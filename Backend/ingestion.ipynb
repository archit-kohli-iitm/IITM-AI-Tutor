{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3uRbkzFUzp8",
        "outputId": "78fe7ea5-4cbb-4d34-9db6-211ea137d602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h2WFHTkhUEVq",
        "outputId": "d6430698-40c6-4588-da18-487f25459323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: qdrant-client[fastembed] in /usr/local/lib/python3.11/dist-packages (1.13.2)\n",
            "Requirement already satisfied: fastembed==0.5.1 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (0.5.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (1.70.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (1.70.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (1.26.4)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.10.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.28.1)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.7.3)\n",
            "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (4.1.0)\n",
            "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (1.20.1)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (10.4.0)\n",
            "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.1.3)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (0.21.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.5.1->qdrant-client[fastembed]) (4.67.1)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.33)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.29.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client[fastembed]) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.3.4)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->qdrant-client[fastembed]) (2024.10.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed==0.5.1->qdrant-client[fastembed]) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.3.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->qdrant-client[fastembed]) (1.3.0)\n",
            "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf qdrant-client[fastembed] langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV98Jf0UTS3f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "from typing import Optional, Dict, List\n",
        "from qdrant_client import QdrantClient, models\n",
        "from fastembed import TextEmbedding, SparseTextEmbedding\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_x_xJq1TS3i"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnwDc5V3TS3j"
      },
      "outputs": [],
      "source": [
        "def preprocess_pdf_text(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts and preprocesses text from a PDF file, maintaining continuity across pages.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Preprocessed text from the PDF with maintained continuity.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = []\n",
        "\n",
        "        for page in doc:\n",
        "            text = page.get_text()\n",
        "\n",
        "            # Remove page numbers and headers/footers\n",
        "            text = re.sub(r'^\\s*Page \\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "            text = re.sub(r'^\\s*-+\\s*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "            # Remove excessive whitespace\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            full_text.append(text)\n",
        "\n",
        "        # Join all pages' text\n",
        "        continuous_text = ' '.join(full_text)\n",
        "\n",
        "        # Fix hyphenation at end of lines/pages\n",
        "        continuous_text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', continuous_text)\n",
        "\n",
        "        return continuous_text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
        "        return \"\"\n",
        "    finally:\n",
        "        if 'doc' in locals():\n",
        "            doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "collapsed": true,
        "id": "6A_Qqyl-TS3j",
        "outputId": "06730003-9510-48fb-f482-89eb85a4ebd9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Class and Objects So, continuing with our discussion of slightly more exotic aspects of Python, let us look at classes and objects. (Refer Slide Time: 0:15) So, most often classes and objects arise in the context of what are called abstract data types. So, we have data types as we know, in Python, we have lists, we have dictionaries. And when we have a data type, we have certain permitted operations on these. For a list, for example, you can append to it, or you can combine two lists using plus you can concatenate them, with a dictionary, you can create a new entry with the key, you can update it, and so on. You can get X, extract all the keys of a dictionary, extract all the values and so on. Now, sometimes we need to create our own data type. And this data type will typically have two parts; it will have some information that is stored in it. But there may also be some discipline or some required way of controlling access to this information. So, a typical example that most people use for this is a stack. So, what is a stack? A stack is what you think in English, it is just a pile of things come one on top of the other. Now, if I have a stack of books, for example, in a table, what can I do? I can add one more to the top of the stack. So, this is what is called in stack terminal, you ‘push’, or I can take the top most book of the stack, and this is called a ‘pop’. Now, I cannot take out this book, until I take out the box on top of it. Otherwise, things will fall down haphazardly. So, the idea of a stack is that I have a sequence of values. So, x1, x2 up to say some xn, and when I push I add a value on the end, and when I pop, I can only take out the last value. Now I may represent this information as a list. And if represented as a list, Python will allow me to take out or update, say the second element in this list. But as a stack, that is not allowed. If I do not respect this last in first out thing as it is called, then I cannot guarantee that the information that I stored has whatever property I would like to associate with it being a stack. So what we want to do is have an implementation of a stack, a way of storing the information and a way of implementing this push and pop. But this implementation should only be manipulated using this public specification, I can only use pop and push, I cannot start inserting into the stack or extracting a value from the stack or updating a value just because I know it is a list. So, so the implementation should be private and you should only use the publicly allowed functions on it. So, this is what an abstract data type essentially means. So, one of the ways of implementing abstract data types is to use this idea of a class. So, a class is a template, it tells you an abstract description of this abstract data type in terms of how the data is stored, and how the functions manipulate the stored data. And now you can make as many copies of this function as of this template as you want. And these are what are called objects. So, I can describe what a stack should look like. And then I can create multiple stacks, it is just like I can use multiple lists, multiple dictionaries, if I update one dictionary, it does not change the other dictionary. So, when I have built in data types in Python, I can create as many of each as I want. Similarly, a class is like a user defined it, I want to define a stack. And now I want having defined the stack, I want you to be able to create as many stacks as you want those are the objects. (Refer Slide Time: 3:35) So, let us look at a concrete example, supposing we want to look at geometric points. So, geometric point, by geometric point just mean that we normally you would have seen this, of course in maths that you have this x y coordinate and I take a point. So, how do I describe this point? Well, it will have some x coordinate a and some y coordinate B, which says that basically, I am a distance away from here on this direction. And I am b distance away from here in this direction. So, the second thing tells me how I am. The first thing tells me how much to the right or left of the center. So, this is my point. So, I want to now have a way of storing these points and manipulating them. What do I do manipulating I might want to take this point and I may want to shift it there. I might want to shift it by some a plus delta x and B plus delta y, I may want to shift it to a new position where I tell you move it 2 steps, right and 4 steps up. So, these are the kinds of manipulations that I might want to do. So, that is called for example translation. So now, in Python, the way we will do this as we use this class definition. So, we say that I want to define a class point and the first and most important thing you need to do is to create points, so for that we have this kind of special function, which is called a constructor, which is always called init it with underscore and score on both sides. So, that it is a special function is not just so this underscore underscore is part of the name of the function. Now one of the things that we need to do is remember we have many points, so each point has an identity. So, it has to talk about its internal values and the values of other points that it might encounter. So, there is this parameter called self, which every function inside a class has, which is an identification of itself. I mean, so every object has a notion of myself or itself and other points. And now when we create a point, we have to provide its location. So, we provide two arguments a and b and internally, it stores it as x and y. So we have self, my copy of x, my copy of y, so self dot x self dot y, and I initialize them to a and b and as in other functions in Python, if I provide some default values, I can create a point without providing a and b and then it will be at the origin. So, this is the starting point of our class point, we have this constructor called init which takes two parameters, the x coordinate and the y coordinate and initializes this point to have that x coordinate and that y coordinate. (Refer Slide Time: 6:15) So, as I decided this, discuss just now one of the things you might want to do is to take a point and shift it. So, we want to make take a point at x comma y, and shift it to x plus delta x y plus delta y. So now, inside this point, this is now a public description. So, the functionality of point permits you to translate as it is called in geometry, translate a point from here to there, translate means shift it by a certain quantity. So, I have a point already. So, I already have a self dot x self dot way, and I want to shift it by delta x delta y, remember that this parameter is always, so I take self dot x and update it to self dot x plus delta y. So, remember, this notation, i plus equal to 7 is a shortcut for i equal to i plus 7. So, whenever you are updating the same value on the right hand side, you can collapse this, this part into this plus equal to. So, this is just a shortcut for saying self dot x gets updated to self dot x plus delta x, self dot y gets updated to self dot y plus delta y. So, this is my translation. (Refer Slide Time: 7:21) Now, I might ask some other question, which is, how far is this point a comma b? How far is it from the origin? If I draw a straight line from (0,0) to this how far is it? So, by Pythagoras theorem, this is going to be a, this is going to be b, so I am going to get square root of a squared plus b squared, because that that distance line is like the hypotenuse of a right hand, right angle triangle. So, d is given by the square root of self dot x squared self dot y squared. And remember that this is a function in math. So, I take self dot x self dot x, which is the x squared, self dot y times self dot y, which is the y squared and apply square root of it, and return that value d. So, so far, so good, we have defined kind of our class, which can allow us to store two dimensional points and do a couple of things with it. One is to translate that point by a certain displacement and the second one is to compute the distance from the origin. So now, what more can I do with this? (Refer Slide Time: 8:20) So, I might, for various reasons, decided to represent this point differently. So, if I have a point, a comma b, an equivalent way to describe it is to actually compute its distance. Okay, and compute this angle that it makes with the x axis. So, this is what is called polar coordinates. So, in polar coordinates, now notice that this is again unique, because the angle fixes at what angle it is an r tells me exactly where it is. So, if I know r and theta, it is not difficult to imagine that it fixes a point uniquely. And this is equivalent to giving the x and the y coordinate. So, these two are obviously then interchangeable. So, we know that r is given by this x squared plus y squared square root as we saw before, because it is nothing more than the distance from the origin. And if you know trigonometry, then this height divided by the adjacent thing is tan of theta. So, this is the definition of the trigonometric function tangent, the tangent is the opposite against the adjacent. So, the inverse of that ratio, tan inverse of y by x, which is the theta for which this is the tangent gives me the value of theta. So, I can start with x, y, and I can compute r and theta. So, that is what is happening here now. Now, the point that the thing that we are trying to emphasize is that this is a different private implementation of the same public point. So, as a user, you still create the point giving this a comma b value, you have no idea whether internally storing it as r comma theta is storing it as a comma b. So, inside the definition of this class, now this init function takes the same values, but what it does is it instant instead of initializing self dot x it initializes self dot r, by using the square root of a squared plus b squared, remember that this is math. So, it is math dot square root. Similarly, it initializes that theta to be the tan inverse, which is in the math functions called a tan, arc tangent sometimes, a tan of b by a. But there is a problem because a could be 0. So, if a is 0, then what we are saying is that we are somewhere on this y axis, because a is the displacement in the x direction. So, if a is 0, we are on the y axis. So, the angle is really 90 degrees, which all these angles are actually represented in radians. So, 90 degrees pi by 2, so we will take it as pi by 2. So, if the a coordinate, that is the x coordinate of the point I am trying to create is along the y axis, then I will declare the angle to be 90, otherwise I will take it to be the tan inverse. So, at this point, from a user's perspective they should not care whether it is r theta or x, y, because the information is actually interchangeable. So, now let us look at the two functions that we had defined, the two functions defined what translate and this distance from the origin. (Refer Slide Time: 11:29) So, distance from the origin is very easy now, because I am actually representing it explicitly as this r quantity. So, if I am asked the diff, the distance from the origin of a point, instead of having to compute this x squared plus y squared and taking the square root, I can just return the current value of self dot r. So, this becomes easier. So, this could be one justification, if you are going to often ask the distance, then this representation makes that calculation easier because you calculate the self dot r once and for all, every time the point changes, after every call to distance just report set value, you never have to do this calculation again. So, that these are some of the reasons why you might choose a different representation, depending on how the functions are going to be called the work that you have to do down the line might improve. (Refer Slide Time: 12:13) On the other hand, now if I have to translate, I have to do a lot of work. Because the translation, remember is going to be still expressed in terms of this x plus delta x and y plus delta y, but I do not have x and y. So, I have to go backwards, I have to go from r theta to x, y. Now again, using trigonometry, if this is x, and this is y, and this is theta. Then sin of theta. And this is say the hypotenuse, the sin of theta is this large, sin of theta is y by r, and cos of theta is x by r. So, from this, you get the x is r cos theta and y is r sin theta. So, you can translate backwards from r to x, r theta to x comma y, then you can apply this transformation at the x y level and then convert back so you have to do this conversion in both directions to go from r theta to x y, apply this plus delta x plus delta y and go back. So, this is what is happening here. So, you compute a new value for x and y, given the current values inside for r and theta. By applying cos and sin, then you translate. But x and y are not my internal representation, I am just using it as an intermediate thing. So, you have to convert back. So, this code is essentially the same code that we had when we did the init. So, the distance from the origin becomes simpler, translation becomes more difficult. In the x, y thing, the translation was easier. But this is a margin required computing the square root. So, it is a trade-off, you have to decide which is which. But the important thing is that from the user's perspective, nothing has changed, you still create a point by providing the x coordinate and the y coordinate. You still provide the translation function by giving the x displacement in the y displacement, you still ask for the distance, and get it back as a distance from the origin. So, the interface, as it is called, has not changed. So, what the user sees, as far as the class is concerned, has not changed. And this is the whole point of this whole discipline of using classes and objects that you can change the internals of a class, but provide the same public face. So, the other person's code does not change. So you, you do not end up. So, this is something that we do not want to do that. Because I change my code, somebody else's code stops working. It might work better or worse, it might be more efficient, less efficient, that is a different matter, but it should not give wrong answers. So, that is one of the things that we get. (Refer Slide Time: 14:41) So, to wind up this discussion of classes this look at some special functions, so we already saw one the constructor function has this underscore underscore init. Another useful function is this function called str, which converts an object to a human readable form as a string. So, for instance, we have might want to take our point and write it out, if I want, if I ask for the value of p, you might want to see it in this form as a string with open bracket, close bracket and x comma y if separated by commas as you would normally read it in a math notation, right. But we have to do that explicitly. So, we will have to take define a string function, which will take self dot x self dot y, convert them into strings, and then using the normal string operations, take these special symbols, open bracket, comma, close bracket and put them around and in between, so we are really assembling a string by taking the open bracket, the string value of x, comma, the string value of y, and close bracket. Now, we normally do not use this explicitly, but it is always used implicitly by print, whenever we print a value of a variable, whenever I say print x, for any x, k, what happens is that this is actually converted to a string of x. So, if I print a number, what happens is the number is first converted to a string and then print. So, print can only print strings in some sense. So, a string is called implicitly. So, the same thing happens here. So, if I want to actually if I say print p, where p is a point, so what will happen is that it will go through this process, and it will come out. So, that is why the string is an important function. (Refer Slide Time: 16:25) Similarly, you can do functions which overload operators, supposing I want to have the possibility of taking a point p, and taking a point q and now defining a point p plus q. So, what does p plus q do it will have this plus this in the x axis and this plus this in the y axis. So, if I have, for example, 3 comma 4 as p, and I have 5 comma 7 as q, then I will end up with a point which is 3 plus 5, 8, and 4 plus 7, 11. This is what p plus q is. So, how do I define that? Well, I define it using this function called add. So, now this is a situation where you see this no use of self. So, I am a point self and I get another point p and I want to add myself to it, or p to me. So, I take my x and I take the px that is the new x coordinate, I take my y and I take the new y that is the new y coordinate. But now this is a different point. This is not myself, this is not p. So, I have to create a new point. So, I have to construct a point out of this by passing this new x and this new y value that I have calculated to point which creates a new point, this class creates a new point and returns a new object. So, when I say p plus q, p does not change q does not change, I get back a new point whose coordinates are the sum of the coordinates in each direction of p and q. (Refer Slide Time: 17:49) So, just like add, you can also do multiply, you can also use, you can define how to compare points. For instance, you might say that one point is smaller than other point if it is smaller, and both the x and the y. So, you can use less than, greater than, greater than equal to, so you can define all these functions. So, then you can manipulate these user defined objects, and put them into conditions and print statements and all that exactly as you would a built in type in Python. So, it gives you the flexibility of using your objects, you can sort them for example, you can take points and sort them in increasing order because you have a way of doing these comparisons directly on points. So, you do not have to write a complicated thing each time. You can just use a normal notation that you use for sorry if this is less than that exchange and so on. So, it is very convenient to be able to work with these objects and classes directly.\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = preprocess_pdf_text('./drive/MyDrive/PDSA_Transcripts/Week 1/Classes and Objects.pdf')\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9rujSvUTS3k"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text: str, max_chunk_size: int = 1000) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Chunks a large text into smaller segments, ensuring sentences are not cut off.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to be chunked.\n",
        "    max_chunk_size (int): The maximum size of each chunk (default: 1000 characters).\n",
        "\n",
        "    Returns:\n",
        "    List[Dict[str, str]]: A list of dictionaries, each containing a chunk of text and its index.\n",
        "    \"\"\"\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    chunk_index = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # If adding this sentence would exceed the max chunk size and we already have content,\n",
        "        # start a new chunk\n",
        "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
        "            chunks.append({\n",
        "                \"chunk_index\": chunk_index,\n",
        "                \"text\": current_chunk.strip()\n",
        "            })\n",
        "            chunk_index += 1\n",
        "            current_chunk = \"\"\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk += \" \" + sentence\n",
        "\n",
        "        # If the current chunk is now longer than max_chunk_size, add it to chunks\n",
        "        # This handles cases where a single sentence is longer than max_chunk_size\n",
        "        if len(current_chunk) >= max_chunk_size:\n",
        "            chunks.append({\n",
        "                \"chunk_index\": chunk_index,\n",
        "                \"text\": current_chunk.strip()\n",
        "            })\n",
        "            chunk_index += 1\n",
        "            current_chunk = \"\"\n",
        "\n",
        "    # Add any remaining text as the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            \"chunk_index\": chunk_index,\n",
        "            \"text\": current_chunk.strip()\n",
        "        })\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jab8yw4VTS3k",
        "outputId": "97c71670-64be-44d9-ea60-630654104579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'chunk_index': 0,\n",
              "  'text': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Class and Objects So, continuing with our discussion of slightly more exotic aspects of Python, let us look at classes and objects. (Refer Slide Time: 0:15) So, most often classes and objects arise in the context of what are called abstract data types. So, we have data types as we know, in Python, we have lists, we have dictionaries. And when we have a data type, we have certain permitted operations on these. For a list, for example, you can append to it, or you can combine two lists using plus you can concatenate them, with a dictionary, you can create a new entry with the key, you can update it, and so on. You can get X, extract all the keys of a dictionary, extract all the values and so on. Now, sometimes we need to create our own data type. And this data type will typically have two parts; it will have some information that is stored in it. But there may also be some discipline or some required way of controlling access to this information. So, a typical example that most people use for this is a stack. So, what is a stack? A stack is what you think in English, it is just a pile of things come one on top of the other. Now, if I have a stack of books, for example, in a table, what can I do? I can add one more to the top of the stack. So, this is what is called in stack terminal, you ‘push’, or I can take the top most book of the stack, and this is called a ‘pop’. Now, I cannot take out this book, until I take out the box on top of it. Otherwise, things will fall down haphazardly. So, the idea of a stack is that I have a sequence of values. So, x1, x2 up to say some xn, and when I push I add a value on the end, and when I pop, I can only take out the last value. Now I may represent this information as a list. And if represented as a list, Python will allow me to take out or update, say the second element in this list. But as a stack, that is not allowed.'},\n",
              " {'chunk_index': 1,\n",
              "  'text': 'If I do not respect this last in first out thing as it is called, then I cannot guarantee that the information that I stored has whatever property I would like to associate with it being a stack. So what we want to do is have an implementation of a stack, a way of storing the information and a way of implementing this push and pop. But this implementation should only be manipulated using this public specification, I can only use pop and push, I cannot start inserting into the stack or extracting a value from the stack or updating a value just because I know it is a list. So, so the implementation should be private and you should only use the publicly allowed functions on it. So, this is what an abstract data type essentially means. So, one of the ways of implementing abstract data types is to use this idea of a class. So, a class is a template, it tells you an abstract description of this abstract data type in terms of how the data is stored, and how the functions manipulate the stored data. And now you can make as many copies of this function as of this template as you want. And these are what are called objects. So, I can describe what a stack should look like. And then I can create multiple stacks, it is just like I can use multiple lists, multiple dictionaries, if I update one dictionary, it does not change the other dictionary. So, when I have built in data types in Python, I can create as many of each as I want. Similarly, a class is like a user defined it, I want to define a stack. And now I want having defined the stack, I want you to be able to create as many stacks as you want those are the objects. (Refer Slide Time: 3:35) So, let us look at a concrete example, supposing we want to look at geometric points. So, geometric point, by geometric point just mean that we normally you would have seen this, of course in maths that you have this x y coordinate and I take a point. So, how do I describe this point?'},\n",
              " {'chunk_index': 2,\n",
              "  'text': 'Well, it will have some x coordinate a and some y coordinate B, which says that basically, I am a distance away from here on this direction. And I am b distance away from here in this direction. So, the second thing tells me how I am. The first thing tells me how much to the right or left of the center. So, this is my point. So, I want to now have a way of storing these points and manipulating them. What do I do manipulating I might want to take this point and I may want to shift it there. I might want to shift it by some a plus delta x and B plus delta y, I may want to shift it to a new position where I tell you move it 2 steps, right and 4 steps up. So, these are the kinds of manipulations that I might want to do. So, that is called for example translation. So now, in Python, the way we will do this as we use this class definition. So, we say that I want to define a class point and the first and most important thing you need to do is to create points, so for that we have this kind of special function, which is called a constructor, which is always called init it with underscore and score on both sides. So, that it is a special function is not just so this underscore underscore is part of the name of the function. Now one of the things that we need to do is remember we have many points, so each point has an identity. So, it has to talk about its internal values and the values of other points that it might encounter. So, there is this parameter called self, which every function inside a class has, which is an identification of itself. I mean, so every object has a notion of myself or itself and other points. And now when we create a point, we have to provide its location. So, we provide two arguments a and b and internally, it stores it as x and y.'},\n",
              " {'chunk_index': 3,\n",
              "  'text': 'So we have self, my copy of x, my copy of y, so self dot x self dot y, and I initialize them to a and b and as in other functions in Python, if I provide some default values, I can create a point without providing a and b and then it will be at the origin. So, this is the starting point of our class point, we have this constructor called init which takes two parameters, the x coordinate and the y coordinate and initializes this point to have that x coordinate and that y coordinate. (Refer Slide Time: 6:15) So, as I decided this, discuss just now one of the things you might want to do is to take a point and shift it. So, we want to make take a point at x comma y, and shift it to x plus delta x y plus delta y. So now, inside this point, this is now a public description. So, the functionality of point permits you to translate as it is called in geometry, translate a point from here to there, translate means shift it by a certain quantity. So, I have a point already. So, I already have a self dot x self dot way, and I want to shift it by delta x delta y, remember that this parameter is always, so I take self dot x and update it to self dot x plus delta y. So, remember, this notation, i plus equal to 7 is a shortcut for i equal to i plus 7. So, whenever you are updating the same value on the right hand side, you can collapse this, this part into this plus equal to. So, this is just a shortcut for saying self dot x gets updated to self dot x plus delta x, self dot y gets updated to self dot y plus delta y. So, this is my translation. (Refer Slide Time: 7:21) Now, I might ask some other question, which is, how far is this point a comma b? How far is it from the origin? If I draw a straight line from (0,0) to this how far is it? So, by Pythagoras theorem, this is going to be a, this is going to be b, so I am going to get square root of a squared plus b squared, because that that distance line is like the hypotenuse of a right hand, right angle triangle.'},\n",
              " {'chunk_index': 4,\n",
              "  'text': 'So, d is given by the square root of self dot x squared self dot y squared. And remember that this is a function in math. So, I take self dot x self dot x, which is the x squared, self dot y times self dot y, which is the y squared and apply square root of it, and return that value d. So, so far, so good, we have defined kind of our class, which can allow us to store two dimensional points and do a couple of things with it. One is to translate that point by a certain displacement and the second one is to compute the distance from the origin. So now, what more can I do with this? (Refer Slide Time: 8:20) So, I might, for various reasons, decided to represent this point differently. So, if I have a point, a comma b, an equivalent way to describe it is to actually compute its distance. Okay, and compute this angle that it makes with the x axis. So, this is what is called polar coordinates. So, in polar coordinates, now notice that this is again unique, because the angle fixes at what angle it is an r tells me exactly where it is. So, if I know r and theta, it is not difficult to imagine that it fixes a point uniquely. And this is equivalent to giving the x and the y coordinate. So, these two are obviously then interchangeable. So, we know that r is given by this x squared plus y squared square root as we saw before, because it is nothing more than the distance from the origin. And if you know trigonometry, then this height divided by the adjacent thing is tan of theta. So, this is the definition of the trigonometric function tangent, the tangent is the opposite against the adjacent. So, the inverse of that ratio, tan inverse of y by x, which is the theta for which this is the tangent gives me the value of theta. So, I can start with x, y, and I can compute r and theta. So, that is what is happening here now. Now, the point that the thing that we are trying to emphasize is that this is a different private implementation of the same public point.'},\n",
              " {'chunk_index': 5,\n",
              "  'text': \"So, as a user, you still create the point giving this a comma b value, you have no idea whether internally storing it as r comma theta is storing it as a comma b. So, inside the definition of this class, now this init function takes the same values, but what it does is it instant instead of initializing self dot x it initializes self dot r, by using the square root of a squared plus b squared, remember that this is math. So, it is math dot square root. Similarly, it initializes that theta to be the tan inverse, which is in the math functions called a tan, arc tangent sometimes, a tan of b by a. But there is a problem because a could be 0. So, if a is 0, then what we are saying is that we are somewhere on this y axis, because a is the displacement in the x direction. So, if a is 0, we are on the y axis. So, the angle is really 90 degrees, which all these angles are actually represented in radians. So, 90 degrees pi by 2, so we will take it as pi by 2. So, if the a coordinate, that is the x coordinate of the point I am trying to create is along the y axis, then I will declare the angle to be 90, otherwise I will take it to be the tan inverse. So, at this point, from a user's perspective they should not care whether it is r theta or x, y, because the information is actually interchangeable. So, now let us look at the two functions that we had defined, the two functions defined what translate and this distance from the origin. (Refer Slide Time: 11:29) So, distance from the origin is very easy now, because I am actually representing it explicitly as this r quantity. So, if I am asked the diff, the distance from the origin of a point, instead of having to compute this x squared plus y squared and taking the square root, I can just return the current value of self dot r. So, this becomes easier.\"},\n",
              " {'chunk_index': 6,\n",
              "  'text': 'So, this could be one justification, if you are going to often ask the distance, then this representation makes that calculation easier because you calculate the self dot r once and for all, every time the point changes, after every call to distance just report set value, you never have to do this calculation again. So, that these are some of the reasons why you might choose a different representation, depending on how the functions are going to be called the work that you have to do down the line might improve. (Refer Slide Time: 12:13) On the other hand, now if I have to translate, I have to do a lot of work. Because the translation, remember is going to be still expressed in terms of this x plus delta x and y plus delta y, but I do not have x and y. So, I have to go backwards, I have to go from r theta to x, y. Now again, using trigonometry, if this is x, and this is y, and this is theta. Then sin of theta. And this is say the hypotenuse, the sin of theta is this large, sin of theta is y by r, and cos of theta is x by r. So, from this, you get the x is r cos theta and y is r sin theta. So, you can translate backwards from r to x, r theta to x comma y, then you can apply this transformation at the x y level and then convert back so you have to do this conversion in both directions to go from r theta to x y, apply this plus delta x plus delta y and go back. So, this is what is happening here. So, you compute a new value for x and y, given the current values inside for r and theta. By applying cos and sin, then you translate. But x and y are not my internal representation, I am just using it as an intermediate thing. So, you have to convert back. So, this code is essentially the same code that we had when we did the init. So, the distance from the origin becomes simpler, translation becomes more difficult. In the x, y thing, the translation was easier. But this is a margin required computing the square root. So, it is a trade-off, you have to decide which is which.'},\n",
              " {'chunk_index': 7,\n",
              "  'text': \"But the important thing is that from the user's perspective, nothing has changed, you still create a point by providing the x coordinate and the y coordinate. You still provide the translation function by giving the x displacement in the y displacement, you still ask for the distance, and get it back as a distance from the origin. So, the interface, as it is called, has not changed. So, what the user sees, as far as the class is concerned, has not changed. And this is the whole point of this whole discipline of using classes and objects that you can change the internals of a class, but provide the same public face. So, the other person's code does not change. So you, you do not end up. So, this is something that we do not want to do that. Because I change my code, somebody else's code stops working. It might work better or worse, it might be more efficient, less efficient, that is a different matter, but it should not give wrong answers. So, that is one of the things that we get. (Refer Slide Time: 14:41) So, to wind up this discussion of classes this look at some special functions, so we already saw one the constructor function has this underscore underscore init. Another useful function is this function called str, which converts an object to a human readable form as a string. So, for instance, we have might want to take our point and write it out, if I want, if I ask for the value of p, you might want to see it in this form as a string with open bracket, close bracket and x comma y if separated by commas as you would normally read it in a math notation, right. But we have to do that explicitly.\"},\n",
              " {'chunk_index': 8,\n",
              "  'text': 'So, we will have to take define a string function, which will take self dot x self dot y, convert them into strings, and then using the normal string operations, take these special symbols, open bracket, comma, close bracket and put them around and in between, so we are really assembling a string by taking the open bracket, the string value of x, comma, the string value of y, and close bracket. Now, we normally do not use this explicitly, but it is always used implicitly by print, whenever we print a value of a variable, whenever I say print x, for any x, k, what happens is that this is actually converted to a string of x. So, if I print a number, what happens is the number is first converted to a string and then print. So, print can only print strings in some sense. So, a string is called implicitly. So, the same thing happens here. So, if I want to actually if I say print p, where p is a point, so what will happen is that it will go through this process, and it will come out. So, that is why the string is an important function. (Refer Slide Time: 16:25) Similarly, you can do functions which overload operators, supposing I want to have the possibility of taking a point p, and taking a point q and now defining a point p plus q. So, what does p plus q do it will have this plus this in the x axis and this plus this in the y axis. So, if I have, for example, 3 comma 4 as p, and I have 5 comma 7 as q, then I will end up with a point which is 3 plus 5, 8, and 4 plus 7, 11. This is what p plus q is. So, how do I define that? Well, I define it using this function called add. So, now this is a situation where you see this no use of self. So, I am a point self and I get another point p and I want to add myself to it, or p to me. So, I take my x and I take the px that is the new x coordinate, I take my y and I take the new y that is the new y coordinate. But now this is a different point. This is not myself, this is not p. So, I have to create a new point.'},\n",
              " {'chunk_index': 9,\n",
              "  'text': 'So, I have to construct a point out of this by passing this new x and this new y value that I have calculated to point which creates a new point, this class creates a new point and returns a new object. So, when I say p plus q, p does not change q does not change, I get back a new point whose coordinates are the sum of the coordinates in each direction of p and q. (Refer Slide Time: 17:49) So, just like add, you can also do multiply, you can also use, you can define how to compare points. For instance, you might say that one point is smaller than other point if it is smaller, and both the x and the y. So, you can use less than, greater than, greater than equal to, so you can define all these functions. So, then you can manipulate these user defined objects, and put them into conditions and print statements and all that exactly as you would a built in type in Python. So, it gives you the flexibility of using your objects, you can sort them for example, you can take points and sort them in increasing order because you have a way of doing these comparisons directly on points. So, you do not have to write a complicated thing each time. You can just use a normal notation that you use for sorry if this is less than that exchange and so on. So, it is very convenient to be able to work with these objects and classes directly.'}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = chunk_text(res,2000)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRAzswIZTS3k"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXhbwoDtTS3k",
        "outputId": "30516d60-ad05-4121-cf03-7af60bd18846"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing PDFs: 100%|██████████| 80/80 [02:56<00:00,  2.21s/it]\n"
          ]
        }
      ],
      "source": [
        "dataset = []\n",
        "MAX_CHUNK_SIZE = 2000\n",
        "\n",
        "\n",
        "total_pdfs = sum(len([f for f in os.listdir(f'./drive/MyDrive/PDSA_Transcripts/Week {i}/') if f.endswith('.pdf')]) for i in range(1, 12))\n",
        "\n",
        "with tqdm(total=total_pdfs, desc=\"Processing PDFs\") as pbar:\n",
        "    for i in range(1, 12):\n",
        "        PDF_DIRECTORY = f'./drive/MyDrive/PDSA_Transcripts/Week {i}/'\n",
        "\n",
        "        for filename in os.listdir(PDF_DIRECTORY):\n",
        "            if filename.endswith('.pdf'):\n",
        "                pdf_path = os.path.join(PDF_DIRECTORY, filename)\n",
        "                pdf_content = preprocess_pdf_text(pdf_path)\n",
        "                chunks = chunk_text(pdf_content,max_chunk_size=MAX_CHUNK_SIZE)\n",
        "\n",
        "                for chunk in chunks:\n",
        "                    dataset.append(\n",
        "                        {\n",
        "                            'id': str(uuid.uuid4()),\n",
        "                            'metadata': {\n",
        "                                'chunk_idx': chunk['chunk_index'],\n",
        "                                'week': i\n",
        "                            },\n",
        "                            'source': filename,\n",
        "                            'content': chunk['text']\n",
        "                        }\n",
        "                    )\n",
        "                pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x6DfHE0QTS3k",
        "outputId": "71c31959-4399-4c98-a358-5269520702a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': '62097995-61b8-4c82-9775-0559bdbe7076',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Class and Objects So, continuing with our discussion of slightly more exotic aspects of Python, let us look at classes and objects. (Refer Slide Time: 0:15) So, most often classes and objects arise in the context of what are called abstract data types. So, we have data types as we know, in Python, we have lists, we have dictionaries. And when we have a data type, we have certain permitted operations on these. For a list, for example, you can append to it, or you can combine two lists using plus you can concatenate them, with a dictionary, you can create a new entry with the key, you can update it, and so on. You can get X, extract all the keys of a dictionary, extract all the values and so on. Now, sometimes we need to create our own data type. And this data type will typically have two parts; it will have some information that is stored in it. But there may also be some discipline or some required way of controlling access to this information. So, a typical example that most people use for this is a stack. So, what is a stack? A stack is what you think in English, it is just a pile of things come one on top of the other. Now, if I have a stack of books, for example, in a table, what can I do? I can add one more to the top of the stack. So, this is what is called in stack terminal, you ‘push’, or I can take the top most book of the stack, and this is called a ‘pop’. Now, I cannot take out this book, until I take out the box on top of it. Otherwise, things will fall down haphazardly. So, the idea of a stack is that I have a sequence of values. So, x1, x2 up to say some xn, and when I push I add a value on the end, and when I pop, I can only take out the last value. Now I may represent this information as a list. And if represented as a list, Python will allow me to take out or update, say the second element in this list. But as a stack, that is not allowed.'},\n",
              " {'id': '01d91f78-2928-402d-8dc2-3c3dee91f1a3',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'If I do not respect this last in first out thing as it is called, then I cannot guarantee that the information that I stored has whatever property I would like to associate with it being a stack. So what we want to do is have an implementation of a stack, a way of storing the information and a way of implementing this push and pop. But this implementation should only be manipulated using this public specification, I can only use pop and push, I cannot start inserting into the stack or extracting a value from the stack or updating a value just because I know it is a list. So, so the implementation should be private and you should only use the publicly allowed functions on it. So, this is what an abstract data type essentially means. So, one of the ways of implementing abstract data types is to use this idea of a class. So, a class is a template, it tells you an abstract description of this abstract data type in terms of how the data is stored, and how the functions manipulate the stored data. And now you can make as many copies of this function as of this template as you want. And these are what are called objects. So, I can describe what a stack should look like. And then I can create multiple stacks, it is just like I can use multiple lists, multiple dictionaries, if I update one dictionary, it does not change the other dictionary. So, when I have built in data types in Python, I can create as many of each as I want. Similarly, a class is like a user defined it, I want to define a stack. And now I want having defined the stack, I want you to be able to create as many stacks as you want those are the objects. (Refer Slide Time: 3:35) So, let us look at a concrete example, supposing we want to look at geometric points. So, geometric point, by geometric point just mean that we normally you would have seen this, of course in maths that you have this x y coordinate and I take a point. So, how do I describe this point?'},\n",
              " {'id': '20dca736-1348-4c09-afdf-9a0bffe64a5f',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'Well, it will have some x coordinate a and some y coordinate B, which says that basically, I am a distance away from here on this direction. And I am b distance away from here in this direction. So, the second thing tells me how I am. The first thing tells me how much to the right or left of the center. So, this is my point. So, I want to now have a way of storing these points and manipulating them. What do I do manipulating I might want to take this point and I may want to shift it there. I might want to shift it by some a plus delta x and B plus delta y, I may want to shift it to a new position where I tell you move it 2 steps, right and 4 steps up. So, these are the kinds of manipulations that I might want to do. So, that is called for example translation. So now, in Python, the way we will do this as we use this class definition. So, we say that I want to define a class point and the first and most important thing you need to do is to create points, so for that we have this kind of special function, which is called a constructor, which is always called init it with underscore and score on both sides. So, that it is a special function is not just so this underscore underscore is part of the name of the function. Now one of the things that we need to do is remember we have many points, so each point has an identity. So, it has to talk about its internal values and the values of other points that it might encounter. So, there is this parameter called self, which every function inside a class has, which is an identification of itself. I mean, so every object has a notion of myself or itself and other points. And now when we create a point, we have to provide its location. So, we provide two arguments a and b and internally, it stores it as x and y.'},\n",
              " {'id': 'e7aa6850-8868-4717-9992-5746ff10fc0a',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'So we have self, my copy of x, my copy of y, so self dot x self dot y, and I initialize them to a and b and as in other functions in Python, if I provide some default values, I can create a point without providing a and b and then it will be at the origin. So, this is the starting point of our class point, we have this constructor called init which takes two parameters, the x coordinate and the y coordinate and initializes this point to have that x coordinate and that y coordinate. (Refer Slide Time: 6:15) So, as I decided this, discuss just now one of the things you might want to do is to take a point and shift it. So, we want to make take a point at x comma y, and shift it to x plus delta x y plus delta y. So now, inside this point, this is now a public description. So, the functionality of point permits you to translate as it is called in geometry, translate a point from here to there, translate means shift it by a certain quantity. So, I have a point already. So, I already have a self dot x self dot way, and I want to shift it by delta x delta y, remember that this parameter is always, so I take self dot x and update it to self dot x plus delta y. So, remember, this notation, i plus equal to 7 is a shortcut for i equal to i plus 7. So, whenever you are updating the same value on the right hand side, you can collapse this, this part into this plus equal to. So, this is just a shortcut for saying self dot x gets updated to self dot x plus delta x, self dot y gets updated to self dot y plus delta y. So, this is my translation. (Refer Slide Time: 7:21) Now, I might ask some other question, which is, how far is this point a comma b? How far is it from the origin? If I draw a straight line from (0,0) to this how far is it? So, by Pythagoras theorem, this is going to be a, this is going to be b, so I am going to get square root of a squared plus b squared, because that that distance line is like the hypotenuse of a right hand, right angle triangle.'},\n",
              " {'id': '6c65edd3-4bf7-4ab2-8620-a5af71aa79f7',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'So, d is given by the square root of self dot x squared self dot y squared. And remember that this is a function in math. So, I take self dot x self dot x, which is the x squared, self dot y times self dot y, which is the y squared and apply square root of it, and return that value d. So, so far, so good, we have defined kind of our class, which can allow us to store two dimensional points and do a couple of things with it. One is to translate that point by a certain displacement and the second one is to compute the distance from the origin. So now, what more can I do with this? (Refer Slide Time: 8:20) So, I might, for various reasons, decided to represent this point differently. So, if I have a point, a comma b, an equivalent way to describe it is to actually compute its distance. Okay, and compute this angle that it makes with the x axis. So, this is what is called polar coordinates. So, in polar coordinates, now notice that this is again unique, because the angle fixes at what angle it is an r tells me exactly where it is. So, if I know r and theta, it is not difficult to imagine that it fixes a point uniquely. And this is equivalent to giving the x and the y coordinate. So, these two are obviously then interchangeable. So, we know that r is given by this x squared plus y squared square root as we saw before, because it is nothing more than the distance from the origin. And if you know trigonometry, then this height divided by the adjacent thing is tan of theta. So, this is the definition of the trigonometric function tangent, the tangent is the opposite against the adjacent. So, the inverse of that ratio, tan inverse of y by x, which is the theta for which this is the tangent gives me the value of theta. So, I can start with x, y, and I can compute r and theta. So, that is what is happening here now. Now, the point that the thing that we are trying to emphasize is that this is a different private implementation of the same public point.'},\n",
              " {'id': '0a09b531-2108-41f2-a758-95757f5fc45b',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': \"So, as a user, you still create the point giving this a comma b value, you have no idea whether internally storing it as r comma theta is storing it as a comma b. So, inside the definition of this class, now this init function takes the same values, but what it does is it instant instead of initializing self dot x it initializes self dot r, by using the square root of a squared plus b squared, remember that this is math. So, it is math dot square root. Similarly, it initializes that theta to be the tan inverse, which is in the math functions called a tan, arc tangent sometimes, a tan of b by a. But there is a problem because a could be 0. So, if a is 0, then what we are saying is that we are somewhere on this y axis, because a is the displacement in the x direction. So, if a is 0, we are on the y axis. So, the angle is really 90 degrees, which all these angles are actually represented in radians. So, 90 degrees pi by 2, so we will take it as pi by 2. So, if the a coordinate, that is the x coordinate of the point I am trying to create is along the y axis, then I will declare the angle to be 90, otherwise I will take it to be the tan inverse. So, at this point, from a user's perspective they should not care whether it is r theta or x, y, because the information is actually interchangeable. So, now let us look at the two functions that we had defined, the two functions defined what translate and this distance from the origin. (Refer Slide Time: 11:29) So, distance from the origin is very easy now, because I am actually representing it explicitly as this r quantity. So, if I am asked the diff, the distance from the origin of a point, instead of having to compute this x squared plus y squared and taking the square root, I can just return the current value of self dot r. So, this becomes easier.\"},\n",
              " {'id': '7cd320e8-a218-4745-9b6e-1b1b6dc9cb07',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'So, this could be one justification, if you are going to often ask the distance, then this representation makes that calculation easier because you calculate the self dot r once and for all, every time the point changes, after every call to distance just report set value, you never have to do this calculation again. So, that these are some of the reasons why you might choose a different representation, depending on how the functions are going to be called the work that you have to do down the line might improve. (Refer Slide Time: 12:13) On the other hand, now if I have to translate, I have to do a lot of work. Because the translation, remember is going to be still expressed in terms of this x plus delta x and y plus delta y, but I do not have x and y. So, I have to go backwards, I have to go from r theta to x, y. Now again, using trigonometry, if this is x, and this is y, and this is theta. Then sin of theta. And this is say the hypotenuse, the sin of theta is this large, sin of theta is y by r, and cos of theta is x by r. So, from this, you get the x is r cos theta and y is r sin theta. So, you can translate backwards from r to x, r theta to x comma y, then you can apply this transformation at the x y level and then convert back so you have to do this conversion in both directions to go from r theta to x y, apply this plus delta x plus delta y and go back. So, this is what is happening here. So, you compute a new value for x and y, given the current values inside for r and theta. By applying cos and sin, then you translate. But x and y are not my internal representation, I am just using it as an intermediate thing. So, you have to convert back. So, this code is essentially the same code that we had when we did the init. So, the distance from the origin becomes simpler, translation becomes more difficult. In the x, y thing, the translation was easier. But this is a margin required computing the square root. So, it is a trade-off, you have to decide which is which.'},\n",
              " {'id': '8e2fbd36-107d-49b0-87a4-ad658a3c531c',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': \"But the important thing is that from the user's perspective, nothing has changed, you still create a point by providing the x coordinate and the y coordinate. You still provide the translation function by giving the x displacement in the y displacement, you still ask for the distance, and get it back as a distance from the origin. So, the interface, as it is called, has not changed. So, what the user sees, as far as the class is concerned, has not changed. And this is the whole point of this whole discipline of using classes and objects that you can change the internals of a class, but provide the same public face. So, the other person's code does not change. So you, you do not end up. So, this is something that we do not want to do that. Because I change my code, somebody else's code stops working. It might work better or worse, it might be more efficient, less efficient, that is a different matter, but it should not give wrong answers. So, that is one of the things that we get. (Refer Slide Time: 14:41) So, to wind up this discussion of classes this look at some special functions, so we already saw one the constructor function has this underscore underscore init. Another useful function is this function called str, which converts an object to a human readable form as a string. So, for instance, we have might want to take our point and write it out, if I want, if I ask for the value of p, you might want to see it in this form as a string with open bracket, close bracket and x comma y if separated by commas as you would normally read it in a math notation, right. But we have to do that explicitly.\"},\n",
              " {'id': '0220cdf8-d1b1-4428-9f40-94cbd27ffff4',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'So, we will have to take define a string function, which will take self dot x self dot y, convert them into strings, and then using the normal string operations, take these special symbols, open bracket, comma, close bracket and put them around and in between, so we are really assembling a string by taking the open bracket, the string value of x, comma, the string value of y, and close bracket. Now, we normally do not use this explicitly, but it is always used implicitly by print, whenever we print a value of a variable, whenever I say print x, for any x, k, what happens is that this is actually converted to a string of x. So, if I print a number, what happens is the number is first converted to a string and then print. So, print can only print strings in some sense. So, a string is called implicitly. So, the same thing happens here. So, if I want to actually if I say print p, where p is a point, so what will happen is that it will go through this process, and it will come out. So, that is why the string is an important function. (Refer Slide Time: 16:25) Similarly, you can do functions which overload operators, supposing I want to have the possibility of taking a point p, and taking a point q and now defining a point p plus q. So, what does p plus q do it will have this plus this in the x axis and this plus this in the y axis. So, if I have, for example, 3 comma 4 as p, and I have 5 comma 7 as q, then I will end up with a point which is 3 plus 5, 8, and 4 plus 7, 11. This is what p plus q is. So, how do I define that? Well, I define it using this function called add. So, now this is a situation where you see this no use of self. So, I am a point self and I get another point p and I want to add myself to it, or p to me. So, I take my x and I take the px that is the new x coordinate, I take my y and I take the new y that is the new y coordinate. But now this is a different point. This is not myself, this is not p. So, I have to create a new point.'},\n",
              " {'id': 'e9002eab-abee-491d-bd64-e7491aee93e2',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 1},\n",
              "  'source': 'Classes and Objects.pdf',\n",
              "  'content': 'So, I have to construct a point out of this by passing this new x and this new y value that I have calculated to point which creates a new point, this class creates a new point and returns a new object. So, when I say p plus q, p does not change q does not change, I get back a new point whose coordinates are the sum of the coordinates in each direction of p and q. (Refer Slide Time: 17:49) So, just like add, you can also do multiply, you can also use, you can define how to compare points. For instance, you might say that one point is smaller than other point if it is smaller, and both the x and the y. So, you can use less than, greater than, greater than equal to, so you can define all these functions. So, then you can manipulate these user defined objects, and put them into conditions and print statements and all that exactly as you would a built in type in Python. So, it gives you the flexibility of using your objects, you can sort them for example, you can take points and sort them in increasing order because you have a way of doing these comparisons directly on points. So, you do not have to write a complicated thing each time. You can just use a normal notation that you use for sorry if this is less than that exchange and so on. So, it is very convenient to be able to work with these objects and classes directly.'},\n",
              " {'id': 'e55529a3-bd97-42da-b121-43cfd1d79733',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Exception Handling So, when we looked at the gcd problem and the primes examples, we covered some of the more standard aspects of python in terms of control flow, so we saw conditionals if, we saw loops for and while, we saw data structures like lists and dictionaries, we also looked at breaking out of a loop, we also considered the problem of initializing a variable properly so that python knows what type it is, so we saw all these standard things just to remind ourselves. Now, let us look at something slightly more exotic which is we will encounter in code so it is important to understand it, so this is about how to handle errors in our code or what are called exceptions. (Refer Slide Time: 0:54) So, when we run code things go wrong, I mean if all code ran perfectly it would be a very simple world and sometimes they go wrong because there is something wrong with what we wrote the logic is wrong, sometimes they go wrong because the values are not what we expect. So, there are many different types of errors that a code, piece of code could generate. So, the most fundamental ones are those to do with values. For instance, you could have an arithmetic expression, you could be dividing x by z but unfortunately at this moment when you are dividing x by z you forgot to check and z actually happened to be 0 so this division is not defined or there are functions python supports type conversion. So, in particular if you read input from the keyboard then it is always a string and now if you want to interpret it as a number you have to apply this function int in order to convert it to a number. But what if that string does not represent a number, supposing it has a decimal point or it has characters other than 0 to 9, then python will throw an error.'},\n",
              " {'id': '872ac87c-b8db-491b-8f28-a9b97c841d3a',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, in this case it is the problem is that the string is not a valid integer, it is a string but it does not represent something that python can understand as an integer. Another type of error is that you actually use an uninitialized variable. So, you put an expression y equal to 5 times x but so far you have not assigned a value to x, so 5 times x has no meaning, so it cannot assign a value to y either or you have a complex structure like a list and you are trying to access a position in the list but the position you are trying to access is outside the bounds of the list. So, the list has positions remember from 0 to the length minus 1 and you are trying to access an i which is outside that. Similarly, you could be doing it for a dictionary also you are trying to access a key from a dictionary but that key does not exist. And finally, there are things which could be outside the scope of the program itself, the values are not wrong inside the program but you are interacting with something outside the program. A typical example when you are trying to read a file so supposing you try to open a file but the file name that you are trying to use does not exist, somebody has renamed the file or deleted it, then your program will throw an error. Similarly, when you try to write a file, you might well find that the disk is out of space, you want to write a large file there is not enough space on the disk. Now, these are things which you cannot do much about within the program, so the question is can you anticipate this? So, what the goal of exception handling is to recover gracefully from errors wherever it is possible. So, we want to try and anticipate the kind of mistakes that might occur when our code runs and provide these alternative paths, a contingency plan as it were saying if this happens, if it goes wrong in this way do this, if it goes wrong in that way do that or report something to the user so that they know.'},\n",
              " {'id': '26a1c811-5f90-4024-8c4e-70de15835fc1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'Supposing, I asked the user to give me a file name and I was not able to read the file, instead of the program aborting and throwing some error it is better that I print out a message to the user saying the file name you provided does not exist try again, for example. So, this broadly is what is called exception handling. (Refer Slide Time: 3:56) So, the good thing for us is that python actually gives us information about each type of error and this confirmation comes in two parts. So, first there is a kind of a name, a type of error and the second is a description. So, the most common type of error is a syntax error, this is not valid python code. Now, if it is not valid python code it cannot run and since it cannot run, we cannot take any corrective action about it. So, we will not bother about syntax there is an exception handling because nothing you can do, syntax errors come before the code runs, exception handling comes when the code is running. So, unless you have valid python code there is no question of exception handling, so there is not much we can do about this. So, we are interested in errors when the code is running so if you have actually run python code and looked at the errors you will see some of these and recognize them, for instance, a name error, python calls a name error, an error which occurs when you try to use a value which is not defined, you have a variable x which is not defined. So, this is like our earlier thing where we said y is equal to 5 times x but x was not yet assigned a value then this would generate a name error. The other example we saw before where you try to evaluate an arithmetic expression where the numbers are defined but the denominator is 0 for a division will give you a 0 division error. So, in each case there is a fixed kind of a terminology that python uses.'},\n",
              " {'id': 'fdc28a00-a429-4a24-9aa9-d52ebce51a0e',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, every time you get a name error you will get this message name error, but there will be a diagnostic string, a kind of explanation which will give you a little bit more information which was the name that was wrong. So, that comes as a bonus for a human to read but as far as we are concerned for exception handling what we are really interested in is the first part because this will tell us what kind of error occurred in our code. So, the final thing that we looked at was errors dealing with data structure, so index error is that a list is out of range and for example a key error would be that a dictionary key that we are trying to access does not exist. So, in all such cases we want to be able to take corrective action. (Refer Slide Time: 6:05) So, what happens when code generates an error is called raising an exception. So, when it raises an exception the code creates these two things, it creates the type of error and creates a message. Now, we will see that we can actually raise exceptions ourselves, so we can create an error if somebody uses our code in the wrong way. For instance, supposing you define a function called factorial and somebody asks you to compute factorial of minus 10 so we know that factorial is not well defined for negative numbers because the factorial is 10 times 9 times 7 up to 1 but if I start with minus 10, I cannot keep going down because it will go infinitely far. So, what you do when you see factorial of minus 10? Well, there are two things, one is you can return some default value or you can actually raise an exception saying do not call this factorial with function with a negative number. So, raising exception we can do but we are not going to focus too much on that, we will see a small example later on. But the code that is running will in general raise exceptions and what we want to focus on is how we handle this exception. So, we know that the code that we call could generate an error so how do we deal with that?'},\n",
              " {'id': 'f17f8cab-9944-4ff4-9c0d-879de93eef9c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'And if we do not deal with it what will happen is that our code will crash. So, we call a function that function generates an error, we do not have code to cope with that error, so our function will also crash because a function that we called crashed. So, it is in our interest to avoid that and that is why we want to handle the exceptions. So, to handle an exception we use this kind of format so that we take the code that we are trying to run. So, this is the actual code that we had, we are trying to run this code but we know that potentially there are some things in that code which might generate errors, so we anticipate that by putting it inside this thing called a try. So, the word try is self-explanatory, try to run this code. And now, if it generates errors take the following action, so that is what is except does. So, except suggest what to do when different types of errors happen, so with except there is an indicator about what exception or what error we are handling. So, if the code generates an index error then we will execute this code. Now, it happens in sequence, so if it is an index error it will come here if it is not an index error then this code will not apply, so it will go to the next line and see is there any other except. The same except can handle more than one thing. So, this is saying if it is a index error the first one happens but if it is a name error or a key error then do this. So, we could have the same exception handling code, handling more than one type of error and these things are checked in sequence. So, it will first check for index error. Remember the error will be only be of one type, an error comes to us its of one type, if it is an index error it will match this block, if it is not an index error it will not look at this at all, it will directly come to this. If it is not a name error or a key error, it will keep going down. So, maybe there is some errors that we do not know about, but we still want to not have our code crash.'},\n",
              " {'id': 'faf4660b-ad66-4ff5-b785-1f2044daa96a',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, then we can write an empty except, except with no side qualifier saying which exception it applies to and such an empty except will kind of trap everything. So, everything that reaches this point will execute this code. So, in this case there are three types of errors which we explicitly deal with, index error, name error, key error, any error of any other type will come to this third except and stop here. So, therefore obviously this kind of a except with no qualifier should be at the end because we put it at the beginning it will not pass anything to any of the other things, this is the most general trap, it is like you are having some kind of a net to catch, a safety net supposing you have a football field, so you have a net and then you have another net and then you have another net, but the biggest net is on the outside if you put the biggest net on the inside nothing will reach the smaller nets, so that is the goal. And finally this is something which happens even in for loops and while loops, sometimes you want to check whether the for loop or the while loop exited normally. I did not break out of it and I did not abort it. Similarly, the default case hopefully is that this code actually executes, the code at the top actually executes without error. So, if I come without error then I will come here, so we can attach an else to this try and this else basically will indicate that I actually managed to finish that try block as it is called without encountering an error, so this allows us to distinguish in our code the situation where an error happened and also the, if there is something we want to do in case an error did not happen then we can do it here. So, this is the broad structure of how we use this exception handling in our code. (Refer Slide Time: 11:08) So, exception handling is not always used only to track errors, it can also be a style of programming in certain situations and most notably with dictionaries and keys.'},\n",
              " {'id': '696be998-8ba4-450a-b6a3-4c483b2e3a99',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, here for instance is a dictionary which stores the scores of different batters in cricket. So, the keys are names and the values that you store is a list of scores so far associated with that person. So, when I want to update this list there are two possibilities as always with the dictionary that is the person whose name I am going to, whose score I am going to update already has an entry in the dictionary, in that case I just append the current score or this person does not have an entry in which case I want to create this, we saw the same thing when we were doing this counting the differences of primes. We said if this difference has been seen before increment that difference for this key otherwise create a new entry in the dictionary for this difference because it is the first time we are seeing it. So, the way we did it there was a traditional way we check whether the new key that we are looking at already exists. So, if the batter that we are looking at is already available in this dictionary as one of the keys then I append the score for this batter otherwise I create a new list with this one score and this is the first score that I have added for this batter, a new key and a new list, so this is the traditional way. So, you check whether it is there if it is there you append or you increment or whatever, update in some way otherwise you create a new entry. So, how would we do this in an exception handling way? So, supposing we assume the key is there, so we assume the key is there and we try to update it, if it is there it will work, if it is not there then it will throw this key error, so here is how we do it, we try to update it that is our default, we assume that this key already exists and we try to append the score. But if this is not the case then python will complain saying that there is no scores of b so this will give us a key error, so we say okay, if there is a key error then do this instead.'},\n",
              " {'id': '194d1649-b470-435c-9541-89f2565e1450',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, this is not a case where we are flagging some kind of drastic error but rather we are using this exception to signal which of the cases, so it is a complicated way in one sense of doing if then else. But if you read it in English this is much easier, try to do this and if you cannot do this do that instead which is more or less what you are saying before but you are saying it in a more complicated and convoluted way, instead of saying try to update and then create you are saying if this key already exist then update otherwise create, so it is a matter of taste and style which one is more easy for you to understand, but this is just to explain that exceptions can be used in a sense in a positive way, they do not have to only be used in a negative way to catch errors. (Refer Slide Time: 14:07)  So, to conclude this discussion of exceptions let us see how this whole thing actually works. So, supposing I have this main code which calls a function f so this will transfer control to f which is defined somewhere else. Now, f in turn may call another function g, so this will again transfer control to g which is defined yet another place, and g in turn may call an h, so we have the sequence of calls so this called this, this called this, this called this. So, we started with calling f and eventually unknown to us in some sequence h has got called. And now h generates an error and it does not internally handle it. So, what happens to this error? So, what happens to this error is that we said before that if an error is not handled then something will abort, but in this case it kind of is unfair to ask this to abort because an error happened deep down, so it will actually abort in stages. So, this error will get passed back to g, so the call that g made to h will terminate in a kind of incomplete fashion and now it is up to g to figure it out. So, it inherits this index error.'},\n",
              " {'id': '27c2db69-272d-4d00-9293-13a9f306f229',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 1},\n",
              "  'source': 'Exception handling.pdf',\n",
              "  'content': 'So, g will see with respect to h an index error, so supposing there had been a try here, except index error then g would have been able to handle this. But maybe g did not. So, if g did not handle it then the same index error goes back to whatever called g. So, in this case it goes back to f. So, again the possibility is that there was a try here which handles this. (Refer Slide Time: 15:52) But if there was not then it goes back to where it called. So, the sequence is that the index error or whatever error it is goes back up the calling stack as it were. So, you keep calling functions and the error goes back in the same sequence and wherever it is handled it will get handled, if it does not get handed it will go all the way back to the original code which started this whole sequence which is the code here. So, at this point now if I did not anticipate it there is nothing more I can do because this was the beginning, this was the main function that was running. So, if I do not handle this then python has no option but to abort the execution of the program. But if at any intermediate stage there had been this try except thing which caught it, then we could have taken some corrective action and fix this error.'},\n",
              " {'id': '3337cb5c-b188-4ed5-81ea-321efa41645e',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithm using Python Professor Madhavan Mukund Why Efficiency Matters So, to wrap up this introductory week of the course, let us look at an example to illustrate why we are interested in efficiency and why we are interested in both algorithms and data structures? What is the kind of benefit that we hope to get out of this? (Refer Slide Time: 00:26) So, let us look at a kind of real-world problem that you might imagine. So, as you probably know, when you go to get a SIM card, you have to give your Aadhaar details. So, every SIM card needs to be linked to an Aadhaar card. Now, if you have an unscrupulous dealer, handing out SIM cards, it could well be that they are putting fake Aadhaar details just to sell you the card. So, the government would like to check that the Aadhaar details on each SIM card actually match with what the Aadhaar number says. So, A the Aadhaar number must exist and whatever details are there about the person who holds the SIM card must match what the Aadhaar number says about that same person. So, this is the problem, every SIM card needs to be linked to a valid Aadhaar card. And we want to validate that the Aadhaar numbers which have been, so let us assume that every SIM card has been actually provided with some Aadhaar details, we want to now check whether the Aadhaar details in all these SIM cards are actually correct or not. So, this is something that we saw a long time ago in computational thinking. This is basically a simple nested loop, for every SIM card, we need to check whether its Aadhaar details are valid or not. So, the only way sensible way to do this is to go through all the Aadhaar cards and look for a match. So, we take each SIM card. And then for that SIM card, we run through all the Aadhaar cards till we find a match. And we find a match saying that this Aadhaar number is Aadhaar number which is mentioned in this card, then we will check that the details match.'},\n",
              " {'id': '74922404-a035-4532-a884-12da82b5ec7c',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'Of course, if there is no valid Aadhaar number for the card, that is also a mismatch. So, at the end of this, either this card will match or it will not match. So, I can separately now take this and create a list of all the SIM cards which are problematic. But fundamentally, I have to run this nested loop, which compares every SIM card against every Aadhaar card. So, the question is, how long will this take? So, supposing there are M SIM cards and N Aadhaar cards, then it is easy to see that the outer loop will run M times. And each time the outer loop runs, the inner loop will run in general N times if I go through all the Aadhaar cards. So, let us assume that all the SIM cards have invalid Aadhaar numbers, then I will have to scan through all the Aadhaar cards to discover this. So, in the worst case, I will end up with M times N. So, this is a kind of complexity of this naive solution. So, what in practice are M and N? This is why we are looking at this example to get an idea about what happens in the real world when we deal with data. Because data is what we are going to deal with a lot in machine learning. And these kinds of scales are typically what we have to deal with not the toy examples that we construct by hand. So, here we have Aadhaar cards and SIM cards. So, how many Aadhaar cards and how many SIM cards are there in India. So, by now almost everybody in India has Aadhaar card. And as you know the number of people in India is more than 1 billion, which is 100 crores, which is in scientific notation 10 to the power 9. So, at the least there are 10 to the power 9 Aadhaar cards, it might surprise you to know that there are as many SIM cards. It is not true, of course that everybody has a SIM card. Babies do not have SIM cards, small children may have Aadhaar numbers, but they will not have SIM cards. But many people have more than one SIM card. So, it all kind of balances out.'},\n",
              " {'id': '6b900d3b-f270-4d97-8d7e-262db318a1cb',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'And if you actually count the number of SIM cards, which are there in the country, it is roughly equal to the number of people in the country. So, both M and N are actually of this order 1 billion, 10 to the power 9. So, that is the problem that we are dealing. (Refer Slide Time: 04:10) So, let us assume that, for simplicity, that they are actually equal to, they are actually bigger than this, but let us assume they are just exactly 1 billion. So, there are exactly 1 billion Aadhaar cards exactly 1 billion SIM cards. So, now let us do the math. So, the nested loop, if you remember, runs M times N times, so 10 to the N times 10 to the 9 multiplied by 10 to the 9, this is M times N, and this gives us 10 to the 18. So, I have this naive search through a nested loop is going to execute 10 to the 18 times. (Refer Slide Time: 04:43) So, we have already calculated that using the timer and all that, that Python actually can run about 10 to the 7 operations in a second. So, even if we assume that every time, we run this loop all this checking happens in some unit operations, this is a very simple thing that we have to do here. Even if there is just a basic step here. We have to do this step 10 to the 18 times, and each of this thing is going to take one operation. So, 10 to the 7 operations per second means that I am going to use 10 to the 11 seconds. This is just 10 to the 18, divided by the number I can do in 1 second, which is 10 to the 11. So, 10 to the 11 seconds, is my estimate for how long, it is a very conservative estimate for how long this code is going to take to run. So, what is 10 to the 11 seconds. So, 60 seconds, make a minute. So, if I divide 10 to the 11 by 60, I get something like this, I get something like 1.67 times 10 to the 9 minutes. So, this is still a little abstract to understand.'},\n",
              " {'id': '86d0231b-8776-4caa-9b2c-c6ed438fbc74',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'So, 60 minutes makes an hour, so if I divided 60 again and I get something like 2.8 into 10 to the 7, so 2.8 into 10 to the 7 is 28 into 10 to the 6, so 10 to the 6 is 1 million. So, this is 28 million hours. Still not very clear. So, let me divide by 24 to get days. So, if I divide 2.8 times 10 to the 7 by 24, I get the total number of days, so I get more than a million days. So, what is a million days? Well, let us take a year to be 365 days. So, if I divide this by 365, I get 3200 years. Now, this is something we can understand. So, 3200 years is a long time, it is probably going to be much beyond the lifetime of the planet as we are currently running it. And we certainly cannot wait so long to validate whether every SIM card has a valid Aadhaar number. So, this is just to illustrate that, if you just take a problem at face value and address it at the way it is presented to you without doing any thinking, you might well end up with a problem that is unsolvable, simply because it is physically not possible to solve that problem given the resources and constraints that you have. So, we need a different way to do this. So, how do we solve this problem? So, clearly, this cannot be the only way to solve this. Otherwise, there is no point in associating Aadhaar numbers with SIM cards because nobody is ever going to be able to check that they are valid or not. (Refer Slide Time: 07:19)  So, let us switch to a different problem. So, supposing I want you to guess my birthday. And the way it works is that you are allowed to make a guess. And I will either say yes, you got it. Or I will tell you no, it is wrong, because my birthday is earlier than the day you presume. And or it is later. So, this is just a date. So, I am just asking for a date like September 7 or October 24th. I am not asking you to guess the year at this point, just the date within the year.'},\n",
              " {'id': '7dcd395e-74dc-4bf8-a8fb-b284305703ec',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'So, there are 365 or if you count leap years 366 possibilities, I want you to figure out which one of them is my birthday. So, let us suppose that my birthday is 12th of April. So, you might ask questions for instance, you might ask, is it the 12th of September? And I would say no, it is not my birthday is earlier than that, then you would logically ask a date. I mean, after I tell you it is before September 12. Presumably, you will not ask me a date like November 15. Because you know, it is before September 12. So, you might ask me something at the beginning of the year. So, you say is it February 23rd? I say no, it is not February 23rd in fact it is later. So, now what have you done? You know, it is between February 24th. Because it is not February 23rd. And it is latest between February 24th and September 11th. So, you will have something in that range. So, maybe you might ask July 2nd. Now, I will say no, it is not July 2 it is earlier. So, now you know it is between February 24 and July 1st. So, our question is, there a systematic way to ask these questions? What is the best sequence of questions to ask in order to guess my birthday? This is the rule that I can say yes. Or I can say it is before or I can say is after. So, as we saw, what we are doing is we are actually taking an interval of possibilities and narrowing it down. So, initially, I have all 365 days after I asked one question, the interval narrows. So, the best way to narrow it is to take a question in the midpoint. So, if I take the midpoint, then the next time I am going to look at half the interval. Now see, September 12. If it had been after September 12, I would have got lucky because I would have narrowed it down to the last three and a half months of the year. But because it was not after but before and still left with 8 and a half months, which is not great.'},\n",
              " {'id': '9e682c58-cc60-43f3-8b76-48d2d367bd27',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'So, instead of having this imbalance with 8 and a half and 3 and a half, it will be better to have something in the midpoint so that whichever way the answer goes, I am guaranteed that I am making an equal amount of progress. The midpoint is best. So, if I take the middle month, end of June or beginning of July, so let us say I take June 30th. So, the answer is still the same. I am still asking questions about 12th of April. So, if you ask me June 30th. No, I will say it is earlier. Now, you have range from January 1st to June 29th. So, January, February, March, April, May, June. So, at the 3 months point so, you will ask say March 31st. So, now that you say March 31st, I will say it later. Now, you have narrowed it down to from 6 months to 3 months. So, it is now in April, May or June between April 1st and June 29th. So, now you take the midpoint in that the middle month is May. So, you take the midpoint say 15th of May. So, you will say is it May 15th? And I will say no, it is earlier. So, now my range is between April 1 and May 14. So, this is like 45 days roughly. I have cut it down from 12 months to 6 months to 3 months to one and a half months 45 days, so 45 days, so, about 22 days is the midpoint. So, I may task 22 days from April 1st, so, I may say April 22nd. Now, if you ask April 22nd I say it is earlier, so, now, my range has come to April 1 to 21 because it is before April 22 and it is after March 31. So, this is a 21-day range, so I might ask 10 to 11 days into this range. So, I maybe I will ask April 11th. So, now, it says later, so now it is April 12 to 21 this is my range. (Refer Slide Time: 11:25) So, I take the midpoint of this. So, I may say 16th, it is the 16th or 17th I mean the midpoint sometimes could be one of two values, so I ask 16th, is earlier, so, now it is between 12 and 15th, it is definitely after 11th before 16th. So, 12, 13, 14, 15. So, I may be ask 13. And this is earlier, so now I know it is after 11. And it is less than 13.'},\n",
              " {'id': '369f5a11-0af7-4100-80d3-7b62151884f3',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'So, I have no choice. I mean, unless I have been lying, and I am not born at all this year, I have narrowed it down to an interval of 1. So, it must be April 12th. So, I have come down this list and answered your question. And in this particular case in 8 questions. (Refer Slide Time: 12:00) So, how does this work in general? Well, the interval shrinks, so initially, we are looking at 365 days, then we look at half of that, because one number gets knocked off. So, 364 divided by 2, so 182 days, then 182, one number gets knocked off, I have 90 or 91. So, in the worst case, I will have 91, and so on. So, 91 becomes 45, 45 becomes 22. So, each time I am subtracting 1 and halving and getting roughly that number, if it is not even I am taking the bigger of the 2, so 22 becomes 21, because it is not that day. So, I have 10 on one side, 11 on the other side. So, 11 is the bigger of the 2. So, the worst case I might get 11, 11 one will go so I will get 5 and 5. Similarly, 5 will go so, I will get 2 and 2. And then when I ask an interval of 2, then I will basically be asking the midpoint and either I will get it right or it will be bigger or smaller than the number I ask. So, I will finally end up with 1, so you can count 1, 2, 3, 4, 5, 6, 7, 8, 9 in general, here, I asked 8 questions and got to this interval. So, certainly it is under 10 questions. So, this is the guessing the birthday thing. So, what is the property that we are using, we are using the property that I can say earlier or later. So, the birth dates are arranged in a sequence. And I am asking you about a particular date. And I know whether to go backwards or forwards. So, this order is important. I have got a sequence of dates. And I am asking you and you can use the same principle to look for, say, you know, words in the dictionary or whatever we are quite used to this. So, when you have a sorted sequence, searching for something and going backwards and forwards, you would not find a page number in a book.'},\n",
              " {'id': 'cdb656c8-2563-41f8-9641-217639882f2a',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'It is very easy, supposing somebody says open this book to page 784, you will first open it at random, and you will see whether the page you have opened is before 784 or after 784. And then you will go forwards or backwards and you will quickly arrive at the page that you need, you will not flip through the book one page at a time to reach 784. Now, how do we apply this to our earlier problem of SIM cards and Aadhaar cards? (Refer Slide Time: 13:53) So, let us assume that this Aadhaar database is actually sorted. So, we can talk about Aadhaar numbers and go backwards and forwards. If I look for Aadhaar number, I can say whether the Aadhaar number I am looking for is before or after the current one I am searching for it, so I can probe this Aadhaar list just like a probe this birthday list, I can say look at the middle number in my Aadhaar list and then decide whether the number I am looking for is before or after, that is more or less what we did in the birthday, I asked you a date in the middle of the year. And I am told with this before or after by you. So in this case, by looking at the Aadhaar number I can tell if I look at the middle Aadhaar number in my list, I know whether the Aadhaar number I have is smaller or bigger. So, I can use the same halving strategy that we did for guessing the birthday in order to keep halving the interval to search for the Aadhaar card. So, in other words for each SIM card S I use this halving strategy to find the Aadhaar card matching it. Now in the birthday case, I always found the birthday because that birthday existed. Now, in the Aadhaar case, there is a situation where the Aadhaar card may not exist. So, what will happen is you will come down to an interval of 1 and that one card you only have one number left to look at and that is not the number that you have, then you will say it does not exist.'},\n",
              " {'id': '1e3abd41-6c9e-4391-af99-63c0c1e091bc',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'Because if it were there, it should be this number, but I have because I have narrowed down this interval down to one number, either this number is the one I am looking for, or there is no such number in this list. That is something I can say. So, we will look at this more formally. But that is essentially the idea that I keep halving it. And when I finally come down to this interval of one, either I found it or it does not exist. So, how long does this this probing take? How long does this take? (Refer Slide Time: 15:30) Well, if I halve many times, then I am dividing by 2. So, if I halve 10, times, if I start with an x, and I divide by 2 and then divide by 2, and so on, then if I divide 10 times, then I divide by 2 to the 10. And 2 to the 10, is roughly 1000, 2 to the 10 precisely is 1024. So, if I halve something 10 times I reduce it to 1000th of its size. (Refer Slide Time: 15:58) So, therefore, after 10 queries, remember my Aadhaar interval, there are 10 to the 9 possible Aadhaar cards from the smallest one to the biggest one, if I have now halve this thing, 10 times, I have brought this down by a factor of 1000 to an interval of size 10 to the power 6. Now, I keep halving. So, this halving applies at every step, so this 10 to the 6 after 10 more queries will come down to 10 to the 3. So, I am now have come from 1 billion to 1 million after 10 queries. From 1 million I have now a range of 1000 after 10 more queries, and now 1000 range in 10 more queries, I will narrow down to one because 1000 by 1000 is 1. So, after 10 queries, I had 1 million after 20 queries, I had 1000 after 30 queries, I have narrowed down to a single card either this is the Aadhaar card I want to check all this is not the Aadhaar card, there is no Aadhaar card of my SIM card in this. So, in 30 queries, I have completed the search for each SIM card. So, therefore overall, there are 10 to the 9 SIM cards and each of them I might spend 30 steps looking for the thing.'},\n",
              " {'id': '108bd431-4f96-4688-a5a5-ba000c8ed2ad',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': \"And then let us assume that you do some basic operations. So, this is my estimated work. So, earlier, if for each SIM card, I had to scan through 10 to the 9, so it is 10 to 9 times 10 to the 9, I have converted that second 10 to the 9 work, instead of scanning through the entire list by probing this sorted Aadhaar list, I brought it down to 30. So, remember that this is going to take 100 seconds, because 10 to the 7 is what I can do. So, 100 times 30 is the number I am looking at. So, I have 3000 seconds. If I divide 3000 by 60, then I get 50. So, therefore, this process is going to take me something relate off the order of 50 minutes. Now, when it was not sorted, we had something which would take 3200 years. And by just the simple fact that we have arranged the Aadhaar cards in a nice way, we are able to use this birthday probing, halving the interval and bring it down by enormous number of amount of time from 30 to 100 years to 50 minutes. So, 50 minutes is not fast, but at least you can start this thing and go and have lunch and come back and it will get done. So, it is something which will happen in an hour's work. So, it is something that you can start off and go and finish and come back. So, it is not, and of course, this is in Python, if we do it in another language, it will actually happen in less than a minute. So, we have taken something which is wildly infeasible and made it extremely efficient. So, of course, there is a step which we have to do, which is to sort the Aadhaar cards. Because every time somebody creates a new Aadhaar card, this list will it is not a onetime operation. So, this Aadhaar card list has to periodically be resorted because new Aadhaar cards keep getting added to this list. So, we need an efficient way to sort the Aadhaar cards which we are not discussing now. So, assuming that this Aadhaar card sorting does not take 3000 years, if it took 3000 years to sort the Aadhaar cards, then we are anyway doomed.\"},\n",
              " {'id': 'b066cc58-7969-4387-a47d-50f3b520abe0',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 1},\n",
              "  'source': 'Why efficiency matters.pdf',\n",
              "  'content': 'But assuming we can also sort the Aadhaar cards in some reasonable amount of time. Then we can apply this halving trick to solve this problem in 50 minutes. So, the main purpose of this story is to show you that we were able to achieve this efficiency by assuming something about how the data is arranged. So, we had to first put the data into this sorted form and only then we could apply this halving strategy to get to our answer. So, both were required I could not, so when we did GCD for example, and we came up with Euclid’s algorithm, we did not exploit anything about how the numbers were given to us M and N is given to us, but we exploited some number theoretic properties of the remainder in order to convert it to a simpler problem. So, that was an algorithm which did not represent depend on how the data was presented to us. Here, we are actually changing the way the data is presented to us we are assuming that the Aadhaar cards are sorted and if the Aadhaar are sorted then we can do this efficiently. So, the moral of the story is that both are important. So, this is why this course is called data structures and algorithms. So, it is important sometimes how the data is organized. So, in the very least thing we have the organization in this case, so, it is not a structure as such, but it is just saying that the arrangement is in a particular way. We will also see examples where the format in which the data is presented makes it easier to look up things. So, those are data structures. So, if you have good data structures, then you can think of cleverer algorithms. So, both data structures and algorithms are important. And it can make a problem which is immensely useful to solve and bring it down from something which is wildly impractical to something which is highly efficient.'},\n",
              " {'id': '38c20a56-53fd-42b3-9990-f83049c8db3a',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 4).pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithm using Python Professor Madhavan Mukund Implementation of Python Codes-Part 4 So, since we cannot construct this list of Aadhaar cards and SIM cards, let us look at this efficiency in our running situation with respect to our GCD algorithms. (Refer Slide Time: 00:22) So, this is our original GCD algorithm. The one which computed the list of common factors, and reported the last one. And what we observed was that this takes time proportional to the minimum of the 2 numbers because this for loop runs for that long. (Refer Slide Time: 00:40) So, let us look at 2 GCD of 2 large numbers. So, let me take smaller numbers. So, let me take two 7-digit numbers. So, let me so this is now GCD of 5678123, which is a 7-digit number, and 8765423. So, let me first execute this so that this Python function is, so now GCD, has been defined. So, now if I run this GCD on this number, then it reports an answer, reasonably fast as 1. So, it actually says that this is, now if I suppose I increase the number of digits by 1. (Refer Slide Time: 01:11) So, supposing I make this two 8-digit numbers. If I make these 2-into-8-digit numbers, and then I run this GCD, then you can see that it is really taking a long time, because now the minimum of 2 numbers is now an 8-digit number, and if I take yet another digit, so supposing I make them 9-digit numbers, right, then actually, if I start running this, this will run for an enormous amount of time, I mean, so I can just keep talking and nothing is going to happen. So, you can see that having a kind of naive algorithm will seriously limit the efficacy of what you are doing. (Refer Slide Time: 01:51) So, now, I just stopped that. So, now let me use instead, this first recursive algorithm that we defined, which uses this minus a minus b format. So, let me use this, a minus b format on this. And now let me go and try to re-evaluate this thing, which was taking a long time. So, I take this 9-digit number.'},\n",
              " {'id': '8c7a25b8-eb8b-41a2-92dd-859024d2d684',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 4).pdf',\n",
              "  'content': 'And now if I do a minus b format, fairly fast, it gives me 1. So, switching to this recursive algorithm, gives me a 1. But we already saw that this a minus b format itself has problems because it will keep subtracting and coming down very slowly. (Refer Slide Time: 02:27) So, supposing I take something like 2 and a large number of 9s. So, this will take a long time. But it will also explore some other thing about Python, which is that Python has something called a recursion limit. So, this is, every time you run a recursive function, it has to remember the previous thing. So, actually what this gives us is something more serious, it gives us a recursion error. Now, it is possible to fix this recursion limit and make it larger. But in this particular case, if I want to make it large enough to do this, it actually still does not work. So, but even if I take a smaller thing, I mean, this recursion limit is going to be a problem. (Refer Slide Time: 03:05) But now let us look at Euclid’s algorithm. So, Euclid’s algorithm was the same recursive thing except I used the remainder operation for subtracting, so I take in the base case, I check whether b divides a, if b divides a I return b, otherwise, I compute the GCD of b and a remainder b. So, let us now use this as our operational definition. (Refer Slide Time: 03:30) So, first of all, we can go back and try the first one that our earlier thing failed to do. And you can see that it does is very fast. And I can probably add a lot more digits to it. Let me see, I hope it will work. So, you can see that our claim was that GCD, Euclid GCD actually grows proportional to number of digits. So, if I add 2 more digits, it grows only as plus 2, not time 10. (Refer Slide Time: 03:55) And in particular, if I do this one, which our earlier thing failed, because of the limit GCD, the Euclid GCD, very quickly tells me that the GCD of 2 and a large odd number is 1.'},\n",
              " {'id': 'ec773b67-a2a0-4243-9077-db13f98e878e',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 4).pdf',\n",
              "  'content': '(Refer Slide Time: 04:05) So, this is the real power of Euclid’s algorithm. So, it actually gives you a very efficient way to calculate GCD. And hopefully this gives you some feel for the dependencies on some things which are proportional to the number value as a magnitude and the number of digits in that value. So, Euclid actually operates in number of digits. (Refer Slide Time: 04:27) So, if I give you a large number of this is basically operating on the length of the string. So, I can actually put much so Python, as you know, can take very large numbers. So, I can take much larger numbers and hopefully run this. And Euclid’s algorithm works. So, it can actually handle very large numbers because it is actually it is like multiplying, I mean, multiplying two very large numbers, if I add few more digits, it just takes a little bit more time. Euclid’s algorithm is like that. So, I hope you get a feel for this from this running example.'},\n",
              " {'id': '291288cf-ac65-4305-b921-df9be8aa4b33',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Python Recap - III (Refer Slide Time: 0:20) So, let us continue our recap of python and go back to the gcd which we saw in the first of these recap lectures. So, remember that the gcd is the number which, the largest divisor of both m and n and we had two versions of gcd that we had computed, one which actually computed the list of common factors between m and n and then found the last one, so this is the last common factor or we said we do not actually need the entire list because only the last one is important, so I can just incrementally keep track of this most recent common factor. But in both of these the operational aspect that has to be executed is this for loop, so I have to run over all i ranging from 1 to the minimum of m and n and this is something that we wanted to improve on. (Refer Slide Time: 1:02) So, let us see whether there is a kind of different strategy to look at gcd, instead of, so what we have done right now is what you might call the naive or the brute force strategy. We took the took the definition of factors and we just literally computed all the common factors and tried to find the largest common factor. So, sometimes you need to think in a kind of different or orthogonal way about the problem in order to find a better way to solve it. So, here is a different way of thinking about it. So, supposing d is indeed a common factor of m and n, so d divides m and d divides n, that means I can write m as a multiple of d some a times d, and b is also a multiple of d some b times d, n is a multiple of d b times d, so m and n are both multiples of d because d by assumption is something that divides both m and n. But now let me look at m minus n then with some very simple algebra you can say that m minus n is a d minus b d and therefore it is a minus b times d.'},\n",
              " {'id': '559408a0-1d57-453c-9f04-ea7569b64f3c',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'So, assuming that, let us assume for the moment it does not really matter if m is bigger than n, a minus b is going to be positive if m is smaller than n, m minus, a minus b is going to be negative, but the main point is that d is also a number that divides m minus n. Now, m minus n is a smaller number. So, what we are really saying is that if we want to solve a problem with m and n, we can actually reduce it to a problem with smaller numbers, so we can keep simplifying the problem we are solving and come up with a final solution which is based on very small numbers. (Refer Slide Time: 2:44) So, these are what are called recursive definitions, so we want a function which is recursively defined so we have to have a situation where we can declare that we know the answer and the situation we know the answer is when one of the numbers divides (()) (2:56), supposing I ask you what is the gcd of 9 and 3, then you know that 3 actually divides 9, so you can just return 3 because if the smaller of the 2 numbers actually divides the bigger number directly then it is the gcd, because it is the largest factor of itself and there is no larger factor of 3 than 3 and 3 divides 9. So, if the second number, we are assuming the second number is smaller in this category. So, if n divides m then we can just directly say that we are done, if n does not divide m then we have to do some work. So, earlier our work consisted of actually explicitly computing all the factors of m and all the factors of n, up to the minimum of m comma n. Instead what we are going to do is exploit this observation and say let us take the smaller number n and look at the gcd of that number with the difference m minus n. So, we reduce this problem of m n to the problem of n m minus n where n we assume is small. So, here is how we write it, so we have this gcd of m comma n but we do not know of course whether m is bigger than n or smaller than n.'},\n",
              " {'id': 'bddaf15d-e5a1-48ad-8876-92cd385c097f',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'So, what we do is we first figure out which is bigger and which is smaller and call them a and b. So, a is the bigger one the maximum, and b is the smaller one the minimum. So, now if the smaller one that is b divides a, so a divided by b has remainder 0, then b is already the gcd and I am done so this is the base case. Otherwise I take the smaller number which I know is b because I have already computed that b is a minimum and I take the difference a minus b. So, you might be tempted to think that after this a minus b will be smaller than b but it is not the case in general. So, supposing I say something like gcd of 97 comma 2 then 2 does not divide 97 because 97 is an odd number. So, I will end up saying that b is 2 and a minus b is 95. So, the next call will be gcd of b and b minus a, a minus b, so it will be 2 comma 95. So, in general each time when I call this gcd recursively I cannot I am not guaranteeing that I am calling it with the right order bigger and smaller which is why I need to fix that inside the function, so I get two numbers I first identify which is smaller which is bigger and then I proceed. But otherwise this now you see that there are no factors, no list nothing, I am not explicitly keeping track of the factors of any number, I am only signaling when one number is smaller than the other number, divides the other number. So, now because of this property, supposing a number divides both m and n then it must also divide m minus n. So, in particular if you look at the gcd, the largest number that divides m and n that number is also a d of some form, so the largest number that divides m and n also divides m minus n, so at every point when I reduce the problem I am not changing the value I report, the gcd before and the gcd after are the same, anything that divides m comma n also divides n m minus n.'},\n",
              " {'id': '8d5b1358-8fc7-48ad-8bd2-a0a0dc0427e5',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'So, by reducing this problem each time I am guaranteeing that the answer does not change and finally I reduce it to a simple case where I just check whether smaller number divides a bigger number. So, this is our recursive function. (Refer Slide Time: 6:32) Now, the problem with this is actually that this does not solve our original problem, our original problem was when we were computing this list of factors it was taking time proportional to the minimum of m and n. Now, here actually is going to be a little worse, it is going to take time proportional to the maximum of m and n. So, to understand this let us look at an example. So, supposing I take the gcd of 2 and some large odd number, then as we saw b minus, b is going to be 2 and a minus b is going to be the large odd number minus 2. So, 9999 is going to reduce to 9997, that is going to reduce by 2 to 9995, so I am going to keep doing this approximately so I am reducing by 2 so this is approximately 10000 the second number. So, if I keep reducing by 2 after about 5000 steps I will come down to 2 comma 3 and when I do 2 comma 3 and I do this thing now 2 becomes a smaller number so then I get 3 minus 2 the difference is 1 and so I get gcd of 2, 1 and now since 1 divides 2 finally I use the base case to get the fact which was kind of obvious right from the beginning that I have 2 and I have an odd number, then I do not have any possibility that that number is divisible by 2. So, the gcd is 2 saying there is no common factor between 2 and any odd number. (Refer Slide Time: 8:00) So, the problem here is that this takes approximately 5000 steps, so this is not a nice thing as we will see, so we would like to do something much faster than this. But we can use the basis of this recursive thing to come up with a more clever recursive thing. (Refer Slide Time: 8:18) So, this is what is called Euclid’s algorithm.'},\n",
              " {'id': 'a8daeab1-c487-4634-bdc9-ef782a36358b',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'So, suppose now earlier we looked at common factors we said suppose d divides both, now let us suppose that we are not in the base case, the second number does not divide the first number that is where we have to figure out what to do next. If the second number n divides m then we are done, so the question is what do we do when we are not finished, how do we reduce it to a smaller problem. So, in the earlier case we took m minus n but now we want to do something more clever than that. So, if n does not divide m then when I divide m by n there is a remainder. So, this is written as, this is if you remember from school this is called the quotient and the remainder so m divided by n it goes a certain number of times so that it goes into it q times and leaves a remainder r, so I can write m as q times n plus r. (Refer Slide Time: 9:13) Now, let us look at a possible common divisor d for both m and n. So, as before if d divides m and d divides n, I can write each of them as a multiple of d. So, I can write m as a times d, I can write n as b times d. So, now if I look at the equation above m is equal to q n plus r, I can replace this m by a times d and I can replace this n by b times d. So, I have on the left something which is a multiple of d and I have something on the right which is a multiple of d plus something. Now, this is not very difficult to show but you can easily justify to yourself that if the left hand side is something times d then the right hand side must also be something times d and if d is a factor here then d must also be a factor there. So, from this expression you can conclude that r must also be of the form something times d, so let us call it c times d. So, in other words when I find that n does not divide m not only does m minus n as we saw before have d as a factor but m remainder n also has d as a factor. (Refer Slide Time: 10:24) So, this gives us what is called Euclid’s algorithm.'},\n",
              " {'id': '815c9e37-3913-42e2-8f21-a32480a98c8b',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': 'So, in Euclid’s algorithm the difference is that we instead of taking m minus n as in the previous case we take in mathematical terms m mod n, m mod n is the remainder which in python terms is this percent operator, so the only difference between the previous thing and this is earlier it was a minus b, now it is a percent b, everything else is the same, I compute a to be the maximum, b to be the minimum, if b divides a then I return b as before, the smaller number divides the larger number it is the gcd, otherwise I go down to the smaller number and the remainder not the difference. (Refer Slide Time: 11:06) So, we can actually show that this is a dramatic improvement, so this takes time proportional to the number of digits so earlier we had looked at for example the gcd of 2 and 9999, and we said that this is going to take something like 5000 steps because it takes time proportionate to the maximum of the two numbers but this gcd will actually take something proportional to 4 because there are 4 digits in this. So, in general when we are dealing with numbers this is what we would like, so think of multiplication, if I take 32 times 4 and I take 322 times 6, my number on the right has multiplied by a factor of 10 but clearly if I do the multiplication the extra work that I get is I have to multiply one more digit. So, a reasonable problem, a reasonable algorithm for any numerical problem like this should grow as the number of digits, it would be tremendously difficult for us if in order to add two 4 digit numbers it takes 10 times as long as it takes to add two 3 digit numbers, it does not, we just have to carry one more step. So, if I have 4 digits it takes me so many time, if I take 5 digit I have one more carry to answer, one more column to add. So, we really want things which are proportional to the number of digits and it turns out we are not going to prove it here, but Euclid’s algorithm has this property.'},\n",
              " {'id': '41fcebee-216c-4be0-bbac-da1b8e0738bd',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Python recap - III.pdf',\n",
              "  'content': '(Refer Slide Time: 12:32) The other important reason to look at Euclid’s algorithm other than it being a nice example is actually is one of the first examples that we know of a non-trivial algorithm. So, remember that we have started with the problem talking about divisors and factors and all that and we have completely translated it into some properties of numbers and remainders. So, we are kind of approach the problem from a different angle and come up with a significantly better solution. So, this is what I mean by a non-trivial algorithm, it is not an obvious algorithm and it significantly improves the performance of the obvious algorithm.'},\n",
              " {'id': 'a64c076b-535b-48b9-af36-dbaeb7f49cd3',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Timing our code.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Timing our code So, in this course, we are going to be focusing on efficiency. And one of the things that is important in efficiency is how long it takes code to run. So, we will be doing mostly theoretical calculations, but it is sometimes useful to do practical computations to understand how long Python actually takes to execute code. So, let us look at a way of measuring how long a piece of Python code takes to run in Python itself. (Refer Slide Time: 0:33) So in general, the running time of code will vary from one language to another. So, one way of measuring these things is from outside, you can take your operating system, and you can call a function or call a program and measure the time it takes for that program to run. But this will not be useful if you want to know how much part of the program. Suppose you want to know how much time each function that you call inside the program takes, you might be sorting something in the middle of it, how long did the sorting, so then you actually need to embed this timing into your program. So, Python has a library called time, which gives us some useful functions for doing this. So, one of them that we will use now is something called the performance counter, which is invoked by this function called perf time. So, perf time does not give us a useful value in itself. If I call this function perf time, it will give me some number; this number has no meaning in itself. But if I call it twice, if I call it now when I call it a little later, then I will get to values whose difference is meaningful. So, if I take two consecutive readings of perf time, I will get a difference. So, this is like having a watch. So, if I have this watch, but the watch is not showing correct time, so maybe it is showing time in a different time zone, I have just travelled from some other country and I have not reset my watch.'},\n",
              " {'id': 'e3216fff-1b7e-4b44-b70d-a215ddede093',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Timing our code.pdf',\n",
              "  'content': \"So, if I look at the watch, and I look outside, they will be mismatched, my watch may be showing 12 o'clock, it might be 6:30 outside. But if I look at it, 12 o'clock, and then I look at it five minutes later, and it shows 12:05, I know that 5 minutes have elapsed between the time I saw the watch the previous time, and that time I saw the watch now. So, the absolute value that the time is showing on my watch is not useful. But the relative difference between the time I read now, so if I want to do something after 10 minutes or 15 minutes, I can measure that with my watch, My watch is not set correctly. So, perf time is like that. So, perf time you would think of as a clock that is not set correctly. But if you call it twice the difference will tell you the interval between the two calls. And by default, this is in seconds. So, how would you use it? Well, typically, you will like any other library, you will import it. And now you want to measure the time that a certain piece of code takes. So, before you start this code, you call this perf counter function and store the return value in a variable say let us call it start after your code, you store it another variable, let us call it end. And finally, this is the meaningful thing, you can compare the difference, you can compute the difference of n minus start. And this will actually tell you how much time elapsed. So, this is the way in which you can use a performance counter. So, we can now embed this call into a class and create a timer object. (Refer Slide Time: 3:15) So, we want a timer object so that we do not have to explicitly call this perf counter ourselves, but we want to say start the timer stop the timer. So, this is like if you have a phone or if you have something else you can have a stopwatch you can say start and stop. And then you know how much time it is taken.\"},\n",
              " {'id': '772a9dd2-078c-4475-9065-666c6f20da07',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Timing our code.pdf',\n",
              "  'content': 'So, this is how we measure for instance performances and athletics when the person starts, you start the timer when the person finishes, you finish it press the timer, and you know how much time so we want to create a timer class. So, obviously, we have to import the time library to do that. And we will keep track of two things right when the timer started and how much time I measured the last time when I stopped it. So, I will start and stop and I will have to. I would like to store the thing. So, when I start the clock, I remember when I started the first call to perf meter when I finish, I, am not interested anymore in when I started, when I finished. I am interested in the time that elapsed. So, I will store that. So, this is a simplistic version there is a more elaborate version, which we will look at when we look at the code, but initially let us just assume that when the timer is created, we set both of these values to 0. Now, the functions that I need are start and stop. (Refer Slide Time: 4:23) So, what will start do, so start will start the timer? So, it will basically call this performance counter in the time library and assign it to my start time. So, now my timer is running implicitly I have set the starting time. Now when I say stop, I call this performance timer again, it is not important to me what that value is as such, I do not need to store it, what I want is the difference between that and the time that I started and I will store that in this elapsed time thing. So, that when I call this function elapsed, I will get the elapsed time right, so this gives us a timer class, I will show you separately a more elaborate version of this and how to use this. And with this, we can actually measure the time that Python takes to run and it will turn out that Python actually executes something like 10 to the power 7 operations in a second.'},\n",
              " {'id': '4c3e412b-2753-4160-8008-a5a597ffa7be',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Timing our code.pdf',\n",
              "  'content': 'This is considerably slower than languages like C++ and C, which usually do ten to the eight or more, so it is at least, so pythons is at least a factor of 10 slower than other languages. It is sometimes it matters sometimes it does not matter. But this 10 to the 7 is useful just to calibrate for ourselves, how long things are going to take and to understand why things need to be done more efficiently in certain cases.'},\n",
              " {'id': '7695d6f1-b5da-4b20-a8d0-e4b7ed1de2b4',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 2).pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Python Codes (Part 2) So, as we discussed classes and objects, we looked at this example of this point. So, let us see how it works here. (Refer Slide Time: 0:16) So, here is the basic definition of point with the x y coordinates. So, we have a class point. And this has this initialization, where it takes the values a and b as the initial coordinates of the point. And then we have this function translate, which shifts x by delta x and y by delta y. And we have this function odistance, which computes the Euclidean distance, as is called by Pythagoras formula, x squared plus y squared, whole square root. So, now I can create these points by using this. So, I, but of course, I must, nothing has run, so I must first run the code there. So, I run the code to create the point class. And now I run this and now it has created two points for me called p and q at 3, 4 and 7, 10. Now at this point, if I try to print out something like p plus q, then it is going to say that there is no remember we said that you have these ad functions and string functions. So, even if I just try to print p, for instance, if I just try to print p, I will get some kind of useless output, which says that this is a point object. It does not tell me the coordinates or anything, it just tells me that this is a point object is, the same thing happens if you try to print for example, a list or a dictionary sometimes without, you know getting the values out properly. So, for instance, if you try to print range of something, which is not a list, it will just say this is the range function? So, that is the kind of useless output. But on the other hand, I can compute the distance. So now, P is 3, 4. So, if you remember your Pythagoras theorem, then 3, 4, 5 is the right angled triangle. So, the distance of this point 3, 4 is 5 from origin. So, if I compute the distance of p from the origin, it is 5.'},\n",
              " {'id': '2b222c29-729e-4c89-a21a-bbf2c4e966bb',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 2).pdf',\n",
              "  'content': 'And if I compute the distance of Q from the origin, so notice that this kind of gives you this helpful hint as to what function to complete it, but let us not worry about that. So, in a Python notebook, I can do this kind of thing, I can implicitly call two things, and that will give me a pair. So, it says that P is that distance 5, q is a distance 12.2 because that is 7 squared plus 10 squared, which is 184 square root, which is something like 12.2, sorry, 149 square root, which is something like. (Refer Slide Time: 2:43) Now, if I ask what is p dot r, then it will say that this is not defined, because there is no internal value called r. (Refer Slide Time: 2:53) Then the next thing we did was we defined this r theta form. So, now I have a new definition of point in which I use this r theta forms. So, remember that we convert x y to r by using the distance formula and we use this tan inverse to construct the angle and so on. So, now, if I take this definition and run it, remember in the Jupyter Notebook, the last thing that ran works, so this is now my new definition of point. (Refer Slide Time: 3:20) So, now, if I take this and if I again execute this, I get two new points, p and q, but these are now defined in terms of r and theta. So, again, if I try to print p, I will still get some meaningless thing. If I try to compute the distance, okay I get the same distance, but this time is coming from r it is not coming from a calculation involving p.x and p.y. And I can check that I am using the new format by asking now what is P.r and it tells me p.r is 5.2. So, similarly, I can ask what is p.theta? So, it tells me that, this mistake here, this should be self-supposed to be safe. This is what happens in Python if you forget the self in an object, then it does not and so it takes theta as an arbitrary value. So, everywhere this should be self. So, let me run this again. So, now I have updated to self.r and self.theta, which I had forgotten the self part of it.'},\n",
              " {'id': '475ba586-3d04-424a-acc2-7c4a25d5fe16',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 2).pdf',\n",
              "  'content': 'So, now let me create these points again, and I can print the point. So, notice theta was not being used for the distance, that is why old distance worked. Because I was just looking up r and getting it, but now if I say what is p.r and p.theta, I get that p.r is 5 and p.theta is 0.972. But remember that this is not in degrees but in radians. So, just remember the unit. So that is the two different formats. (Refer Slide Time: 5:04) Now, we also said that you could have these special functions, string and add to convert. So, let us compile these things. So notice that this thing which was obscured in the slide, so this is how we convert to a string, I take self dot x self dot y, convert each of them to a string, and then embedded inside this open bracket comma and close bracket. So, I create a new string, it has an open bracket, the string version of self.x, comma, the string version of self .y, close bracket. So, this is my output string that I create. And this is what str does. And add if you remember, take self.x, a new point p.x adds them up and then creates a new point. So, now let me replace this is my third definition of point is the, I have gone back to the x comma y definition but I have added this self.str and self.add. (Refer Slide Time: 5:56) So, now let me go back up. So, this is again the advantage of Jupiter note I do not have to type anything again I can go back and reexecute it. So, now I reexecute this code. So, again, now it has created two points, but also a new definition. And as proof that is a new definition I can now say, tell me what is the value of p plus q?'},\n",
              " {'id': '9ce9558c-d877-46e2-888a-ef3fb7d5e37e',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 2).pdf',\n",
              "  'content': 'Now, this has two things; one is p plus q has to be executed it has to give me a point, what is that point, it should have 3 plus 7 as the x coordinate 10; 4 plus 10, 14 as the y coordinate and it should then separately, because print should convert it to a string it should not give me this kind of strange output which says it is a point so and so hexadecimal, it should give me the values. So, if I execute this, indeed, I get the output 10 comma 14. So, two things have happened, it has added and it is converted to a string. So, this is just to illustrate how this class and object definition work. And again, an illustration of what how in this notebook format, you can keep updating the code very naturally and keep rerunning the code without typing a lot of stuff back and forth. So, it is very convenient for this kind of incremental creation of code.'},\n",
              " {'id': '196ed963-b295-4c19-b209-cee121d356ae',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Python Recap - II So, let us continue with our recap of python by looking at another example, first we looked at gcd, now let us look at the problem of dealing with prime numbers. (Refer Slide Time: 0:23) So, we would like to check primality, so remember that a prime number is 1 that is divisible only by 1 and itself, it has no proper divisors other than 1 and itself, so prime number has exactly 2 prime factors, a prime 2 factors 1 and n and it has (())(0:38) of 2, so it is not enough to have only 1 factor so if you look at 1, the number 1, it has only 1 factor namely 1 because it is itself 1. So, we need two different factors 1 and n so technically speaking 1 is not a prime the smallest prime as you should know is 2. So, one way to check whether a number is prime is to compute the list of factors of that number and then check whether that list of factors contains exactly 1 and n. So, the list of factors of a number is very similar to the loop that we wrote to compute the common factors between two different numbers. So, in the earlier gcd example that we looked at we computed all the common factors between m and n by running from 1 to the minimum of m and n and for each number they divided both m and n, we appended it to this list of factors. Here we do not have two numbers we only have one number so we just define this factor list remember like in the common factor list earlier we have to assign it to empty to make sure that python knows it is a list. Now, we run from, so we are looking factors of n, so we run from 1 to n plus 1 because we want n also to be in our loop, so we run try out all numbers 1, 2, 3, 4, up to n whenever we find a number which divides n such that the remainder is 0 when divided by i for each such i we appended. So, this is very similar to the earlier common factor list except it is the list of factors of a single number n.'},\n",
              " {'id': 'da608827-fa10-4ef3-b0fe-f65db0eaaacb',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'And now having got this it is a simple matter to write a function which checks if a number is prime you just compute the factors of the number you are trying to check and verify that it is the list 1 comma n. So, this again tells us that 1 is not a prime because if I say factors of 1, what I am going to get is just the list 1 which is not equal to 1 comma 1. So, that is why this function will correctly compute that the smallest prime is actually 2. (Refer Slide Time: 2:40) So, now that we know how to check a given prime let us see what we can do with this. So, the first thing we might ask is what are all the primes up to 100 say or what are all the primes up to 1000 so we want to list all the primes up to some number m, so again it is a very simple matter we just start off with an empty list of primes and we run a for loop saying for everything from 1 up to m, so remember the range function has to go to m plus 1 so that we capture m, for everything from 1 to m if the number i is a prime and we can use that earlier function to decide this if the earlier function returns true then i gets appended to this list of primes. So, it is a very straightforward thing to compute the list of primes from 1 up to any given number m. On the other hand, supposing we do not want the list of primes up to a given number m but we want the first m primes. Now, primes are infinite but there are gaps so a priori it is not very clear if I ask you for the first 2000 primes, it is not very clear what is the 2000th prime in value, how many numbers should I search for the first 2000 primes. So, here instead of using a for loop we will use a while loop and count the primes as we get them and when we reach the number m we will stop. So, we keep a counter to count how many primes we have seen, we keep incrementing i and when we have reached m primes in our counter we stop. So, one thing to note in this function is that we have used this feature in python called multiple assignment.'},\n",
              " {'id': 'cae9af95-1920-406b-9672-135eb3c51598',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': \"So, we want to keep track of a count, how many primes we have seen so far and that is initially 0. We want to now use a while loop for this because we do not know how many so in the earlier thing we ran i from 1 to a fixed number m plus 1 but here we do not know how many i's we have to look at, so we start with i equal to 1 and we explicitly increment it inside the while loop. So, this is the second initialization and the third one is we need to of course keep track of the prime list. So, these are three separate assignments count equal to 0, i is equal to 1 and pl is equal to the empty list, all collapsed into one single tuple assignment. So, the first element of the tuple count is assigned to the first element on the right 0, i is assigned to 1, pl is assigned empty. And now what is our while loop say it says as long as I have not yet found m primes I will check if the current number I am looking at is a prime and if it is a prime then I do two things, I increment the count so again this is a multiple assignment or a multiple update, i count becomes count plus 1 and pl now I use this plus for list which basically concatenates two lists, I already have some primes and I want to add 1 to, i to it earlier I had use the append function here I am using a plus function so this takes a list of primes already given to me, it takes a singleton list with the new prime I have just found and combines it into a longer list pl plus i. And whatever I have done whether I found a prime or not I must move on to the next i so I have to increment i so that after 1 I will look at 2, after 2 I will look at 3 and so on but i is not determining how when the loop ends, i will keep on incrementing it is only when count reaches the value that I want the threshold that I want that this loop will terminate. So, I cannot tell you in advance even knowing m it is difficult to predict what value i will reach, I will only know at the end what is the largest prime that I saw.\"},\n",
              " {'id': '640bc266-8255-4280-9015-377fcacab35b',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': '(Refer Slide Time: 6:21) So, this is used so this two examples are basically to illustrate the difference between these two types of loops that python has and almost any other programming language has. So, there is a for loop and a while loop. So, a for loop is something which is predictable, we know in advance how many times we are going to execute it. So, in the kind of for loop we have used so far, we are running from 1 to m but in general as you know in a python for loop you can run over a sequence or a list, so you can say for i in l. So, it runs over but l is a fixed list you should not be changing that list for example, so that is a very bad and dangerous practice, if you are using a for loop do not change either the index variable or what it is ranging over inside the loop otherwise your program will be very unpredictable. So, the whole purpose of a for loop is to be predictable I know I am going to run this loop a fixed number of times. On the other hand, when I do not know how many times I am going to execute the loop but the loop will terminate based on some condition that will eventually become true then I use a while loop, like here when I am trying to compute the first m primes but when I am writing a function with a while loop you must make sure that the while loop will actually terminate. Because if you do not ever make that condition true for example if this condition does not become false rather, if this condition remains true forever then the while loop will keep on running indefinitely. So, you will have a function or a program which does not terminate. So, you have to make sure that you are doing something inside your loop, in this case count equal to count plus 1 which is taking me towards this progress of making sure that the loop will eventually execute, will eventually exit.'},\n",
              " {'id': '9338b53e-1e89-4613-9657-bae809f51b56',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': '(Refer Slide Time: 8:04) So, here is a different strategy for checking primes, we do not actually have to compute the entire list of factors and then check that is 1 comma n. All I have to do is check whether there is any factor between 2 and n minus 1. So, this is again to illustrate a style that we have seen long ago in computational thinking what also you should have seen in other situations. We will assume that a number is a prime. So, we start off assuming in this name result that the number n we are considering is a prime. Now, we are looking for proper factors so we are not interested in the factor 1, we are not interested in the factor n, so we are going to run from 2 to n minus 1, so the range function goes from 2 to n because if I have the upper limit as n the actual upper limit that we are looking at is n minus 1. So, in this range from 2 to n if I actually find a prime then I mean if I actually find a factor rather then I know that this is not a prime so my initial assumption gets reversed and I change it to false. At the end of this loop either I did not find a factor in which case my assumption held and it was true or I found a factor and it became false and it is not a prime so whatever it is at the end I return the result. So, this is a more direct way of checking if a number is prime, you just cycle through all the factors and if you find a nontrivial factor between 2 and n minus 1, you switch your rating of this number from true to false. (Refer Slide Time: 9:34) Now, of course a nonprime number will have many factors. So, there is no point in looking once we have found the first factor that we are looking for, once we have found 1 factor we are done. So, we can actually terminate this thing as soon as we find a factor and that one way to do that is to use this function called break.'},\n",
              " {'id': '86e51662-2df3-49fa-a8d1-b98269ebfa7f',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'So, what break does is that it breaks out of the loop inside which it is so this break will terminate the for loop the moment I find some i between 2 and n minus 1 which sets this result to be false. So, I do not if I find 2 itself for example supposing the number is very large but it is an even number the moment I know that 2 divides that number I can stop I know it is not a prime. (Refer Slide Time: 10:22) So, of course breaking out of a loop is sometimes unavoidable, but very often it leads to confusion about what is actually happening in the code, so you should use breaks with caution, it is nicer usually to have this kind of a loop controlled by a while. So, what we can now say is that I will use this result itself to control the loop, so long as I have not contradicted my assumption that the number is a prime I will keep searching and the moment I find a prime that is not a prime I will exit. So, I assume that result is true and I start with 2. So, I am basically going to simulate this range 2 to the n by a while loop as we did earlier. But now what I do is I check whether I have two things now, I check whether I have violated this so if the result becomes false that is I have found a factor I will exit the loop or if I have exceeded the number of the range of factors I am going to try. So, if I have cross from n minus 1 to n then I no longer have to continue I can terminate with whatever result I have. So, I need both result to be true, I have still not discovered that it is not a prime and I have not run out of factors to try and if that is the case then I try out the current factor and if the current factor actually divides or the current candidate factor actually divides then I set the result to false so the next loop will terminate and then I increment i so that progress so eventually even if no factors are found i will eventually reach n and in both cases this loop will terminate.'},\n",
              " {'id': 'f93b1d7d-924a-4963-b0fe-684c65858898',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'So, remember when you write a while it is your job to make sure that the while actually terminates at some point. (Refer Slide Time: 12:05) So, finally one thing we can do with primes is to speed up this thing a little bit because factors as we all know occur in pairs so if something, if 2 is a factor then n by 2 will be a factor, so if 2 divides 16 then 16 by 2, 8 is also a factor so I do not need to separately check 8, every time I find a factor I find a matching factor. So, in particular the factors divide as those which are smaller than square root of n and those which are bigger. So, remember square root of n is a number, so that square root of n is itself its pair, so every number, every factor smaller than square root of n will have a factor bigger than square root of n. Of course in general square root of n will not be an integer unless n is a perfect square but it is enough to look from 1 to square root of n because all the factors beyond square root of n are paired with all the factors before square root of n. So, if n is a prime I just need to check for factors from 2 to square root of n rather than 2 to n minus 1. So, in order to use square root I need to import this math function so I import math and now the difference between the previous one and this is that instead of checking earlier I was checking i less than n, now I am stopping at a smaller number i less than square root of n everything else is the same. So this marginally improves it I mean in a sense it drastically improves because square root of n will be smaller than n so if say for example n is say 1 million then square root of n will be 1 thousand, so there will be a drastic improvement in that sense. But actually we would like something even more efficient than this but that is beyond the scope of this course. (Refer Slide Time: 13:51) So, as a final exercise using primes let us look at some properties of primes.'},\n",
              " {'id': '135068bb-694d-4722-958f-5900c08aab55',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'So, one of the things that is a very classical result about prime numbers is that there are infinitely many of them, there is no such thing as the largest prime, there are any number of proofs and if you do not know some proofs I encourage you to find out because there are some very simple proofs of this. Now, primes become larger and larger so one of the natural questions that people have asked is how are they distributed, how are the gaps between the primes. Obviously as you get larger and larger you would some, you can notice even if you look at small numbers like you know numbers between 1 and 1000 between say 1 and 10 you have prime numbers like 2, 3, 5, 7 so they are pretty close. But if you start going up the ladder then by the time you reach say the 40s you have 41, 43 then you have a big gap like 47 and so on. So, the gaps become larger between the primes as the numbers become bigger so how are the primes distributed. So, you would be inclined to think that as the numbers become larger the gaps will keep on increasing. So, a particular interesting difference is two. So, we say that two primes are twin primes if they are difference of two. And why will there be a difference of two? Typically, they are two odd numbers and they are usually of the form 2 to the k minus 1 or 2 to the k plus 1 this is a typical type of format of two prime numbers. So, it is not always the case but there are many interesting primes which have this kind of a distribution. For example, 15 and 17, no 15 and 17 are not prime so let us not worry about this but there are these twin primes and some of them have this format but twin primes do exist like 17 and 19 are twin primes because they are 2 apart, 5 and 7 are twin primes because they are 2 apart and so on 41 and 43 are also twin primes. (Refer Slide Time: 16:01) So, the twin prime conjecture is one of the questions which is yet not resolved in number theory is are there infinitely many twin primes.'},\n",
              " {'id': '19e21e42-53b0-48e7-bed0-521fb5d9a426',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'So, as we go into the larger and larger numbers and look at all the primes will we still find primes of the form p and p plus 2. So, we will of course not be able to solve the twin prime conjecture using python, but we can at least ask the following question which is if I give you a range of numbers like we saw earlier how to compute the primes up to a number m. So, instead of just computing the primes up to a number m supposing I ask you what are the differences that I observe between the primes between 1 and m. Now, you want to keep track of just the differences, we are not interested in the primes themselves, we are interested in the difference between each prime and the previous prime. So, this brings us to another very common data structure that is available in python. So, we have seen lists which have you have used so far to compute the list of prime numbers but now we do not want a list, we want to keep track of how many times each difference occurs. So, we want to keep track of a certain numbers and for each of them we want to keep a counter. So, this is what we call a dictionary. So, we have a dictionary which will take a key and associate with it a value, so the key will typically be in our case a difference and the value is going to be the frequency of that difference. So, if I see that the number of twin primes is actually infinite then the number of times I see a difference of 2 we will keep on growing, but if maybe for some other number like 17 maybe or 18 rather the number will be finite. But what we want to do is generally compute for a finite range between 1 and m say what are the differences that I get and how many times do I see each of these differences. (Refer Slide Time: 17:48) So, the first difference will be between the second number and the because if I start with 2 as a smallest prime it has no difference with the previous prime because there is no previous prime. So, for this prime difference function it makes sense to start from 3.'},\n",
              " {'id': '7dcdaeea-4e63-48bf-b680-4ab9d97af53c',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 1},\n",
              "  'source': 'Python recap - II.pdf',\n",
              "  'content': 'So, the difference between 3 and 2 is the first difference that I observe and then the difference between 5 and 3 will be the next difference and so on. So, I will keep track of the last prime number that I have seen and I will initialize this to 2. So, I assume that I have seen 2 as the smallest prime because 2 is always the smallest prime. And now I have a dictionary which is going to keep track of the frequency of each difference. So, the keys of this dictionary are going to be the differences, it is a difference, it is the number which represents a difference between 1 prime and the next prime and the value is going to be how many times I see 2 primes with that difference. So, I start with 3 and I go until n so as usual range goes to n plus 1. For each prime number that I see I compute this difference what is the difference between the prime number I just found and the last prime number which I had seen before and then I reset this number to be the last prime because next time this is going to be the last prime. Now, having done this now I have to update my dictionary. So, I check whether I have seen this difference before. So, does this difference appear in the keys of this dictionary or not, if this appears in the keys of dictionary I have already seen this difference before so I just increment the current value of that difference by 1. If I have not seen this difference before then I create a new entry for this dictionary. See these are all very familiar patterns which you have seen before right from computational thinking onwards but it is important to just recognize how to use these things, so we are just reviewing this. So, this is a kind of an introduction to various things, so we have seen for loops, while loops and we have seen dictionaries in the context of computing prime numbers.'},\n",
              " {'id': '6d64a282-46e9-4bda-9b67-a0cbd9f9410f',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 1).pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Python Codes – Part 01 So, let us take a look at how this Jupyter notebook works in Colab. (Refer Slide Time: 0:14) So, if you go to this Colab url, colab.research.google.com and you have logged in, then it might show you what you have seen, done before or it will give you an empty menu and ask you whether you want to import something say you can take something from your Google Drive or you can upload something from your computer and so on. So, here we have already, I have already done something, so let me just open something that we have, I have already used. So, this is the code which I was, had used as the backdrop of the slides of the first lecture. So, notice now that we have cells. So, we have a cell here, we have a cell here, we have a cell here and so on. So, this is that one dimensional kind of spreadsheet structure that I talked about and if you look at the left of each cell there is a kind of bracket there, so this indicates whether cell has been run or not. So, at the moment none of these cells have been executed. The other thing is that we have this text over here and we have code over here, so these are these two different types of cell and Colab allows you to add new cells. So, if I say plus code on the top it adds a cell in which I can type code and if I say plus text it will add a cell in which I can do documentation. So, now if I go to a text cell for example, and I edit it right then you see this Markdown syntax. So, this Markdown syntax is basically saying that this double hash is giving me this kind of a heading. If I use a single hash, you will see that this thing will actually become bigger, so you notice that it actually displaying, Colab displays this, Jupyter notebook will not display, you have to execute the cell, but if I come out of the cell now it has become bigger.'},\n",
              " {'id': 'd7abb10b-7268-4a4b-a6ab-5000ab53a4f6',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 1).pdf',\n",
              "  'content': 'The other things I can do here for instance is I can kind of create, so I said that you can use text like formatting. So, you can say, so you can use formatting like this, this hyphen and this will automatically come out as you can see on the right as some bulleted items in the list, so of course, now in this Colab environment this is not true in the normal Jupyter notebook that you would run on your laptop, it actually allows you to do this using this interface where you can say that I want to kind of add, say a bulleted item and so on. So, you can actually do this directly, but normally you have to use this Markdown. So, it is still doing it in Markdown, as you can see it is using star, so minus gives you a bulleted item, star also so, Markdown is flexible, so you get this kind of text and it gives you documentation. (Refer Slide Time: 2:56) Now, when you come down to the code for instance, now at the moment none of these have been run. So, here I have, so this code let me explain what is doing. So, this first function factors is creating the list of factors of a number n, so it is running from 1 to n and it is checking all the numbers, which divide n and putting them into a list of factors. So, that is factors of n. And then there are two definitions of prime, the first definition of prime checks whether the factors of n is precisely 1 comma n, the second definition of prime checks whether the length of this factors list is exactly 2, both are the same, because if there are exactly two factors there must be one comma n, but these are just two different to show you that you can have the same function defined in two different ways coexisting inside the notebook. At the moment neither of these is active because I have not run anything. And finally here is a function a piece of code, it is not a function actually. It is a piece of code which actually computes all the primes from 1 to 100.'},\n",
              " {'id': '2d3c0375-2b19-412e-9ebb-8943e9ea15e1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 1).pdf',\n",
              "  'content': 'So, it takes i in the range 100, and if i is a prime then it appends it to the prime list and finally it prints the prime list. Now, see that there is some output at the bottom. So, this code actually was run before. And this is what I meant earlier when I said you can export the notebook with the output, so even though I have not run the code even once, the output is there so that you can show it to somebody, saying this is what happened when I ran the code the last time right, so the output is preserved when you save this file or in Colab it is automatically saved like a Google document. So, when you reload it or you send it to somebody they will be able to open it and see the same output that you saw when you ran it. (Refer Slide Time: 4:33) So, now let us see what happens if I try to run this code. So, if I kind of hover over these things I get this run symbol, so if I run this code now it is going to complain because I have not run the code before it, so in the current environment the functions prime and factors which are used here are not there, so in particular I get the standard Python error which says name error, the name prime is not defined. (Refer Slide Time: 5:00) So, that means IS must go and define one of these two prime, so let me say take the second one. Now if I try to do that then it goes through, but now if I try to run this thing, now prime is defined but I am going to get a different error saying that inside prime the name factors is not defined. So, these are all things which are only available at runtime. So, when I compile a function in some sense, when I run a function definition it does not check that those values at the function definition uses are defined because it will not know that, few factors could have been some kind of a global variable as far as the function is concerned, so it has no idea what to do with that factors.'},\n",
              " {'id': '2a89592d-af1b-4962-92e2-6470a0fa2f88',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 1).pdf',\n",
              "  'content': '(Refer Slide Time: 5:37) So, now that I know that it still does not run I go back and I run factors right and now that I have run factors this prime list will now run, now I can run prime list and this time it will execute and produce the same output that I saw before. So, two things to understand, one is that this output was there before, but these functions were not run and now if you look carefully, it is not easy to see, but there are numbers inside these boxes. So, it is, actually I executed these cells in a particular order, so there is some one, two, and this is called four, because I ran this a third time in between. (Refer Slide Time: 6:16) Now, I can now for instance take this function, this definition of prime and run it. What this does is it takes the previous definition of prime and replaces it by this definition. So, the most recently defined version of prime is available in manual, so if I run this code again, of course, in this case there is no big difference because it is going to be the same thing, is going to produce the same output. But supposing I make a mistake, so supposing I, instead of equal to equal to I say not equal to, so supposing I say a number is prime if the list of factors is not one comma n, in other words I have exactly negated the thing, so I have got factors other than one comma n. Now, supposing I call this my definition of primes and now I run this then what I will get is? I will get a list of non-primes, I get 2 and 3 are skipped, I get 1, which is not a prime, 2 and 3 are skipped, which are primes, I get 4, I skip 5, I get 6 and so on. Now, if I go back and I take the correct definition of prime which I have not changed and I run this again, then I get the primes again.'},\n",
              " {'id': '5967944c-d8d9-41a7-86f9-55d72f67d83e',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 1).pdf',\n",
              "  'content': 'So, this is the nice thing about this notebook that you can dynamically… So, in this case I made a mistake, but you can have two different definitions of a function, you can kind of interplay between them and you can come back and run your code, without having to go back and do a lot of copying and pasting and editing and all that, so some subset of these cells are available and the current state of the cells is what is basically according to the dynamics of how you ran them. (Refer Slide Time: 7:45) So, let us look at this other example. So, here is that sine curve which I had shown in the slide. So, the sine curve, again the previous plot is available, it is not that it is run, so now if change, for example, this function sin to cosine, so if you know your trigonometry, you will know that cosine is basically going to be the mirror image of sin. So, this particular thing goes up and then down, so cosine will go down and then up. So, if I run this code now, it is going to recompute that plot and now it is going to start from above and go down, so I get a kind of flipped version of the sin thing because sin and cosine are flipped. So, this is a kind of dynamic environment in which I can, now if I save, so this file now will have, at this juncture will have whatever changes I made. So, if I share this with you now, you will see whatever updates I made now and in general I can change it yet again and present you a version which is instructive in terms of text and in terms of outputs and then you can take it and further run it, so that is the advantage of using this Jupyter notebook.'},\n",
              " {'id': '10981b90-ef76-4a1c-8d23-ce30ba313d9f',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 3).pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Python Codes (Part 3) We had created in the lecture a class which would represent the time that a piece of code takes, so a timer class. (Refer Slide Time: 0:18) So, here is a more elaborate version of the class. So, let us go through the code just to understand what this elaborate version is doing. So, first of all if you remember code can throw exceptions. So, what we are going to do is make this kind of this timer sort of little bit proof again except, misuse by saying that you must stop it only after you start it and so on. So, we will have a new error that we are creating and this is just a way of creating a new error. So, we say that timer error is a new class which is of type exception, it inherits from exception. So, just, so these quotes inside these double, triple quotes, these are what are called documentation string. So, they are the things that you get when you hover over a function. So, let me see if we can see that. Let me just execute this and then if I go and for instance if I sometimes if I hover over something, I might see it. So, it is loading something, but anyway we will come back to that, but anyway. So, the main thing is now we have this timer error exception. So, when we start the timer instead of setting it to 0, we will put the start time and elapsed time to none to say that it is not set. So, now when you start the timer, it must be a timer which is not running. So, that is the difference now. We can check that this timer is not running by saying is the start none or not, if the start time is not none, it means the time is already running, so we can raise an error saying timer is running, please use stop. So, do not restart the timer because it is already running, this is a mistake. Otherwise, if there is no error, if you do not raise. This word ‘raise’ is basically to create an exception.'},\n",
              " {'id': '97c460cd-d9b2-48d5-a715-964debae6972',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 3).pdf',\n",
              "  'content': 'So, if you do not raise an exception, you do what we had seen in the slide which is you set the start time to the current value of performance counter. Remember the current value performance counter has no independent meaning; it is just used as a reference to subtract later. (Refer Slide Time: 2:36) Similarly, when you stop the timer you check whether the timer is running. So, if the timer is start time is none, it means the timer is not running and you give an error saying timer is not running, please use start. Otherwise, you create, you set the elapsed time to the current value of the performance counter minus the start time and you reset the start time to none. So, now the timer can be run again because you have stopped it, but you have saved the time elapsed time. So, you can look up the elapses time, but the timer is not running, and it is ready to run again. And finally, elapsed as we said, will check that actually that the elapses time is something meaningful, it is not the first time when you never set it. So, if the elapses time is not none, it will give you the elapses time, otherwise it will throw an error. And finally, we have if you just want to dump the value of the timer, then you get the elapsed time. So, that is the string function. (Refer Slide Time: 3:25) So, now this is the piece of code that I wanted to show you. So, this is just a trivial piece of code. It is just a loop, which is running this n equal to n plus i for i ranging from 1 to some large numbers. So, what is that large number, I am doing it for 10 to the power j, where j ranges from 4 to 9. So, I am doing it for 10 to the power 4, which is 1 with four zeros. So, it is 10,000 100,000 that is 10 to the power 5, 1 lakh, 10 to the power 6, and 10 to the power 7, and then 10 to the power 8 because 4 to 9 will be 4 5 6 7 8. So, it is running it for each of these values.'},\n",
              " {'id': '70618ce3-68b5-44f8-8913-2a681655a18e',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 3).pdf',\n",
              "  'content': 'And what are we doing before we start the loop, we are running, starting the timer and when we finish the loop, we are stopping the timer. So, this is just a simple loop to tell us h ow long Python takes to execute something as trivial as n equal to n plus i that many times. So, if you run this, and you can see that the first time, the time when it takes 10,000 it runs in 0.01 it is probably difficult to see on the screen but I will share this code with you and you can run it, you can trust me to say when j equal to four the output of the timer is 0.001 that is one thousandth of a second. When this is 10 to the power 5, it is 0.01 is one hundredth of a second, when it is 10 to the power 6 it is 0.13, it is about 0.1 seconds, when it is n to the power 7 and this is what I promised you in the lecture that 10 to the power 7 operations per second it takes 1.3 seconds and 10 to the power 8 it takes 13 seconds so you can see that there is an clear 10 factors slow down when I increase the number of operations by a factor of 10. So, when I go from 1000 to 10,000 from 10,000 to 1 lakh, from one lakh to 10 lakhs, 10 lakhs to 1 crore each time, this loop takes 10 times as much. And it is useful to calibrate now our expectations when we run code, we will see later on that when we are trying to, for instance, if you are trying to sort a list or you are trying to do some other complicated operation on a large structure, then you might actually end up taking some 10 to the power some number of steps as we will see. And it will be useful to estimate how long it is going to take without actually running the code, we have a kind of a clear idea that if it is going to be, if I have to traverse a list, for example, whose size is 10 to the power 7 is going to take me 1 second just to go from this end to that end because power 8 is going to be 10 seconds just to go from this end to that end without even doing anything. So, we will come back to this later on.'},\n",
              " {'id': '58d0412d-b08b-4b8e-9709-1c6819af9e00',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Implementation of Python codes (Part 3).pdf',\n",
              "  'content': 'But it is useful to have this kind of timer to give us an idea about how long things actually take when they run because we can do a theoretical calculation but there is nothing like seeing it in kind of black and white to know exactly how long your code is taking.'},\n",
              " {'id': 'a634c9ac-fbff-410b-986b-3758029ba346',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Python recap - I.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Python Recap - 1 Let us begin with a quick recap of Python. So, I am not going to go into great detail but through some examples we will just revisit the basic syntax and just understand what we need to remember. (Refer Slide Time: 0:21) So, my first running example is going to be the greatest common desire is a problem gcd. So, remember that the gcd which is also sometimes called this hcf is the largest common factor or the greatest common divisor of m and n. So, you want to take all the numbers which divide m, all the numbers which divide n and find the largest k that divides both of them. So, for instance if you take 8 and 12, then the gcd is 4, because 4 divides 8, 4 divides 12, 4 times 3 and 4 times 2, so there is no larger factor which divides both 8 and 12. On the other hand if you see 18 and 25, both of them have factors, so 18 is actually 9 times 2, so it is 3, 3 and 2 are the factors of 18 and 5 times 5, but there are no common factors other than 1. So, the gcd of 18 and 25, the largest number that divides both 18 and 25 is 1. So, 1 divide everything as we know and that means that 1 is always available as a factor of both m and n. So, this is good because gcd of m and n is always defined, we do not have to worry about whether gcd exists or not. In the worst case if they have no common factors, like in the earlier case of 18 and 25, they have no common factors, then gcd will be 1. So, our question now is how to compute this gcd. So, first of all any factor of m is going to be smaller than n, the factors of m run from 1 to n, similarly any factor of n is going to run from 1 to n and if something has to be a factor of both m and n, it has to be smaller than the minimum, so the gcd is always going to be smaller than the minimum of the two numbers because it has to divide both numbers.'},\n",
              " {'id': 'd5879022-36ec-42b4-8c64-ba991240a003',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Python recap - I.pdf',\n",
              "  'content': 'So, if a number is bigger than m, it cannot divide m, if it is bigger than n, it cannot divide n. So, the most obvious way is to run through all the numbers from 1 to the minimum of m and n and check if a number divides both m and n, so look for all the common factors, collect these in a list and return the last element in the list. So, here is the code for that, which we will look at in a minute in more detail. So, you start by setting this list of common factors to be empty and now you run i from 1 to this minimum plus 1 and for each i you check whether it divides m, so when it divides m it means that this remainder operator percent, the remainder of m divided by i is 0, that is there is no remainder. So, it divides m. It also divides n and if this is the case, then you add it to this list of common factors and finally at the end you are interested in the biggest one of these. So, these are being added in sequence from 1 and increasing order, so the largest 1 will be the last one and that will be at index minus 1. So, let us just look at this code a little more carefully to understand all the nuances in that. (Refer Slide Time: 3:26) The first thing is that we need to do this initialization here, we cannot just start off with cf inside the body of the loop because we are applying this append function, so the append function applies to any name which is holding a list, but in Python as you know, there is no way to announce to the Python interpreter that cf is a list without actually setting a value. So, the only way that Python can associate types with names or variables is to look at the value that that variable holds. So, a value a variable which has not been initialized has no type and if it has no type, then we do not know what operations are legal. For example, is append legal or not, is plus legal or not? So, that is why we have to first announce to Python that we have an empty list. so that when we start adding things to the list using append it is legal.'},\n",
              " {'id': 'a22d3e42-2a07-452c-866c-aef3fb8accc3',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Python recap - I.pdf',\n",
              "  'content': 'So, this is the first thing to note about this code. Then of course, we are using two basic elements of Python control flow, so the assignment statement is this one which assigns an expression or a value in general to a name, but we need to run through a sequence of statements in some order or may be repeated. So, we have conditional statement, so this statement is executed provided the condition that I divides both m and n is true and overall this statement is repeated a certain number of times and that is what for if, so we have loops and we have conditionals. The other thing is that python has this general principle that whenever you take a sequence of numbers… So, in particular the range function is something that generates a sequence of numbers from a lower limit to an upper limit, in this case separated by plus 1, you can of course modify it to get a range in which you skip elements, but the important thing to remember is it always stops before the last element and that is why we have to, if we want to look for factors from one to the minimum of m and n, the range function has to have this extra plus 1. Otherwise, the last factor will not be considered. So, for instance supposing we were looking at 8 and 12, then the minimum of 8 and 12 will be 8, and we need to check whether 8 is a factor or not otherwise we do not get the correct gcd. And if we stop at the minimum in the range function, then it will only go up to 7, so this is why we have to put that plus 1. So, these are all minor points which I am sure that you are familiar with, but it is worth repeating, so that you do not make silly mistakes when you are writing your code. The other thing is that lists are indexed starting from 0 as usual, but Python has this interesting option of counting backwards, so you go forwards from 0 to n minus 1, let me say m minus 1. So, supposing the list is m, length m, then the indices run forwards from 0 to m minus 1, but they also run backwards from minus 1 to minus m.'},\n",
              " {'id': 'f74a2345-ff90-4cdb-9e83-67de579eda9f',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Python recap - I.pdf',\n",
              "  'content': 'So, this way it is much more convenient to talk about the last element of the list as having index minus 1, which is what we are doing here rather than saying it is the length of cf, so this is the same we could have said length of cf minus 1. This would have given us the same position in the list, but obviously writing minus one alone is much less cumbersome than writing length of cf minus 1. So, these are some interesting things which we have captured in this very simple function, which captures gcd in a very naive way. (Refer Slide Time: 6:54) So, one of the things we can realize is that we really do not need all the common factors, we are only looking for the largest common factor, in particular we are only looking for the last element of this list that we are constructing and if you are only looking for the last element of the list that we are constructing there is no point in remembering all the earlier items in the list. So, we can actually eliminate the list right and just keep track incrementally of the most recent common factor. Now, remember we are computing common factors in ascending order, we are starting with 1, 2, 3, 4, so every common factor we find will be the latest candidate we have for the greatest common divisor, because once we have found it all the earlier ones are now superseded, so here we have a different version of the same code where we eliminate the list, so we have the same for, and we have the same if. But now whenever we find a common factor, we just update this name mrcf - most recent common factor to be 1. So, we have not initialized this. Is this a problem? Will we reach this return statement and return a value which has never been defined? Recall that 1 is always a divisor, so 1 will be always a common factor, so even though we do not initialize mrcf here, like we did for the list. When we first encounter 1 we are going to get a value here.'},\n",
              " {'id': '544ce8ba-4877-4445-8120-61f16a08ae1a',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Python recap - I.pdf',\n",
              "  'content': 'Now, the difference between this loop and the earlier loop is that when we first encountered a number in the earlier loop, we had to append it to the list, so we had to apply some operation to the value we were storing, namely the list, so we needed to know that it was a list, that is why we have to initialize it. Here we are not appending, we are merely assigning, so the very first time we find a common factor we are assigning it to the name, so name gets a value and the next time we will update that value we are not going to operate, we do not have to put a plus or a minus or a times or something, so Python does not really care what the type of that name is as long as the name is updated and because one is always a common divisor we will definitely return a valid value. So, how long do these two functions take? This is going to be one of the items that we are going to look at in detail in this course, namely how efficient are the algorithms or the pieces of code that we write. So, as you can see it has a for loop which runs from 1 to the minimum of m and n, so this is going to take time proportional to minimum of m and n because that is how many times this loop is going to execute. Whether we do it with a list or whether we do it in this mrcf version or whether we do it with a list. So, although we have changed the data structure in one case we are keeping track of only the last common factor we found and in one case we are keeping track of all the common factors we found; fundamentally the time taken does not change both are proportional to the minimum of m and n. So, we will see later that we can do much better than this, but for now let us move on to another example.'},\n",
              " {'id': 'b81b5951-ddd4-4661-80f9-11d68e84d517',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Jupyter Notebooks So, welcome to the course Programming Data Structures and Algorithms using Python. So, to begin with we will look at some of the things that you are already familiar with just to get a refresher on Python and one of the things I will start with is the environment in which we are going to run Python code. So, I am going to be talking to you today about Jupyter notebooks. (Refer Slide Time: 0:32) So, normally when we want to write and run code there are many options available to us, perhaps the simplest option is what you see here, so you have a text editor, so you type your code and then you open a kind of a console or a terminal and then you load the code that you have typed in in your text editor and then you run it. Now, this is of course, easy to do but it is a little bit tedious because if you decide to make a change over here then you have to go back, reload it, so going around in this kind of cycle of edit and run is a little bit cumbersome if you are using a separate text editor and using directly the console. (Refer Slide Time: 1:16) So, this gives rise to a more convenient interface, which you are familiar with REPLIT called an Integrated Development Environment or IDE. So, in an IDE you have on one side something where you can edit your code and side by side you have this command window where you can run the code and see the output. And because you can now see the output and edit your code in the same interface it is much easier to cycle back and forth and see how your code can be changed and how the output changes. So, you have this quick cycle of updating your code and running it, and of course, an IDE has other features also, so it offers you typically a debugger. A debugger will allow you to interrupt your code and inspect the values of various variables to see what is going on.'},\n",
              " {'id': '48a3f65e-b162-460b-a3ce-1cdbc652fa7c',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': 'You might also be able to prepare some test cases in advance and run them to see how your code performs and so on. So, this is the default that most people use for developing serious amounts of code, and of course different IDEs are designed for different programming languages. Some like REPLIT support multiple languages; other IDEs you might find are dedicated to certain languages like Python or Java or C plus plus and so on. So, we already have an IDE but I am going to present to you yet another way of writing and running code and the question is why would one want one more when you already have an IDE. (Refer Slide Time: 2:48) So the main point that I would like to emphasize is that writing code whether for yourself or for teaching, like I am teaching now, also involves collaboration, very rarely do you write code only for yourself, you usually work in a team. So, as a team you might be together developing the code or this might be a larger team. For instance, it could be some kind of a research environment and this is very common, say in machine learning, you are trying to solve a problem, coding is part of the solution but is not the solution in itself. So, when you run the code not only it is possible that more than one person may be running the code or developing the code to run. But there are maybe people who are not running the code, who are not even aware of how the code works but who might want to know what the code does. So, then you want to be able to share your results. Now, not notice that when you have a typical IDE, when you run the code you can see the results, but what happens after you run the code. You cannot take the code, you cannot take the IDE open to another office and show somebody what you have done. You would like to have some way of preserving the output for later, without having to go through a sort of cumbersome process of saving it to a file or preparing a report or something like that.'},\n",
              " {'id': 'c4f997b8-dd9f-4e0b-a1b0-1b9aa2eef97c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': 'So, here on the right you see a typical view of what a Jupyter notebook, which is an interface that I am going to introduce today looks like. So, you have on one side the ability to write text, so in this case is just a simple title, but you can also, as we will see, introduce more annotations which describe what you have done or what you are planning to do and then you have in this block, you have the actual code exactly as you would have typed it into a standard editor or into an IDE and now at the bottom you have outside this grey area, you have the output of the code what you would normally see in a separate window on the side in an IDE. So, the documentation is interleaved with the code, so you have the documentation here, you can have more documentation, so in between your code fragments or different parts of your code, you can introduce documentation without looking at it in the comment section of a Python program. So, in python of course, you can include comments by writing this hash and then writing something after that. But this does not look very neat and it is also very difficult to format. The idea of this notebook is that you can actually format your text and interleave it with the code so that it looks readable. The other thing that you can do in this environment which is difficult to do when you are working in an IDE is to replace one piece of code by another piece of code without abandoning the previous one. You do not want to throw it away, you want to say what if I replace this by that, so I might have two different definitions of the same function and I may want to try out both of them without having to go through a tedious process of loading a different file or deleting one and replacing it by the other, so I want both but only one to be active and we will see that in a notebook this is possible because it really depends on which was the last copy of that function that you executed.'},\n",
              " {'id': 'e0135f29-4162-42c3-b109-921c11b9a296',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': '(Refer Slide Time: 6:04) And finally, as I said you would like to keep the outputs available for somebody else to see. Now, this could be for two reasons, one is you are just reporting it, you want to prepare a kind of documented output after you have run your code so that you can evaluate whether it did the job that it was supposed to do or you want to run it again. And the other thing is also that if somebody else wants to run your code they should be able to see what you had got with your output, so that when they run it they can see whether they get the same output or they get some other output. So, preserving the output is part of saving this project as it were for future use. (Refer Slide Time: 6:41) So, the concrete interface that we are going to look at is something called a Jupyter notebook. So, you may be familiar with a spreadsheet. So, what is a spreadsheet? A spreadsheet is basically a large, in some sense an indefinitely large square matrix of cells and in each of these cells you can put a value, you can put some text, you can put a formula. So, it is unstructured in the sense that it does not tell you specifically what each cell should contain, but it is structured in the sense that there is a position, each cell has a kind of location you have a column number and a row number and then you can write formula saying this cell should be the sum of two other cells or it should be the sum of a column of cells or a row of cells and so on. So, Jupyter notebook is like a spreadsheet except you have only this one column, so you do not have multiple columns but you can see that you have one cell, then another cell, then another cell, then another cell, so you have a sequence of cells from top to bottom, so that is what a spreadsheet looks like. Now, in a spreadsheet you can do whatever you want with a cell.'},\n",
              " {'id': '5c3fde11-6d5d-476e-ab0f-235f38f22916',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': 'You can put many things, you can put text, you can put numbers, you could put formulas, you can put even charts, you can put diagrams and so on, so in a Jupyter notebook each cell holds either a piece of code or it holds some text which you want to insert to explain something about the code to somebody else. So, it is either code or text. And as I said before the text is not just a comment as you would put in a Python program, it is something that has formatting. So, how do you specify the formatting? Well, the Jupyter notebook supports a format called Markdown. So, this is a very simple type of format, which is like the type of formatting you would do if you were just composing some text in a normal text mode without a word processor. So, you would basically have a hyphen to indicate a bulleted item and all that and the thing with Markdown is that it will convert it into nice formatted output. So, there are various resources on the internet, so here is one, which will tell you how to actually use Markdown to generate formatted output, so I will leave you to look that up, it is not very important for this particular course, except that it can be done. So, the main thing about a Jupyter notebook is that like in a spreadsheet, we can dynamically change the spreadsheets contents by either updating a cell with a new value or when we update some values we can rerun some formulas either explicitly or implicitly to recompute some other values. So, in the same way in a Jupyter notebook you can add, code or update code and then rerun that code. So, you can change definitions of functions, you can rerun an output and see how it changes with each edit.'},\n",
              " {'id': 'c521ba47-d396-4841-b3fe-5a8830f1f330',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': '(Refer Slide Time: 9:30) So, you might wonder about the name Jupyter, so the notebook as such, this is a spreadsheet like format, this notebook as such is not specific to Python, so it is a generic interface to any programming environment, which allows you to kind of interpret your code where you can write code and then immediately get it to be recognized and run. So, that is what happens in Python, we change the code and then automatically the python interpreter is able to read the updated code and run it. So, Jupyter was designed to work with three different languages, Julia, Python and R. So, R you may have come across in some statistical context, so Julia is also a kind of scripting language for these kinds of calculations. So, if you take the first few letters of Julia, Python and R you get Jupyter and that is the origin of the name. We of course, will not be using it for anything other than Python, but it is important to note that Jupyter as a framework is not limited to only Python, you can use different what they call kernels, you can have different programming languages supported behind the notebook as to the code that you run. So, one interesting, one reason we are focusing on the Jupyter notebook format is not that it is necessary in some sense to understand data structures and algorithms in this course, but it is an extremely popular format in machine learning, which is the broader scope of this whole program. So, if you look at code which is available for many machine learning projects, it is typically saved and disseminated using the Jupyter notebook format. In particular if about the site called Kaggle, so Kaggle is a competition for ML – Machine Learning, where they post problems, so some of these are synthetic problems, some of them are real problems, some of them even have price money because somebody wants a problem solved and they are looking, it is like crowd sourcing, they want a number of people to attempt it.'},\n",
              " {'id': '11cda686-d404-42c5-9d27-c28e18d868f8',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': \"And after the competition closes many people will post their solutions and these solutions are typically in this notebook format, so you can then access the solution which will have both the code and the documentation and then you can examine it, rerun it, tinker with it and so on. So, in particular this means that you get the opportunity to take somebody else's code for a given problem and see how it works by modifying it and trying to enhance it. So, the Jupyter notebook has become so successful, especially with the growth of interest in Machine Learning that it has won a number of awards, so the Jupyter project which is behind the Jupyter notebook, one in particular this ACM Software Systems award in 2017, which is given for Significant Software Systems, which are used by the community. So, it is a fairly powerful tool. (Refer Slide Time: 12:18) So, of course you can run Jupyter notebook on an individual system, whatever system you have, whether it is Windows or Mac or Linux by installing it and like any other Python system you also have to import and install the relevant libraries that you need. We on the other hand will be using a publicly available form of the Jupyter notebook which is put up by Google in what is called Co-lab, which is short for the Co laboratory. So, colab.research.google.com leads you to an interface where you can create these notebooks and save them and it is most importantly free to use. So, this has a slightly different look and feel from the Jupyter notebook that you would install on your own system, but essentially it is the same broad structure, which has been customized by Google for their internal use and then released for public use. So, it is a customized Jupyter notebook. And one of the things which will be useful for you, not necessarily in this course but outside this course and the other courses that you are doing is that Colab has all the standard packages for machine learning pre-loaded.\"},\n",
              " {'id': 'a22c9744-1048-4c40-8426-66502539ac73',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 1},\n",
              "  'source': 'Introduction to Jupyter notebooks and Google Colab.pdf',\n",
              "  'content': \"So, in particular there is a very popular pack library called scikit-learn, which has a lot of the standard machine learning models already implemented in it which you can call and use. And there is Google's own library called TensorFlow which is used for deep learning or deep neural networks, plus because it is running on the cloud you also have access to hardware which is beyond the limitations of your personal computer. In particular Google makes it possible for you to run some of this machine learning code on what is called a GPU. A GPU is a Graphical Processing Unit and it is very useful to run the kind of large scale matrix calculations which run behind the scenes in machine learning. So, many calculations which would take an enormous amount of time for you to run on the laptop will run in a much more reasonable amount of time on Colab using a GPU. (Refer Slide Time: 14:19) So, to summarize we will use Jupyter notebooks because it is a convenient interface to develop python code. In particular this ability to edit save and share the code is useful for me as an instructor also to be able to distribute the code that we discuss in this class, to you after the class. So, you can incrementally update and run your code. You can write documentation in between your code using this Markdown syntax. And most importantly as I said you can preserve the state of the notebook, in terms of what you have run, what outputs you have generated and export it. And this is extremely useful for collaboration and for sharing, whether you are sharing it with a colleague or you are sharing it for teaching purposes like we are doing here. And the particular version of the Jupyter notebook that we are going to be using is Google's Colab which is free to use and it is configured for Machine Learning.\"},\n",
              " {'id': '37cedb66-f6e5-4c9c-9c87-5cde6a297d2e',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor. Madhavan Mukund Selection Sort So, binary search is a success story for us because it works in log in time. But to apply binary search, we first have to have the list sorted. So, now we will look at this problem of sorting a list. (Refer Slide Time: 0:19) Sorting a list not only allows us to do binary search, it allows us to do a number of things better. For instance, sometimes we want to find the median value, we want to find that value such that half the values in the list are bigger and half are smaller. So, if I have managed to sort the list, the median will automatically be the middle value. So, by just probing the middle value in the list, I know that this is the median. Another possible problem is to find out whether this list has got unique values or whether there are duplicates, like are there two SIM cards, which have been registered with the same Aadhaar number. Now if I can sort the thing by Aadhaar number, then if I find duplicates, they must be next to each other because all the values are sorted, two identical values will appear adjacent in the sorted list. So, if I go through the sorted list, and if I find, if I do not find any value, which is the same as the previous of the next value, I know that all the values are unique. If I, if there are duplicates, I will find them. And more general than this duplicate problem is if there are multiple values and many things like for example, these are marks in an exam or something, if I sought my marks, and I want to know how many students got 44, and only students got 72, and how many silver 99. In this sorted list, all the blocks will come together all the 40 fours will come as a block all the 99 will come as a block and so on so I can build a frequency table. So, sorting is a general first step, which makes many subsequent steps easier to solve.'},\n",
              " {'id': 'bc74a44c-a5c7-4a8b-9614-95517812932b',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So, it is a very important and very fundamental question in computing, how to sort a list efficiently and effectively. So, that is the question we are going to address in the remaining lectures in this week. How do we sort a list and what is the complexity of sorting each of these different ways of sorting. So, we are going to start with two very intuitive ways of sorting, which you would typically do when you are asked to do something by hand. So, let us look at a very practical problem, which you may be asked to do by hand, supposing are the teaching assistant for a course, and the instructor has graded the exams. So the instructor is graded the exams. But of course, the exams are graded in the order in which they got, came to the instructed could be in roll number order; it could be the order in which the person physically handed in the exam paper as they left the hall. So, there is no particular logic to or correlation between the order in which the exams are and the marks that the students have. So, now what the instructor would like to do is assign grades. So, you need to sort the papers. So, you need to get the papers in ascending or descending order. So, let us say you want the highest from the top, so let us say descending order. So, now, as the TA it is your job to do this. So, you are given this huge stack of exam papers in areas to sort it in descending order with the highest marks on the top. So, how would you go about it? So, here is one natural strategy, which we do use quite often, when we are looking at quantities like this and trying to establish this, we will scan the entire list. And then as we are going along, we all know how to keep track of the maximum and the minimum in a list, you keep the first value as your hypothetical minimum or maximum every time you see a smaller or larger number, you will update the minimum or the maximum as the case may be. So, by doing one scan of the list, you can find the maximum value.'},\n",
              " {'id': 'c1efd422-a868-46f1-9e27-480143d69680',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'Let us assume for simplicity that all these values are distinct, there is only one mark, one paper with each mark, it does not really matter, because if you find it again, you will find it again. So, it is not a problem. But let us assume that there is only one, one paper for each value of mark. So, you find the maximum and then what you do, you take it and you move it aside, or you find the minimum and you move it aside depending on which so your problem here is you are trying to find the final list in order. So, it is better to put the first the minimum at the bottom. So, you have this whole pile of papers. So, you find the minimum one, and then you move it from here to the bottom. (Refer Slide Time: 4:17) Now you do the same thing again. So, you take that pile. And then you again, find the minimum and put it on top of the first paper, you want to find the minimum put on top the second paper. So, in this way in the second list is growing from minimum to maximum, because you are finding the minimum at each time and then appending it to this list. So, this is a simple strategy where eventually the second pile is going to be in order from bottom to top from minimum to maximum, which is what your instructor asked you to do. (Refer Slide Time: 4:43) So, let us say that this is the hypothetically, this is our top and this is our bottom. So, it is easier to do left to right. So, this is my initial set of marks, six marks sorted like this. So, the first, the lowest mark is sorry. I think they should take it. Yes, bottom to top, the lowest mark is 74 and the highest mark is 6, I say the top most paper is 64 mark. So, first I scan through this whole thing and I find the minimum. And the minimum in this case is 21. So, I take this 21 out, and I move it to a new list. Now I have five papers remaining. So, again, I look through the whole thing, and I find the second minimum, the minimum amount, what remains and that is this 32. So, 32 is my next candidate.'},\n",
              " {'id': '0c624b78-7788-432a-8500-56c56b886597',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So, I will take this 32 and move it aside, then I will find this 55 and move it aside and so on. So, 55 then 64 then 74 and then 89. So, this is the algorithm in work. So now at this, again, this is my top, and this is my bottom. (Refer Slide Time: 5:50) So, this is called selection sort, because we select the next item in sorted order and move it into the correct place, move it into the correct business cases, just append it, we do not have to do any work on the second pile, the second pile is growing naturally in sorted order. We are just adding it to that. Now, for many reasons, we would like to avoid using a second list. So, we would like to ideally use the space that we have in the list. So, supposing we have a large list, we do not want to duplicate it. So, we want to make this same algorithm work within the same list that we have. So, a strategy for that is that you take the list and now you go through it. And so you take this list, and you go through it. And perhaps you find the minimum here. So, what you now do is you move this to the beginning, and you move this here, so we just exchange the values here and now the minimum as the first position. Now I go through this whole list, and maybe I will find the second minimum at this position. So, now I move the second minimum here and move this year. So, this makes it possible for me to do this strategy without creating a new list. Because remember, in the previous example as I move things, I cross them out, so they were no longer there. So, here the crossing out in this thing corresponds to saying that they are moved into position and now their position has been taken by someplace and not value, which I have not seen before. So, eventually this list will be rearranged. If I keep moving the minimum to the beginning of each segment, I will rearrange it in ascending order. So, here is a Python implementation of this selection sort. So, what you do is you first compute the length of the list.'},\n",
              " {'id': 'd4b6ad0e-b1da-4518-bc3c-2b80e223b18d',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So, if the list has is empty, if the length is 0, and is less than one, then I just return the list as it is, I do not have to do any work. Now otherwise, I am going to build up this segment, so at any given point, I am going to be looking at some index i so I am looking at L of i and I am going to scan from here onwards and look for the minimum and then move it to L of i. So, I am not going to touch the part on the left. So, basically 0 to i minus 1, this part is already sorted. So, we are going to assume that the slice or the prefix of the list up to i minus 1 index is sorted. So, when I am at 0 this means that nothing is sorted because there is no i minus 1. So, now, in order to find the minimum, I use our usual algorithm, I assume that the current position is my minimum and I keep looking at all the values from the next position onwards, and if I find a value which is smaller than the minimum that I assumed, then I replace my minimum position from i to j. So, this just computes j as the minimum, I mean, it scans through all j and keeps updating mpos, to be the minimum position, the value with position with a minimum value from i onwards. Once you have done this scan, now what you want to do is swap that thing, so that is what just, swap that to the current beginning. So, we take a L of i, which was where we started the scan, and exchange it with the minimum position. Now if L of i was already the minimum position, it just gets exchanged with itself, which is no use. But if we did find a smaller one further to the right, then we would bring it here. So, what have we achieved? We have achieved that now I have moved this boundary from i, i minus 1 to i. So, earlier before I did this pass, 0 to i minus 1 was sorted. Now I found the correct thing to put in position i so that 0 to i is sorted and I keep doing this from for i going from 0 to n minus 1.'},\n",
              " {'id': 'e50c2244-ce7f-4a79-8374-e82fe823a7ad',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So, eventually, every time I move, one more extension of the sorted segment happens and the whole thing becomes sorted. So, it is important to kind of think of these algorithms in this way because unless you do this, it is not going to be possible to really convince yourself you have handled it properly. So, these are what are called invariance. (Refer Slide Time: 9:58) So, this is what is called an invariant. So, at the beginning of the loop, the invariant that I have is up to I sorted, that is 0 to i minus 1 in terms of indices and what the loop does is it extends the invariant to one more step. So, earlier I had L colon I was sorted now, L column i plus 1 is sorted and then you have to just verify that what happens here is a valid calculation to ensure that we go make this progress from L 0 to i to L 0 to i plus 1. (Refer Slide Time: 10:38) So, the correction, correctness of the algorithm follows from the invariant but correctness is an important step that we should we should never ignore, because it is totally pointless to have an efficient algorithm which is wrong. So, ultimately, when you design an algorithm, the first thing you have to make sure is doing the job that is supposed to do, then you can worry about its complexity. So, establishing correctness is cannot be dismissed, you must show that you are always going to work. So, here I claim that these comments you have written, you have to will, so I have kind of informally convinced you, but you have to kind of formulate convince yourself that this invariant holds the beginning, each iteration will actually update the invariant as I claim and therefore, at the end of the loop, the invariant will establish the property I want, which is the entire slice from 0 to n minus one is actually sorted. So, that is the first thing. Next, we have to worry about the efficiency. So, how long does it take? Well, here, we can use this, because there is no recursion and all that we can just look at the loop structure.'},\n",
              " {'id': '761a044c-5e3a-4dcb-a91a-964bf8fbaf66',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So, we can see that there is an outer loop which works, which takes n steps and inside this outer loop we have this inner loop. And how much time does this inner loop take when it goes from i to n minus 1. So, it is going to take n minus i steps to find this minimum. So, therefore, for each of the outer iterations, I am going to take n minus i steps, and i is going to keep changing. So, I am going to take initially, I am going to take go through on any elements to find the minimum, absolute minimum and bring it to the first position, then I am going to go through n minus 1 elements, find the minimum, bring it to the second position, and so on. So, the total time algorithm is going to take is n plus n minus 1 plus up to 1. So, you should probably know what this adds up to. But in case you do not, here is a simple way to do it. So, have n plus n minus 1 plus 2 plus 1. So, what does this add up to? So, this is a trick which is attributed to Gauss as a school child. So, he said, let me write it in reverse the same sum, let me write it again. So, I have just written it out from right to left. But now if I add it, column by column, then this column adds up n plus 1, this column adds up to n plus 1. So, every column adds up to n plus 1. And how many columns are there? Clearly, there are n because n to 1. So, there are n columns, each of it adds up to n plus 1. But the total sum is n into n plus 1, but that is 2 times this because I have added the list once forwards and ones backwards. So, this is 2 times the total amount. So therefore, my total is going to be n into n plus 1 by 2. So, if you do not remember this formula, this is one easy way to reconstruct it. (Refer Slide Time: 13:47) So, T of n, if n if it is a summation from 1 to n is just n into n plus 1 by 2, it is the number of elements times the number of elements plus 1 by 2. And as we have seen, this is big O of n squared. So, all we need is the highest term. So, this is n squared by 2 plus n by 2.'},\n",
              " {'id': 'a5b503d2-9b63-40ef-9565-17ed4f31a4d6',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 2},\n",
              "  'source': 'Selection Sort.pdf',\n",
              "  'content': 'So basically, in our asymptotic, order of magnitude way of doing things, we throw away this, we throw away this and we say this is order of n squared. So, selection sort is actually an order n squared algorithm. (Refer Slide Time: 14:21) So, it is an intuitive algorithm to sort a list because we do it all the time actually, when we do it with small values, when we are asked to find them. Whenever somebody gives you a kind of set of things and asks you to arrange it, this is very often the kind of thing that you might want to do. So, we just repeatedly find the minimum or the maximum and append it to the sorted list. Now, one of the features of this algorithm is that not only is the worst case big O n squared, but actually every case is big O n squared and that is because if you look at this loop, so this loop will scan the entries from i plus 1 to n regardless of what the status of those entries is, it is not really influenced by whether those entries are already in sorted order or not, it is just going to scan every one of them and try to update the minimum. So, it has no performance improvement on whether this list was already sorted, not sorted in a particular order, it is as good or as bad whether you had a completely random order or whether you had some nice order to start with. So, this is actually some case where the worst case is really every case. So, selection sort is really big O of n squared in every case, so there is no advantage, even if the list is arranged carefully before sorting.'},\n",
              " {'id': '228b2e4e-c9b6-4120-8cea-dd658f8b6d42',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Comparing Orders of Magnitude (Refer Slide Time: 00:13) So, we are talking about the analysis of algorithms. So, we said that we are interested in measuring up to orders of magnitude. And we are looking at this asymptotic complexity. So, we want to know what happens is n becomes large. So, we would like to say that n cube eventually grows faster than 5000n squared. So, we now want some way of writing this down precisely. How do we argue? Or how do we describe the fact that f of n is a bigger function of n than g of n. So, how do we compare functions so we can say this algorithm is better than that algorithm asymptotically? So, in order to compare one function against another function, we have this notion of an upper bounds. So, this big O notation as it is called so this big O. So, this O here is a capital O, and it is called Big O. So, we say that f of x is big O of g of x. If in some sense g of x sits above f of x. Now, because we are ignoring constants, and orders of magnitude is what matters, we are allowed to make g of x sit above f of x by multiplying g of x by some suitable constant. And like we saw with that n cubed and n squared example, this may not happen initially. So, they you might need to go to large enough n for this to happen. So, you need to find two values, you need to find an x not. And you need to find a c, such that if you go beyond x not, then f of x is below c of g of x. So, if you want to write it more mathematically, you say that for every x bigger than equal to x not, f of x is smaller than equal to c times g of x. So, you have to find I mean claim that f of x is over g of x, you have to find so g and f are given to you. You are told what is f of x, you are told, what is g of x? The question is, is g of x an upper bound for f of x. In other words, f of x, in general, smaller than g of x as a function of x.'},\n",
              " {'id': '54d334d6-f86a-494e-9e3e-eddc0c7aceb3',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, for this, you need to find these two constants, c the multiplier for g and x not, the threshold beyond which this holds. So, here is a picture on the right, so this is our f of x and for some suitably chosen, chosen g, this is our c times g of x. And this, at this point is our x not. So, if you believe that this function continues to behave similarly to the right beyond what we cannot see, you can see that in this interval, the relationship between f and c times g is a little bit ambiguous. Sometimes f is below g, sometimes f is above g. But once it crosses x not, you have this property that f of x is always below c times g of x. So, this means that in such a situation, I can declare that f of x is big O of g of x. (Refer Slide Time: 02:52) So, just to give an idea about how the, so basically it is saying that one graph is above the other graph that is what we are saying, well, if you draw it. So, here are some of the graphs that we talked about when we saw this table of functions. So, here is a particularly simple graph, which is just a constant function. So, this is a so not many algorithms have this kind of constant behavior. But you can imagine an algorithm which takes a list and returns the first element of the list. So, it does not really matter how large the list is, because it is always going to just pick up the first element which is available and give it to you. So, this is a typical example of an algorithm which takes constant time its input does not matter. So, then you obviously have a straight line at the bottom. Now, log n is kind of hugging the bottom, it does not grow very fast, square root of n also does not grow very fast, but it grows faster than log of n. So, square root of n is an upper bound for log of n. So, you can say log of n will be big O of square root of n, but not vice versa. This is our y equal to n, where I have a straight line, which is kind of at 45 degrees. And now n log n is it looks almost like a straight line.'},\n",
              " {'id': 'f7408383-6740-4580-b933-d81dcd6b957c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'But it is, basically at this scale, it looks like it is closer to n squared. But as this line grows up, you can find out that n log n is much closer to n than it is to n squared. So, n squared is this parabola n cube, we have not drawn two to the n is even steeper than n squared, and n factorial is even steeper than two to the n. So, this we saw in the table last time and now you can see it in the graph. So, in Big O sense, each of the lower curves is going to be in big O of the upper curve. So, what I said before is that to show something formerly is big O, you have to demonstrate these two constants, c and x not. So, let us do a couple of examples. And then we will see a simpler way of doing it informally. So, let us look at two functions 100n plus 5, and n squared. So, I want to argue that 100n plus 5, so this is my f of n. And my g of n is n squared. So, I want to argue that f of n is big O of g of n where g of n is n squared. So, I need to find this constant. So, I do a little bit of manipulation, I look at 100n plus 5. And I say that, 5 will be less than n, if n is bigger than 5. So, if I take 100n plus 5, and instead I write 100n plus n, then the right hand side is going to be bigger than the left hand side, provided n is bigger than 5. So, I choose n bigger than 5, then I have 100n plus n, but 100n plus n is nothing but 101. So, 100n plus 5 is less than equal to 101n. Since n is always positive, remember, we are looking at functions where n describes an input size, and obviously, the input size is not going to be negative or something. So, since n is only going to be positive 101n for n bigger than 5 is going to be smaller than 101n squared. So, therefore, if I take now 101n squared, it is of the form c times n squared and n square to the function I was looking at. So, my constant that I have discovered is 101. And the lower bound beyond which this happens is 5. So, n not is 5.'},\n",
              " {'id': '0de7ce1a-4bb7-411c-97c3-ac88d9e96e9e',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, for all what we are saying is for all n greater than n not, which is 5, in this case, 100n plus 5 is less than or equal to 101 times n squared. So, this is the kind of analysis that we have done to show that 100n plus 5 is big O of n squared. Now, it turns out this is not unique, I mean, this choice of c and n not really depends on your cleverness at finding out how to establish this and there may be more than one way to do it. (Refer Slide Time: 06:35) So, for the same example, instead of writing 100n plus n. I could write 100n plus 5 times n, that is also true, except now it becomes true even for n bigger than equal to 1, because once I have n equal to 1, 5n becomes 5, so this is 5 and this is 5, and this is 5, so they are the same. And if I go to something like two, then 5 n will become 10. And so, this right-hand side will become larger than 5. So, therefore, for n greater than 1, we have 100n plus 5 is smaller than 100n plus 5n. And now, if I combine these two, I get 105. So, now I have a different choice, I say now, if I take any for all n bigger than 1. I have that f of n is smaller than equal to 105 times g of n. So, now I have a different n not in a different c. So, this is one way of doing this. (Refer Slide Time: 07:28) And it shows that the choice of n not and c is not unique. Now, let us look at something where both sides have the same kind of highest degree. So, we have here a quadratic 100n squared plus 20n plus 5, this is our f of n. And we have another quadratic which is does not have any coefficients does not have any linear terms and so on just n squared. But I still claim that 100n squared plus 20n plus 5 is big O of n squared. So, again, you can do a similar trick, you can kind of convert all of these into n squared form. So, you can say that for n bigger than equal to 1 this 100n 20n multiply by n to get 20n squared, 5n multiply by n squared to get 5 n squared.'},\n",
              " {'id': '913c4e63-4fbb-4dd8-be7e-50b34187325c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, I get that this whole thing is smaller than 100n squared plus 20n squared plus 5n squared. So, if I combine that I get 125n squared. So, now, we can say that the left hand side though it has a kind of a bigger coefficient for the n squared term, and it has these linear terms, it is still below 125 times just n squared. So this left hand side is big O of n squared. So, what really matters, in some sense is the highest term. So, what we are saying is that any polynomial in which the highest term is 2, is big O of any other polynomial is basically big O of n squared. So n squared is as big as any polynomial of degree two in this order of magnitude notation. So, in particular, we have this constant 100, which does not matter, we also have this extra linear term, which is going to be dominated by the quadratic term. So, all these things go away. So, really, we are looking at the highest power when we are comparing, say polynomial, then the highest power is what matters. So, by the same token, if I look at n cube, then it cannot be n squared, O of n squared, because if it is O of n squared, then I will get something like c times n squared. And I will (())(09:23) n cube to be less than c times n squared for all n above a certain thing, but we know that once we cross c, remember, we saw the particular example 5000n squared right at the beginning. So, once we cross c n cubed is going to get higher than c times n squared no matter what c we choose, so we can never find such as c. And so something with a higher power n cubed can never be big O of something with a smaller power. So, the simple rule of thumb is you just look at your function, especially when it has polynomial terms. And you can look at the highest power and decide based on that and of course, sometimes it is more than just a polynomial. It could be something like n squared log n or something.'},\n",
              " {'id': 'b9954e3d-c8d5-4539-aa81-d64409ed6d2c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, now n squared log n would be smaller than n cubed, because log n is smaller than n, but n squared log n will be bigger than n squared, and so on. So, you can do a rough and ready. We go calculation. But to formally verified, you have to go through the mechanics of using the constant and finding the minimum level beyond which it holds. (Refer Slide Time: 10:24) So, one of the useful properties of this big O notation is that if I add two functions, then they lie below the maximum of the two in terms of big O. So, we will see why this is important. So, supposing I have a pre processing step, like for instance, in the Aadhaar card example, this is my sorting step. And then I have an actual computation step. This is a search, search all SIM cards. So, now, I might do some kind of analysis and get some f1 as the complexity of that, and I might do some analysis f2 as a complexity of this. So, I have two different algorithms, one which sorts the cards, and one assumes the cards are sorted and then compares. So, the total time that I want to reason about for this algorithm is f1 plus f2. Because the first step to do f1 work, then have to do f2 work. So, that is what this is saying? So, f1 of n plus f2 of n taken input, I first do f1 work, and then I do f2 work. What can I say about this? If I know something about each individual step. So, in each individual step, I know that f1 is within g1 of n and O is f2 this is g2 as an. And the claim is that the whole thing f1 plus f2 is within the maximum of g1 and g2. So, it will not exceed the maximum of g1 and g2. (Refer Slide Time: 12:00) So, why is this the case? Well, just from the definition, we know that f 1 of n is below c 1 times g 1 for some n above n 1, and it is below f 2 is below c 2 times g 2 for n above n 2. This is just the big O definition being stated concretely for f 1 and g 1. So, I have these four constants c 1, c 2 and n 1, n 2. So what I will do?'},\n",
              " {'id': '59b08a73-d1c4-4b24-81f0-bd6cfbac98f0',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'I will take the maximum in each case, I will let c 3 be the maximum of the 2 c is c 1 and c 2. And I will let n 3 be the maximum of n 1 and n 2. So, now I want to look at what happens for f 1 n f 1 plus f 2. So, I will look above the maximum. So, I will look above n 3. So, since I am above maximum of n 1 and n 2, I know that f 1 is below its upper bound. And I know that f 2 is also below its upper bound, because both of them have already crossed their threshold, because n 3 is bigger than n 1 and n 3 is bigger than n 2, because it is the maximum of the 2. So, for both f 1 and f 2, I can use this fact that they are big O and get this inequality saying that they are below c 1. Now, because c 3 is bigger than both c 1 and c 2, I can write this as c 3 g 1, plus c 3 g 2. This is also valid, because if it is smaller than c 1 times g 1 it is smaller than c 3 times c 3 is even bigger than c 1, same to c 2. So, if I take that c 3, and then I collapse it, I get c 3 times g 1 plus g 2. So, I am just doing some simple algebraic manipulation here. So, I have taken c 3 g 1 plus c 3 g 2 and collapsed it into c 3 times g 1 plus g 2. Now, I have g 1 plus g 2. So, in general, if I have some 2 values, x and y, if I take the bigger of the 2, let us assume that y is the bigger of the 2, then I will have y plus y. So, I claim that x plus y is always going to be smaller than 2 times the maximum of extent x and y. Because instead of the smaller value, I put the maximum instead of the bigger value it is itself the maximum, so that is 2 times the maximum. And what I started with is smaller than that. So, therefore this is going to be less than 2 times. So, c 3 times this is there and I am going to replace this by 2 times the maximum. So, now with the new constants 2 times c 3 and n 3, I have shown that f 1 plus f 2 is big O of max of g 1 g 2.'},\n",
              " {'id': '379bee1b-8c4e-4cab-aebb-389b2e8501b4',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': '(Refer Slide Time: 14:21) So, going back to our earlier discussion about that sim card, Aadhaar card case, the importance of this is now when I have these algorithms which take multiple phases, and I analyze each phase at a time. So, I tell you that the first phase takes gA of a phase A takes order of gA time and phase B takes order of gB time. From this I can conclude that the algorithm as a whole takes a maximum of order of gA and gB. So, in practice what this means is that the each individual block contributes some time and the largest of these blocks dominates the overall running time of the algorithm. I do not have to add them up and get some new number. It is the maximum of the running time of each block or each phase. So, the least efficient phase is the bottle neck. So, this is what we are saying if I have to do a sequence of steps and for each step, I have calculated this asymptotic complexity individually, then to get an estimate for the whole algorithm, I can take these steps and find the worst step worst section and say that dominates. So, that makes it much easier to analyze algorithms, because I can now break up the algorithm into blocks. And I can say, this block takes so much time. And after this block ends, it goes to this block. So, this block takes so much time, so it is f 1 plus f 2, but then the maximum of these 2 will dominate the running time. So, this is for upper bounds. Sometimes we might want the other way, we want to say that a function is above another function. So, this is called a lower bound. So, earlier, we said fx is below gx. Now, we want to say fx is above gx. So, this is written using this Greek letter omega capital omega in particular. And so similar to the other one you want to find these constants, but here, instead of saying that f of x is smaller than or equal to g of x, you are saying f of x is bigger than or equal to g of x. So, this says that now, g of x is a lower bound.'},\n",
              " {'id': '14ac44c8-8a23-46af-aa0b-ee88ec8f105c',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'It saying that f of x will never go after a certain point, if I know something of g of x, I know f of x is always above. So, it is going to take at least so much time, if I can say that is what you say, for a lower bound. It cannot take less than so much time. So, g of x is some known quantity, you will say f of x, I do not know what it is, but it is O big omega of g of x, it cannot take less time than g of x. So, here is an example you can say that, remember, we said that n cubed cannot be big O of n squared, but n cubed is omega of n squared, because simply put n cubed is bigger than n squared for all n. So, if I just take trivially my horizontal limit to be n equal to 1. And my constant factor to be 1, I get that for every n bigger than 1, n cubed is bigger than n squared and so n cubed is n squared is a lower bound for n cubed. Now, this is not something that we typically establish for an algorithm individually. We do not say that this algorithm is a lower bound for that algorithm or something like that. Rather, what we say is that, for a problem as a whole, I must take at least too many steps. So, this is a lower bound for how many steps of work this problem requires to be solved. For example, If I take an unsorted sequence, and I asked you to search for a value, which is not there. I claim that no matter how you do it, whether you scan it from left to right, right to left. Whatever way you do it, if you do not have any particular structure to the order of the values, there is no way to determine that the value is not there without examining every value. So, I can then argue that searching for an element and an unsorted list mean this is what I gave you is an informal argument. But you can formalize this day, that searching for a value an informal in value which does not exist in an unsorted list is going to have a lower bound of the size of the list. Unless I look at every element, I cannot conclusively declare that this value is not there.'},\n",
              " {'id': '29ff7e8f-c538-47ef-be28-9ab0328f1a98',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'Similar example happens in sorting. Now, in most sorting algorithms that we will see, the way that you saw it is you compare 2 elements in an array and you want to put it in a particular ascending order, you want the first element to be smaller than the second element. And if it is not, you exchange them. So, this compare and swap, so you compare 2 elements of what you are allowed to do to sort an array is usually to compare elements, and to rearrange them. So, in this model, where you are comparing and rearranging, it turns out that you need to compare at least n log n times. So, this is a problem, this is a lower bound for sorting as a problem. So, it says that no matter how clever your sorting algorithm is, you are not going to be able to use this compare and swap type of algorithm and get away with less than n log n comparisons, because that is a lower bound. So, independent of whatever algorithm we choose. So, the same thing with the earlier thing about searching for a value, which is not present in a list, independent of how you scan an unsorted list, whatever strategy, you look at all the odd positions and all the even positions, you look at left to right, right to left, you start in the middle and go forwards and backwards. All of these are going to have to force you to see the entire list. So, you cannot do better than the order n time. (Refer Slide Time: 19:27)  Now, in some rare situations, we are able to show that there is a lower bound for the algorithm that I am trying to solve into for the problem I am trying to solve. And I have an algorithm whose upper bound is the same as that lower bound. So, this is what is called a tight bound, and it is written theta. So, as you would expect, what you want is the same function like you want to say that I have the best possible sorting algorithm.'},\n",
              " {'id': '650072f0-8bb0-4eb6-ae44-05f8e0e8b2ca',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, if I argued that n log n is a lower bound for the number of comparisons, and I do have an actual algorithm which performs in order n log n comparisons, then I have something which is effectively the best I can do. Because it does as many comparisons as is required and no more. So, you want the same function in this case n log n, to be both an upper bound and the lower bound. So, you want to say that there is a constant such that f is bigger than c 1 times g. And there is another constant such as f is smaller than c 2 times g. In which case now, f is sandwiched, it is essentially the same as g. So, here is a simple example. So, we want to claim that n into n minus 1 by 2 is theta of n squared. So, we have to first show that there is an upper bound and then we have to show this lower bound. So, here is the upper bound. So, I just take n into n minus 1 by 2 and I expand it, so I get n squared by 2 minus n by 2. Now, because n is positive, I can this am subtracting some positive quantity rates in the worst case and subtracting 0, but when becomes 1, I am subtracting half and becomes 2 and subtracting 1. So, I am subtracting something positive from n squared by 2. So, if I do not subtract, I get something bigger. n squared by 2 minus n by 2 must be smaller than equal to n squared by 2. So, I am now claiming I am looking for something with respect to n squared. So, I claim that this factor half is my multiplier that I want. So, for all n, n into n minus 2 is going to be upper bounded by half times n squared. Conversely, if I do the lower bound, I do the same expansion. And now I say, instead of subtracting this I subtract more so, I subtract n by 2 times n by 2. n by 2 times n by 2, if n by 2 is a fraction, when you square it becomes smaller, half squared is 1 by 4. 1 by 4 squared is 1 by 16. So, a number smaller than 1, when you square them make them even smaller. But numbers bigger than 1 become bigger.'},\n",
              " {'id': '33f31e4b-9c99-4d0a-88d3-275729475979',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'So, long as this quantity is bigger than 1, n squared, n by 2 whole squared is going to be bigger than n squared. So, this is a same thing as saying that n is bigger than or equal to 2. So, for n bigger than equal to 2, if I subtract n by 2 times n by 2, what I get to something. Where I am subtracting more than I am subtracting here. So, this quantity is bigger than this quantity. So, this is what I need for an upper bound. But what is this quantity, this is n squared by 2 minus n squared by 4, which is equal to n squared by 4. So, we are saying that n squared by 2 minus n by 2 is bigger than 1 by 4 n squared. So, I have another constant now 1 by 4. So, now with these 2 constants, I have that 1 by 4 of n times n squared is smaller than n into n minus 1 by 2 is smaller than half n squared. So, this is the form in which I want this thing to show that this is theta. And what is the value it because in 1 case, I need 2 in 1 case, I need zero. If I choose n to be bigger than 2, both of these inequalities will hold. (Refer Slide Time: 22:55) So, to summarize, we have saw these 3 notations to compare function, so big O is an upper bound notation. So, f is big O of g, if g is an upper bound for f, so beyond a certain point, some constant times g is always sitting above f. And this is useful to describe upper bounds for worst case running time. For problems as a whole and rather than individual algorithm, sometimes we want to, we will try to establish lower bounds, you must do at least so much work, no matter how you try to solve it. So, it is not very useful to establish a lower bound for an algorithm individually, but for a problem as a whole. And this is given using omega, we just invert the condition you say beyond a certain point, f is always bigger than c times g. And finally, you might have a situation where you have a matching upper and lower bound, and this is a tight bound.'},\n",
              " {'id': 'f01c5c9c-e5be-4bf8-8f20-2ad7953f5188',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 2},\n",
              "  'source': 'Comparing Orders of Magnitude.pdf',\n",
              "  'content': 'And if it so happens that you have an algorithm whose upper bound matches the known lower bound for the problem you are trying to solve, then you are in good shape because you are found an optimal algorithm for that problem.'},\n",
              " {'id': 'ba9d04cf-239f-490a-9516-15ea9062d1b0',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor. Madhavan Mukund Analysis of Merge Sort (Refer Slide Time: 0:09) So, let us analyze merge sort. So, Merge Sort remember was this divide and conquer algorithm, which divided the listen to two halves sorted the first half sorted the second half and then merge the two halves into B and merging basically looked at the first element in each list and move to smaller one. And if any of the lists become empty, we just copy the other list. So, this is just a summary of the basic high level thing of merge sort. So, merge sort on the left, just tells you that you sort the two halves and then merge them and the merge operations on the. So, we want to now analyze how long Merge Sort takes. So, we have to basically analyze both these things separately, we have to check how long merge takes, and then how long it takes. (Refer Slide Time: 0:48) So, let us try to analyze merge. So, remember that this is our merge function. So this, in this functions on where there is a table, yeah, so in this function, we basically want to merge two lists of length m and n and remember that we have this condition, which terminates the loop. So, that should give us a clue as to what the complexity of this function is going to be. So essentially, the output list has m plus n elements, because all the elements of A and all the elements of B have to go into the list and the crucial thing is that when I go around this loop, then I make progress by moving at least one element. So, in these two cases, I move one element to C either the first element of A or the first element of me moves to C. In some cases, I make a lot of progress. So in these two cases, I actually move a whole chunk of elements. But in the worst case, it might happen only at the end, I might be alternately moving. So if I have something like 0, 1, 2, and 3, 4, 5.'},\n",
              " {'id': '7772c1df-d6dc-4d95-8419-d55258428eaf',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': 'So I might, or 0, 2, 4 and 1, 3, 5 then I might move as follows, I will first move the 0, then I will move the 1 then I am going to move 2, then I move the 3, then I move the 4, and then I move the 5. So, I will only move one at a time. But in the worst case, I am not going to do any worse than that; I have to move m plus n elements. And an every time I go around this loop, I moved at least one. So, I am going to definitely finish an implicit time. (Refer Slide Time: 2:12) So basically, it is easy to see that merge takes time proportional to the sum of the total number of elements to be merged m plus n. Now, in a situation like merge sort, we are applying merge in a special case where the two lists are almost the same size, because we are doing half and half. So, even if it is not exactly half, because say there is an odd number of elements or something, they are roughly the same. So, we are looking at a situation where m is approximately equal to n. And we have seen before that if I take m plus n, it is going to be less than 2 times a maximum. So, we had seen this when we did this asymptotic complexity, we said f plus g, f 1 plus f 2 will be 2 times the maximum of g 1, g 2. So, m plus n will be 2 times the maximum, but the maximum of m and n when m and n are almost the same as just n or m whichever, because they are almost the same. So, merge will take essentially it will take time order n which makes sense. (Refer Slide Time: 3:09) So, now what about Merge Sort? Now Merge Sort is a recursive algorithm, Merge Sort of a requires me to solve my sort of half of A. So, let us first of all assume that the n that we are dealing with, because we are going to keep dividing by 2, let us assume that the n we divide, dealing by dealing with is actually a power of 2. So, every time we divide by 2, we will come down nicely to n number and in some uniform way.'},\n",
              " {'id': '2897c37d-f76f-42f1-9bc5-48ecdd43d3a1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': 'So, T of n is what we want to calculate where we are assuming n is actually of the form 2 the power k, it does not really matter, as you will see for the analysis, but it is simpler to calculate. (Refer Slide Time: 3:47) So, now our recurrence basically says that, if n is 0, or 1 then I return immediately because n is less than or equal to 1 return immediately. So, T of 0 and T of 1 are both 1 and otherwise, I have to do two times T of n by 2 work, because that is the cost of sorting half the array, twice the first half and the second half in binary search, we only had to do either the first half or the second half. Here we are doing both. So, we sort both halves and then we are merging them and we saw that merging two lists of roughly the same size is basically big O of that size. So, it is going to take n steps to merge n plus 2 plus n, n by 2 plus n by 2. So we have this is our merging cost. (Refer Slide Time: 4:32) So now, as usual, we unwind, so we start with this recurrence. We start with 2 T n by 2 plus n and then we take this and we expand this is 2 T n by 2 by 2, which is n by 4 plus n by 2. So, basically, I am substituting this n by 2 in this function, so I am getting another factor of 2 here and this thing is getting copied here. So, I am just using the same expansion. This is what we always do for our unwinding. So, I am replacing T n by 2 by 2 T n by 4 plus n by 2. Now I am going to do a little bit of cleaning up like we had done. When we did binary search, I am going to combine these 2s as 2 squared, and I am going to combine this and I am going to come, I am going to do some cleaning up. So, first of all, notice that there are several things happening. So, this plus this is, is 2 squared. This is again, 2 squared and this cancels with this. So, I get an n from here, and I get an n from here.'},\n",
              " {'id': 'f59afd4a-9a48-4e6b-8c44-f8601634bfc3',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': '(Refer Slide Time: 5:31) So, if I expand this out, I claim I have 2 squared, this 2 squared times T of n by 2 squared and then because I have this 2 times n by 2 plus this n, I have 2 times n. So, now let us do it one more time just to see what happens. So, I take this and I expand it, and I get 2 times T of n by one more power of 2 at the bottom, so 2 times n by T cubed, plus n by T squared n by 2 squared and then I have this 2 n. Now again, I do this calculation. So, this 2 squared, 2 squared cancels, I get one more n, so I get 3 n, and then this 2 cube, and this 2 cube comes here. So, you can now see a pattern after two expansions, I get to 2 squared, T of n by 2 squared plus 2 and after three expansions, I get to cube T of n by 2 cubed plus 3 n. So, you can work out that after k steps, I will get 2 to the power k, T of n by 2 to the power k. So, this is just saying that I am dividing that interval by 2 k times and k times n. Now, when do we kind of reach this base case when this becomes 1? So as usual, we look at the case where k is log n. So, if k is log n, then n by 2 to the k becomes 1. So, t of n by 2 to the case T of 1 is 1. So at this value of k, I get T of n is 2 to the power k, so that is 2 to the power n log n, times T of 1, which is one and then this is again k. So, we are looking at this expression. And we are plugging in k is equal to log n. So, for k is equal to log n, I am getting this log n. With this K, I am getting this log n. And for this n by 2 to the k, I am getting 1. So, this is just 1, 2 to the log n is just n, 2 to the, you can just check that for yourself, but 2 to the log n is just n. So, this whole thing on the left hand side simplifies to n, and this right hand side becomes n log n. And because n log n is bigger than n, I can throw away the n term and say that this is big O of n log n. So actually, merge sort is big O of n log n, which is really what we want.'},\n",
              " {'id': '4ab09a7b-1601-400b-84b1-074a2c20bc03',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': 'Because we said that finally, when we are using that SIM card thing we need to do n log n work to search. So, if the sorting takes more than n log n time, we are in bad, bad shape. But now we are saying that Merge Sort will actually take n log n time. (Refer Slide Time: 7:54) So, this takes n log n time and I claimed without justifying it, you can look it up if you are interested that you cannot do better than in login and such kind of sorting, where you are comparing and exchanging. So, it can be used effectively on large inputs. Another thing is that that merge function that we saw here actually can be used in a number of contexts. So one thing is, if you take two lists of values, without duplicates, you can take the union and remove duplicates. So basically, whenever I move from A and B to C, if I see the same value at A and B I keep only one copy, but I move both the pointers. Similarly, you can do intersection, if I see the same value, I move it, if I do not see the same value, I skip it. So, the same merge function, there are lots of variations. And they can do very interesting things with two sorted lists. You can also do the list difference, everything which is an A but which is not in B, and so on. One of the drawbacks of this thing is that merge needs to create a new list. There is no obvious way to put those lists back into the list that you started with. Remember that an insertion and selection, we had a clever way of moving the things to the beginning so that we did not have to create a new list. But in merge, there is no obvious way to actually put it back because you have no idea really where it is going to come. So, you cannot really take and reuse space and A and B to store the merge of A and B. You do not know which of them is going to move faster compared to the other. So, you have no option but to create extra space. And this extra spaces an extra resource that we have to use.'},\n",
              " {'id': 'af128a79-489f-413b-ba05-86ad73f9ed77',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Analysis of Merge Sort.pdf',\n",
              "  'content': 'And the other thing is that this Merge Sort there is no way to avoid this recursive call. I mean, we saw for Insertion Sort, for example, that we could do Insertion Sort with recursion without recursion. And sometimes it is easier to do one than the other. But merge sort is kind of inherently because there is no easy way to describe Merge Sort without recursion. So, we will try to address some of these issues by looking at yet another sorting algorithm.'},\n",
              " {'id': 'c6014b17-b2ec-40e2-8613-e9781b5535bc',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhvan Mukund Searching in a List So, with this background about analysis of algorithms, let us start looking at some basic algorithms, starting with one set we already examined. So, the first problem we looked at in the SIM card case was to search for a value in a list. (Refer Slide Time: 00:22) So, the search problem says given a value v and given a list l, is v present in l. So, as we said if we know nothing about the list, then the Naive solution is just the simple Python loop, which says for every x in l. Check if the value of v is a value x that you are looking for; if so, you return True. If you have exhausted all the elements in l and not found in v, then you return False. So, this is a Naive solution, which scans the entire list. So, the input size as we said for a list is the length of the list; and in this case the worst case as we have discussed before will happen, when the value v is not present in the list, because you will have to go through every element of the list. So, x will pass through every element in the list, and each of this x will not be v; and eventually you will come out of that loop and then return False. So, here using this analysis in our earlier terminology, we argued that the worst-case complexity of this particular algorithm is big O of n. So, notice this is a worst case, because I could very well have situations, where v is found in a very first position. So, there is kind of best-case scenario, but I am not interested in best case scenarios; because they do not tell me much, they are like lucky shots. So, I am looking at what is the worst thing that the algorithm do; so, the worst case scenario here is O of n. (Refer Slide Time: 01:36) So, then we said what if this list is sorted in ascending order? So, then we came up with this strategy that guess my birthday strategy; where I compare v with the midpoint of v, interval I am searching in.'},\n",
              " {'id': '04dd894b-37cd-4bc0-a7e8-5cea515cc129',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'So, I have 0 to n minus 1 as positions in my list, such that the values are in ascending order; so, I search at the midpoint. Now, if the midpoint happens to be the value I am looking for, I am done. If it is not the value I am looking for, then the value I will looking for is either beyond the midpoint or before the midpoint. So, depending on whether it is bigger than the midpoint value or smaller than the midpoint value; I either search the second half of the list, or the first half of the list. So, that is what this function binary search as it is called list doing. So, if we first checks that if I have run out of values to look at; remember I am shrinking this thing. So, when I shrink it down from n to n by 2 to n by 4 and so on; eventually I will come to a list whose length is 1. Now, if it is not the value 1, then I will look at left of 1, which will be the empty list; or the right of that one element, which will again be the empty list. So, I will have to search in an empty list; but obviously I cannot assign v in an empty list. So, if the empty list is where I have reached and return False. Otherwise, I use this integer division to get the length divided by 2 as a midpoint. I check whether the midpoint is the valid one. If the midpoint is equal to v, then I return True; if this is not the case, then I must now decide whether to go left or right. So, if the value I am looking for, it is not the midpoint; so, it is either strictly less than or strictly greater than. So, if it is strictly less than, then I will search in the slice up to, but not including the midpoint. So, remember in Python, this will go up to m minus 1; it will go from 0 to m minus 1. And not hit m, which is the midpoint which we have already seen. Otherwise, if it is not smaller, it is bigger; and if it is bigger, then I want to start beyond the midpoint. So, m is the midpoint, so I start which I have already checked; so, I go to m plus 1 till the end of the list.'},\n",
              " {'id': '319b2855-5e4a-4f0b-a502-dd7914d18c9a',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'So, this slice notation is very convenient in Python, to express the beginning of the list to m; or m plus 1 to the end of the list. So, in both cases I am skipping m, here because m is not included in this range calculation. And here because I am starting at m plus 1; so, this as we said is called binary search. (Refer Slide Time: 03:55) So, how long does this take? We have argued before many times, as this takes time proportional to how many steps you need to halve the interval down to 1. So, each step halves the interval and so we stopped when this interval becomes empty. So, log of n remember log of no with no base is always to the base 2 by assumption. Log of n is a number of times we divide n to get 1; and now I have an interval of size 1 and I divide by 2 again, I get 0. So, log of n plus 1, but we will ignore the plus 1; so, this is going to be big O of log n. So, that is one way to do the calculation, just to analyze at a kind of at the level of the meaning of the program. Try to argue what it is doing and make the kind of high level of analysis of that. (Refer Slide Time: 04:41) Now, there is a more explicit way to do this, especially for functions like this, which essentially, I just shortened to make it fit on the page; so, binary search uses itself. So, this is a recursive function. So, function on the full list calls itself one half the list. So, let T of n as we normally say be the time it takes to search a list of length n. So, we have two cases if the length is 0, it is an empty list; then we just check and leave. So, this is one basic operation checking whether the list is empty or not; so, in one step I get my answer, which is always False. On the other hand, if the answer is not that, then I have to do some work. So, I have to compute the midpoint, check this and all that. So, in the worst case remember this is always worst case; what I have to is I have to search either the first half or the second half.'},\n",
              " {'id': '83e1d3d1-d70c-4b91-846d-6df4912c8841',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'So, I have to solve the same problem for half the list; so that is T n by 2. And I have already spent one step deciding which half to do and all that, more than one step. But, as I said we have collapsed it all that saying this whole thing is of three steps or for steps or whatever it is that, you want to count for these comparisons and divisions and all that. So, we have spent certain amount of time, but a fixed amount of time to decide which half to call, and then we have to solve that. So, it is T n by 2 plus 1; so this is what is called the recurrence. So, T of n for binary search is 1 if n is 0; and if n is bigger than 0, it is T n by 2 plus 1. So, how do we solve this? The easiest way to solve this, if you can manage to find out a pattern is to unwind it. So, unwind it means you take the definition and you keep expanding it again and again. So, we start with T of n, so what is T of n? by definition it is T of n by 2 plus 1. So, I do not know what this value is? So, I again apply the definition to it. So, if I trick n by 2 as the input, then it will expand as T of n by 4, half of it plus 1. So, this is an expansion of this part and then I have this old plus 1 coming here; so, this is what mean by unwinding. You take the definition whenever you have a expression which you do not know the answer to apply the definition again. So, T of n, I apply the definition, got T of n by 2; T of n by 2, I got the definition; I apply the definition again I got T of n by 4. (Refer Slide Time: 07:00) So, now if I kind of simplify this and rewrite it; I will take this two once together and say there are 2 ones. But I will also rewrite this and saying that I divided by 2 twice. So, instead of writing it as 4, I will write it as divided by 2 to the power 2; showing that I divided twice. I started with n and divided by 2, started with n by 2 and divided by 2 again.'},\n",
              " {'id': '98d1bb5a-cca3-43e4-bc53-1c6ab9c342dc',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'So, in general now if I do this k times, that is why if I applied this binary search case k times, I have gone left and right left and right. Then the interval would have been divided by 2k times; so I would have an interval of size n by 2 to the k. And each time I would have spent one step of work to try and decide whether to go left or right. So, I would have spent k times one work; so I would have T n by 2 to the k plus k times 1. Now, of course we said and we have seen many times, as when will n by 2 to the k reach the limit that we want, when it becomes 1. So, after log n steps, I will have T to the 1; so 2 to the k is equal to 1. So, after log n steps, I will end up with T to the 1 plus log n of course. So, T to the 1 plus k, where K is log n. but, what T of 1? T of 1 is T of 0, because 1 by 0, 1 by 2 is 0 plus 1. So, if I expand this T of 1 one more time, because I do not have an expression T of 1. I get T of 0 plus 1 plus log n; log n because I stopped with k equal to log n. So, I get this plus 1 that is 2, because T of 0 is 1, I have a 1 here; so, 2 plus log n, so, again it is big O of log n. So, the interesting thing here is that without trying to analyze what is happening; so notice that I am not this this whole thing here that I have done on the left hand side. It was merely a kind of algebraic manipulation based on what is happening in the code. So, I have not try to understand what is happening in the code. So, the earlier analysis, which I gave you said, I am halving the interval and the interval keeps halving; and at thumb point interval become the trivial interval, and how many times it will take. So, here I am not asking anything about what the algorithm is doing. I am just saying this is the call; if I call it with n, it ends up calling itself with n by 2. And in order to do that, I have to do a certain amount of work in between.'},\n",
              " {'id': '6161de85-bb40-4ca3-b21d-4c0d6ea8c80a',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Searching in a List.pdf',\n",
              "  'content': 'So, this is a more syntactic way of analyzing an algorithm; it does not require me to understand at a higher level, what process the algorithm is doing and yet I arrived at same answer. So, this is useful to know that when you have a kind of recursive algorithm like this; it is very important to be able to write this recurrence. And then usually you can solve it by unwinding it like this. (Refer Slide Time: 09:48) So, to summarize the first non-trivial algorithm that we have seen in this course is searching. So, given a value, given a list, search for this value in the list; if the list is not sorted, you cannot do anything better and big O of n. So, we have to scan the entire list and the worst case is when the value is not present. On the other hand, if the list is sorted, then we can use its binary search by looking at the midpoint. And an order log n time by, halving the interval to search at each time; I will find or not find the value in log n steps. So, this is really remarkable if you think about it; because it says that log of say 10 to the power 6 is roughly 20. So, that means if I give you one million sorted values, and I give you a new value which is not present in these one million sorted values; you will only look at 20 of them. So, ninety nine (thous), nine hundred and ninety nine thousand nine hundred and eighty values, you will have to you can ignore. So, these many values can be ignored; you have start with one million values. So, this truly a remarkable property that almost the entire list you can ignore; and still, you can convince me that the value I am looking for is not there. So, this is truly remarkable.'},\n",
              " {'id': '2778a747-807b-4377-a014-e6ebfa7906eb',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor. Madhavan Mukund Merge Sort (Refer Slide Time: 0:09) So, we have seen two intuitive algorithms, insertion sort and selection sort. And both of them happen to be n squared. And we know that n squared is not good because if we go beyond say something like n to the power 4 already, you will start getting algorithms which take too long to be feasible in practice. So, we would like something which is better than n squared. So, how do we do this? So, we need some other strategy and so here is a strategy which is perhaps less intuitive than the one that we use for selection and Insertion Sort. So, in this strategy, we divide the list into two parts into two equal parts, in fact, two halves, we separately sort the left half and the right half. So, if we are still thinking in terms of that TA problem that we had, so this instructor gives the pile of papers to the TA, supposing the instructor has two TA, then the instructor can give half the papers to one TA, and the other half to the other TA and notice that these two halves can be sorted without reference to each other. So, each of them sorts their own half within that half. But now the two halves have to be reconciled, because some papers in the bigger half in the first half might have lower marks and some in the second half, and so on. So, then we have to take the two halves and combine them to get a fully sorted list. So, there is some work to be done to combine the individual TA answers or the TA sorted bundles into one full sorted bundle. So, this is the strategy that we are going to look at now. (Refer Slide Time: 1:34) So, in order to do this, let us focus on the second part, which is the two TA of come back and they have brought me two sorted bundles of answer books, how do I combine them into a single sorted bundle. So, I take two sorted lists A and B, and I want to combine it into a single sorted list, C.'},\n",
              " {'id': '3519bd76-64af-4036-8170-4fa733235239',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'So, this is something you can imagine, again, physically, you put them down, so each of them is sorted, with the highest mark on top. So, which is the highest mark overall? Well, it is either the highest mark in the second pile or the highest mark in the first pile. So, I look at the top two, and I decide, oh, this is higher than this. So, this must be the highest overall and I put it aside. Now I compare the highest here and the second paper that is visible here, again, the higher of the two must be the highest overall and what is here, so I moved that there. So, now I will kind of in some sense, in this physical process, I am reversing the order, but the highest paper comes down, then the second highest paper comes down and so on. So, by gradually, you know, running through these two piles systematically, I will be able to take them and run them down to get the pile in a single sorted order. So, I compare the first element move the smaller of the two or the bigger of the two to C and repeat until you exhausted. So, let us look at this example. Supposing this happened, a trivial thing. So, the first pile has only three papers, 32, 74, 89 and let us say that 32 and 21 are the papers which I can see. So, they are the top of the pile. So, in my earlier thing, so this is slightly. So, this is now my top. So, I start from the left hand side. So, I compare 21 and 32 and I want the new list to also be in ascending order. So, I compared 21 and 32 and the smaller of the two is the smallest overall and move the 21 out. Now I am focusing on the 55 compared to the 32. So, these are the two that are visible to me, if they were physically sitting there, I moved the smallest one out. And this is what is visibly there. So, I now find the 32 smaller and remove it. Now I am looking at these two, because the other two are gone. So, now, 55 is smaller. Now I am looking at these two. Now 64 is smaller and finally I moved 74 and 89 because there is nothing to compare them with anymore.'},\n",
              " {'id': '53929a29-bcb2-4375-b354-f5892f6293ef',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'So, I only have to move from the first pile. So, this is how I would combine two sorted lists into a single sorted list. So, this is what is called merging. So, merging is the operation that I need after I have got the things sorted by two different TA. (Refer Slide Time: 4:04) So, now, I have to get the thing to sort by these two different TA. So, how do I do it, this is the Merge Sort part. So, I let n be the length. So I will first start the beginning, and then I will sort the end. So, if I take the length and divide by 2, then I will get two segments of roughly equal length. So, I will sort the first half and sort the second half. And then I will merge using the procedure, which I just described how to take to merge links and bring, give me back a fully merged list. So, I am merging these two sorted half into B. And how do we sort these two half? Well, we use the same procedure that is we will take each of them break them up into two parts. Sort the first half or the second half merge. How will we do that? Again, we will break it up into two parts. And when do we stop when we stop? When we come to a list which has only one element when it has only one element, then there is no more work to do. So, let us look at an example. So, supposing we have eight elements then I do not know how to sort these eight elements. So I divided into the first half and the second half, and I asked my two TA to sort them, so they do not know how to sort them. So, they find two friends, and they pass them each, the two halves of the first half gets split into two lengths of two, the second half gets split into two lengths of two. So now I have 4 parts with to each. Now these two friends do not know how to do this either. So they each call two more friends, and give them one paper each. So now basically, this has come like this. So this is how they have split.'},\n",
              " {'id': '749b1547-6e18-42d1-9237-2d1e98f1fe70',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'So now the last guy in this list, the last set of TAs who have been roped into this job have an easy job, because they have just got one paper. So they say, Oh, this is easy, I can just give it back to you. So Mr. 43 says I am done. Mr. 32 says I am done, and so on. So, all these last eight TA report that they have sorted their work. So now the second level TA have to merge them. So I have to take these two and merge them. So if I merge them, then 32 is smaller than 43. So I will replace the list at this level. By the sorted version, I will do the same thing with 22 and 78. Here, nothing changes. The next one 157 comes before 63. The next one 113 comes before 91. So now I am finished with this level, and I can throw away the singleton thing. So all those TA and are dismissed. (Refer Slide Time: 6:25)  So, now I have the sorted list of length two. So, I combined the sorted list of length two into two sorted list or list of length four. So, from the left hand side, I now get the list 22, 32, 43, 78, which is the sorted version of this. And from the right hand side, I get 13, 57, 63, 91, which is a sorted version of this, how do I get a sorted version of this? I apply that merge function, I look at the smaller of the two, I pull out the 13, then I pull out 57, then I pull out the 63. And I pull out the 91. Now again, I have got these two sorted sequences of length 4, so I can throw away this smaller sequences that I had used in between. (Refer Slide Time: 7:01) And now I do a similar merge here. So I say, I want 13 first, then I want 22, then I want 32, then I want 43, then I want 57. Then I want 63 then I want 78 and then I want 91. So, this is my merge. So, if I apply this merge, I get the sorted sequence on top, and I am done. So, then I can throw away this and this is my final answer. So this is how merge sort works.'},\n",
              " {'id': 'c3fb0bfe-c171-47ab-a9a6-398ded57adc7',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': '(Refer Slide Time: 7:33) So, Merge Sort is an example of something called divide and conquer, you break up the problem into disjoint parts solve each part separately, remember, the two TA could go off into their respective places and work without having to talk to each other. And then when the solutions are done, you have to combine. That is a general step. So, there are two parts. One is breaking it up. This case breaking it up was easier; I just take the first half in the second half. And then there is a combining part, I take the solution given to me by the first TA, solution given the second TA and I have to merge them. (Refer Slide Time: 8:06) So, that is divide and conquer. So, let us look at this merging thing in more detail. So, if I want to combine two sorted lists, and I want to write the actual code for this, then I have to look at these boundary conditions, which we came to at the end of the thing that we were executing. So, if one of the list is empty, we just copy the other list. So, if I have run out of things in A, then I just take everything that is in B and put it at the end of C. Similarly, if everything in B is empty, then I just copy whatever is in A into C. And if they are both not empty, then I look at the first element that is visible, or the first element. I am scanning currently for both these lists. And I take the smaller of these two and append it to C. So, this is how merge works. So, we repeat this until everything has been moved. So, this is the merge function, which we will quickly look at. So, we have two lists, and they could be a different length. So, A is of length m and B is of length n. So, now, what I will do is I will have my three lists, I have A and B, which are of length m and n and I have my C. Now at any given point, I am going to be looking at some element here which I call i some element here, which I call j and some element here, which I will call k.'},\n",
              " {'id': '212a8857-c36e-49fa-b42e-0fb4ac2aea81',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'So, as I progress, I am walking down these three lists the two lists that I am scanning to examine and the third list C which I am building up. So, I have these three indices i, j and k which are 0 initially and add this new list C which is empty. So, this is what I need to do to manipulate. (Refer Slide Time: 9:39) So, now I go through these three steps. So, these three steps are here. So, if A is empty, so when is A empty when i have reached, i has reached the end. So, if i is equal to m that means A is empty. Then I just copy B to C. So, I use this Python function called extend which will take one list and extend it to with another list. And what I am going to do is I am going to, in that process, move k by a whole lot. I am going to move k by however many letters I moved values I moved from B, which is it, was a j and I moved from j to n. So, n minus j elements have been shifted. If that is not the case, if A is not empty check if B is empty, in which case symmetrically I have j has gone to n. In which case I extend C by A and I move k by n minus i should be sorry, n minus i. And finally, if neither of them is empty, then I compare A[i] and B[j] and if A[i] is smaller than B[j], then I append A[i] and I move i and k and otherwise, I append B[j] and I move j and k. So, if I push the indices, either i moves and k moves or j moves or k moves. Now, when do I stop? Well, I have to eventually move m plus n elements into k, into C. So, k has to go from 0 to n plus n. So, long as K is not yet m plus n, I have work to do. But when k reaches m plus n, I am done. So, that becomes my terminating condition for this merge function. (Refer Slide time: 11:12) So, having got the merge function, what does Merge Sort look like? Actually, it is much simpler than merge as you would expect. So, if you have a list, which has at most one element, there is nothing to be done. Otherwise, you sort the first half or the second half and merge them.'},\n",
              " {'id': '238784ea-8f8f-4312-9be0-ceb8765758b7',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Merge Sort.pdf',\n",
              "  'content': 'So, this is what it is, you find the length of the list you want to do, if the list, length of the list is one or less, then you just return the list that you got saying is already sorted, otherwise you recursively call merge sort on the first half and the second half. Now remember that this will stop at n by 2 minus 1. So, the lists will be disjoint, but which will cover the entire thing nothing will be left out. So, 0 to n by 2 minus 1 will be n part L and n by 2 to n the n will be in part r. Now having got both of these, I just apply my merge function to get a list which combines the two into a new list B and I return B. So, this is Merge Sort. (Refer Slide Time: 12:12) To summarize, Merge Sort uses divide and conquer two sorted lists. We divide the list into two halves so what each half and merge the sorted halves. So, why did we come to this at all is because we said that insertion sort and selection sort were not acceptable in terms of their complexity because they were order n squared and we wanted to get something which is better than order n squared. So, to complete our discussion of merge sort, we have to analyze it which will be a separate exercise.'},\n",
              " {'id': '520c328c-4e86-4d9b-91ec-6985ba0d11b8',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor. Madhavan Mukund Implementation of Searching and Sorting Algorithms (Refer Slide Time: 0:09) So, we have seen a lot of searching and sorting algorithms abstractly. So, let us try and understand in a more concrete setting how the claims that we made about the complexity actually work. So, so here are some experiments that we will run. So, we will start remember, in the beginning, we talked about this timer class, which will be used to measure time. So, this is just the same class. So, it is called timer error. So, remember, it allows us to start a timer and to stop a timer. So, that is basically, we will use this to show in physical time, how much time execution takes. (Refer Slide Time: 0:45) So, the first thing that we were looking at was searching a list. So, we had two strategies, so we had this naive search. So, naive search basically, was one thing, which just scan through the entire list. And whenever it found the item, it would return true otherwise, it would eventually come out from the list saying that the item is not there and return false. So, the worst case is when the item is missing. And then we had binary search, which would basically divide the interval into two and search either the first half of the second half if we did not find. (Refer Slide Time: 1:15) So, let us just check that these two implementations are correct by looking at some trivial example. So, we just set up a list here, which is the list of all the even numbers from 0 to 50. So, now, I searched then for all the numbers from 0 to 50. So, it should say that all the even numbers are found and all the odd numbers are not found, which is indeed the case. So for 0, naive search; so the first line is naive search. Second line is binary search. So 0, 2, 4, 6, 8 are true 1, 3, 5, 7 are false. So, this is just to convince myself that I have not made any silly mistake in writing those two functions.'},\n",
              " {'id': 'a382d9c3-2ce2-4535-81b0-4b79c2080a45',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': 'So, now what I really want to do is compare the performance of these two algorithms. (Refer Slide Time: 1:59) So, one way to compare it is to run it on large inputs, and also to run it multiple times. So, just for the interests of making sure that this does not take too long, I am going to run 10 to the power 4 Worst case searches in a list of size 10 to the power 5. So, what is the worst case search, it is going to be a search, which is not going to work. So here, I am going to take 10 to the power of 5. So, I am taking notice here 10 to the power 5, I am taking all the even numbers between 0 and 10 to the power 5. So, this is actually half of 10 to the power 5. And then I am going to take a bunch of odd numbers 1000 odd numbers from 3000 to 13000. And I am going to search for them in this list. And for every search thing, I am going to start a timer, then search and then stop it and then print the timer. So first, I will do it for naive search. And then I will do it for binary search. So, let us run this thing. So, it takes some amount of time. The naive search for this takes 10.6 seconds. And binary search takes less than one second. So, there is a factor of 10 speed up, it should possibly be more dramatic than this. But at least it is clear that binary search is much better than naive search. And so we will come back to this issue about why binary search is not as good as it seems, compared to naive search, but naive, which is certainly worse. (Refer Slide Time: 3:31) So, next thing we looked at was some naive sorting algorithms. So, the first naive sorting algorithm was selection sorts. So, this is an implementation of selections or so remember, in selection sort, say we are doing it in ascending order, we take the we find the minimum element and move it to the beginning. So, we assume that we have sorted up to position i minus 1, we take the value at i onwards find the minimum and put it at position i. So, this is selection sort.'},\n",
              " {'id': '03cbd885-8381-410f-b20c-b3c688ffb80c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': 'So, this scanning and finding the minimum is not going to change whether or not that suffix of the list or sequence is sorted on. So, whether it is in random order, or ascending order or so on does not matter. So, let us set up an experiment with that. (Refer Slide Time: 4:13) So, what we do is we take 5000 elements, and we create 5000 random elements. So, we start with a random library of Python and what this says is, it creates for i in range 5000. So, it creates 5000 random elements between 0 and 10 to the power 5, so I am basically creating a random list of size 5000. And then I am creating an ascending list of 5000, which is 0 to 4999. And then I am creating a descending list of 5000, which is 4999 to 0. So, I create these three lists, and I put each of them in a dictionary called input lists with the key random ascending and descending. And now what I do is I put this whole thing in the loop. So, I have this timer which I create, and then for each of these random ascending descending, I first make a copy of the list, I do not want to change it remember that these are in place sort. So, I just take a slice. And then I started the timer, run the selection sort on this list that I am passing it, stop it and print it. So, if I run this, then you will see that I have chosen numbers to run this fast. So, each of them takes about a second, but they take the same amount of time, this is the important thing. So, if it is random or ascending or descending, it does not matter, they all take roughly as you can see, 1.2 seconds or so. (Refer Slide Time: 5:41) Now, here is Insertion Sort, which was another naive sort that we looked at. And in insertion sort, we said that the performance actually should vary according to whether it is ascending or not. So, if we run insertion sort on the same thing, we would expect that it will work better on the sorted arrays than on the unsorted arrays.'},\n",
              " {'id': 'e63bc577-887b-4123-8857-c4cf36311fbb',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': 'So indeed, if I, if I run insertion sort on the same random elements, on the random elements, it actually takes a little longer than selections sort, there it took 1.2 seconds here is taking 2 seconds. But on ascending order, it takes a fraction of a second, because an ascending order, the insert operation is trivial. In any descending order, it is taking something like 5 seconds, so it is actually much worse, because in descending order, essentially whenever we are taking the minimum element, we have to insert it all the way to the back, because each element that we see is smaller than everything before it. So, we have to the maximum number of inserts. (Refer Slide Time: 6:38) What about a recursive Insertion Sort? So, a recursive Insertion Sort. Now will, if we run it on the same inputs, we first see something interesting, which is that we will get an error, which is because of something called the recursion depth. So, you may have seen this before in the Python course. But Python by default has a very small tolerance for recursion, because it is worried that you are out of control. So typically, the recursion depth is. So, you can have about 1000 nested calls, which is not enough for Insertion Sort sorting, say an ascending or descending order list where it has to keep doing something for, say 5000 elements. (Refer Slide Time: 7:26) So, one way to fix that is to set this thing called the recursion limit. And it turns out that the largest value you are allowed to set in Python is this number 2 to the power 31 minus 1. So, you can do this thing of setting the recursion limit by importing this system library and calling it with this number. So, once we do this, then we can run our Insertion Sort as before. And now I am running it on even smaller lists. So there, initially I was running into on a 5000 size list. Now I am running it on a 2000 size list, which is only 40 percent. If you remember insertion sort on the iterative version took 2 seconds.'},\n",
              " {'id': '99bc7f37-74ff-4ccf-9ada-2a4ae716b7ea',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': 'Now here on something which is much smaller, it is taking 12 seconds. And there, if you remember, the insertion sort on the sorted version took really a very small amount like 1 10000th of a second. And here it is taking almost 1/10 of a second. So, what this is saying is that, although they are equivalent as algorithms, the recursion basically makes the whole thing much slower. Because every time we have to call and return from a function, there is an extra cost involved. So, our basic counting is not working as well when we have to deal with these function calls. (Refer Slide Time: 8:43) And finally, we have merge sort, so we have this merge function. And then we have merge sort, which calls this merge function. And we said that merge sort is an order n log insert, so it should work much better on even large inputs. So again, we can run it on a trivial thing. So, we just take sorting the even numbers from 0 to 1000, followed by the odd numbers from 0. So, we construct a list which is 0, 2, 4 followed by 1, 3, 5, 7. And we sorted and indeed Merge Sort produces a sorted output, but our real interest is in the performance. So, supposing now we take a seriously large array which we have not tried at all with insertion sort and selection sort, if you try it, you will find it takes forever, but 10 to the power 6, remember, if I do n log n and 10 to the power 6, log n of 10 to the power 3 is about 10. So, n log n on 10 to power 6 is going to be more than 10 to the power 7. And we said that 10 to the power 7 is what we would expect in 1 second. So, n log n should take between 1 second and 10 seconds is what we would estimate.'},\n",
              " {'id': '28be52a6-634d-4328-a6ee-dc7f0d8a095c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Implementation of Searching and Sorting algorithms.pdf',\n",
              "  'content': '(Refer Slide Time: 9:41) So, if we run this again on our three types of inputs, random ascending and descending, but this time the numbers are 10 to the power 6 and not those 5000 which we are doing with Insertion Sort, then you will find that indeed, the random one takes a little less than 10 seconds and the ascending and descending will take about half the time. So, this shows that merge sort is indeed, significantly faster than the naive swords working in n log n time.'},\n",
              " {'id': 'a34de5e5-9aa4-4d42-abb2-15338422a690',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhvan Mukund Analysis of Algorithms So, in the first week we looked at some motivation for studying algorithms. And then towards the end, we looked at one specific problem regarding SIM cards and Aadhaar cards, where we showed that doing the clever thing can save you an enormous amount of time. So, this is the design of an efficient algorithm; but we also need to analyze these algorithms. So, let us spend a little bit of time trying to understand what analysis of an algorithm means. (Refer Slide Time: 00:36) So, essentially, we are trying to compute the performance or the expected performance of an algorithm. So, remember, this problem where we were trying to validate whether the other entries in SIM cards were actually valid with respect to the other database. So, the Naive approach was to compare each SIM card along with every other Aadhaar card; so, have a nested loop for every SIM card, check whether the Aadhaar card appears in the other database. And this would be something which we roughly said will take n squared time, if n is the number of Aadhaar cards. So, we said that typically we assumed a realistic case where we had 1 billion; we actually have more than that more than 100 crore, Aadhaar cards in India. But, 100 crore Aadhaar cards and a similar number of SIM cards, we said if we actually tried this Naive approach to take us several 1000 years. On the other hand, if we were able to arrange the Aadhaar cards so that they were in, say, ascending order; then we saw this clever strategy, where we keep checking the midpoint and reducing the search space that we needed to look for the SIM card value by half each time. And then we cut it down to a significantly smaller amount of a few minutes. So, this was a kind of informal analysis that we did. So how do we formalize this? So first of all, what are we interested in when we are computing the performance of an algorithm?'},\n",
              " {'id': 'bcc60442-967e-4759-9b4e-4893cfee3ad5',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': \"The obvious thing is time? This is what we were talking about when we were looking at the Aadhaar card and SIM card example. How long does the algorithm actually take to run? A less obvious thing is space. How much memory do we need to store all the values that the algorithm computes on its way to computing the final answer? So, time of course, depends to some extent, on what we are running the process on. So, I had claimed that Python takes can compute 10 to the power 7 operations per second; and I did not qualify on what machine I was talking about. Well, it turns out that hardware is such that over the last 10 or 15 years, there has not been really much dramatic speed up in terms of the processing power of the CPUs that we have in our. You know, in the laptop or the desktop that we are using at home, they are all pretty much the same. So, this 10 to the power 7 applies in all of these. And if you remember, we ran the experiments to show that it takes 10 to the power 7 on Google's co lab, which is on the cloud. So, if you are running on a normal machine, you can assume that. So basically, you are running your stuff on some computer, and it is very hard to change the configuration of that computer. Of course, you could buy a faster machine. But we will see that changing to a faster machine that is even 10 times as fast will only have a limited impact. So, if you have something which is inherently long to compute, it is not helped much to change the hardware. So, you really have to look at something which is efficient, independent of the hardware; and that is how we will compute our time. Now, storage in a sense is a little easier to modify, because we can more easily typically add additional memory to our laptops, then we can add power to our CPUs. So, in essence, storage is easier to augment, and so partly for this reason and partly; because storage comes with a lot of other issues, which are not easy to assess.\"},\n",
              " {'id': '05c0be36-cae0-4d88-9cd6-9c751fcd5daf',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'We will focus when we talk about performance more on the time rather than the space aspect of the algorithm. (Refer Slide Time: 03:55) So, clearly the time that an algorithm takes is not invariant; it is not that every time I run the algorithm, it will take the same amount of time. And the most obvious dependence that this running time has is on the input size. So, say for example, I am trying to rearrange a list or an array in sorting order in ascending order. Obviously, the larger the list is, the more elements that I have, the longer it is going to take me to to arrange them. Similarly, if I am searching for a value, the larger the list, the longer it will take me find the value. So, input size clearly determines the performance of the algorithm. So, what we have to do is talk about how this algorithm actually behaves as a function of the input size. If I, if I double the input size, what will happen to the time will it double? Will it go up by a factor of 4? Will it go up by a factor of 400? So, we talked about the time for an input of size n t of n; so we think of it as a function. A function which takes the input size as an input, and produces an estimate of the running time as the output. Now, even when we fix n input size, it is not clear that all inputs of the same size are equally hard to process. For instance, if I am rearranging a table in ascending order or a list in ascending order, and it is already in ascending order; then clearly, I do not have to do much work. So, maybe I can just do one scan to verify that it is in ascending order and say nothing needs to be done. Or maybe I searching for a value sometimes you might find the value very fast; it might be the first position that will look in the list that has that value. So, clearly, there is going to be a lot of variation even for the same input size. So, we will have to come back to this a little later and argue about what input we are looking at.'},\n",
              " {'id': 'c12033e9-4efe-4d2a-8f3b-0ea9af2a7881',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'You know which input of size n are we looking at, or which inputs of size n are we looking at. So, with this minimal background, we will come back and address these points more formally; let us go back to our SIM card-Aadhaar card example. So, there we said that n the input size was roughly 10 to the 9, which is 1 billion or 100 crores; and this is related to the population of India. So, roughly everybody has an Aadhaar card, and the number of SIM cards is equal to the population; not because everyone has a SIM card, but because as we said, there are multiple SIM cards with some people. So, we said that the Naive algorithm, if we look at it as a function of n takes time proportional to n squared; because we had these two nested loops, where you had an outer loop, which goes cycles through n SIM cards. And for each of the n SIM cards, you will cycle through the n Aadhaar numbers to look for the correct match; so, n times n, n squared. So, that is how we got that n squared. And when we gave this clever algorithm, which would have the interval to search, and if you think about that having process; basically, takes an interval of linked then, and then it keeps dividing it until it comes down to an interval of size 1. So, we start with n, we divide by 2, we divide by 2, we divide by 2, and we come down to 1. So, if you reverse the process, we start with one we multiply by 2, we multiply by 2, and we reach end. So, basically 2 to that many multiplications is n, so two to the power k is n. So, the number of times we have to divide n by 2 to reach 1, is the same as the number of times you have to multiply 1 by 2 to reach n. So, it is that power, 2 to the power k is n, so this is the log. So, the log to the base 2 of n is how many times I have to probe or look into that Aadhaar card list, for each SIM card. There are n SIM cards, so n times log of n.'},\n",
              " {'id': 'a19fde17-09d9-4a91-b438-fc8719a7d254',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, we will typically write just log, we will not use this 2; but for the moment, we will use to emphasize that we are dividing by 2. But usually, if we do not write the base of the log, you can assume it is 2. (Refer Slide Time: 07:32) So, now this Aadhaar card example is not the only case where one can get a dramatic improvement; which makes a difference between something being practical and something being wildly unusable. So, let us imagine a hypothetical video game; so, in a video game, there are usually several items on the screen. So, let us assume this is one of those games where there are spaceships and aliens and, and whatever obstacles flying around. And these are being tracked by you the game player and of course by the game designer, because the game designer also has to keep updating the screen every time something happens. So, let us assume that one of the basic things that the game designer needs to do is to calculate among all the objects which are currently on the screen, which two are closest to each other. So, it could be that for instance, it is trying to calculate something about some hostile piece, which is going to attack some other piece, and so on. So, this is a basic step, say we want to calculate the closest pair of objects. So, we have a large number of objects on the screen. And at every instant that the game progresses, we need to keep track of each pair of objects is the closest to each other. So, let us assume that now the number of objects is our input; so, we have n objects, and we want to find out which pair among these n are closest to each other. So, how would you do this naively? You will take each object and measure the distance from that object to every one of the n minus 1 objects on the screen. So, will have to compare n objects to n minus one objects. So, of course, you can divide by 2 because you do not have to compare it in both directions; but still, it is roughly n into n minus 1 by 2, which is n squared.'},\n",
              " {'id': 'e43bf1fc-0f17-4543-a36a-90534d9b5917',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, if we want to compute all pairwise distances, and then determine the minimum one; then we have to actually do n squared work. Now, just like we saw in the earlier example, you can actually replace this brute force or Naive n squared by an n log n; there is actually an n log n algorithm for this problem also. So, what would the impact of this improvement be in this case? So, let us assume that we are working on some standard hardware. So, if you look at a typical laptop, the type of laptop that you have at home; it will have maybe 1500 to 2000 pixels horizontally and about 1000 pixels vertically. But, if you are really playing on some fancy gaming console, you might have a large number of pixels like 4000 this way and 2000 this way; so that is about 8 million pixels. So, at the moment, we are not particularly concerned about the pixels except that the more pixels you have, the more objects you can have on the screen. So, let us assume that this game designer has actually programmed this game; so that there can be at any given time up to one lakh, one hundred thousand, 10 to the power 5 objects on the screen. Now, if we do our naive algorithm for computing which pairs are closest to each other, then we are going to take roughly 10 to the 5 times 10 to the 5, which is 10 to the 10 steps in order to make this update, to compute which pair is closest to each other. And if you do a simple calculation based on our earlier estimate of how fast Python is, this will take us 1000 seconds; so, 1000 seconds is 16 minutes. So, this means that if this is part of the game, that is every time you make a you move your joystick or whatever, you move your cursor or you shoot something; this has to take 16 minutes to recalculate the state of the screen. Obviously, this game is not going to be very exciting to play. Now, you might think okay, I would not do this in Python; I would do it in c plus plus use a faster language.'},\n",
              " {'id': '9327f325-8ac0-49e6-a6cd-f6a690475885',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, it is C plus plus is about 10 times as fast as Python; so what will happen in c plus plus is this will become 1.67 minutes, so a minute is a long time. So, after you make a move, if it is going to take hundred seconds for it to update, it is not going to be an effective game. So, this is why an n squared algorithm will be totally useless in this scenario. Because the response time as a user of the system, you expect a response time will be almost instantaneous; so, that you can play it in real time. You cannot make a move and then think about go back and sit for half a minute or one minute, and then come back and make another move. And what happens if we use this n log n thing? What is log of one hundred thousand? Well, 2 to the power of 10 is roughly 1000, because it is 1024. And 2 to the power 20 is 10 to the power 6, because it s 1000 times 1000; this is 10 to the power 5. So, you can make a kind of if you take this, then 2 to the power 19 will be roughly five lakhs; it is 512,000 and this is hundred thousand. So, it is going to be something similar to 1718, certainly less than 20; so, log of one lakh is less than 20. So, we have now 20 times n is 10 to the 5, which is something like 2 into 10 to the power 6. So, 10 to the power 6 is one tenth of 10 to the power 7. So even in Python, this will take only point two seconds; and if you are using a faster language like c plus plus will take point 02 seconds. So, it will really be almost within your is certainly faster than our human reaction time, which is typically closer to point one second. So therefore, this will now be almost like any move you make, the world changes instantly. So, this is just another illustration as to why designing an algorithm which goes from n squared to n log n, can make a huge impact on how useful that algorithm is in practice.'},\n",
              " {'id': 'cbeee8b6-a8bf-4a8f-bd1d-2f0b564f6463',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': \"(Refer Slide Time: 12:59) So, now let us get back to the more technical aspect, which is how do we actually evaluate and compare these different algorithms formally. So, we cannot just wave our hands and say, this looks like n squared that looks like n log n and so on; we need a way to actually compute this and compare this. So, the first thing is that we will always be interested in comparing these functions. Remember, t of n is the time taken for an input of size n; but we will be interested on t of n up to some order of magnitude. In other words, we will ignore what are called constant factors. So, for instance, if you look at something like n cubed, and look at n squared, it seems obvious that n cubed is bigger than n squared. But what if I stick a 5000 in front of n squared? Then is n cubed bigger than n squared? In some sense, it is still bigger; because even though for small n, 5000 n squared will be more than n cubed. So, supposing I take n is equal to 8, for example; on one side, I will write 8 cube and here I will get 5000 times 8 squared. So, it is clear that in this situation for a small value of n, and 5000 n squared is not as good as n cube. But we are not interested really in these inputs of size 8 and 10, and so on. We really want to know what happens because our algorithm is going to run on arbitrarily large inputs. So, we are really looking at what is called the asymptotic complexity; as n becomes large, which of these two algorithms is going to be better, which of these two functions is going to go faster. So, here you can check that once I reach 5000, on the right side, I have 5000 times 5000 squared and on the squared; and on the left-hand side, I have 5000 cube. And now if I go beyond that like 6000, I will have 6000 cube; and here I'll have 5000, which is a fixed constant times 6000 squared.\"},\n",
              " {'id': 'cac7d16b-7b76-4583-a97a-706fc84b8ccb',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, once I cross 5000, the function on the right-hand side 5000 n squared is going to go below n cubed and stay below n cubed; so, this is the kind of comparison that we want to make. So, we are interested in this t of n in different forms or shapes; so, we have seen this logarithmic form. So, logarithmic complexity comes for instance, when we are doing this half interval searching what is called binary search. When we keep dividing by 2 the size that we are searching in; then in login steps, we reduce our search interval to one point, and then we have found it or not found it. So, that is something which is logarithmic. Now, we already saw examples of polynomials; we have seen n squared algorithms, these two nested loops. You can have three nested loops; you will get n cubed and so on. So, these are all called polynomials, you could have exponential something which takes 2 to the power n. So, 2 to the power n is the number of subsets of n elements. So, imagine that you have a situation, where you have a large number of objects of different shapes and sizes; and you want to fit them into a loading load them into a delivery truck. Now, you want to find out which subset of objects will fit this truck most effectively, so that you minimize the number of trips that you make on the truck. So, the Naive solution would be to try every subset; and once you pick a subset, you will try to fit it in as well as possible. And finally, you among all the subsets that you examine, you find the one which leaves the least empty space in the truck and you choose that one. So, this would be something to take 2 to the power n. (Refer Slide Time: 16:31) So, now we can tabulate these different functions, and try to see how much time a typical input will take in terms of number of steps as a function of n. So here, on the left-hand side, we have a column, which is the input size which is growing in multiples of 10.'},\n",
              " {'id': 'cb7eb2bb-c863-4b67-a312-0699103bddb3',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, we start with input of size 10, which is a very small input; and we go all the way up to 10 to the power 10, which is 10 billion, or one hundred crores. It is in a powers of 10, we increase the input and then we see what happens; so the columns correspond to different functions, so, this is our logarithmic function. This is a polynomial with degree one is a linear function, this is n squared and cubed and so on. In between n and n squared, we have n log n. This is what we saw when we talked about the SIM card-Aadhaar card algorithm; or that video game algorithm, which we claimed had an efficient n log n algorithm. And then I described to you a situation, where 2 to the n might make sense, if you are trying out all possible subsets of some set-in order to determine which is the best one. And if you are trying out all possible subsets, but up to rearrangement; supposing you are looking for all possible permutations, which do something and which one is best. Then you might end up with something which is proportional to N factorial, because there are n factorial ways of arranging n objects in different sequences. So, we have these different things; and now the question is what happens when say n is 10. So, when n is 10, if you look at the first row here, then already n factorial has become quite large; it is already within one tenth of pythons one second limit, all the other values are reasonable. But now, if I go to 100 at this point, this number has already gone well beyond anything reasonable that we can do; 10 to the power 157 operations is going to take forever. Even 2 to the power n has now exploded, because remember that 2 to the power 100, is going to be 2 times 2 times 200 times; if I just multiply 10 times I get 1000, a and I am going to keep multiplying it. So, I am going to get something like 10 to the power 30.'},\n",
              " {'id': '6fc91544-bbfe-4d73-96ee-be7f97f92495',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': \"So, 10 to the power 30 again, if you think about even with a very fast language 10 to the power 8, or 9 or maximum 10 to the power 10 operations per second, you are going to do. You are going to take 10 to the power 20 seconds in order to get things done; so this again has become infeasible. So, what you can see is that if your complexity of an algorithm is a large function of n; then very small inputs will drive it to an infeasible range, where you cannot execute. You cannot take a 2 to the power n algorithm and hope to even execute it for 100 objects. So, if you are shifting house, and you have 100 objects in your house, then you cannot apply this algorithm. So, what happens now as we go up the line, so we go to 1000 at 1000 n cubed is already approaching an infeasible limit. So, in Python, this will be talking about 100 seconds; 100 seconds is like 2 minutes. And that is acceptable in some situations, but not acceptable and others like in that video game situation. So, if we go to 10 to the 4, then already n cubed is off our list; because 10 to the 12 seconds or 10 to the 12 operations is going to take something like 10,000 seconds, that is not good. So, now as we see, at 10 to the power 6 for instance, n squared also goes up above any reasonable limit; and at 10 to the power 7, 8, 9 we still can manage with n log n. So, what we see in this thing if you want plot it, is you can kind of draw a feasibility line. You can say that up to here, maybe it's feasible, up to here is feasible; anything below this red line is not feasible. Then when I come here, above this is feasible, and I come here above this is feasible and so on. And then maybe here, you can say that at this point, this is say let us say 10 to the 10 is feasible; and then you can say that up to here. So, this red step staircase tells us that, you can go up to very large inputs if your function is up to n log n. But if it goes to n squared, you are already down to inputs of the size, one lakh or less.\"},\n",
              " {'id': '92e59c19-7d8f-49c6-83da-8b4376944408',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'Typically, 10,000 is more like it, because with 10,000 you can at least hope to do it in a few seconds. With one lakh already, you are pushing it too; so maybe we should actually draw this line above 10 to the power 10, just to be more. So, so what we are saying is that there are two sides to this story. So, the one side of the story is estimating how fast your algorithm will work; and from that, you can determine what your algorithm can be used for. What type of problems, what sizes of problems, it makes sense to use it. Conversely, if you know what size a problem you are going to use, then you can determine whether you can get away with an inefficient algorithm or search for a better one. (Refer Slide Time: 21:19) So, we still have this problem of measuring running time. So, in this Python example, I had defined for you a timer class, which actually measured the running time of the code as it was running. But then that does not really help us much, because it is should be independent of the. I mean, we should be able to make an independent assessment of the time, like we have done with our other things. Without really having to physically run it and see how long it takes on a particular input of that size. So, we will typically measure this running time in terms of what we will arbitrarily call basic operations. So, a basic operation you can imagine is one line of Python code, which does not involve calling a function. So, what is the basic operation if I write and if for a while; it says if x less than y, or while n less than 100. Then you have to check that the current value of x is smaller than the current value of y, or check that the current value of n is less than 100. So, this comparison of two values can be a basic step. Similarly, if I say assign y equal to x plus three, then that is also a basic step; so, assignment or a comparison is a basic step.'},\n",
              " {'id': 'a955aa07-a190-4d11-8994-ff03ede8fa21',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, this notion of a basic step is somewhat flexible, because it also depends on what language you are looking at. So, in Python, you would have seen this kind of clever way to use pairs or tuples to exchange; in particular to exchange two values. I mean, this is used as it is essentially a multiple assignment, you can say x comma y equal to zero comma zero. But in particular, you can assign each of them to the other one; but in practice, this does not work. So, you can imagine that these are values, supposing these values are actually physical objects. So, supposing you have two bottles, one bottle has got some water and the other bottle has got some juice. And you now want to transfer the water to the juice bottle and the juice to the water bottle; you have enough space to store them in the end. But in between, you cannot do it without using a third bottle; because, if you try to start pouring the water into the juice, it will get contaminated and vice versa. So, you will need a third bottle into which you will pour one of the two and then you will exchange. So, the same ways, how you would normally do this in a standard programming language? You will create a new temporary name called t and you will park the value of x that you have now there. So, that when you replace x by y, because that is what you want to do, you want to swap x and y; so, x has to take the value of y. When you take the value of y, the value of x that you had before is not lost, it is stored in t. So, then you can go to t and copy that value back into y. So, in essence this, what we look think of as a Python with basic operation on the left; takes three steps on the right. So, we do not want to really bother about this. So, if we ignore these constants as we said, we would do and look at orders of magnitude. If something takes three times, another thing it just could be a way of implementing these basic operations.'},\n",
              " {'id': '054d9f38-b1d0-43df-a3ce-8e9a63b247bb',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'And we do not want to worry too much about having to do this kind of low-level accounting. So, in order to simplify our accounting, in some sense, we will focus on just the statement level execution of programs. And we will be able to collapse multiple statements into one block saying, this block takes seven, seven steps. But it will always take only seven steps; it will not take steps proportional to the input. So, we will just count it as one block. So, we do not have to be very precise about defining the basic operations. And this is one motivation for looking at this running time in terms of orders of magnitude. (Refer Slide Time: 24:50) So, the other thing that we have to be clear about is what constitutes the input size. So, we said of course that if you are manipulating lists and arrays; the number of elements in the list the size of the list is a natural parameter. The larger the list, the more time you are likely to take to search or sort or whatever. If we are rearranging objects, like we were looking at, for example, trying to find an optimum collection of objects to fit in our delivery truck; then the number of objects would be the natural thing. We have seen before and we will see in this course also graphs as a very powerful way of representing relationships. And then algorithms on graphs have to manipulate these graphs. So, graph as you remember consists of these vertices and some edges between them. So, there are pictures like this, where you have the nodes or the vertices connected by edges. Now, there are two natural parameters, there is the number of vertices, in this case 5; and then you have the number of edges, which in this case is also 5, not 6. So, there are six ages and five vertices. So, you might think that if I fixed the number of vertices, the number of edges anyway cannot grow beyond a certain level, because every edge has to connect two points.'},\n",
              " {'id': 'd5eb1999-7ca3-4d21-bec1-cfe08ac60997',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': \"And so, I only have a fixed number of points, the number of edges is bounded by the number of points squared roughly. But you could have graphs in which you have very few edges, and that actually makes a difference. So typically, it will turn out that when we are looking at graphs, the input size consists of the number of vertices, and separately the number of edges. Both of them are important for us, they are separate parameters. Another very typical class of problems that we are interested in when computing is problems about numbers. The most famous such question would be to identify whether a number is prime or not? So given n is n a prime. So, how should we measure the complexity of an algorithm which claims to determine whether n is a prime? Should it be something proportional to the value of n? So, if we say something is proportional to the value of n; that means that if I give you some number like 387, and ask you whether it's a prime. And then I give you a number like 4364, 5; that is not going to be prime, okay, 4367 and onwards prime. So, this is right going from the top to the next number; so, this is more than 10 times. But, do you expect that it will take 10 times the effort to check whether a four-digit number is a prime compared to a three-digit number of prime. So, this is not typically a case and we know this; because when we do arithmetic, we now work on numbers, we know that we do not do this according to the magnitude, but according to the number of digits. So, if I add two numbers like this, then I will work from right to left; and I will do work proportional to the number of digits. So, I will say 9 plus 3 is 12, carry the 1 then I will say this is 11, carry the one and I will get 112. Now, if I had another digit on the right, then I would say 7 5 is 12, carry the one; then 9 plus 3 is 12 plus one carry the one; and I will get 1132, so this will be my new number.\"},\n",
              " {'id': '37394581-1cdd-4acc-82a3-ac0f76c4ff81',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'So, it did not take me again 10 times the work, to go from adding two-digit numbers to adding three-digit numbers. It only took me one more column of work in terms of moving carries. So, addition, subtraction, multiplication, division, square roots, anything that you do, will typically work based on the length of the number as represented in that format, say in decimal, or in binary, or whatever. So, it turns out that we are interested really in that representation. So, we are really interested in number of digits. So, we want to say that as the number of digits grows, how does the thing grow? Not the value. So, the number of digits close by one, the value actually in decimal will multiply by 10. So, we really want it to be proportional to the logarithmic logarithm of the number that is the input size. (Refer Slide Time: 28:52) So, now let us get back to the question that we had at the beginning, which is that I fixed the input size. But now if I look at an input of size n, there are very many different inputs of size n, and they all behave very differently. So, for instance, as I said before, if you are searching for an element in an list; it could be a very large list. But you could just be lucky the value that you are searching for right is at the beginning of the list. Or if you are using that binary search, it could be at the midpoint. So, with by just sheer luck on a particularly large input, your algorithm might actually execute very fast. So, ideally of course, we do not want to look at these extreme cases; these extreme good cases in particular, and be very optimistic about our algorithm. What we would like to know is, if I try out different inputs, what is going to be the typical behavior across the different inputs. So, we would like some notion of average behavior, on an average how does it do? Now, unfortunately, this is a tricky problem. So, what is average mean? It means that I have to take the time.'},\n",
              " {'id': '99f536d5-b919-49d3-8ae5-8223c8a6d676',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'It takes for many different things and then you know take the mean; but first, I have to enumerate these many things. How does it how do I generate all the possible graphs that I will look for a graph algorithm for instance? How will I possibly generate all the possible different objects of size n with different values for their sizes? So, even just enumerating all these inputs before checking how how they do on these inputs is hard. Secondly, there is a question of whether all these inputs are actually equally likely? So, are you actually going to look for every possible combination of objects of different sizes? Or are some more typical than others? So, then we need a probability distribution also. Not only do we need to list out all the inputs, we also need to say what is the likelihood of each particular thing; and then we have to take technically what is called an expectation. We have to multiply the probability of each input multiplied by the time it takes for that input and add it up. So, this is very hard to do, it is almost impossible; so that is why we cannot, in most situations, talk about average case. So, instead we go for an easier thing, which is the worst case? If we look at the algorithm, we can say this algorithm is going to get really stumped by this particular input, it is going to have to spend a lot of time to answer it. So, a case in point is when I am searching for a value in a list when searching for a value in a list, then when do I get stuck? I get stuck, when it is not there in the list. I have to go through the entire process, to find out whether the value is there or not. So, if I am doing a linear scan, you know the one where it is not sorted, and I go from beginning to end. Without looking at every value in the list, I cannot be sure that it is not there.'},\n",
              " {'id': '89bf1528-78de-418d-b6c5-1cbe7b0f46c7',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': \"I cannot just look at so supposing some your mother asked you to search for something in the kitchen; and you come back and say I looked on three shelves, and it is not there, they are not likely to get very far. So, you have to convince her that you looked in every possible shelf, and it was not there. So, if you do not look at exhaustively at every elements, and something is not present; you will not be able to tell, so this is now a worst case. You know that this algorithm takes a maximum amount of time; it must scan every element in my list, if the element I'm looking for is actually not in my list. So, this is something which we can typically extract by looking at how the algorithm performs. We can look at the algorithm and say, this is a worst case. So, now if we can do this worst-case analysis, this gives us some idea about how our algorithm does. It says that there are situations where it does this badly. The problem is this is pessimistic, because this worst case may not actually happen many times; so, it might be a rare case. But, the good thing about doing a worstcase analysis is you can actually get a reasonable upper bound, like those n log n cases. Even if all the SIM cards had invalid Aadhaar cards; the algorithm that we looked at there, we claim will take n log n time. Because it will take log n time per SIM card to determine that the Aadhaar card Aadhaar number is not there in the other database, and we do this for n. So, if you can get a good upper bound for the worst case that means that overall, you have a good upper bound. So, that is one reason why we are quite interested in worst case analysis. When we are able to prove good upper bounds, worst case tells us a lot. When we are not able to prove good upper bounds, worst case may or may not be a realistic estimate of how good or bad an algorithm is.\"},\n",
              " {'id': 'd9e4f19f-08bc-4aea-81b0-dfcada4ed85c',\n",
              "  'metadata': {'chunk_idx': 18, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'There are many algorithms which have not so good worst cases, but which work well in practice; because the kind of worst cases where they do badly, do not occur very often. (Refer Slide Time: 33:17) So, to summarize, there are two important measures that parameters that we look at, when we are thinking about the performance of our algorithm, running time and the space required memory required. But we will mainly focus on the time for because that is really more fundamental for us. So, we said that the running time is not just an absolute value; it is a function of the input size. So, we are interested in how long the algorithm takes on an input of size n, and we are interested in this up to some order of magnitude. So, we will ignore constants, we will kind of collapse things into basic operations; and not worry too much about what constitutes a basic operation. And finally, we are looking at this thing asymptotically. So, we are not interested in the performance for a fixed n, but rather what happens as n becomes large. So, as this function, as this algorithm works on larger and larger inputs, how does the behavior change? So, one of the things we saw in that table was that if we know something about the upper bound on the running time of the algorithm. It also tells us for what classes of inputs, it will be feasible. So, it is a very important part of using an algorithm to be able to tell what its running time is; because only when you know what is running time is. Will you know whether the problem we are trying to solve has a feasible solution using this algorithm or not? And finally, we said that there are many inputs of a given size, and we can hope in some situations to come up with some kind of average performance, but this is very hard. So, instead we will typically look at what is called the worst case; so we will reverse engineer from the algorithm.'},\n",
              " {'id': '4b03aebf-02fd-455d-a0ac-7c56daa2d9dc',\n",
              "  'metadata': {'chunk_idx': 19, 'week': 2},\n",
              "  'source': 'Analysis of Algorithms.pdf',\n",
              "  'content': 'The input that takes the longest time, and based on this we will give an upper bound on the running time of the algorithm.'},\n",
              " {'id': '6c4bf6cd-5011-419d-901c-6a0cfc690579',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor. Madhavan Mukund Insertion Sort (Refer Slide Time: 0:09) So, we have seen one intuitive algorithm for sorting selection sort. So, here is another one. So, remember the scenario you are the TA, the instructor has asked you to arrange the graded papers in order of marks. So, the second algorithm works as follows. So, you look at the first paper and you put it into a new pile, so you start off a pile. And we are now going to the earlier case, you made sure that the pile was going to be sorted by making sure that you carefully put the minimum value there, now, you are just going to put the first value you see over there. Now, this does not guarantee it is going to be sorted. At least for one, it is sorted. But now when you move the second paper, you have to put it correctly. So, if it if you want the minimum at the bottom of the pile, and the second paper is smaller, you have to slide it below. If it is bigger, you put it on top. So, depending on the value of the second paper compared to the first paper, you have to put it in the position. Now if you come to the third paper, there are three possibilities, so you have paper 1, you have paper 2 and now paper 3 could go below, both of them go above both of them or go in between, depending on what its value is. So, in general, you have to keep doing this for every paper for every paper, you take the new pile, which is already has been sorted because of way you created it. But now in order to maintain the fact that it is sorted, you cannot just dump it on top as you did in selections, or you have to find the correct position to insert. So, that is why this is called insertion sort because every time you have to insert the next value into the sorted list that you are creating. (Refer Slide Time: 1:33) So, let us do this Insertion Sort now, just we will do it from left to right.'},\n",
              " {'id': '738a2908-a1ca-4441-b2be-f064623453c0',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'So, I start from, I start building up the new list from the left, so I pick up the 74 and I create a new list. Now when I pick up the 32 because 32 smaller than 74, it has to go to the left. So my new list changes. So, 74 gets pushed 89 when I pick up next, is now bigger than everything. So, it goes to the top of the pile. So, it just stays at the top and nothing gets disturbed. For 55, I have to move it down and push it into correct place. 21 goes all the way to the bottom and pushes everything to the right. And 64 again goes in the middle and pushes 2 elements to right. So, this is the way of insertion. So, basically 64 came and inserted itself between the 55 and the 74. Hence, Insertion Sort. (Refer Slide Time: 2:21) So, an insertion sort, basically you build a new list, you pick the next element and insert it into this list. So initially, you build a list of size one, which is sorted by the fact that it has only one element, every one element list is by definition sorted. Now once you have two elements by insertion, you make sure that the two element list is sorted, then the three element list is sorted and so on. So, if you want to do this iteratively, rather than it is like our selection sort, if you do not want to actually build a new list, what you can assume is that so far, I have somehow scan from 0 to i minus 1 and I have inserted them so that they are sorted. Now I take this L[i] and I move backwards and insert it into the position, then I would have grown my sorted prefix up to here. So, in selection sort the sorted prefix was created by swapping the minimum with the beginning of the prefix. Here, instead, what we do is we take the next element, which is not in the correct, which is not yet part of the sorted prefix, and I inserted by moving it back, so the sorted prefix will shift a little bit and make space for it. (Refer Slide Time: 3:35) So, here is Insertion Sort.'},\n",
              " {'id': '2845b96e-55c8-4f64-b0cf-d38ba0ca405a',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': \"So, it is actually simpler to write than selection sort of you remember the amount of code there. So, what we do is that again, if we compute the list length, and if the length is 0, then we return otherwise, now we assume that we have already sorted again, like selection sort, there is an invariant that we are looking at position i, we assume that 0 to i minus 1 is already sorted. So, we start with the ith position and now, we want to go backwards. So, if L[j] is smaller than L[j-1], then we want to swap and continue. So, supposing I had, whatever it was, so say 21, 34 56, 72 and now, I find say, the next element that I am looking at is a 43. So, what I will do is that this is my i, so I will compare these two, and then I will exchange them. And now I will shift my attention to this position, because that is where I am. So, I will do j equals j-1, then I will compare these two and if necessary I will swap them. So, I will now make this into 56 and this into 43. And now I will move my attention to this position, and then I will compare these two positions and see and now I do not need to swap them. So, I will finish. Now the other possibility that could happen is that I could go all the way and eventually supposing that we not 43 but say 13, then eventually it'd have gone all the way there. And then I would have reached the beginning of the list. And now if I start looking at j minus 1, it is not a valid value in Python will throw an index error. That is why I have this extra thing saying; do not try to look at the previous element if you are already at the beginning. So, if you happen to insert and push it all the way back, then just stop, So either you stop, because you have reached the beginning, you have inserted it by pushing it all the way to the end, or you stop because there is a point beyond which it should not move anymore, because the previous element is actually this element is bigger than or equal to the previous element.\"},\n",
              " {'id': 'c3d81375-41dc-44db-af27-d77bce062249',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'It is not smaller than so this is the insert thing. And once you have done this by moving this L, L of i backwards to the correct position, you can claim that the sorted prefix has been extended from i to plus 1. So, the invariant has been extended. And in the limit, as I go to the last position, I will have extended the prefix to the entire thing. So, the whole thing will be sorted. So, this is Insertion Sort. (Refer Slide Time: 6:00) ` Now, we can also do this, just for illustration, recursively. So, we can assume that this thing is sorted as before, and now we can insert it, but the sorting happens inductively. So, this is also useful to just think about it, because you need some practice are also looking many problems that are actually easier to formulate recursively. So, in this case, it is not very difficult to do it both ways. But let us look at it recursively. So first, let us look at insert. So, we have this operation here which is actually inserting, so this is actually an insert operation. So, let us see how to do this insert in a recursive way. So, I have a list L and I want to insert a value v from the right. So, L is sorted and I am going to do exactly what I did before, I am going to insert it from the right recursively. So again, the base case is that the list is empty, I do not have anything to insert into, then the new list I get is just a list consisting of the new value v, I am inserting it. So, I have the old list plus the value v in the correct position, the old list is empty. So, the new list is just the value V in the correct position, which is the single. Now if the value v is actually bigger than everything in the list, then I just take it at the end. So, L minus 1, remember that L minus 1 is the last value. So, if v is not smaller than the last value, in other words, is bigger than equal to the last value, then I just take L and I append value v to it and I return that list. Otherwise, I will move in one step.'},\n",
              " {'id': '549130f3-087e-4147-86b4-5aa5e502bf20',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'So, I will recursively, this is where the recursion comes; I will insert v into the slice excluding the last element. So, I will try to insert it one step before, but I must remember to restore the last value afterwards. So I am basically bypassing so I have this. And I have checked that we must go before this. So, I am trying to now insert it in this segment, and then add back this, that is this value. So, this is a recursive insert and once I have a recursive insert, then insertion sort is very simple. So, I take the list, if it is empty, I return otherwise, I take the last value in the list and inserted into the recursively sorted list up to the n minus 1 value. So, this is a recursive Insertion Sort. Now there is another point to note between these two versions. So, if you look at the iterative Insertion Sort like the selection sought, it is actually maintaining L and updating it in place, because all these operations exchanging Li, Lj, Lj-1, all these things do not disturb the identity of L. So, though we are returning L. Actually L is also being updated inside the function. So this is actually sorting l as you go along. Whereas when you are doing a recursive thing, typically you are constructing a new list. So, each time you insert, you are constructing a new list. So actually, this function will not disturb it. You will get actually a new list back and the old list does not change. Now there is this. If you look at Python, actually, there are two ways to sort a list built in there is something called L dot sort and there is a function called sorted, which takes a list. So this is an in place sort or that is it will actually take L and update it to a sorted version of itself. But sometimes you do not want that sometimes you want to keep L but you want a sorted version of its values. So, then you should do this. So, then you will have to say L 2 is equal to sorted of L. So, L will not change but L 2 will be the sorted version.'},\n",
              " {'id': 'bb2109ea-25f1-4e09-b967-1b46639deae0',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'So, it is useful to have both versions. But in this case, just to highlight the recursive version of Insertion Sort will create a new list whereas the iterative version will actually update the list in place. (Refer Slide Time: 10:01) So, let us analyze these two separately so, analyses of the iterative version. So, the correctness follows from the invariant just as for selection sort. And if we look at the efficiency, well, the outer loop again, we have this outer loop, which executes n times and inside we have an inner loop and this inner loop is really this insert. So, we have to take L of i and insert it into 0 to i -1. So, 0 to i -1 has i values. So, I have to look at each of these values in the worst case, because it might be smaller than L0. So the, it takes i steps to insert L of i into the prefix 0 to i - 1. So therefore, by the same logic, as the selection sought, the first time, I have to do no insert, so it is 0, the first element is just really in place, the second element has to be inserted with respect to the first element, the third has been inserted, the first two and so on, so is 0 plus 1 plus 2 up to n minus 1. And using that same summation rule as we had before, because now it is from 0 to n minus 1, the summation is n into n minus 1 by 2, not n into n plus 1 by 2, but still, it is big O of n squared. So, that is how we can analyze insertions are the same complexity, essentially a selection. (Refer Slide Time: 11:15) So, in the case of the recursive version, so first of all, I am I am glossing over the correctness, but you have to argue that the recursive formulation is kind of correct, which is much easier to do usually than iterate, because just by looking at the structure of the function, you can claim that the recursive version is correct. But now, if I want to do an analysis, I have to analyze two functions, I have to take the time here, which I am going to call T of i and I am going to take the time here, which is called T of S, TS.'},\n",
              " {'id': 'ce8d4747-32ce-4311-985c-a616b039b1e2',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'So, Ti of n is a time taken by insert for input of size n, TS of n is the time taken by insertion sought for input of size n and Insertion Sort calls insert. (Refer Slide Time: 11:55) So, let us look at insert first. So again, if it is 0, it immediately returns the list containing only the new value of v. So, T of Ti of 0 is one and Ti of n in the worst case is going to have to call itself with n minus 1 values, and then do some work to append back the value that I skipped over. So, T i of n is going to be Ti of of n minus 1 plus 1 and if you unwind this, you will get 1 plus 1 plus 1 n times. It is not very difficult; you can just check for yourself that it goes down and get you get 1 plus 1 plus 1 n times. So, Ti of n nd this is the intuitive thing that we said earlier. Also to insert in a list of i elements, we said you need i steps. So, this is saying to insert in a list of n elements, you need n steps. What about T S? Well, TS again, when it is 0, you return immediately. Otherwise, you first insert and you sort, you have to do both. So, you have to sort an n minus 1 elements and you have to insert an n minus 1 elements. And so, you can unwind this and you can figure out that this thing is actually going to be 1 plus 2 plus n minus 1 and so this is going to give me n squared. (Refer Slide Time: 13:13) So, to summarize insertion sort and selection, sort of both intuitive algorithms and these are both algorithms that we use quite naturally. For instance, insertion sort is typically the kind of algorithm you use, if you if you ever play cards, you have to pick up a card and put it into the correct place in the hand that you have, usually use Insertion Sort, it is already sorted, you find the correct place to put it in. So here, in both cases, you create a new sorted list. But in this case, unlike the earlier one, where you actually the sorted list was automatically built up in sorted order here, you have to correctly insert it in the correct place.'},\n",
              " {'id': 'cdd64642-fff7-4463-9d34-b877fb180670',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 2},\n",
              "  'source': 'Insertion Sort.pdf',\n",
              "  'content': 'So, the worst case complexity is order n squared. But unlike selection sort, not all cases take time n squared. Because if you look at this insert operation, especially in the iterative cases, very clear. If you look at the iterative insert operation, notice that if this element is already bigger than the previous one, then I will not go into that loop at all. So essentially, for each element, if it is already in ascending order, the next element will be bigger than the previous element. So, I will not do this while at all. So, you could actually get almost a linear time algorithm out of this depending on how the input is originally. So, Insertion Sort, performance can be wildly different depending on whether you have an already sorted input, which is a good case. But it has to be sorted in the way if it is ordered in the opposite way. Supposing you are trying to sort in ascending order and it is given to you in descending order, then you are going to spend a lot of time because it needs time, you are going to have to kind of move it back to the beginning. So, supposing I have 7, 8, 6 as my lists, then I will put 7 then I will have to bring 8 in here. Then I will bring 6 in here, and I will bring 5 and 8, so then I am going to do a lot of work, but if it was 5, 6, 7, 8, Then I am going to put 5 then 6 is going to come after that, then 7 is going to come after that 8 is going to come after that. So, depending on the order, it may be much more efficient than the worst case of n squared.'},\n",
              " {'id': '671ab2b2-58ac-4694-807a-556f7e9aab3d',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhvan Mukund Calculating Complexity So, we have seen how to express orders of magnitude using this big O notation. So, before we get into some actual algorithms, let us look at some examples of how we would look at very simple pieces of code, and try to estimate the complexity of the code. (Refer Slide Time: 00:27) So, typically there will be two types of code that we look at; either it will be an iterative program, where we run some loops perhaps nested loops. Or it will be a recursive program, where the program calls itself. So, let us start with some examples of iterative programs. (Refer Slide Time: 00:43) So, here is a very simple program that we all know, which is how to find the maximum in an in a list. So, what we do is we start assuming that the beginning of the list; the first element is the maximum. And then we scan through all the elements, and wherever we find a value, which is bigger than the current maximum; we updated to that value. So, in one scan of the list, we just keep the track of maximum value that we see. So, the complexity of this is very easy to estimate. So, first of all we have to calculate what is the input size. So, a natural notion of input size for this is the length of the list; because obviously we have to find the maximum and a longer list, it will take more time. And so, it is reasonable to say that the input list size or the input list length is the parameter of interest. Now, this is a single loop, so there is a for which goes through all the elements in the list. And is going to always scan all the elements, because we have no particular information about this list; we do not know whether the maximum value occurs in the beginning or in the middle or in the end. So, this is an example of a function that will always run that loop n times, where n is the size of the input list. So, this has worst case order n, but it is also in some sense every case order n.'},\n",
              " {'id': '01ba5df0-1285-459a-802e-69a4ff909ad9',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': '(Refer Slide Time: 02:00) Now, let us look at the slightly more complicated question. We have a list of elements which is a no particular order, and we want to find out if the list has two duplicate elements; that is at least two elements which are the same. So, for this we run a nested loop. So, what we do is we scan through every element; so we have this whole list. So, when we are looking at element A Li, we compare we compare this to everything to the right of i. So, we have i plus 1 up to n minus 1; so that is the nested loop. So, for every i from 0 to n minus 1, for every j from i to n minus 1; so, maybe this should be i plus 1. From i plus 1 to n minus 1 we check whether Li is equal to Lj. If we ever find that Li equal to Lj, we will return False. If we find no Li is equal to Lj, then we would return True. So, this now is a nested loop; so there is an outer loop which runs n times, and there is an inner loop which runs. The first time it will run n minus 1 times, because it run from 1. If the outer loop is at 0, it will run from 1 to n minus 1; if the outer loop is at 1, it will run from 2 to n minus 1 and so on. So, it is going to run, the inner loop is going to run n minus 1 times the first time; then n minus 2 times the second time and so on. And finally, the last time is going to run once; and effectively there is going to be a zeroth thing, which we will not worry about. So, if you add up this then this turns out to be n into n minus 1 by 2. So, we saw last time that n into n minus 1 by 2, we just discard the lower powers of n. So, this is n squared by 2 minus n by 2; so we get rid of this, and we get rid of the constants. And we can declare that this particular nested loop has worst case complexity order n squared. Now, when is it going to reach at worst case is going to reach worst case if it is finally going to return True. That is it has to scan through every pair and decide that no pair is the same; so, there could be much better situations.'},\n",
              " {'id': '3bc9c873-14b1-40cc-9a0b-11b97895dbca',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'So, there could be situations where the very first element L0 is equal to L1. Then write at the beginning of this nested loop, you will find that it is False and return. So, there could be good situation, there could be bad situation. But as we said we are looking at the worst case situation. And in the worst-case situation here is one where there are no duplicates, and it is going to take n times order n squared. (Refer Slide Time: 04:22) Let us look it at more computational examples; so let us look at the problem of matrix multiplication. So, if you remember matrix multiplication, we take two matrices which are of compatible size; and then we have to multiply them to produce an element here. So, if this element here is in row i and column j, then we take row i in this matrix. We take column j in this matrix, and we multiply pairwise every element A i k with B k j. So, that is how matrix multiplication happens. So, in particular if this has m times n that is m rows and n columns; and this has n rows and p columns, then the output is going to have m rows and p columns. So, the input matrices have to have this compatibility; the number of columns of the first matrix must be equal to number of rows in second matrix. And the final output matrix will have the same number of rows as the first matrix, is the same number of columns is a second matrix. So, we will now use a very simple representation of matrices. So, we will take a matrix like this and represent each row as a list. So, the matrix as a whole is the list of list; so each row is the list, each row is one list. So, 1, 2, 3 is the first list; the second row is 4, 5, 6 is the second list; and the whole matrix is a list of these rows. So, now if you look at the code what we have to do is first extract m, n, and p. So, we are going to assume that A and B can be multiplied, so the number of rows in A, which is a number of elements at the top level of the list is m.'},\n",
              " {'id': 'ff1e7474-111e-487c-a5f2-2716ae313872',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'The number of columns in A is equal to number of rows in B. So, we can take the number of rows in B as n, and the number of columns in B is the number of entries in the first element of B. Because any of these rows has an equal number of entries; so we can just pick the first one. So, B0 will be the first row of B, and the number of entries in that the length of that will be p. So, we having got m, n and p; we have to setup the output to be a list of m rows with p columns. So, this is p zeros, 0 for i in range p will setup a list of p zeros; and then we took m of them, so we get m rows of p0. So, this is our initial output matrix and now we do this update rule. We take each for each C i j, we run k over all n; and we take A i k times B k j and add it to the current value of C i j. So, we go over the number of rows in A and the number of columns in B. So, this now is three nested loops, so this is going to take m iterations; this is going to take n iterations and this is going to take. Sorry, this is going to take p iterations, and this is going to take n iterations; so, m times n times p. So, the number of rows in A times a number of columns in B, times the number of columns in A, which is the same as the number of rows in B. So, we are going to take m times n times p. And if we are actually multiplying square matrices that is m is equal to n is equal to p; then this is going to be n times n times n or n cube. So, here we see a natural example of a problem which is actually higher than n squared; so, far we saw nested loops of n squared. Now, here is a problem which is actually n cubed. (Refer Slide Time: 07:43) At the other end of the spectrum, supposing we want to find out the number of bits or number of binary digits in the binary representation of n. So, the usual way we do this is we keep dividing by 2, and we get each bit as we go along.'},\n",
              " {'id': 'e4ef36e5-8014-400a-8324-393ac2f4a12c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'So, we can just keep a counter initialize it to 1, and we just want to see how many times we can divide by 2 till we reach 1. So, we keep incrementing every time we divide by 2, and we stay above 1; and finally, we return this counter. So, this is what we said before when we looked at this binary search and so on; you know when we were halving the interval. The number of times it takes to (have) n down to 1 is basically log n; so, this is going to take order log n steps. So, for a list type of algorithm, where the input is of size is n; a function which takes order of log n would be considered very efficient. But we said that first number theoretic problems like this, we should not count n, the number n itself as the input size. We should rather look at how much space it takes to write down n, which is roughly the number of digits in n. So, we are saying really that the number of digits in n is proportional to log of n; and the output is taking time log n to compute. So, this is actually a linear function, even though it looks like it is taking log n time. Log n itself is the size of the input and that something we have to keep here. Now, one thing you realize is that we might be writing it in decimal and taking it out in the binaries. So, if you say log 10 of n and log 2 of n; you might ask what is the connection between these two. It is not difficult to see that these are connected by a constant factor. So, if it is log anything of n to any base, its order of log any other base to n; so, this is also log of n. So, the input is say in base 10, so the size is a number of digits written in in decimal. The output is logarithmic to base 2; but they are both of the same order of magnitude. (Refer Slide Time: 09:45) Such a final example let us look at a very famous problem; which is like a puzzle. So, you are supposed to take these discs which are of different diameter, and are arranged in decreasing order of size.'},\n",
              " {'id': 'c12654c8-dd17-4851-879d-adfb9f7cb7e0',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'And it is supposed to transfer them from one peg to another peg. But the constraint is you never allowed to put a bigger peg on top of the smaller peg. So, supposing you move the smallest one here, then you cannot put the next biggest one on top of it. So, you have to move biggest one to the other one. So, now the question is how are you going to transfer these n pegs from A to B? So, you are given a third peg. So, since you are given a third peg, in this situation what you can do, is supposing this is smallest peg, this is the next biggest peg. Now, you can move this here and create a pile of two on that peg. So, this is how you can use the third peg in order to make space to move things around. So, there is a very well-known recursive solution to this, and that says that I have these n pegs. Now, if I keep this peg at the bottom, this largest peg at the bottom; then anything can sit on top of it. Because it is a largest disc at the bottom, anything can sit on top of it. So, I can as well assume that this disc never moves. So, now I take these n minus 1 pegs which are above, and I transfer them to the third one. Now, this is a smaller problem. And effectively I have all three pegs at my disposal, because I do not have to use this one. When I gets to the bottom, I can just put anything on top of it. So, having done this, now I have I have moved this n minus things here; now, I can move this biggest one to the peg that I wanted to be. And now I can again solve the same problem, recursively move this n minus 1 things here. So, I can move n minus 1 pegs from A to C which was supposed to be the temporary peg, thinking of B as my intermediate peg. So, I do not use B as my final peg; then I moved my biggest disc to the final peg. And then I can now again use the original thing A as intermediate thing, and move everything back from C to B. So, I have moved to n minus 1 things from A to C, one disc from A to B, and n minus disc from C to B.'},\n",
              " {'id': 'b3bb37c9-ab8a-49de-bfec-120b0082d4ab',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': '(Refer Slide Time: 12:14) So, when I have a recursive solution like this, then the best way to express the complexity is to think of it, as a recursive formulation or a recurrence. So, let me say that the number of moves M, for an input of size n; if I have to move n discs M of n, it is a function of n. So, if I am moving one disc to move I can always move it; means one disc means I start with three pegs and only one disc. Then I can move it in one move; I do not have any constraints, so M of 1 is 1. On the other hand, if I have more than one disc, then I have to obey this recursive solution. So, what I do is I first move this is from A to C, I move n minus 1 disc from A to C. then I moved this one disc from A to B, and then moved n minus 1 disc back from C to B. So, I have to solve this n minus 1 problem twice, and in between I have move one disc. So, if I combine this and this which are the same, I get two times n minus 1 plus 1. This gives me a kind of recursive dependence of M of n on M of n minus 1. And the usual way to solve this is to expand or unwind the recurrence by substituting. So, I start with M of n being two times n minus 1, n minus M of n minus 1 plus 1. And now I can do the same thing to; I do not know what M of n minus 1 is. But I can apply this same formula to that. So, I get two times M of n minus 2 plus 1, so this comes from expanding this, so, what is inside this bracket. And I have this outer two and I have this plus 1. So, if I expand this out again and kind of regroup; this 2 times 2 becomes 2 squared. These 2 times 1, plus 1 is here; and I have of course M times M of n minus 2. So, this is the kind of cleaned up version of this one step expansion. So, now I do I expand this again; I will get something in terms of M of n minus 3. So, I will get 2 times M of n minus 3 plus 1; and again if I combine and clean up, I will get 2 squared times two is 2 cube times of M of n minus 1, n minus 3.'},\n",
              " {'id': 'd97cc004-7919-42ee-ade2-db0fb44b596d',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'And then I will get two squared times 1 4, and I already I have a 2 and I have a 1. So, 4 plus 2 plus 1 and if you read this carefully, it is actually 2 squared plus 2 to the 1 plus 2 to the 0. So, each time I will get a higher power of 2; next time I will get plus 2 to the 3. And if you know this or you should check this, 1 plus 2 plus 4 is 7, which is 8 minus 1. If I add 8 to that I get 15, which is 16 minus 1. So, if I add up the powers of two up to n, I get 2 to the power n plus 1 minus 1. (Refer Slide Time: 15:02) So, I should use that fact to say that if I expand this k times, this 2 to the 3 will become 2 to the k. This n minus 3 will become n minus k; and this summation will be 2 to the power k minus 1. Because I would have 2 to the 0, plus 2 to the 1, up to 2 to the k minus 1. So, this becomes the general form after k steps of substituting this recurrence; and eventually when I reach 1, I will have 2. If I do this n minus 1 times, I will have n minus minus 1, which is equal to 1. So, after n minus 1 steps if I take to be n minus 1, then I will get M of 1, and this k is again n minus 1. So, I have 2 to the n minus 1 times M of M of 1, plus 2 to the n minus n minus 1. But what is M of 1? M of 1 is just 1; M of 1 was known to be 1. So, this is just 2 to the n minus 1, plus 2 to the n minus 1 minus 1. So, is this plus this minus 1, which is two times this is 2 to the n minus 1. So essentially, I have now by kind of laboriously substituting and coming down to the base case, I have argued that this tower of Hanoi solution actually takes exponential time. So, if I give you n discs is going to take you, 2 to the power of n minus 1 moves to execute that recursive solution, in order to transfer all the discs. (Refer Slide Time: 16:27) So, to summarize what we have seen is that if we are looking at iterative programs with loops, then what we really need to understand is how many times the loops execute.'},\n",
              " {'id': 'dce8f459-b4be-4872-9bd9-0dd8c4d1c3a9',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 2},\n",
              "  'source': 'Calculating Complexity.pdf',\n",
              "  'content': 'If there are no loops, then it is just a straight line of code is equals to take the constant amount of steps. So, any interesting program will have a loop which will depend on the input. We will run through the loop; if it is a for loop, it is a very easy to calculate. Because we know exactly how many times loop is going to execute. If it is while loop, then we have to be a little bit more careful to calculate how many times it will take for the while to become 1. So, this is something that we saw, for instance, when we looked at that number of digits thing; so, here we had a while loop. So, when we have a while loop, then we cannot tell a priory that is going to take n steps. So, we have to figure out how long it is going to take for that condition to become False and the loop terminates. So, if we have an iterative program, we are basically looking at the loops. If we have a recursive program, then we have to understand the recurrence. So, we have to say, how the time complexity for the full input depends on the recursive or the inductive parts into which you are breaking it; and then we have to solve this recurrence. S o , w e t y p i c a l l y s o l'},\n",
              " {'id': 'c0ff00c1-8b0e-4c69-a335-0632c12cf70d',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'Programing: Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Lists in Python So, we saw, that a list is a flexible structure, which is distributed in memory, but which allows us to manipulate things locally so that we can add and delete items in the middle of a list at low cost, as opposed to an array, which actually happens as a contiguous block of memory. So, it is efficient to randomly access the ith element of an array. But if you have to insert or remove things in an array, then you have to shift everything up or down by one position, so that can be an expensive task. So, arrays are not flexible, but they support random access, lists are flexible, but to get to the ith element, you have to start from the beginning and spend time proportional to the position you want to reach. So, now in Python, we have this built-in list and we also have a library called numpy, which provides arrays. So, let us try and understand how lists actually work in Python. (Refer Slide Time: 0:59) So, we saw that, sequences can be stored either as lists or as arrays, so lists are flexible. You can insert and delete items in the middle of a list quite easily, but getting to the ith element takes time proportional to i. So, in general, if I have to walk across a list to get to the nth element will take me order n time. On the other hand, arrays are declared contiguously in memory. So, we are starting from the first position, we can compute the offset of the ith element and get there in one step. So, these arrays support random access, but because they are contiguous and must remain contiguous, we cannot have holes in them. So, if we want to insert an element, we have to push everything. If we delete an element, we have to shrink the array, and this can take again order n time.'},\n",
              " {'id': 'ade92e11-d08b-407c-903d-1e0cceb1d41d',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': \"So, what we said is, when we do an algorithm like sorting or searching, we do these operations, of looking up A[i], A[j] exchanging them, and so on, so we need to be conscious about which implementation we are using of this list or sequence in order to make sure that the analysis we do is accurate. So, these are the classical understanding of listen arrays, but now we have a built-in list type in Python. So, the name, of course, suggests that it should be a flexible list. And we do have operations which allow us to insert items at the beginning of the list, to replace a smaller slice by a bigger slice or a bigger slice by a smaller slice, so we can shrink and grow lists quite arbitrarily. So, the question is, is the built-in list type in Python, really a flexible linked list of the type of implementation that we saw? We also have separately a library called numpy, which provides arrays in Python. So, are these numpy arrays actually faster than lists in Python? (Refer Slide Time: 02:47) So, let us answer the first question. In fact, lists in Python are not implemented as flexible linked lists, rather, they are actually implemented as arrays. So, when you define a list in Python, Python actually allocates a contiguous block of memory, which is much larger than the list is because the list, of course, when you start, it is typically empty. So even if you declare an empty list, you get a large block of memory. And till that memory fills up, the list will be accommodated in the array. And once your list grows beyond the size of the array that you have, then you will get a new array, which is double the size. So, effectively, when you're working on a list and the size does not change much, you are manipulating it within an array, and when you get a new list, you grow the list beyond the size of the array, then you get a new array, which is double the size. So, inside this array, so now we have this array, which we think of is a block of memory.\"},\n",
              " {'id': '6149f1af-caf3-4a58-b446-e5e4c8d9da4b',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, this is my L0, L1 and so on. And in general, there will be at the end of the array, some unused space, because the array that is given to me is bigger than the list that I am actually storing in it. So, what Python does is, it keeps track of this position. So, it keeps track of the last position. So, now, if you want to add an element to the array, it is very simple. You just remove, you put a new element here, you move this pointer here, and now your free space starts at this point. So, extending an array by adding an element to the right is easy. So, extending a list in Python by adding a limit to the right is easy provided, of course, you do not overflow that boundary. So, Python also provides an operation called pop, which returns the last element. So, it removes the last element and then returns it back to you and shrinks the array by one. And for the same reason it is also easy, because I just have to move this pointer up, declared this to be free and return whatever was there as the return value. So, append and pop are constant time. Now, we said that it is constant time, but what happens when we have to expand the array and create a new array? So, as I said, at some point, you will have to double the size. So, doubling the size might require you to move, you may not have space right here, so you might have to double the size somewhere else. So, you might go from an order n size array here to an order 2n size array somewhere else in the memory. So, you might have to copy all these n elements and then extend it to the n plus oneth element. But that is a linear operation, that is order n, and that happens once. So, it happens once, when you do this extension beyond that. So, if I amortize the cost, in order to reach that order n operation, I must have grown the array n times. So, those n adds could have been distributed among the n appends. So, effectively each append we can think of as having taken two steps on an average.'},\n",
              " {'id': '2610565e-deca-4829-a26b-486572d0ebdd',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, I do one plus one plus one n times, and then I do one single n. So, this is what happens when I have to expand beyond the array that is given to me. But if I think of this whole thing as a chunk, this is around two n. And how many operations have I done? I have at least n operations. So, 2n steps have been spent over n operation, so each step, each operation roughly took two steps instead of one step. So, that is what we mean by saying that this append and pop. Because they happened at the end of the array, and only occasionally they cause you to spend a little bit of time transferring the contents from a smaller array to double the size, these are amortized 0 order 1. Oder 1 means, remember, order 1 means it is less than c times one for some C, so it just means it takes a constant amount of time. So, there is no dependence on the input. On the other hand, as you would expect, because this is an array, and exactly the same problem, happens in an array. So, when I have a sequence, and I want to delete this element, I have to shrink everything from the right to the left or if I want to insert an element here, I have to push everything to the right. So, insertion and deletion in the middle of an array will be expensive, because I have to push everything one position to the right or pull everything back one position to the left. So, insert and delete actually require order n. So, effectively, lists in Python are more like arrays in a normal programming language than lists in a normal programming language. So, a list in a normal programming language will behave like the flexible linked list structure that we described earlier, where you have these nodes, which point to the next node and so on, and you start from the head, and you have to navigate your way through the list, which will be scattered in memory. That is not how a Python list works.'},\n",
              " {'id': '64b8a9e6-a241-4651-9b16-cd5bc874296a',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, we have programming notation, which makes it look like we are dealing with a flexible structure, as I said, because you can take a slice and shrink it and you can take a slice and increase it. You can insert in the middle of an array, you can pull them apart, put them together. So, it looks like these lists are actually flexible, but the implementation because most often we use them like arrays, the decision in Python is to keep these as arrays and the underlying implementation. (Refer Slide Time: 07:48) So, arrays are useful for representing matrices. So, we saw that, in the maths course, that one of the reasons you will use a matrix, which represent a graph, and we will come across these in this course as well. So, matrices will play a role in terms of dealing with graphs. So, if we take a two-dimensional matrix, for example, which has a row with 01, and another row with 10, the typical way you will represent it using list is to write it out row by row. I mean, you could also do it column by column, but the convention is row by row. So, you write out the first row. So, this first row is this one, and then, this second row is another list, and this whole thing is inside a nested list on the outside. So, it is a list of lists. So, each inner list is a row, and the outer list corresponds to the set of columns. This is of course, a square matrix, but in general, you could have a rectangular matrix, so each row will have a fixed length, and the number of columns will be fixed, but the two need not be equal to each other. So, what happens if we use lists in Python to represent arrays? The problem comes because lists in Python are mutable. So, let us look at this example. Supposing we want to create a three-dimensional array, which looks like this. It has a 3x3 matrix, which has all zeros. So, it might be tempting to first create because it is easy to write down one row. So, we have one row, which is represented as we said by a list, so each row is a list.'},\n",
              " {'id': 'b033d2d3-973b-47f8-a24a-2fedc35ce682',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, we write down the first row and call it 000 and then we just write three copies of that, and say that, this is the first row, second row and third row, so now this is an array, which has three rows and three columns. So, the length of each list inside is the number of columns, and the total number of lists inside is the number of rows in my array. So, this is a zero matrix, but there is a problem. (Refer Slide Time: 09:48) So, supposing I now take an element in this matrix. So, I take this matrix, and I take the center element. So, the center element, so remember, the columns and rows are numbered from 0, so This is 0 1 2 and this is 0 1 2. So, I take this element, and I assign it to be 1. So, if I now do this, so I just take 0 matrix 1, 1, that is the list number 1, and entry number 1, and assign it to 1. And if I print the 0 matrix, now suddenly you find that the first row and the last row also have a 1. And the reason for this is because of this way that lists are organized. So essentially, we had this 0 list, which was pointing to 0,0,0. And then when I created 0 matrix, and I said, I have three copies of 0 list, essentially, I have all these three entries pointing to the same object, the same item in memory. So, now, if I take this through any one of those copies, like if I go through here, and then I replace this by 1, then unfortunately, for the other copies, also that copy is replaced by 1. So, when we have this mutability we unintentionally, sometimes alias different parts of a list or different lists together. So, when we set up a matrix, we have to do this quite often. We quite often have to initialize a matrix to an mxn or an nxn matrix of 0. And it is tempting to do this, but it is dangerous, you should not do this. So, what we can do, for instance, in Python, the most common way to do this is to use this list comprehension.'},\n",
              " {'id': 'bc3bfc69-7048-46db-82d6-3874d853e8e8',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': '(Refer Slide Time: 11:31) So, what you say is that I first create a row of two independent zeros, I mean, three independent zeros, and I do this three times. So, each of these zeros is now a separate 0, and now this actually creates an entry in which all the 9 entries are pointing to distinct elements in memory. So, you need to use this kind of cumbersome notation, in order to work with matrices in this list language of Python. So, even though the underlying representation is as an array, the notation for lists in Python does not have a convenient way of actually expressing an array as we would like. .we cannot just say I want an mxn array, I want an mxn list, I have to grow that list from an empty list. (Refer Slide Time: 12:16) So, the solution is to use numpy, which is a library, which provides among other things, arrays. So, this is the normal way you use it. You import numpy, and just to save on space, you normally import it with an alias, so you call it np. So here, for instance, is the same code that we wrote before to get a 0 matrix, which is 3x3 np numpy has a function called zeros. So, you pass the shape that you want, so the shape tells you how many rows and how many columns. So, this is a function, which returns to you a 3x3 matrix or a 3x3 array of zeros. So, in general numpy allows you to create an array from any kind of a sequence. So, here for instance, is the same thing, which, so this is, this should not be. So, you can say for example, new array is equal to np dot array, and we can take our earlier thing. For example, remember 0,1 and 1, 0. So, this was the, we had said earlier that this represents that matrix 0,1,1,0. So this nested thing. So, this sequence, if I pass it to this array function in in numpy, this will actually produce an array with exactly that structure.'},\n",
              " {'id': '56f81fac-d1ae-4c24-ba44-5dca9a3245e8',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, you can take any normal Python sequence, either a list or even a tuple, and pass it to this array function, and it will create one array, which is called the corresponding entries, but in in numpy’s array format. (Refer Slide Time: 13:56) So, one of the ways that we can construct lists from say, i to j is to use the range function, so the same thing is there in numpy. And just to distinguish, it is called a range because it is an array range function and not the range function. So, this will create an array, which has 0,1,2, a single row with 0,1,2,3,4. So, then the representation is, I mean, the notation is similar, but typically you have these extra a’s hanging around to tell you that this is a numpy function. (Refer Slide Time: 14:28) So, one advantage of working with numpy, is that, you can actually do operations on arrays as a whole without writing these nested loops. If I have to operate on all elements of a nested list representation, then I have to write a for loop which runs through all the rows and all the columns and updates every ij. Now, if you want to say, take two matrices or two arrays of the same size and add them together element by element, then normally in mathematics, you will write a plus B. And now numpy supports that. So, for example, this will take the array a and multiply every element in A by 3, so this is like a scalar multiplication of every element in A. So, this takes, for example, if I had 1,0,0,0,2,3,1,4,7 then this will take every entry and make it 3 times. So, this will become 3, this will remain 0, this will become 0, this will become 6,9,3, 12,21. So, this will be the new array which is three times A. And then if I add b, then each element wise will become added and I get a new array C out of this. So, this kind of a block operation can be expressed on arrays exactly as you would do with numerical variables. And you can also do matrix operations. So, this is the numpy function with just the matrix multiplication.'},\n",
              " {'id': '2f4d6808-6335-4390-b90c-7559219c3f1f',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'So, if you remember matrix multiplication, you take a row of A and a column of B and then point wise you multiply them and add it up right. So, you want A ik times B kj, for all k. And this will give me one entry C ij. So, the ijth entry in the target is obtained by taking the ith row the jth column and then adding them up multiplying and adding them. So, these things are very useful because we use these kind of operations a lot in many of data science manipulations. So, for this reason, numpy has become a very popular library in Python. But for us in this course, our main interest in numpy is that it allows us to write these kinds of definitions to create NxN arrays quickly without going through this cumbersome list comprehension notation and worrying about whether we are in this mutable aliased mode where something will accidentally disturb something else. (Refer Slide Time: 16:44) So, to summarize, Python lists despite the name of the data structure, Python lists are not lists in the conventional sense that computing calls lists. They are not these flexible linked lists with nodes pointing to each other, rather, they are implemented as arrays with a pointer to the end of the list within the array. So, the list starts at the beginning of the array and it gradually grows. And when it overshoots, the amount of space allocated, then the array that is used for the list will be doubled. So, Python will automatically allocate a new array of double the size, move everything there and let you continue. So, in this process, append is cheap, because I am just adding and moving appointed to the right. And even though I periodically overshoot and have to go across we said this amortized cost remains order 1. On the other hand, if I have to insert something as we will see, if I have to insert something, then I have to push everything. So inserting is actually an expensive operation, because it takes order n time.'},\n",
              " {'id': 'b3b7de49-a027-471e-9946-1b78340035f6',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 3},\n",
              "  'source': 'Implementation of Lists in Python.pdf',\n",
              "  'content': 'Then we said that arrays or matrices as we will use are actually can be represented as multi-dimensional lists, but then we have to be careful about this mutability, aliasing and all that. S o , f o r t h'},\n",
              " {'id': '73351b2a-6b52-4924-8c98-69d5f6ce622e',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'Programing Data Structures and Algorithms using Python Professor Madhavan Mukund Designing a Flexible List and Operations on the same (Refer Slide Time: 00:10) So, we talked about flexible lists, which can expand and contract, I mean, which are like a train. So, they have this kind of linked structure. So, let us try and understand, how we might actually program such a list. So, a list is now a sequence of nodes. Each node consists of a value, and something that points the next node. And we argued that using this plumbing operation, it is easy to insert and delete locally. But in order to navigate to a particular point in the list, you will have to start from the head each time, so it is going to take you some time to reach that point. So, the typical way to represent these is as collections of these nodes. So, we need something which represents one node in the list, and then we will string them together. (Refer Slide Time: 00:52) So, we define a class node, and this class node will have two parts. So, it will have one part which stores the value, so let us just call itself dot value. So, remember, this object-oriented thing that we discussed in the very first week. So, each object refers to itself through this name, self. So, self dot value is my value field, and self dot next is my name for the next node in the list. So, if there is no next node I will just use the default Python value none. So, this is the initialization. So, when I initialize a list, I pass a value, I set the value to that list. And initially, this list has only one node, so the next is always going to be none. So, one of the important things we have to deal with is this notion of an empty list. So, when we write something like in normal Python, if we want an empty list, we have to write this. Because if we do not write this, and later on we try to add something to it.'},\n",
              " {'id': '0e3f3c15-6445-44e7-8214-c9ec70816913',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'Supposing I want to start with a list with the value 0, and I write l dot append 0, then Python will say, I do not know why you are allowed to use append on l, because you have not told me it is a list. So, we always need to initialize value to an empty data structure, data type of that value in order to tell Python, what is the legal operation allowed on that? So, this initialization is important. Now, the problem with Python is that there is no way to tell it, other than to initialize it. There is no way to say l is a list. So, other programming languages, if you are familiar with programming languages, like C or C plus plus or Java, whenever you use a variable or a name, you first have to tell the compiler which type of value it is going to hold. So, you have to say I am using i and is going to be an int, I am using l and it is going to be an array. So, this information is there independent of the value that you store, so there is a separate piece of meta data, as it is called about your variables, which is known in advance. Now in Python, we do not have that. So, we have to be able to represent these empty lists. So, in our context, think of a node as a box with two parts. So, this is our value and this is our next. So, we know that if it is a singleton, the next is none. So, what we will use as a convention is that if the value is also none, then this is an empty list. The only situation in which you could have a node, which has no value is when it is the first node of an empty list. And if it is the first node of an empty list, by definition, there are no other nodes. So, this gives us this motivation for this function. So, this says, is the list I am looking at empty or not. So, it says, if the value is none, yes, it is empty return true. If the value is not none, then it cannot be empty, return false. And if the value is none, implicitly, the next must be none.'},\n",
              " {'id': 'b1118f61-0661-4727-b1ee-ebee11eacb7b',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'We will never ever check the next if the value is none because we will believe it is an empty list. So, the empty list is one where self dot value is none. So, the way we would use this, remember, is to just call this node class, as we would call a function with an argument, which is optional, because we have this default value. So, if we call node with no argument, we get an empty list. The self dot V by default is set to the value not none. On the other hand, if you call it with a concrete argument like a 5, then we will get a node containing this is in Python notation, this would lead the list 5, and this would be the empty list. So, this is what we are saying. So, if I now for instance, check it, then it will say that l1 is empty is true and l2 two is empty is false, because l1, if I look at it self dot values none, l2, if I look at it self dot values, not none. So, this is our basic building block. So, this is how we build up a list. Now we have to think about how we will actually grow and shrink these lists. So, in a list, there are many different ways you could define the operations, but the most conventional thing to do with a list is to take a list and either add at the end, which we call append or insert at the beginning. Now you can modify it to same, I mean, you can apply the same principle to insert anywhere in the middle also, but the two interesting cases are append and insert and they have different properties. Actually, append turns out to be much easier than insert, as we will see. So, typically, what we have through this kind of thing is, say l1 is pointing to the first node in the list, the first node is pointing to the next node, and so on. So, what we want to say is take the node point the list pointed to by l1, and append a value V to it. (Refer Slide Time: 05:30) So, we want to add a value at the end of the list. So, what are the possibilities? So, the first possibility is that there is nothing in the list, the list is empty.'},\n",
              " {'id': 'f2a52759-37d8-44a6-8de3-617d76e0b223',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': \"So, if the list is empty, then we basically take this list currently looks like none, none and we change it to the list looks like V none. So, we just update the value field to v, and we leave the next field unchanged and we are done. Now, if we are at the end of the list, the last node will have some value, and it will say, there is no, there are no more nodes in this. So, in this case, what we do is, we create a new node. So, we call this node constructor, as it is called. We create a new node, which will be the form V comma none because we need a node, which has the value V that we're trying to append, and we set this to point there. So, self dot next, if self dot next is none, which means, I am at the end of the list, then instead of making it none, I make it point to a new node that I create now. I create a node with the value V and I make this node point to that. And if I am in the middle of the list. So, supposing I am here, and it says that there is, there are some more nodes to my right then I just postpone the work by saying that, appending to this node is the same as appending to the next node, so I just say, append on the next node. So, if I say l dot append I am appending on the first node, the first node has an X node, it will say, okay, append to the second node. Finally, we'll keep walking down in this recursion will end at the last node, the last node will say, why have no next node, and then it will create the node and append it. And there is this base case where the initial node itself was empty, where I just update, I do not create a node, I just change the value from none to v. So, this is a very straightforward recursive implementation of append. It has three cases. Am I empty? Am I the last node or if I am not the last node then recursively call? So, in this case, it is quite easy to think of how you would do an iterative implementation, essentially, you do the same thing. I check it is empty.\"},\n",
              " {'id': 'a7fb0183-a595-40b9-a40b-775c3933e3cb',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'If it is not empty, I keep following this next pointer. I can create a loop, which says, I keep going to next, next, next until next is not then I know I am at the last node and then I do the same thing. So instead of using recursion to go this way, I can use a loop saying while the next pointer is available, go to the next pointer. (Refer Slide Time: 08:04) So, here is an iterative implementation. So, if it is empty, replace the value from none to v, otherwise, we loop through this next until we find the last node. How do we find the last node? Next is none. And then at that point in the same way as we did before, we append. So, here is the thing. So, if it is empty, we do the same thing. If it is empty, we just change the value to V and return, otherwise, we have to start here and we have to keep walking down. So, we use a new name to do this walking, so we call it temp. So, we first point temp the first thing. Then if temp has a next, then we will move temp to the next, so temp will become temp dot next. So, temp will keep moving from one node to the next, so long as the next is defined. It will exit from this when it reaches the last node. So, when I have here and I reached temp, supposing this is the last node, then it will have no successor. So, when temp dot next is none this while loop exit. So, now I know, the temp is pointing to the last node in my list. At this point, I just create a new node V as before and I make temp dot next point to it and I am done. So, this is an iterative version of that append. So, both of these are very simple, because we do not have to do anything except for in some sense go to the end of the list and add something at the end. (Refer Slide Time: 09:16)  For the other operation, as I said is insert. I want to insert not at the end of the list, but at the beginning, so I call that append, I call this insert. So, I have a value V, which I want to stick I want to put this here. So, what is the logical thing to do?'},\n",
              " {'id': 'a3db42cb-3621-4e64-a5dc-2a84f75c97d0',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'The logical thing to do, is to say, that I made this head point there and V point here. Now, unfortunately, this is a problem. Because inside a function, when I call an object, it is like calling a function. If I call a function with the value l which is like this, something which can change, but if I update it inside then it gets lost. So, if I, it will get create, I will lose this list. I cannot take the head and point it inside the function change what is pointing to it. It is the same as if I pass an l to a list processing function and I reassign l inside, then the l inside becomes different from the l outside, so outside nothing changes. So, the same will happen here. If I go inside this thing and I change where head is pointing to, then the old head will remain where it was and this whatever changes I make will happen inside the function, but will not happen outside. So, this insert will have no global effect. So, this is not allowed. So, this would be the easiest solution. Just stick a new node at the beginning and point to that instead of the first node it that, but that is not a lot. So, now what do we do? So, what we do is, we have to do something which is kind of not obvious initially, which is, we have to so, what is the status of this node? Remember that in a linked list, the physical position of a node makes no difference only the logical position matters. So, what we want to do is make this new value V logically come before V0, but we for us, the first value you point to is always in this node. So, the first place I point to must contain V. So, I need to move this V here. But if I move this V here, then where does the V0 go? Well, I have made space for it, so I will move this V0 there. So, what we can do instead is we can exchange the positions of V and V0. So, I create a new node, I have a new node, which I want to add, which has a value V, and I have a list with such as V0, so I just swap the values. So, the nodes are still where they are.'},\n",
              " {'id': 'ea5573fb-9a50-4aff-9c8e-b21506f8fadb',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'So, my head is now pointing to a new list, the same old list rather, which starts with the new value V and then goes to V1 , and V0 is now sitting up there. But now I can do this plumbing, I can make V0, V point V0 and V, so I can make this point there and this point here. So, once I do that, then I am done. So, this is how we insert at the beginning of the list. So, what we do is we swap the value of what head is pointing to with the value we want to exchange and then we do this update. (Refer Slide Time: 12:04) So, here is some code for that. It is very straightforward it is not very complicated. So again, if it is empty, inserting into an empty list is equivalent to appending it to an empty list. If I have an empty list, it is the first and the last element of the new list. So, I just update the value to V. Otherwise, I create a new node, which I am going to add to my list at the beginning, but I cannot add it directly. So, what I do is, I first exchange this value and the value of self. So, self, remember, is pointing to the beginning of the list. So, self is the zeroth value in my old list. So, I exchange the value of the old list head and the new node that I have created and then I do the swapping of links. So, I make the new node point to the next of the old list, and the new thing next point to the old. So, I take self dot next and make it point to the new node, and I take new node dot next. So basically, I had this link, and I have this new node here, so I am now going to change this. So, I am going to say that, instead of pointing here, I am going to make this point there, so that is the self dot next is equal to new node. And instead of having, I mean, right now, this is not pointing to anything, because it is a new node it was none, so I am going to make that point here. So, I do this plumbing, and I am done. So, this is my insert. (Refer Slide Time: 13:19) What about delete?'},\n",
              " {'id': '65cef0b2-4a0c-4d78-bd19-e746f32b2416',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'So, now delete is more interesting when you actually tell me what value to delete. When I insert a value, it is I said that we will look at the case where the insertion happens at a fixed place, either the beginning or the end. The end is append, the beginnings insert, but deleting the first value of node is not typically, of a list is not typically what we want. What want to say is, I give you a value V, please delete it from the list. Now, deleting from a list can mean many things. It could mean, delete all the values or delete the first value or delete the last value or delete some value, so let us just for the sake of concreteness, say that when we say delete a value V, we mean, remove the first occurrence of V. And in particular, if there is no V in the list, then it will just do nothing, right. So, deleting a V from a list that does not contain a V does not create an error it merely does nothing. So, what we have to do is we have to go through the list and look for the first V, and then we have to delete it. Now, the problem is that, I do not know if you are seen, but old style people demolishing a house they will, you find these people who are workers who are standing on a wall, and they will have a hammer and they will be hammering the wall below them. And I have always wondered how they know when the wall is going to be safe enough to stand on. And when they have to jump off and then finish the job standing, but it is kind of easier to hammer down like this. So, in the same way, if you are sitting on the node, which has a V and you want to delete it, it is kind of difficult. So, if you are sitting here and you want to delete this V, this is not a good way to do. So, what you want to do is actually sit here and look ahead and see is the next node that I want to delete. And the next node is the one that I want to delete then I will make myself point to that node, so I can bypass it.'},\n",
              " {'id': 'eee4ad6f-3fdd-480d-826b-6922bf4b2641',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'So, this bypassing is basically just a way of saying that my next pointer points to the node with V. Instead, I make it to point to the node after that. And how do I know that because the node with V tells me where the next node is. (Refer Slide Time: 15:26) So, let us look at this code. So, this is the bypass. So, what we say is, that I want to delete the value V. So, if I, if the current value is V, so this is a difficult case that we will come to. So, the recursive case just says that, if the next, if I am not deleting it here, then I delete the V from the next point onwards. So, but so the difficult cases really now that we are pointing from the head to a node containing V, and then V1, and then so on. So, this is the thing where I want to delete this note, we have the same problem as before with insert, which is I cannot make this head point there directly, because I cannot change the node that the head is pointing to. So, it is exactly the same problem when we said when you insert the beginning, we would like to make the head point to the new node and then come down to the old node, but we cannot do that. So, what did we do there? We exchanged. We exchange the two values and then made this, the new node actually the second node in the list by making this, the zeroth node point to that. So, we can do the same thing here. So, what we do is, instead of trying to delete this, we move this V1 here, so logically, there is no V anymore. And then we delete this. So, that is what is happening here. So, we say that if I want to delete the value that I am pointing to, then what I do is, I check. I mean, there may be no next note, maybe this is a singleton, in which case, I just set it to none. But if I have a next node, then I copy the value from the next node and now I bypass, so this is bypass. So, self dot next, my next is pointing to my next, next. So, whatever the next node says is, the next is the node that is two steps down, is what I am going to point to.'},\n",
              " {'id': '381c8a38-0ba8-4ea7-9d80-7d40b76344c6',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'So, this is a simple recursive version. So basically, it has now three cases. Either the, I have reached the end of the list and there is nothing to delete. So, I have an empty road list and nothing is there, I just get out. Otherwise, if I have to delete the value here, then I first deleted, so I just change this value to none. And now I check whether there was something after me. Because I cannot remember, if I cannot have value none unless it is the empty list. So, if it is not the empty list, I have to do some work. So, in that case, what I have to do is, I have to copy this value and then I have to bypass. So, to draw it again. So, basically, if I have let us say I have three nodes and this is my V and this is V1 and this is V2, then what I will do is, I will first set this to none, then I will copy V1 here, and then I will say self dot next dot next is this this thing. So, I will say move this to there, so that is the bypass. So, this is a recursive implementation. And as an exercise, you can write an iterative version just to check that you understand what is going. (Refer Slide Time: 18:18) So, what we have seen, is that, by taking this class node, which contains a value and a next pointer, and we can string it together like a train and make a flexible list. So, programming a flexible list is not very difficult, we can make one of our own. In this, because we start from the beginning, adding at the end of the list is quite easy, because we do not, all the difficult part happens when we have some changes to make at the node which head points to. So, the node that is pointed to by the first name of the list is the one that is tricky to handle. So, append is easy, insert requires some care. And when deleting basically what we do is, effectively we look one step ahead, and so we try to delete the value at the next node by bypassing it.'},\n",
              " {'id': '0d810a3a-4349-47b7-ae1e-e8b0f35013e5',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 3},\n",
              "  'source': 'Designing a Flexible List and Operations on the same.pdf',\n",
              "  'content': 'But then if we are doing it recursively it amounts to then coming to the node and then deleting it and then we have to do something else. If you are doing it iteratively you will look at the next node and then bypass. So, this is what a flexible list looks like. But as we will see, actually the Python lists as they are implemented in the language are not these flexible list. So, all this analysis of what flexible lists do, does not really hold for Python, because of that.'},\n",
              " {'id': 'f8bd27b9-d80c-4b8a-a652-97cd0ec81665',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Quicksort So, the last sorting algorithm that we will look at for now is called quicksort. (Refer Slide Time: 00:14) So, quicksort is interesting for us, because it overcomes some of the shortcomings that we have observed with merge sort. So, we saw two things which we did not like about merge sort. It was good that it took order n log n in the worst case, but it was bad because the merge function actually forces us to use extra space to show the merge list. So, this creates an extra copy of all the space that we are using to store the original list. And it also had this inherent recursion. We were forced to write a recursive function. And the reason that recursion is bad is because there is a certain amount of the programming language infrastructure for recursion requires you to suspend the current computation, make a call and then restore. So, there is a certain amount of, what you might call, paperwork with recursive calls. And so recursive calls tend to be more expensive in that sense. The function calls are expensive. And so you want to avoid them if you can for, if you are interested in performance, although as we said, in some cases, the most natural functions may be recursive. So, then you use recursion as a tool to write correct algorithms. But efficiency wise, it is useful to see if you can avoid recursion wherever possible. So, why does merging require us? I mean, what is the cost of, cause of merging? So, if you think about an example like this, so these left and right halves are already sorted. But now, I cannot leave them as they are, because I have to move things here. I have to move the 1 and the 3 here. And in principle, I have to move the 4 and the 5 there, so 5 and 6. So, that in some sense, unless I do these shifting things across the two halves I will not be able to assemble the full sorted list.'},\n",
              " {'id': '37ce6b18-3a50-4b0f-b413-496a61e5c4f2',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'I cannot just keep the sorted list on the left and stick the sorted list on the right together and make a full sorted list. So, the premise of quicksort is that what if we could do this division better. So, if we can do the division so that everything on the left is smaller than everything on the right, then I can just stick them together. I sort the left half. I sort the right half, but then the only work I need to do is to put them together kind of concatenate them. I do not need to actually do any merging. So, this is the idea behind quicksort. (Refer Slide Time: 02:41) So, let us think of how to do this. So, supposing you could calculate the median value. So, remember that the median value is the one which divides the list into two parts where half the values are smaller than m and half the values are bigger than m. Then what you can do is you can walk through this list and every time you see a value which is smaller than or equal to the medium, you pull it on the left hand side, every time it is bigger than you move to the right hand side. So, you create two new partitions or two new halves. But now these halves are not by position, but by value. So, those values which are smaller than the median, wherever they happen in the list will come here, those values are bigger than medium will go there. Now, because this is the median, these will be halves. I mean, the property of the median is that it is a midpoint. So, you can expect, let us assume there are no duplicates for simplicity. Then it is very easy to see that exactly half the values will be on the left and half will be in the right. So, then now, if I take the left hand sorted and I take the right hand sorted, then everything on the left is smaller than everything on the right and each of them individually is in sorted order. So, the whole thing must be in sorted order. I do not have to merge anymore. So, if I take a recurrence for this, because of the median property, the two parts are roughly half.'},\n",
              " {'id': 'a1d525ed-d3dc-48b5-a81f-a7de1e9d89b1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, I have to sort two lists of size half. This n cost I incur to create those halves. So, unlike in the case of merge sort, where I just have to find the midpoint and say you take everything before this and I take everything after that, I do not have to actually walk through the list at that point. I just have to give the thing and say you take this and I will take that. Here, one has to actually walk through the entire list and decide which goes left, which goes right. For each value, we have to calculate whether it is smaller than the median or bigger than the median. So, you take order n time to decompose the problem into two parts. But then you do no work to combine them except to just concatenate. But it is still this familiar recurrence. This is the same recurrence we had for merge sort. And we know that if we got this recurrence, then we have an n log n sorting algorithm. So, this would be wonderful if you could do this. If we could find the median and then use the median to split the two lists, the problem is how do we find the median. So, when we started our discussion of sorting, we said that one of the advantages of sorting is that it helps us find the median. So, what we can do is we can sort and pick the middle element as the median. But the problem here is that what we are trying to do is sort. So, we cannot assume that we sorted, find the median and then sort again efficiently, because we have already had to do a sort before this. So, there is no point in doing it twice. So, if we find a median through sorting, then it is not acceptable, because we are using the median for sorting. So, there is a kind of circularity in this argument. So, we cannot use the median independently of unless we can find it independently of sorting and that is hard to do. So, instead, what quicksort tries to do is to just pick some value in L. It does not necessarily mean it is the median. It just picks a value which is traditionally called a pivot element.'},\n",
              " {'id': '38f7e30d-ec5d-406b-a3b5-f98c17052e86',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'And what you do is you do the same thing that we did for the median, except you do with respect to the pivot. Everything that is smaller than the pivot, you move to one side, everything that is bigger than the pivot you move to the other side. So, this is this algorithm called quicksort. (Refer Slide Time: 06:04) So, it is due to famous computer scientists called Tony Hoare, Antony Hoare C.A.R. the A stands for Antony. So, it says choose a pivot element. So, how do you choose a pivot element? We will basically choose it based on its position. You do not know anything about the values in the list. So, maybe we just pick the first element, for instance, as the pivot element. Then what you do is you partition the list into two parts, the upper part and the lower part. The lower part has everything which is smaller than the pivot. The upper part has everything which is bigger than the pivot. And now you move the pivot in between. So, now you have everything smaller than the pivot, then you have the pivot, and then you have everything bigger than the pivot. So, now if you sort these parts, everything is with respect to each other in the correct place. So, then you have to take these two parts which are before and after the pivot and sort them again. So, you, again you quicksort for that. So, you again break them up with a pivot to a smaller half, bigger half and so on. But everything in that will stay on the left, everything here stay on the right. That is important. So, let us look at this kind of at a high level. So, supposing this is my input list. So, first, I have to pick a pivot. So, the usual practice, as I said, is to pick the first element is a pivot. So, let us say 43 is our pivot. Now, with respect to the pivot, I have to scan these elements from left to right and decide which ones are smaller and which ones are bigger. And from that, basically, identify the lower and the upper partition.'},\n",
              " {'id': 'c53bebc6-9fd8-4270-99b5-e209df6dd8f5',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, here, for instance, the blue ones, 32, 22, and 13 are the smaller ones, and the green ones, 78, 63 and 57 and 91 are the bigger ones. So, these are the lower and upper partition. So, now, I will kind of rearrange these things so that all the lower ones go to the left, the pivot comes to the middle and the higher ones go to the right. So, this is the setup now. So, now what I have to do is I have to recursively sort this part, recursively sort that part, and I am done. Why am I done, because whatever comes out of this, in general, is going to, obviously, hopefully, be 32, 13, 22 and 32 is going to always lie to the left of the pivot and everything that comes out of this, which is hopefully going to be 57, 63, 78 and 91 will lie to the right of the pivot. So, I no longer have to do anything with these two parts with respect to each other. I can separately solve the blue guys, I can separately solve the green guys and just let them be wherever they are and they have done their job. So, that is the point of quicksort that you do not have to do a merge step afterwards, so you can in some sense disjointly work on the blue things and disjointly work on the green things and never have to ask them to talk to each other again. So, this is a high level view. But in order to actually implement it, we have to, this step that we did going from here to here is not obvious. It is one thing to go through and flag each element as lower or upper, but how are you going to rearrange them at the same time into this lower followed by pivot followed by the upper. So, this is really the heart of quicksort, this partitioning process, how do you partition the list into the correct sequence. (Refer Slide Time: 09:12) So, here is one partitioning strategy. It is not the only one, but it is one which is easy to remember. So, we will scan from left to right. Remember, the leftmost thing is always the pivot.'},\n",
              " {'id': '9d5fd9b1-eda1-4886-96ff-bf0303058fd4',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, what is left to be partitioned is everything beyond the pivot from element position one onwards. So, position 0 is the pivot. Position 1 onwards are the things that need to be scanned. So, I will kind of break it up into four parts is the pivot, then after the pivot will be all the elements which are known to be lower than the pivot, after that will be all those which are known to be upper, higher than the pivot, and then there will be the part which I have yet to seek, the unclassified ones. So, I will be always looking at the next element as the first unclassified element and I will examine it. And now I have to decide how to adjust the lower and upper parts that I have already found. So, the claim is that if the unclassified element is larger than the pivot, I can just move. So, basically, let us try and draw a picture. So, this is my pivot. Then I will say that this is my lower segment, this is my maybe upper segment and this is my unclassified. So, this is where I am. Now, I will look at this element. So, I will look at this element and compare it to the pivot. So, if it is bigger than the pivot, then it is easy. Then I just move this blue line to here and I have extended the upper thing and I have got one less unclassified element. If it is smaller than the pivot, then I need to put that element in the green thing. Now, naively, that would mean that I have to take all these values and shift them, because I need to make one more space in the green thing. But actually, the clever thing to do is to just, if I have this and I want to move it into the upper thing, the clever thing to do is to just exchange this value and this value. So, if it is less than or equal to the pivot, you exchange. So, you have the upper, you have the lower and you have this boundary and you have this unclassified. So, I am saying you just take this value and you swap it with this value.'},\n",
              " {'id': '169ee8d3-0acb-458a-ac85-b930f533b85f',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, you are now taking a value which is lower and adding it to the lower part and you are taking a value in the upper and moving it from the beginning to the end. But the net result is that your boundary has basically shifted like this. So, your upper thing has shifted from left to right does not preserve the order. The first thing has come to the end. And in the space that you have vacated, you have moved a smaller thing. So, this is the strategy of this partition thing. (Refer Slide Time: 12:25)    So, let us see how it works. So, you start with this 43 this thing. So, now you first identify the pivot and you make a kind of marker of where you are two segments end. So, we are using this Python convention where that marker is one beyond. So, the blue and the green at position one means that the left and right things are empty right now, because they do not include the position of the marker. Now, I look at 32. So, 32 is a lower element. And if I do that thing right now it does not make much sense. But if you do that thing, you will end up shifting. So, now it says the blue segment is actually from here to here and the green segment is from here to here and 22 is my next unclassified element. So, now for 22, again, it shifts. Now, for 78, because 78 is an upper thing, the upper segment grows, for 63 the upper segment grows, for 57 the upper segment grows, for 91 the upper segment grows. Now, this is the case which illustrates what happens most clearly at this point. So, now, 13 is smaller. So, I need to put 13 here. So, what I am claiming is I will move the 78 here and I will move the 13 here and I will move both of these pointers. That is what I am going to do. So, I am going to basically, so look at the previous picture. So, the previous picture, the blue segment was ending below 78 and the green segment was ending below 13. Now, I will move 13 to the position of 78 and 78 to the position of 13 and move both those arrows to the right.'},\n",
              " {'id': '23899920-cc83-4ec8-9a49-662f85345169',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, this is now, at the end of this pass, I now have pivot, lower and upper and nothing is unclassified, because I have moved to the end of the list. Now, I have to move the pivot in between. So, I will use the same trick. So, to move the pivot in between I ideally would have to shift the three lower elements to the left and make a space, but I could as well take the rightmost element in that 13 and swap it with the 43. So, I will just swap the 13 and the 43 and now I have the lower elements to the left of the pivot and the upper element to the right. So, this is my partitioning strategy. (Refer Slide Time: 14:39) So, with this strategy we can now write this algorithm quite clearly. So, I want to do quicksort. In general, I have to do quicksort remember on different segments. So, I do quicksort on the slice from l to r. If the slice is trivial, that is it has at most one element, then as usual, I return. Now, if this slice is not trivial, then what I do is that I will fix the pivot to be the beginning of the slice and I will fix initially, if you remember, the lower and the upper segments were the positions immediately to the right of the pivot. So, they were, if the pivot is at small l, then the lower and the upper things are at l plus 1. Now, I will scan through all the unclassified things. If the unclassified thing is bigger than the pivot, I extend the upper segment. That is what this is. Otherwise, I will exchange the current position with the first upper thing. So, remember, lower is pointing just to the right of where the lower segment ends. So, this index lower is actually the first upper position. So, I will swap those, and then I will shift both the lower and the upper markers by 1. So, this is that partitioning step. Now, at the end of this loop, I finished partitioning.'},\n",
              " {'id': '237be5f5-a9b0-40f2-9abc-8e67ef3944eb',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'Now, I will do this business about swapping the pivot and the last element in the lower partition and then I will move the lower partition limit by one because I have kind of shifted the whole thing down by one and now I call quicksort on the left half and I call quicksort on the right half. Notice that here I am doing it up to lower minus 1 and from lower plus 1, so I am leaving out the position where the pivot is, because I do not want to again, the pivot is already in the right place. So, I am doing everything to the left of the pivot and everything to right of the pivot. So that is this quicksort. (Refer Slide Time: 16:33) So, this is again, a divide and conquer thing, except is doing the division in a more clever way, it is partitioning the list in a careful way based on the value. So, we are avoiding this merge step. And this actually allows us to sort it in place. So, actually, you will see that that code if you run it, you will find that the list actually gets updated in place. And we can also provide an iterative implementation, which I will discuss briefly later, to avoid the cost of recursive calls. And we have given one partitioning strategy which basically starts from the left and kind of builds both these partitions as we go along from left to right. Now, there are other strategies which start at the opposite end. So, I start with this and I will have some kind of lower here and upper here and I will grow them towards the middle. So, this is also a strategy which has been studied and implemented. So, there is more than one way to implement quicksort. So, it is a question also, but it is really, the partitioning algorithm is usually the one where you make a mistake. So, you have to be careful that you are doing the right thing in partitioning, because after that the recursive call is very straightforward.'},\n",
              " {'id': 'e90bb5e4-7861-4e24-9b29-78d372955742',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 3},\n",
              "  'source': 'Quicksort.pdf',\n",
              "  'content': 'So, now, in order to justify this we actually have to analyze the complexity and see if we have really done something which improves on the shortcomings of merge sort without sacrificing the advantages of merge sort.'},\n",
              " {'id': '1246cac7-d7a7-4263-bc7b-e99096acd2e3',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'Programing: Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Dictionaries in Python So, having seen lists and arrays in Python, let us look at another very popular data structure that we use quite often, which is built into Python, namely a dictionary. So, how are dictionaries actually implemented in Python? (Refer Slide Time: 00:23) So, recall, that an array or a list allows us to access an element through its position. So, we can talk about l square bracket i , and i will range between 0 and n minus 1 where I have a list or an array of size n. So, the way we access an element or we name an element in a list or an array is through its position. If it is two-dimension, we will say row i column j. In a dictionary, on the other hand, what we want is to name or access an element through some abstract value. For instance, we might want to attach, for instance, to name, a string, we might want to attach a value, for example, marks. We could have a dictionary, which has marks where the key is the name of the student or the roll number of the student, and the value that we store against that name, or roll number will be the marks of that student. So, we do not want to go through this process of first creating a mapping from all students to 0,1,2 to n minus 1 and then using an array. So, we would directly like to use this key to value mapping, so we want what is called a key value store. But we want it to behave still like an array, in the sense that, if we pass a key to this dictionary we would like it to return the value in equal time for any key. So, this is a principle of random access. It should not depend on what the key is, it should take roughly the same time to give me the value regardless of what key I provide. So, the question we want to know is, how is such a dictionary implemented. (Refer Slide Time: 01:48) So, we know that, an array is the way to get random access memory.'},\n",
              " {'id': 'd381d9fc-464b-4287-ad5a-df29ca4ac8f4',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': \"In an array, we have a contiguous block, and we can get the offset of position i by looking at the first block and multiplying it by the number of i times the value of the size of the data entry, and getting the ith block in constant time. So, the underlying principle of any random access structure has to be this idea that you have a contiguous block and you can navigate an offset within the block. But our problem is that we want to not use the 0 to n minus 1 as our entries, but we want to use these keys, so we need a way to convert these keys into 0 to n minus 1. So, we need a way to go from a key to a position in this array. So, we need a function, which can map keys to 0 to n minus 1. So, we want to give an a key k converted to an offset I, and this kind of a function is called a hash function. So, it takes a set of values. So, the set of values could be roll numbers, names, any set, and maps these values to a fixed range, typically zero to n minus 1. So, it takes an arbitrary set and maps it to a small range of values. And usually, the range of values that you map it to, is going to be much smaller than the range of possibilities. So, you can imagine that if you think of all the possible names that you could have for students in a class or all the possible roll numbers that you could create using the sequences of letters and numbers that you use for roll numbers that is a very large set. But you will use a class will have only 50 people, 100 people, 200 people, whatever, some small number of actual people. So, you don't want to have an enormous possible storage for all possible names and all possible roll numbers, so you would actually like to map this large set of values to a small set. But, of course, now the problem is, therefore, that if I look at all the possible names that people could come with or all the possible roll numbers I could create, in general, it will be that two of these keys will map to the same value, so this is what is called a collision.\"},\n",
              " {'id': 'f317ec25-5776-42b8-a72e-4b2b35c2dec8',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, this hash function h maps s and s prime to the same value, even though it is not equal to s prime. Now, this cannot be avoided, because you are taking a large set and you are pushing everything from here into a small set. So, it is not possible for everything in the large set to map to different things in the small set, because you just do not have that many elements to map to. But what you would like to do is, to somehow distribute this so that it is random. So, it should not be that, if I map a name, and if I map the next name or if I map a roll number, and I map the next roll number the next roll number will go and collide as it is called, so these are called collisions. So, the next number will collide. I would like to distribute, so that if I give you a, so the ideal thing is that if I give you a random set of elements from here, which are kind of spaced out, hopefully they will all go to different things, this is what you would like. You cannot guarantee it, but you would like this to happen. So, if I give you a random collection of inputs from the last set, then with reasonable chance they should not collide. So, this is what a good hash function will do. (Refer Slide Time: 04:58) Just to illustrate, how important hash functions are. So, there is this now, current industry standard called SHA-256. So, this is used for cryptography and other things. So, 256 refers to the fact that the output of this thing is to 256 bits. 256 bits is a large number it is 2 to the power 256. So, remember that 2 to the power 10 is 1,000. So, this is 1,000 to the power 25, roughly something like that. So, this is like 1 with 75 zeros after it. So, that is the space in which you are mapping. But it is not such a large space, if you think about it, because what you can actually do is you can take, for instance an entire file. Now, a file could be a very large.'},\n",
              " {'id': 'd490b15f-538f-485b-ad4b-63a785ba5bed',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, you can think of a file as a large number, a file is a sequence of zeros and ones, so you can think of it as a large number or some value from some large set, and you can now map it to this 256 bit. So, for instance, it could be a movie or it could be a document. So, it could be a very large file much larger than this 1 with 75 zeros in terms of possibilities. So, each individual document will not be so much, but there are so many different documents. So, one application of SHA-256, which might be surprising to you, is that, if you do if you use a cloud storage system, supposing something like Dropbox, for instance. Sometimes, when you upload a very large file, it will actually upload very fast. And the reason is that actually, this system computes this SHA-256 hash. And if it detects that this hash is already present in the storage, it does not actually upload, it just kind of makes a pointer saying one more copy of this file has been uploaded, it does not bother to upload it. So, in principle, if you had a hash, SHA-256, hash with two different files produce the same hash, you would not be able to upload the second file because it will say it is already there. Now, in practice, this does not happen, so it just shows that the SHA-256, has been carefully designed so that these collisions rarely happen. (Refer Slide Time: 07:11) So, the way we use this hash function, as we said, is we want to take the key and map it to a position an array, so this is called a hash table. So, a hash table is an array of size n, combined with a hash function, which maps keys to this range 0 to n minus 1. So, the idea is that h will map the keys that you provide, whatever keys they are, so the keys are not restricted, only the range of the hash function is restricted, it depends on what the keys are.'},\n",
              " {'id': '9d6085f1-13d6-496b-a3f1-6d3495c5da77',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, the ideal situation is that if I give you are new key, k, and I want you to store something as for the dictionary, d of k, then you will map it to this h of k the position in the array and hopefully that places available to you. So, I want to say d of k is equal to V. So, what will happen now, is that, I will go from k to h of k, this will give me a number. This will be some number in 0 to n minus 1. Then I will go into my array, so this will be some number, j. So, then I will go to position j, and I will try to put this value V there. So, this is how dictionary should work. But what will happen is that sometimes because of collisions, that place is occupied. (Refer Slide Time: 08:24) So, if there is already a value at that location then this hash table has to deal with this collision. It cannot just replace because then their old value will be lost It cannot put it in some random place, because if it puts it in a random place, you will never find it. So, you need to have a systematic way of putting it somewhere where you will find it if you look at it later. So, there are two ways to deal with this, which have some more confusing names. So, the first one is called open addressing, but it is called closed hashing. And here, the idea is, you could not go outside the array. So, what you have is that you came here. So, this is the place that you were looking to fill, but then unfortunately for you there is already something here. So now what you do is, you skip, right, so you kind of have a kind of round robin. So, you do not necessarily go to the next position. But you kind of say, okay, if this is not there then let me go plus 10 and see if there is a place there. If it is not there, then let me go plus 10. So, I just keep cycling around, going forward by probing a sequence of alternative slots until I find a free slot.'},\n",
              " {'id': '3b1ccfee-4ef4-4bd6-bdbe-6cc65ef42723',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'And in that free slot, now I must of course have one extra piece of work, I must write the key also because I need to know that this piece belongs to that key. But essentially, this is the way you work, so you create an array which is large enough, so there will be some extra slots. So, even after these collisions, you will still find some extra space. So, if you anticipate that you will need say, 1,000 entries in your dictionary, then you might put say an array of size 5,000. So, you have a lot of free space to play with for this kind of collision. hashing, hashing and avoiding collisions. (Refer Slide Time: 10:04) The other option is to do what is called open hashing. So, this is now the confusion. Open hashing and open addressing are not the same. So, open addressing means you are working within the same array. You are moving around to a new position and hoping it is free. And so it is like you get into, say, supposing you get into a parking lot, and you find that there is no space for you to park in that particular row. Then you go out and you circle around and you enter another row, and you try to look for a space, then you go out circle. Eventually, hopefully, there is a space, you will find it. So, that is more like closed hashing. You just stay within that parking lot. You are looking for a free slot, so the free slot, this first slot you try to find was booked, so you go around looking for the next free slot by randomly wandering around, not quite randomly. But you move around in some fixed sequence so that you can retrace that sequence later on when somebody comes looking for this value again. Now in open hashing, on the other hand, remember that you get into this, and this is my array. So, in close hashing, I actually store the value in the array. Now in open hashing, what I do is I do not store the value in the array instead, for each position in the array, I maintain a list. So, the actual storage happens somewhere outside this hash table.'},\n",
              " {'id': 'c7fbbf04-2c7d-4500-9f14-b189facf3179',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, the hash table is only a collection of what you might call pointers. So, if I hash key to a position in the array, the array tells me, go and look there, that is the list of all values, which had the same hash value as the key you had, go and find it. So, I will maintain this elsewhere. And then now because that list is flexible, you can grow and shrink, I do not have any constraint about it. So, this allows me to keep coalitions under control in that sense, because I can just append to that list. So, these are these two standard ways. And, of course, you can use some heuristics to do these better or worse, but this is the standard ways of dealing with collisions in a hash table. (Refer Slide Time: 11:52) So, you might have wondered. When you are taught to use dictionaries in Python, one of the things you are told is the key to a dictionary must be immutable. You cannot use for example, a list as a key for a dictionary. Why is that the case? Well, now you know the answer. Because, in order to find the place where the value is stored, the hash function has to be applied. So, the hash function is applied, when it is stored, at that point, the key has a value. Now, if it is a mutable data structure, the value can change. So, when I come back later on, and I try to hash that value, it will be a new value, and therefore I will not be able to find that position in the dictionary anymore. So, I must guarantee that the value that I used inside the key remains fixed, because otherwise the hash function cannot return the same value in a guaranteed way each time. So, this is one reason why Python insists that dictionary keys must be immutable, so that the hash value remains fixed across multiple visits to that key when you enter that same dictionary. (Refer Slide Time: 12:58) So, to summarize, dictionaries are extremely powerful data structures, which allow you to store values against keys. And the way you implement a dictionary is by actually using this hash function.'},\n",
              " {'id': '030acbad-2016-4348-bc47-e1d520ad027e',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, underlying it, there is an array of some sort, which has a random access storage from zero to n minus 1, but you want to take a key and you want to put it at position i. So, you want to take key and you want to map it to i belonging to 0 to n minus 1. And this is that function, h , which is a hashing function. So, you want to design a good hash function, which randomizes and does not collide too often. So, creating a good hash function is a lot of work. So, the SHA-256, which I mentioned, was a major effort. It was a major effort to come up with this function, which would give you an output or 256 bits, and which had all the desirable properties. So, to come up with a good hash function requires a lot of mathematical reasoning about the function, and we are not going to get into that, so we assume that we have a hash function. And in fact, in Python, there are hash functions. There are hash functions for different immutable types, like strings and numbers and all that. So, you can actually create your own hash table if you really want by using the Python hashing function. But when we use a hash table, one of the things that we need to do is to deal with collisions. Because it is eminently possible that at different point in time, two keys will give you the same location inside that array that you are using. So, either you can use this probing technique, which is to just move around the array looking for a free space, which is called open addressing, or closed hashing depending on what you want to call it. So, think of it as closed hashing means your world is closed, you cannot move outside this area. So, you are restricted to being inside this array. Whereas open hashing says actually, the place that you are going to go to is somewhere outside. So, each of the slots in the array actually points to a list of values, which shared the same position.'},\n",
              " {'id': '55a50767-0284-46d9-a91c-11fefa743828',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 3},\n",
              "  'source': 'Implementation of Dictionaries in Python.pdf',\n",
              "  'content': 'So, if you have a collision, you just move and append to that list, so each position i now maps to a collection of values. So, you first come here and then go there, and it still does not take much time. Because you assume, again, that you have a good hash, so these collisions are rare. So, those lists will be ideally small. You do not want everything to be collecting in one list, which would happen if all your elements collide. So, if you have a reasonable hash function, it will distribute the values across these lists, so searching those lists will not take much extra time. So, there are many heuristics and optimizations which are available for dealing with collisions which we will not get into.'},\n",
              " {'id': '0ead24e5-e0e2-4ef4-8dcd-91d60d9e87d1',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Concluding remarks on sorting algorithms.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Concluding remarks on sorting algorithms (Refer Slide Time: 00:09) So, let us conclude our discussion on sorting by looking at some other issues. (Refer Slide Time: 00:13) So, one of the issues is something called stability. So, very often we are not sorting single values, but we are sorting compound values. So, imagine that we have a table and we are sorting the rows in the table. So, each element that we want to sort corresponds to one row. So, one row usually corresponds to values from many columns. So, it is a tuple of values. It could be, for example, a list of students, and each student has along the columns, you have roll number, you have the name, you have the marks, and so on. So, you can think of what you are sorting is actually a spreadsheet. So, your list is a list of rows. But each element in your list consists of some sub-parts. So, now when we sort, we typically sort on one or more of these sub-parts. The sub-parts are attributes or columns. So, you might want to sort by name or you might want to sort by roll number or you might want to sort by marks. And quite often you might want to sort something which has already been sorted. So, for instance, it could be that your list is provided to you by roll number. And now, for whatever reason you want to group the students in alphabetical order of name. So, you sort them by alphabetical order of name. Now, the question is, when I sort by alphabetical order of name, I might get two students who have the same name. They would have had a roll number. In the original sorting one of them would have a smaller roll number than the other. In the new sorting by name, are the roll number still in the same order or not or have they got shuffled. So, this is something that we want to know.'},\n",
              " {'id': '88654ddc-6a24-4f85-9bdb-817208efdfec',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Concluding remarks on sorting algorithms.pdf',\n",
              "  'content': 'So, for example, supposing you have sorted in alphabetical order of name and now you want to sort by marks, now when you get the same students with the same marks, are they sorted in alphabetical order of name or do they get shuffled? So, this is called stable sorting. So, stability says that when I sort in another column, it should not disturb the sorting on a previous column. So, later sorting should not disturb an earlier thing. And this is something which is quite natural if you actually work with sorted things in spreadsheets and all that. If you sort on something, you do not want to go back and figure out that something else has got sorted badly that you had already sorted in a previous column. (Refer Slide Time: 02:24) So, unfortunately, the quicksort implementation that we looked at is not going to be stable, because it does all this, if you remember, you suddenly do these kinds of exchanges. So, if these values originally were in some order with respect to some value you are not using right now for sorting and if, while partitioning your sort of swapping things around, then you might very well be exchanging things which are equal in some other key and this is not good. So, the partitioning of quicksort has to be very carefully done to make sure that it is stable. It is possible to do it, but not the partitioning that we saw. Merge sort is easier to do. Basically, what you want to make sure is that when I have something like 1, 3, 7 and merge it with, say, 2, 7, 9. So, maybe this had an A and this had a B with it. So, I want to make sure that in the final list the A7 comes before the B7 this is what stability means. It means that in the final list I want 1, 2, 3, 7, 7, 9, but the order of the 7s should be in the same order in which I was presented to me in the beginning. I should not have 7B followed by 7A, even though A and B are not part of my current sorting procedure. So, I am not looking at the A and B, but I want to guarantee that this does not happen.'},\n",
              " {'id': 'dad4b98d-9d44-45c0-948e-fe27400bc66c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Concluding remarks on sorting algorithms.pdf',\n",
              "  'content': 'So, in merge, what you do is, basically, if I see the same value on both sides, then I will move the one from the left first. So, the value from the right should not overtake the value on the left. If you guarantee that that your merge has this kind of preference for the left list, when you compare equal values, then you will get a stable merge. (Refer Slide Time: 04:01) Now, we have looked at basically criteria which involve the number of comparisons and swaps that we make. But there are situations where actually the data has to physically be moved and it is not an easy thing to move data from one place to another. So, you can imagine that the data sitting in some heavy boxes. So, supposing you are doing a physical sorting or something, then one of the things that you might want to minimize is not just the number of boxes you examine and exchange, you might be willing to look at more boxes to examine and exchange if you have to move your boxes overall. So, data movement is an orthogonal thing compared to anything that we have discussed so far. But this is also an important criterion when looking at certain categories of sorting. So, sorting is a fairly vast. As we said, sorting is used as a first step for a large number of things and there are many different dimensions to sorting which people have looked at. So, you should not think that if you know merge sort and quicksort you have exhausted everything to do with sorting. There is a lot of sorting which is still there and many different ways of sorting which emphasize different aspects of the problem. (Refer Slide Time: 05:08) So, is there a best sorting algorithm? Well, as you can see from our discussion, the hints are quite clear that there is no best sorting algorithm overall. Quicksort is, as we said, very often the algorithm of choice when we are using built in functions and all that people use quicksort, despite its worst case.'},\n",
              " {'id': 'd4d9f4d2-8d5f-4d99-99b3-390b42c0a2e5',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Concluding remarks on sorting algorithms.pdf',\n",
              "  'content': 'But there are situations, like we were talking about those boxes, where you do not have the luxury of sorting everything as a function. So, sometimes you need to sort, example in a database, and in a database you need to put parts of the data into memory and part out. So, this is called external sorting. So, usually, merge sort is used for external sorting. And merge sort is not the only n log n algorithm. There is also an algorithm which we will see later called heapsort, which also does n log n. So, there are other n log n algorithms. And very often, you might actually use a hybrid strategy. So, you might use a divide and conquer strategy when n is large. But then when you get down to a small list, say like 16 or 32, you might switch over to insertion sort. So, a lot of different ways are there of combining algorithms or using the properties of, sorting algorithm for different situations. So, you, it is useful to know that all these exist. Although in many cases, you will typically be using a built-in sorting routine. So, you do not really need to know the sorting algorithm as long as a built-in sort function works. But there are situations where you will need to sort for yourself and then it is useful to know that there are these different options and they all have their pluses and their minuses.'},\n",
              " {'id': '162ace72-7b55-4703-b4a8-e1a1f8542e47',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Implementation of Quicksort algorithm.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Implementation of Quicksort algorithm (Refer Slide Time: 00:09) So, last week we looked at some experiments with sorting algorithms with insertion sort, selection sort and merge sort to find out how they actually behaved on different types of large inputs to understand whether they were uniform across different things or they were better on ascending on descending. So, let us do the same now with quicksort. (Refer Slide Time: 00:26) So, we begin with timer class which will allow us to time the execution and get an idea of how things happen on large inputs. And quicksort is fundamentally a recursive implementation, the one that we have given in the class. So, we are going to use that. And that is going to create problems with this built-in small recursion limit. So, we will, as we did for insertion sort in the recursive case, we will increase the recursion limit to the maximum allowed which is 2 to the power 31 minus 1. So, now, this is our quicksort algorithm that we give in class. So, what we do is we sort from a lower limit to an upper limit and we choose the first element of the list as the pivot and then we go from left to right and we partition the pivot into this lower, the remaining things lower and upper, and then we move the pivot to the middle and then we recursively sort the two parts. So, eventually we come down here and we recursively sort two parts and then we return here. So, one the things that we want to check is, of course, this is correct. So, let us take some sort of random array and just sort it and validate that both. So, this technically sorting in place, but it is also returning the sorted list. So, you can see that if I kind of take this qlist and sort it and return it as qnew, both qnew and qlist are the same. So, it is basically in place sort that we have implemented, because we are doing all these exchanges which will actually happen inside this.'},\n",
              " {'id': '8a1c463d-a021-4613-aaf9-d8db9e9ab336',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Implementation of Quicksort algorithm.pdf',\n",
              "  'content': 'Now, the thing that we want to see is how this behaves on large inputs. So, one of the things we argued about quicksort is that typically it behaves well. So, what this means is that hopefully if we create a random array, it is going to behave fast. But because of the nature of the pivot when I have extreme values at the 0th element, either the largest or the smallest, is going to break up these things unequally into an empty list and a list of size n minus 1. So, I am going to get this n square behavior like insertion sort and selection sort. (Refer Slide Time: 02:24) So, let us compare this performance of quicksort with say merge sort. So, merge sort, remember, will take good time on any input. It does not have this problem ascending and descending, but let us, in particular, look at this random behavior and see which is better. So, this is a last times implementation of merge sort. So, here, we are now doing the same thing. We are creating a random input of size 10 to the 6, exactly the same size as we had for quicksort. And for the ascending and descending for merge sort, it does not matter. We can actually have it of the same size because it is not going to blow up like the quicksort worst case. So, if I run this now on merge sort, the same 10 to the power 6 which took 7 seconds in quicksort is actually going to take a little longer in merge sort. So, this actually shows that though they are both similar algorithms in terms of divide and conquer, we said that quicksort is the worst-case time of n square and we have, average case of n log n, and merge sort is a worst case of n log n. So, we can see that when we have large lists, we always get something between 0 and 10 seconds, but it is interesting that merge on this random array takes actually about 50 percent more time almost than quicksort on the random array.'},\n",
              " {'id': '93255a0d-b5da-4b92-99c4-dab53534a3d6',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Implementation of Quicksort algorithm.pdf',\n",
              "  'content': 'So, this is what we meant by saying that although quicksort does not have a very attractive worst-case time, it seems to be a very effective algorithm and it is one that is often used in practice. So, that is why we typically see quicksort being taught and used even though it theoretically looks like insertion sort and selection sort in having an n square worst time.'},\n",
              " {'id': 'cdc054f5-fed8-465d-bc01-f888a5e5095c',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Analysis of Quicksort (Refer Slide Time: 00:11) So, let us analyze quicksort. So, remember, this is how quicksort works. It chooses a pivot, partitions it, moves the pivot between the lower and right segments, and then recursively sorts the two partitions. So, it all amounts to really asking how well this partitioning works in terms of dividing the array into two or the list into two smaller parts. (Refer Slide Time: 00:28) So, partitioning itself takes linear time, because we saw that in one scan of the list, we can do the partitioning. But the real question is, what are we partition, partitioning with respect to. So, if the pivot is actually the median, then we know that the two halves are roughly equal in size. So, we know that the lower half has half the elements, the upper half has half the elements. So, we get our familiar merge sort recurrence, which is Tn is 2 of T2 times n by 2 plus the partitioning cost of n and we get an n log n algorithm. If this were indeed the case, we would be in business because now we have got an in place algorithm, which I also claimed can be done iteratively. But it does not have the cost of the recursion and the extra space of merge sort, but it is as good theoretically as an upper bound. But unfortunately, this is not going to be the case. So, in the worst case, because you have no control over the pivot, because it is picking it up from someplace in the list, the first element of the list need not be the median. You are just picking it up without analyzing the values. In the worst case is either the smallest or the largest value. So, if it is the smallest or the largest value, what will happen is that every other value will be either smaller than it or bigger than it.'},\n",
              " {'id': 'a1175920-771b-4033-b60b-62cac5c20544',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'So, one of the two partitions, either everything will go into the lower part and nothing will go into the upper part or everything will go into the upper part and nothing will go into the lower part. So, worst case, your partitions will have size 0 and n minus 1, instead of n by 2, n by 2, of course, the pivot will not be there, so n will reduce. But it will reduce in this very asymmetric way, either the lower will be empty and the upper will be n minus 1 elements or the lower will have n minus 1 elements and the upper will have. So, remember this worst case. We cannot avoid the worst case. So, this might happen. So, in this case, then the recurrence in the worst case says that to sort n, I end up having to sort the larger of the two partitions, which in this case is n minus 1 and I have spent order n work getting there. So, I have exactly the same recurrence that we had for selection, for insertion sort when we did it recursively. So, I have Tn is T n minus 1 plus n and this ends up being n square. So, unfortunately, this very clever strategy of avoiding the merge has a worst case complexity, which is n square. And what is the worst case? Well, paradoxically, the worst case is one where in fact, the array is already sorted. So, for instance, if I am sorting it in ascending order and I give you an array in ascending order and I pick the first element as a pivot, then the first element is going to be the smallest one. So, it is going to produce an upper partition consisting all the remaining elements. But remember, they are going to be generated in the same sequence. So, the upper partition will again be a sorted sequence with the first element is the smallest one. So, if I pick that one, again, it is going to produce an upper thing with n minus 2 sorted things and so on. So, the case which is actually bad is the case which should be good.'},\n",
              " {'id': 'b2654e58-2232-42b0-82ca-1db535501fea',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'Like we said in insertion sort I give you an already sorted sequence in the correct sequence sorted in the same sequence that I am looking for. It will actually work well. Here, it actually works very badly. (Refer Slide Time: 03:34) But there is some silver lining. So, actually, one can show that the average case for quicksort, we are not going to prove it, but I am just going to claim it. The average case is m log n. So, we had a discussion earlier about what this entails. Average case means we have to talk about all inputs, some distribution over the inputs, and then somehow look at the expected running time and all that. So, in this case, how does it work? Well, the first thing that is there is that in principle you have infinitely many arrays of a fixed length. Even if I say that the length is n, there is no limit to the number of arrays or lists you can construct of length n, because you can keep changing the values arbitrarily. But when I am doing compare and swap as my sorting, it really does not matter where, if I give you a list with 1, 2, 3 and if I give a list of 10, 20, 30, it really does not matter. They are both the same list as far as the algorithm is concerned, because the first position is smaller than second position is smaller than the third position. So, the actual values are not important. What is important is their relative order, which is the biggest one, which is the second biggest one. So, I can always think of any input of size n as being a permutation of n elements. It tells me the biggest element is in one position, the second biggest is somewhere else, and so on. It really does not matter what the values are. So, this allows me to now first bound the space over which I am calculating the probability. I can say that for input of size n, I am looking at all n factorial permutations of 1 to n. And then I have to make an assumption.'},\n",
              " {'id': '8f70bb9c-c3a5-4071-bc0c-0254927cafcb',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'But in the case of sorting you can imagine that somebody is giving you a list to sort there is no bias, every permutation is equally likely. So, you can assume that for the probability part that each is equally likely. And then because it is exhaustive, you can actually do a count and verify this. So, that is how this thing comes out, saying that the expected running time for inputs of size n over all permutations of size n you can actually calculate its n log n, even though there are going to be some inputs which are going to be worst case n square. So, although, so, therefore, in some sense, n square is a rare case, because n log n turns out to be the average case. So, this is not possible for most scenarios and algorithms, but for sorting it is possible. And in particular, it has been done for quicksort to show that its expected running times n log n. (Refer Slide Time: 05:52) So, there is another way to beat, I mean, to exploit this to beat this worst case in the case of quicksort. So, the real problem with quicksort turns out to be that choosing the pivot using a fixed position gives us a problem. So, we saw that if the first position is our pivot, then if we put the smallest value at the first position each time, we can kind of build up an array or a list which will always give us a worst case behavior. Supposing you say, no, I am not going to take the first position, I am going to take the last position, then I will give you a symmetric input, which will be bad for that. If you say, I am going to pick the middle position, then I can make sure that in the input I construct that the first element, the extreme element is going to be the middle, then I will run your quicksort implementation to figure out what time is left, what happened the lies and then I can put again the second minimum and the third minimum at the midpoints of the two partitions that you find. So, I can always reconstruct a worst case and put if you have a fixed strategy for finding the pivot.'},\n",
              " {'id': '29677304-70ee-4d8a-a842-895a56c4d15d',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'So, what is the solution? The solution is that you do not have a fixed strategy. At every time when you want to run quicksort, you have to fix a pivot and then partition. But you do not fix a pivot by choosing the first element or the middle element or the last element, you kind of pick a random value between 0 and n minus 1, uniformly. So, you just generate a random number uniformly with probability 1 by n between 0 and n minus 1 and you say, okay, for now, this is the pivot, next time will be something else. So, since you are picking the pivot at random, there is no way to, I mean, in some sense, intuitively for somebody adversarial to give you a bad input. So, if you do the calculation in this randomize sense, where each time you pick the pivot, so this pivot is not always the left-hand side of your list, but it is some random position, which you calculate with each iteration, then it turns out that again you can show that you have an expected running time of n log n. So, this is a different way of achieving that average case. And it is an easier way in some sense, because what it means is that when you actually implement it, if somebody gives you a worst case input your algorithm is not necessarily going to be stuck in that worst case. (Refer Slide Time: 07:54) We also mentioned this iterative quicksort. So, I will not go into it in much detail. But just to suffice to say that, basically these calls are happening on disjoint parts of the array. So, since I am anyway telling quicksort to work within a bounded interval from left to right, when I work on this interval it does not influence anything else, when I work in that. So, I can always rewrite this code to work iteratively on each segment between l and r minus 1. So, I would not go into the details, but you can convince yourself that this algorithm can actually be implemented iteratively.'},\n",
              " {'id': 'f3cf4272-b060-4e3e-be4c-e478a9c6a6e5',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Analysis of Quicksort.pdf',\n",
              "  'content': 'So, when we are doing this iterative thing, we have to each time when we are running quicksort, we have to know between which r we are working. (Refer Slide Time: 08:33) So, given all this, you might ask why are we so interested in quicksort? Well, it turns out that quicksort despite its order n square upper bound is actually very fast in practice. So, in many situations you use built in sorting. You take a spreadsheet, you take a column and you say sort it, or you might have a sort function which you can call like we have in Python, like l dot sort or sorted, so you might have a function that you can call for free in a programming language. You do not have implement it. So, in many such cases, actually quicksort is the algorithm that is used. So, that shows basically that despite the worst case it actually works this implementation without the overhead of merge sort actually makes it competitive and very efficient in practice. (Refer Slide Time: 09:19) So, to summarize, the worst case complexity is n square, but actually you can calculate the average is n log n. And one way to kind of achieve this average is to have a randomized strategy to choose the pivot. So, at each time you want to pivot the thing you pick a position with uniform probability between the beginning and the end. So, quicksort overcomes some of the limitations of merge sort, in that it works in place, it does not require you to construct a new array, even though it is can be implemented recursively and it also can be implemented non-recursively, it can be done iteratively. And the main selling point of quicksort is that it is very fast in practice. So, it is often used for built-in sorting functions and it kind of illustrates the point that we made that using upper bound as a prediction of the overall behavior of an algorithm is often very pessimistic and this is one real life situation where this pessimism actually shows up.'},\n",
              " {'id': '8e16dc48-0bef-44ec-b0d9-68396cc486d7',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Difference between Lists and Arrays (Theory) (Refer Slide Time: 00:09) So, we have been studying sorting and searching and we have somewhat interchangeably been using words like lists and arrays. So, let us try and understand formally what is difference between a list and an array in the context of programming. (Refer Slide Time: 00:24) So, we are interested in maintaining sequences of values. So, we have a first value or second value, and so on. And we would like to talk about the ith value and the jth value and maybe look it up or exchange it and so on. So, fundamentally, these two lists and arrays are two different ways of maintaining such a sequence. So, what is the difference? So, list in the conventional understanding of the word is a flexible length sequence. So, it is a sequence that can grow and shrink. So, the idea with a list is that you can modify its structure. And this we see in a built in Python list, for example, because you can take a slice, for example, and replace it by a longer slice or a shorter slice. So, you can expand and contract a Python list, you can append something to the beginning, you can delete a value in the middle. So, the length can vary over time. But because of this, it is hard to put this list in a fixed place in memory, because the place that you require for the list may grow and shrink over time. So, typically, a list is scattered in memory. So, we will come back to what this means. But it is, think of a list as being dispersed. So, you cannot point to a segment of memory and say the entire list is sitting here. It might be sitting in a lot of places. An array, on the other hand, is typically something which is designed not to grow or shrink. So, in the conventional understanding of an array, it is a fixed size sequence. I need a sequence of 100 elements. Now, this makes a lot of sense when we are processing graphs, for example.'},\n",
              " {'id': '7ffaf3d4-f1cc-46fd-b8a9-61929ffafdef',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'In a graph, typically, we need something like the list of all the vertices or we need the adjacency matrix. And for these things, if we know the number of vertices, we know the length of the list and is never going to shrink or grow. We need to always have a list of n vertices. We always need an adjacency matrix of n by n. So, if we have a fixed size, then we know in advance how much space we will need in memory. So, we can go ahead and block that space. So, this makes an array something which sits in a fixed space in memory and this will help us as we will see access the elements in an array faster than accessing them in a list. (Refer Slide Time: 02:30)  So, let us look at this in a little more detail. So, a list in this flexible sense, is typically a sequence of node. So, each node has a value, and it will point to the next node in the sequence. So, you do not have to tell in advance where these nodes are, because you can just ask each node to tell us where the next node is. So, it is like following directions to go someplace. You do not know the way all the way, but you know that everybody in the neighborhood knows the way. So, you as the first person saying I am going in that way, which road should I take. You go to the next guy and he would say which road should I take and you keep following these links. So, the picture that we should have in mind is something like this. So, we have these nodes. So, each node has a value and each node has this information which tells us where the next node is. So, what we know typically is where the list begins. We could, if we want keep information about where the last node in the list is. This might be helpful, for instance, if we want to extend it by adding one more value there without going through the whole list. But in principle what we really need is this structure which is often called a linked list. So, the linking basically refers to the fact that I have this. So, think of it like a train.'},\n",
              " {'id': '595adb40-2400-4980-87ec-65b8a79c4fcc',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'If you have seen a train, a train consists of these independent wagons or bogies, and they are all connected to each other. So, this list is like a train. We can detach it, we can reattach it. And in principle, of course, in a physical train, there is some link is made out of some hard metal, so there cannot be so much distance. But if this link was made out of elastic and infinitely stretchable, you could have a train in which the different parts of it are all over the place. So, the main advantage of having this linked structure is that it is very easy to modify a list. So, supposing I want to insert a new value x between the element i minus 1, Vi minus 1 and Vi. So, what I need to do is basically restructure this list. But it is, the structure is only logical, it is not a physical structure. Each of these notes are sitting somewhere in memory. So, what I need to do is make this Vi minus 1 point to X and Vi X point to Vi. So, what I need to do is basically change from this link which says that Vi connects to Vi, Vi minus 1 connects to Vi, I need to say that Vi minus 1 connects to X and X connects to Vi. So, there is some local, what you can think of is plumbing. So, if you have some block in a pipe, then one way is to of course remove that piece of pipe and replace it or you can just bypass it by moving a pipe around it. So, you can do some local operation and not affect the whole plumbing network. So, you are just taking some local neighborhood of the nodes that you want to update and you are making some changes in how the information flows or how the sequencing happens. So, this is how you insert. Similarly, so now at the end of this, I mean, I can think of it as I have put an X there. Of course, none of these is actually in any physical sequence. These are all sitting somewhere abstractly in memory. So, this is only a logical sequence that I go from 0 to 1, 1 to 2, and so on.'},\n",
              " {'id': 'e0b58dff-ae2c-4bd3-9141-d4022f0b186f',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'And now after this insert, I go from i minus 1 to this new value X, and then I go from X to i. So, this should be i probably. So, now, here is a symmetric situation. I want to delete a node in the middle of a list. So, here, what do I do? Well, it is simpler in a way. I need to just take this and bypass it. So, this is like, again, plumbing. You want to make sure that some piece of pipe which is blocked is bypassed, so you just go passed it. So, you do this. And then logically speaking, this Vi disappears. So, now I directly go from the previous node to the next node. That current node which I want to delete is gone. Now, it is a separate matter in terms of the programming language, how that lost node is returned back to memory so the memory does not get used up with all this junk. That is not something we have to worry. Python does it automatically. So, Python is able to keep track of all these things, for instance, which get lost and take care of it so that they do not become permanently a drain on the memory that you are using. So, in this way, if you have this linked structure, you can see that by doing some local operations, you are able to manipulate the list quite easily. But the problem with this is that you need to actually get to this position. So, you need to follow these links to access the ith thing. So, the, in order to put this here, I have to first get here. And the only way I can get here is to actually walk down this list from the head, because there is no way I know in advance where the ith element sits in memory. So, I need to spend time proportional to i, the position or the index I am looking for in order to reach the ith element. So, this is the flip side of this flexibility. The positive side is that I can make these local changes easily where I am. But to reach the place where I need to make the change, I need to walk down always from the start, because there is no direct way to get there.'},\n",
              " {'id': 'bf41c469-637e-4974-be8c-44b7c1fb7fa3',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': '(Refer Slide Time: 07:25) The other option is to have an array. So, as we said an array will take a parameter n to start with and say that this is going to be always of size n. And usually, it is also assumed that in an array all the elements of the array are of the same type. So, in most programming languages this is also true of lists. A list is also a list of a fixed type. Now, Python has a very flexible thing which allows you to have lists in which some elements are integers, some are dictionary, some are this, some are that. So, in principle, a Python list is actually a list of what are called objects. But in most programming languages is a fixed type. And a fixed type has a fixed size. If I want to store an integer, it will take so much size, if I want to store a float, it will take so much size. So, if I know how much size a fixed value takes and if I need n of them, then I obviously need n times that space. So, I can allocate when I start, if the programming, programmer asks for an array of size n of int, then I know how much space n ints take and I can say, okay, here is a space with n ints and I put it all in one position, in one place. So, from the starting point, I know that the first int is A0, the second int is A1 and so on and this gives us what is called random access. So, if I want to get to this entry and I know where the array starts, then I just have to calculate it from the way the array starts, I have to skip past i times the space for an integer and I know that that position is going to have this. It is like walking on tiles. If I know I have to walk 20 steps, then I can close my eyes and walk 20 steps and I know that I will be there. Except that I do not have to, in this case, physically walk. I do not have to go through 1, 2, 3, 4. It is not like a list. I do not have to say, oh, the next one is here and the next one is here, I do not. I can directly say, okay, I want to bypass to the ith position.'},\n",
              " {'id': 'b4e7a155-3500-42dd-a5cf-82d49c325bc2',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'So, if I am here, this is where I need to be. So, I can actually, it just becomes an arithmetic calculation to calculate where it is. So, this is called random access. So, technically, what it means is that whether I am accessing the beginning of the list, the middle of the list or the end of the list, it takes the same amount of time. It takes time which is independent. So, whether the list has 100 elements or 1 million elements, it is going to take the same amount of time to access it if it is an array, whereas in the previous example, in the flexible list, it took time proportional to where I want it to go. So, if I wanted to work in the early part of the list, it was fast. If I wanted to work in the later part of the list, it would be slow, because I have to walk all the way there. Now, of course, this is not a situation where one is clearly better than the other. So, what we have lost in an array is the flexibility. In a list we could insert and delete things, so we could grow and shrink the list without any problem, provided we knew what to do where we were. But in an array, if we have to grow this list, supposing I want to insert a value here, then everything below it has to shift by one position to make space. So, you know exactly what happens. When you try to put a book into a crowded bookshelf, you have to push all the books. So, that is a lot of work. And in the worst case, if you are putting it right at the beginning of the bookshelf, then you have to move everything to the right. So, it could take order n work to make space to insert an element. And likewise, when you pull out a book from the bookshelf and you leave a hole in the shelf, you need to compress all the books. So, you need to push everything from the right to the left. So, again, it could take order in time. So, for our earlier thing that plumbing was only a two or three step operation. In the neighborhood of the element, I wanted to add or delete, I just had to move a few links.'},\n",
              " {'id': '1e85aaf0-3814-4909-84fc-33d3cf258f0c',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'So, that was a constant overhead, only problem was to find out where I was. Here, even if I know where I am supposed to do it, it is going to take me in the worst-case time proportional to the number of elements which are to my right and that could be in the worst-case order n. (Refer Slide Time: 11:04) So, to summarize, if I look at these two representations of sequences, the typical operations that we do when we are doing things like searching, sorting or manipulating arrays in general, so the one operation is to exchange two values. We do this all the time in sorting. We say okay take the say in selection sort, we find the maximum or the minimum, move it to the beginning or we might take two adjacent values which are out of order and swap them. In quicksort, we take the pivot value and we move it somewhere. Now, in this we have to look up in general a value which is at some position i and some other position j which may be very far away. For an array, this is not a problem. If I know i and j, I can calculate instantly where Ai is, where Aj is, so it is exactly like swapping two values of a normal type. If I want to exchange the values of x and y or a and b is exactly the same cost as compared to swapping capital Ai capital Aj. But in a list, it is not the case. I need to actually walk down to Ai and I need to walk down to Aj. So, it will take me time proportional to the length of the list in the worst case in order to do the swap. On the other hand, if I have, it does not happen in sorting and searching, because we are not manipulating the array. But if we have a need to actually make this sequence shrink and grow, then if I know where I am, if I am at a position and at this point, I want to make a change, then, for the list, the flexible list this is a constant time. I just have to do some rearrangement of these arrows which point from one node to the next.'},\n",
              " {'id': '456a0f85-6215-4d91-aa34-5e9f49ed03b2',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'Whereas for an array in the worst case, if I need to grow and shrink, as we said, you have to shift a lot of value so it could be order n. So, the reason to emphasize this is because the analysis that we do for our algorithms actually depends on what representation we are using, because we have been a little bit casual about this when we talked about searching and sorting, because we have always sort of assumed that we are working with arrays. When we said Ai all these basic operations, when we said, for example, swapping Ai and Aj is a basic operation, it was a basic operation if I do something like x comma y is equal to y comma x, this is not a problem. But if I am saying Ai, Aj is equal to Aj, Ai. Now, the question is to whether this is the basic operation or not really depends on whether we are dealing with an array or a list. So, in an array, this is a basic operation. In a list, this need not be. If I do not know where I am, I have a problem. So, as a concrete instance of this, let us look at this question of inserting in a sorted list. We use this as a part of our insertion sort. So, I have a sorted list and I want to take a value V and I want it. So, supposing this is an ascending order. So, then what we said is we will keep walking one by one and we will find a position and at that position, so till that position, for instance, we could keep swapping or whatever, and then we will insert it there. We typically did it from the righthand side, it does not matter. But this is how we did it. Now, you might ask, okay, why did we do all this, because it is a sorted sequence. So, why not just use what we already have talked about. So, we look here to say, should I insert it there. If it is the right place to insert, it is fine. Otherwise, I need to insert it here or here. So, I can use binary search to find out where to insert. And then having found it, I need to insert.'},\n",
              " {'id': '93faebf4-91dc-48f9-8afc-1d75c8a65293',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'Now, there are two separate operations here; one is binary search and one is inserting by making space. If I assume it is an array, then binary search is good, because I can always go to the midpoint, I mean, the 0 to midpoint, that quarte point or the three quarter point in constant time. So, binary search works well in an array, does not work so well in a list, because each time I have to calculate the midpoint in the list I have to actually walk halfway down the list. So, this part of it is good. Binary search works with arrays, but then I have to insert. Now, if I were in a list and I was told to insert here, it is very cheap. But if when a binary search finds it in an array fast and tells me to insert then it is expensive. So, I kind of gain on one and lose on the other. So, if I have an array, I will gain on the binary search and I will lose on the insert step. And if I have a list representation, I will lose on the insert position finding step. Where to insert, I will take time to find it. But having founded the insertion will be just that plumbing. So, in some cases, the trade off is such that it does not matter. That is why we have been a little casual about insertion sort just saying that the insert is order n. But it is not order n because of one or the other is because whichever you choose one of these two operations will push it to order n. So, the operation that pushes the order n is different. If it is an array, it is because of the insertion operation. And if it is a list, it is because of the finding the position operation. So, this is something to keep in mind. Whenever you are dealing with sequences, try to keep in mind whether you are dealing with lists or the arrays when you are doing the analysis especially.'},\n",
              " {'id': 'd6448674-3d9c-4b83-96e1-590dafd9dd8a',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Theory).pdf',\n",
              "  'content': 'And also, of course, when you need it for a particular requirement, just think about whether the list of sequence you are using is going to be frequently changing, is it dynamic, is it going to be growing and shrinking or is it something which is fixed once and for all. So, all these factors come into play when you choose one of these two as a representation for the sequence that you need in your program. (Refer Slide Time: 16:14) So, to summarize, sequences can be stored as lists or arrays. Lists are flexible, but accessing an element takes linear time. And arrays support random access, but expansion and contraction take linear time. So, when we do an analysis of an algorithm, we have to take this into account. Now, one of the things that we are doing in this course is we are using Python. So, the question is we have this data type called a list in Python. So, is this data type called a list? When we write this, is this list a linked list in the sense that we described here with these nodes which are connected together or is it like an array and what do we do for arrays in Python? So, in Python, there is this library which you have possibly come across called numpy, which allows us to actually define these arrays with a fixed size. But now given what we know about Python lists, are these going to be faster than Python lists or slower than Python lists? So, these are some of the things that we will look at in a later lecture.'},\n",
              " {'id': 'ddacbecb-f878-4d5b-b081-cceb85dadfc7',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': 'Programing, Data Structures and Algorithms using Python Professor Madhavan Mukund Difference between Lists and Arrays (Implementation) Earlier, we ran some experiments on lists and arrays in the context of searching and sorting. So, we tried to investigate, for instance, whether there was a real distinction between the order n squared sorts, namely selection sort and insertion sort. And we saw that selection sort behaves pretty much badly on any type of input, whereas, insertion sort works well on sorted inputs and so on. Now, we have also added into this mix the problem of lists and arrays. So, we have seen that there are two different ways you can represent sequences. And we were trying to understand exactly what a Python list means. Is it a list or an array, and we claimed, actually, it is an array. So, let us see if we can validate this using some experiments. (Refer Slide Time: 00:48) So, we start with our setup as before, so we set this recursion limit to be 2 to the power 31 minus 1, because this is needed in order for quicksort and insertions are to work properly when we use the recursive versions. And then we as before, create our timer class, which we will use to actually run our experiments and time our behaviors. (Refer Slide Time: 01:09) So, the first thing I want to check is this Python list. So, Python, we said, is a list anomaly in a list, growing and shrinking a list should be a constant time operation. But we said that, Python actually has this array implementation, so it allocates an array at a time and it keeps really, in some sense, a prefix of the array at your list with the right pointer, and the right pointer keeps moving by 1, and appending is essentially a constant time operation. Except, when you overflow when you have to kind of double the size, but that happens only once every time it doubles, and so, amortized it is an order 1 operation.'},\n",
              " {'id': 'e785b787-2c84-4719-97c9-8d2ab29eca75',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': 'So, here is something which basically tries to create a list of 10 to the 7, and it does 10 to the 7 append. So, it starts with an empty list. So, we start with an empty list and 10 to the 7 times, we append something to it. So, we append i from i equal to 0 to 10 to 7 minus 1. So, if we run this, and we time it a 10 to the 7 is roughly one second of Python execution. And as we would expect, since this is a constant time operation takes one and a half seconds. So, this validates the fact that append is O1, but it does not validate the fact that this is not a list in the flexible sense. So, what is the corresponding thing we can do? The corresponding thing we can do is to grow this list from the other end. Instead of appending, we can prefix. So, we can, Python has a function called insert, which gives a position and a value. So, instead of appending, which is to take the nth or the n minus oneth position and add something at the end, I am going to take the zeroth position and put it before. So, I am going to do l dot insert 0i. And just because it is going to take a lot of time, I am going to do it for a much smaller thing. So I am going to do it for 10 to the power 5, not 10 to the power 7, because if I did 10 to the power 7 will be sitting here a long time. So, let us run this for 10 to the power 5. I am doing 10 to the power 5 inserts, as opposed to 10 to the power 7 apprentices so it is 100 times less work, and I end up taking two seconds. So, this shows that a 100 times less work takes about the same amount of time. And if you want to validate something is actually quadratic, which is what we claimed it. Every insert needs to push n time, so it is 1 plus 2 plus 3 to n minus 1, it is a bit like selection sort. So, supposing I take this 10 to the power 5, that is 1 lakh and I make it 2 lakhs, I make it 20,000. So, I am doubling, so what would you expect that if it is n squared, if I double it is 2 squared, so it will take me 4 times as much time.'},\n",
              " {'id': '6a5a00fd-5042-4080-8a96-ed2a3aec7e10',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': 'So basically, if this is 2 to the power 4 it should take me something like 10 seconds, so we can validate that. So, we can run this now. So earlier, it was taking 2 to 2.5 seconds and now it is taking a lot longer. And in fact, it takes 9.8 seconds. So, it takes roughly 4, so 2.4 became 9.8. You can do it one more time, if you really want to check, so I make it into 30,000. (Refer Slide Time: 4:10) So, if I started with 10,000, and I do 30,000 it is 3 times, so 3 squared is 9. So, it should take me 9 times 2, it should take me 18 to 20 seconds. It initially 2 seconds, I will now triple the input, so the time is going to take 3 times 3, 9 times. So, if I run this on an input of size 30,000. So, what am I doing? I am taking a Python list, and instead of appending I am inserting. So, when I insert 10,000 times it took me 2 seconds. 20,000 times took me almost 10 seconds 30,000 times it is taking me 22 seconds, which is something like 9 point something times 2.5. So, 9 into 2.5 would be 23 seconds. So, roughly, it is growing. You can see, visibly that is growing quadratically so this validates our claim that Python lists are actually arrays, and so insert are pushing things, delete, contracting, expanding a list from middle is going to be expensive, adding at the end is cheap. (Refer Slide Time: 05:07) Now, let us run our searching things on lists and arrays. So, we have this naive search, which we wrote, which is just a scan of a list and binary search, which is of using lists. And we can do the same thing now with arrays. So, I am using now numpy array. So, as far as the code goes, it does not really distinguish. But the fact that it is an array means that I cannot shrink and grow, so I have to give the left end point and the right end point explicitly. So, the main difference between the naive search and the array searches is I have to actually tell you which segment I am searching, but otherwise, it is the same function.'},\n",
              " {'id': '9cb62a35-032e-4b44-bd93-41c4b9292b32',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': \"So, you don't have to worry too much you can see the code later on. But the point is that if I now do a naive, and binary search on a list of. So, what we have is we have a list of 10 to the power 5 elements, and we are doing about from 10,000 searches on it. And if we do 10,000 searches on that, so it's sorry, 1,000 searches. So, this is 10 to the power 7. So, what we find is that this linear scan basically, is going to look at all the elements, so I am going to do 10 to the power 4, times 10 to the 5, 10 to the power 10 to the power 8 work, which is roughly 10 seconds. So, naive search takes 10 seconds, binary search takes a fraction of a second. (Refer Slide Time: 06:38) So, what happens when we do the same thing on arrays? So, again, I do the same thing. But just because I know in advance it is going to work badly, I have reduced the naive part, not the binary search. The binary search is still doing the same thing. I have 10 to the 5 elements, and I'm doing 1,000 binary searches. But then I think I have reduced it to from 10,000 binary searches to 2,000 binary searches. And the reason is that it takes a long time. So, here I am using notice I am using a numpy array. So, I am instead of using a list of that size, I am using a numpy array of that size, and it takes a lot longer. So, this shows that actually Python lists are actually quite optimized to behave almost like arrays, but numpy arrays as we said are convenient. We will use them for graphs and other things where it is convenient to set up these matrices. So, therefore, here for insertion, not insertion, naive search of one-fifth. Instead of 10,000 I am doing 2,000. For one-fifth work it takes almost double the time. Now, binary search behaves as we would like. So binary search, which was taking something like 0.8 seconds now is reduced to something like 0.2 seconds. So binary search has actually speeded up for a numpy array, but it seems that this is the only one which works.\"},\n",
              " {'id': 'a6cf7958-097e-47e8-b11c-40a8629ba1a1',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': \"Because as we go down and look at other sorting, we will find that this does not help us much. (Refer Slide Time: 07:58) So, let us look at selection sort. This is selection sort on a list and this is selection sorting an array, again, it's very similar, except that I have to extract the size of the array using the numpy shape function, as we said. So, one of the things is that, selection sort performs pretty much the same on all lists. So, we take random lists of size 10 to the power 4, ascending and descending, and we run them all, and all of them will take roughly n squared time. So, 10 to the power 4 will be 10 to the power 8, so it will take something of the order of a few seconds. More than 1 second, roughly 10 seconds. So, you can say that random takes 5 seconds and all of them take about the same amount. Because remember, the selections are basically has you keep scanning, finding the maximum bring it then finding the maximum bring it and it does not really matter. Whereas, we saw that if I look at, so let us see what it does on arrays. So, now an array is just like we saw that this naive search takes a lot longer. Even though it is supposed to be just a linear scan, the fact that you are using a numpy array seems to slow down things. So, even selection sort actually slows down. So, if I do the same thing in selection sort, as I did before, then it takes about 3 times a time. What we were seeing 5 seconds is now taking 16 seconds. So, this seems to suggest that actually this numpy array implementation though it gives you some bonus in terms of the array operations it is not actually giving you some performance in terms of the indexing, which you are primarily using for searching and sorting. So, selection sort is actually slower with arrays, you can verify that the same thing actually happens for insertion sort also. So, I will not in the interest of time, you can take this same thing and run it on insertion sort.\"},\n",
              " {'id': '40409472-02fc-4c08-8bb3-57c2dc808404',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': '(Refer Slide Time: 09:56) So, what we will do is we will go ahead and look directly at merge sort. Because merge sort is also something interesting. So, this is my merge sort on lists, and this is my merge sort on arrays. Essentially the same thing except I have to manipulate the indices differently when I use arrays because listen, arrays, numpy arrays and Python lists have slightly different syntax. (Refer Slide Time: 10:15) So, now, what we did earlier was we took merge sort, and we ran it on inputs of size 10 to the power 7. And if you run it on 10 to the power seven n log n should be 10 to the power 8, so it should take roughly 10 seconds to run is what we think. So, indeed, it takes 9.5 seconds on a random input, and it works slightly better on ascending and descending order, so it takes 7 seconds on ascending. And how much time on descending? It should also takes a similar amount of time, I think 7.5. So, it is slightly better on ascending, but roughly the same. But the point is, it is able to do 10 to the power 7, which we cannot even hope to do with insertion sort or selection sort. But if I do the same thing with arrays, again, what we will find like in the, so the same thing, I am just using the A range function of numpy to create a range of values rather than the range function for lists, and then I am resetting it to some random numbers. So, I want random integers, because it turns out numpy as a float, random, which takes more time. So now if I run this, merge sort on the same size sequence, but as an array and not as a Python list, a numpy array and not a Python list, it actually takes appreciably more time. So, this is learning that, theoretically, we saw that lists have this flexible structure, arrays have random access, arrays should be better than lists, but Python lists are not lists.'},\n",
              " {'id': 'f019e6e5-8d92-4735-89b6-56ebb0497b8f',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': 'Python lists are these kind of arrays masquerading as lists, so we already saw in the experiment above, that insert was slow compared to append because of this asymmetry in the way that lists are handled. And notice here that, merge sort on an array takes 10, 5 times as much time. This is something earlier, we saw their selections sort took about 3 times is a much time. So, there is a certain cost that numpy arrays impose on us, which comes maybe, I do not really have an explanation as to why it happens, but and numpy has its own. It is a layer built on top of with its own advantages in terms of matrix operations. But the cost you pay is that when you try to use them in these things, where the only thing that matters to you is the indexing. Then actually, Python lists seem to work better than numpy arrays. So, you can see now that merge sort has taken 45 seconds, so and it is going to take again similarly, another. So, it is an order of 5 to 10 times slower for these things than it is for the list. So, I hope this shows you that, first of all, there is a difference. So, you cannot just take an algorithm on a sequence and assert something about it without knowing how the sequence is implemented. Now, theoretically, arrays are better than lists. So, theoretically, we should be able to analyze these on arrays and lists saying that arrays or random access lists are not random access, and so on. But then what seems to be a list may not be a list, and this is what we have seen. So, a python list is not a list, a numpy array is an array, but it seems to be less friendly for these kinds of operations then a Python list is. So, you have to be a bit careful about the data structures that you use, for the context that you are using it. So, if you do not need to do any complex operations, you are just maintaining a sequence of values, go ahead, by all means and use a Python list.'},\n",
              " {'id': 'f0931078-d868-4aaf-b87d-28b70534aada',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 3},\n",
              "  'source': 'Difference between Lists and Arrays (Implementation).pdf',\n",
              "  'content': 'If you are going to take a two-dimensional representation, like in a graph and do some operations on it, then it is a real nuisance. We saw that using that list comprehension and all that setting up a two-dimensional Python array is not the best of things. So, then it is much more convenient to work with numpy arrays and then it does not really cost you much because you are not going to be sorting those graphs and so on. So, just keep the data structure in mind when you do the operations that you have.'},\n",
              " {'id': '67269f71-26db-42dc-84f1-a3cf5a446063',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Introduction to Directed Acyclic Graph (DAG).pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor. Madhavan Mukund Directed Acyclic Graphs (DAGs) So, we saw that we can use BFS and DFS to detect cycles in a graph. In an undirected graph, we just look for non-tree edges and in a directed graph, we use the DFS numbering in order to look for back edges and find cycles. Now, there are actually a large class of applications where directed at acyclic graphs are important. (Refer Slide Time: 00:30) Typically, a directed acyclic graph is what you need to represent a set of tasks and their dependencies. So, suppose you are moving into a new office, let us say a start-up is moving to a new office. Now, there are a number of things that need to be done to make the office space ready for occupation. Do you have to lay the tiles, plaster the walls, paint the walls, you also have to lay conduits because there are going to be lots of cables and wires, you have to do the electrical wiring you have to do the electrical fittings, the plug points, but you also have to do the telecom wiring and the telecom cabling, so all this has to be done and there will be some constraints usually over this. So, there will be some constraints. Let us say for instance, that you obviously cannot break open the wall after it is plastered and painted, not really conduit, so conduits have to be put before tiles are put on the floor before the plastering is done, you would probably want to lay the tiles and plaster the wall before you paint, because especially Of course, you have to plaster the wall before you paint. But then if you put tiles after you painted the walls, some of the cement used to lay the tiles might splash on the walls. And finally, you might want to finish painting before you do some cabling and wiring and you might want to do the electric wiring before you do this fitting because you cannot put the light bulbs refill wires are not in place.'},\n",
              " {'id': '1056548f-3ac7-4492-b4b2-6039c1e4559b',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Introduction to Directed Acyclic Graph (DAG).pdf',\n",
              "  'content': 'So, these are typically represented as a directed graph in which the vertices are the tasks to be done and there is a dependency. So, these constraints say you must do this before you do that. So, that is a directed edge u before v. (Refer Slide Time: 02:05) So, let us look at these constraints. So, we have these tasks. So, these are the vertices. So, here is the graph that we might want to construct using these vertices. So, we have these various tasks, you know, conduits, tiling, plastering, cabling, wiring and so on. So, now the constraint, the first constraint is laid the conduits before the tiles and plastering is done and there are two types of conduits, remember the electrical conduits and telecom conduits. So, both the electrical conduits and the telecom conduits have to be done before the tiling and the plastering is done. Then it says lay the tiles and plaster the walls before you paint. So, the first, the second two steps must be done before the painting step. Painting has to be done before wiring and cabling has to be done and wiring has to be done before the fittings are installed in the electrical setup. So, this represents the constraints, this is a directed graph and this directed graph should not have cycles. (Refer Slide Time: 03:06) So, if it had cycles, it would mean that I have to do a before B. But I also have to do B before A. So, that would be an unrealizable thing. So, we want to schedule these tasks so that these dependencies are respected and there may be many ways to do it. So, we can just do it like this. We can take the conduits first and then we can do the tiling and the plastering. Then we can do the painting and then we can do the wiring and the cabling, then the fittings, so this would be one sequence in which you could do it. But if people are not available or there is some other optimization that we can do in terms of using manpower, we could also do some other things.'},\n",
              " {'id': 'a7ac1de6-2490-423d-9574-ccb1c235977f',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Introduction to Directed Acyclic Graph (DAG).pdf',\n",
              "  'content': 'So, we could do conduits of the telecom before we do the conduits of the electrical thing and we can do plastering, before we do the tiling and then after we do the painting, we could do the electrical wiring and then finish it off, maybe the electrician is there, finish off the wiring and only then come to the cabling. So, there are more than one way to process these tasks, while respecting the dependencies. So, that is one thing we want to know, what are the different ways in which we can execute these tasks, maintaining these dependencies and the other thing we might want to know is how long is the whole thing going to take, if I do this with as many people as possible, there is still some constraint that I cannot finish one job till I do the previous job and there may be some other constraint might be that I need the paint to dry. So, if I paint the room today, maybe I will not be able to work in this room for another two days late. So, some of these tasks might have other things, plus we have the fact that we cannot violate this thing. So, if I have all my working staff available all the time, but I still have to do these and each of these has to be done on one or multiple days. What is the minimum number of days, so how long will it take is basically a minimization problem, what is the fastest I can do this? (Refer Slide Time: 04:55) So, this is a directed graph without cycles, which is often called a Directed Acyclic Graph or a DAG. So, finding a schedule amounts to listing out these vertices in a sequence. This is the order in which I am going to process the work, you can think of it, in such a way that I never have an edge going from something which is later in the sequence to something earlier in the sequence. So, I do not do conduits after plastering. So, in the sequence, all the edges must be respected by the sequence.'},\n",
              " {'id': '4d5fab76-fd53-42e5-8a00-9f69b278b6f1',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Introduction to Directed Acyclic Graph (DAG).pdf',\n",
              "  'content': 'So, if i appears before j, then there cannot be an edge from j to i, so this process of numerating the vertices in a consistent way with the constraints, constraints being the edges in my directed a cyclic graph. So, this is sometimes called topological sorting. So, I want to take a graph, which has many possible sequences, which are compatible and produce any one. This is called topological sorting. The other thing, which is to ask how long I am forced to wait, is what is called the longest path. So, if I look at this, for instance you can see that if I start this and then I say, assume that everything can be done in a day, then on day 1 I can do this. Then on day 2, I can do these things and then day 3, I can do these things, day 4, I can do these things and only on day 5, I can do this. So, I am guaranteed that I need 5 days to do this, there is no faster way no matter how I optimise it, there is no, it could take more than 5 days if I delay it, but I cannot do it in less than 5 days, because I have this longest path in my graph, which forces a dependency of 5 days. (Refer Slide Time: 06:45) So, we are going to look at how to calculate these things using our usual graph representations. So, to summarise, we have this notion of a directed acyclic graph. So, it looks like acyclic graphs are very simple and we should not need to worry about them. But actually, directed acyclic graphs are very important, because they very naturally capture this idea of tasks and dependencies. So, anytime we have a constraint on doing things one before the other, this is what we need to analyse in order to understand how we can execute it without violating the dependencies and also how we how fast we can do it. So, topological sorting and longest path. So, these come in various forms. For instance, you might have to satisfy some prerequisites before you complete a degree. So, what is the minimum number of semesters in which you can complete and graduate?'},\n",
              " {'id': 'a63c6014-e2ed-498c-9d00-d023718f4281',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Introduction to Directed Acyclic Graph (DAG).pdf',\n",
              "  'content': 'Or you need to process some food for cooking. So, some things can be overlap, maybe you want to marinate something while you are chopping something. So, maybe you have a lot of people in the kitchen who are helping out. But still, some steps must come before other steps. So, how fast can you actually prepare a dish? Or of course, in a construction project, the kind of thing that we said for preparing the room, also applies to a building as a whole, you have to lay the foundation, then you have to do some work on laying this slabs and then you have to do some work. So, how fast can you actually put up a house or a building? So, all of these things have these natural constraints, which can be expressed in terms of DAGs and then these questions which arise, which is how do you process the tasks to satisfy the constraints and how fast can you do them.'},\n",
              " {'id': '36a7d428-8ecd-49f1-a32e-2323fea52199',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Breadth First Search (BFS) First strategy to systematically explore a graph is called Breadth First Search. (Refer Slide Time: 0:15) So, remember that we are looking at reachability. So, what reachability said was that we start from a vertex and marked it as reachable. And now we systematically mark all the neighbours of already known reachable marking. So, you start with the thing, you know it is reachable look at its neighbours, mark them as reachable, look at their neighbours mark them as reachable, stop when the target is marked, but you want to avoid going around and redoing something. So, you need to keep track of which guys are marked. (Refer Slide Time: 0:37) We also said that we have two representations possible, we have this adjacency matrix, and we have the adjacency list, and we have to choose the correct one. So, what we are going to look at this time is the first strategy, the breadth first search, and next we will look at the depth first search. So, in a breadth first search, we propagate these things one level at a time. So, we start with the vertex which is reachable in 0 steps, then we see what all we can reach in 1 step. Then from the 1 step vertices, we see what all we can reach in 1 more step, so in 2 steps. From 2 what we can reach in 1 more step that is in 3 steps, and so on. So, we kind of that is why it is called breadth first, kind of growing this set of nodes, broader and broader. And in depth first search, on the other hand, I pick one neighbour, I say, I can go from here to this neighbour, then you do not go and look at other neighbours, you say if I can go to that neighbour, where can I go from there? So, I start exploring from that neighbour. And I keep exploring until I get stuck, I cannot go any further. Then I come back and at each point, I say I got stuck now, which are the other neighbours, which I did not explore.'},\n",
              " {'id': '133a77c8-a906-4d29-b1fb-c6b4cd9f3dda',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, I go backwards and forwards and explore all the nodes. And this is called depth first search. (Refer Slide Time: 1:43) So, in breadth first search, we explore the graph level by level. So, we first visit vertices one step away, then two steps away, and so on. So, whenever we reach a vertex for the first time, it has to be explored in the sense that I have to look at its neighbours, which I have not seen so far, and explore them. And but, we must make sure that we do this only once. So, we do not want to come back from, for example, we go from, as we said, we go from 9 and we say we restate, then I do not want to look at 8, explore it, and then come back to 9. So, we must make sure that we do not visit a vertex twice. Or in other words, we do not explore its neighbours twice, because if we do that, then we could go around forever. So, for this, we need to maintain some information, so we need to know which vertices have been visited. And among these, we need to know which ones have been explored. So, those which have been visited, are then to be explored. So, there are two steps. First, I visit a vertex saying this is the first time I have come here, so I need to explore it. But I might be busy doing something else. So, I come back later and I say now I have not visited this, let me explore it. (Refer Slide Time: 2:50) So as usual, let us assume that our vertices are 0 to n minus 1, then what we do is we set up this dictionary or function or whatever you want to call it, which marks each visit vertex as either visited or not visited. So, we have the keys or the position 0 to n minus 1. And each entry is either true or false. So initially, everything is false, nothing is visited. (Refer Slide Time: 3:13) Now we have to maintain those vertices. So, this is telling us which vertices are visited. But it has not told us which of these have been explored. So, there is a two step process, I visit a vertex, and then I explore it that is I look at its neighbours.'},\n",
              " {'id': '5a9a853c-6b9f-4037-8650-8b90309d20ea',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, in order to do that, we need to maintain this information in some systematic way so that does the ones which I have yet to explore will eventually be explored, and only once. So, the standard way to do this is to use a queue. So, what is the queue, a queue is just what you think of as a queue in real life, it is you join a queue from one end, and then when you move up to the queue. And when you reach the end, that way, you are going to get serviced, you move out of the queue. So, supposing standing in a bank, or you are standing, waiting for some delivery in a fast food place, and you join at the end of the queue, and when it is your turn, you will be served at the counter. So, there is an entry end, and there is an exit and otherwise the sequence. So, this is a queue. So, let us, so we can of course describe a queue in Python as a list. And you can say that when I want to add to the queue, I add on the right hand side of the list, I append. And I when I want to exit the queue, I remove the leftmost element. So, let us just for convenience, wrap this up as a class and an object so that we deal with it more abstractly. So, we have this class queue and that class queue internally maintains a list, which is called queue the small queue. So, self dot queue is initialized to empty. So, when I set up a queue, it is empty. Then I have an add operation. So, I want to add v to the queue. What it does, as we said is it appends to the right so it takes a queue which is stored internally and appends the value v to it. Now, when I want to remove from the queue, delete from the queue, I essentially want to take the zeroeth element, return it and reset the queue to start from the first element. So, I am taking the zeroeth element out. So, deleting from a queue or removing from a queue reduces the length of the queue by 1. So, this customer is gone, this customer is out of the queue, otherwise, the next customer not going to get to the head of the queue.'},\n",
              " {'id': '23835888-a423-464c-9f53-9961e675e6ba',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, the first element of the queue is the one that is going to be processed next. And when you process it, you remove that thing from the queue. Now, obviously, there is a problem here, if there is nothing in the queue? If I try to process an element, and the queue is empty, it should not work. So, what we are going to do is not worry too much about it. But we are just going to say that, by default, we assume that there is nothing to process. And if the queue is not empty, so I put in a check before, I do not want this call this thing to give me an error. So, this slice is not going to give me an error. But if I try to access the zeroeth element of an empty list, Python is going to say it is an index error. So, I am going to ensure that it is not empty. So, it is not empty, is basically saying that the internal queue is not the empty list. So, I write a separate function, which basically returns true or false. And if it returns true, in other words, it is not empty, then I will process it. If it is empty, then I will skip it. And finally, just for inspection for debugging, you might want to check what the queue looks like. So, we will have this function which converts this queue object to a string by just returning the string representation of the internal list. So, if I try to print a queue, it will print a list. That is what it says. So, this is now my class representing a queue, which I will be using in breadth first search in order to maintain this extra information about which vertices have been visited, but whose exploration is still pending. (Refer Slide Time: 6:50) So, this to get an idea how the queue data structure works. So, supposing I start with an empty queue, and then for 0, 1, 2, I add them. And then for 0, 1, 2, I remove them. So, what I do is I print the queue at every stage. This is why I said for debugging purposes. So, after I add a 0, my queue looks like this. After I add a 1, my queue looks like this, after added 2 my queue looks like this.'},\n",
              " {'id': '74de512f-bbd3-4fdc-9b2f-5ff95717fa14',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'And now I check whether the queue is empty. And as expected, it says it is not empty, it is false. So, this is the first half of the program. And now in the second half, I remove from the queue 3 times. So, I print the value that is removed and the remaining q. So, initially, the value because this is my q, so the leftmost item 0 is removed. And what remains is 1 2, I do it again, 1 is removed. And what remains is 2. I do it 1 more time, the final item comes out and add the empty list. And now if I query whether the queue is empty, indeed, it is empty and returns me true. So, this is just a simple way to illustrate how this queue data structure works in this object situation. (Refer Slide Time: 8:01) So, with this queue in place, what are we going to do? When we explore a vertex, we look at all its neighbours. So, there are two things remember whether a vertex is visited and whether it is explored. So, when we look at a neighbour, we first check, whether it is been visited, if it is been visited, then we have already done something or something is pending, we do not touch it again. So, if visited of j is false, then we want to visit it. So, we say, if it has not been visited before, I will now visit it, so I will mark it as visited. And I will put this thing into the queue. Putting into the queue means at some later stage, I am going to look at its neighbours I am going to explore it. So, this is the basic step in breadth first search, I start with the vertex I mark it is visited, put it in the queue, then I pull out something from the queue. I check whether its neighbours are visited, put them into the queue. And I keep doing this until I processed everything that has been visited. (Refer Slide Time: 8:59) So, initially visited is false for every v, and the queue is empty. So, if I start my breadth first search at a particular vertex j, then I set that vertex to be visited, I set it is visited value to true and I added to the queue.'},\n",
              " {'id': 'd2948943-d693-4695-8880-946cabf6d481',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'And now I keep removing from the queue. So, now I have something in the queue. So, I would have j and j needs to be explored. So, right at the beginning, I have started off with one vertex which needs to be explored. So, I remove it from the head of the queue. I look at all its neighbours, and all the unvisited neighbours get pushed in the queue. Then at the next step, I will take one of those neighbours and do the same thing and keep going until I have processed everything which I have visited. So, I stop when the queue is empty. (Refer Slide Time: 9:38) So, here is my Python code for this breadth first search. So, I am taking an adjacency matrix. This is the graph that I am trying to explore. And this is my starting vertex v. So, I need to now keep track of these data structures. I need to keep track of the queue and I need to keep track of this visited value. So, the first thing I do as before is I find out how many vertices there are by querying the shape attribute that NumPy tells me. So, remember that this is, logically it should be a square matrix. So, rows should be equal to columns, I just want one of them. So, what I do now is I initialize this visited thing which I will keep as a dictionary to be false. So, for every vertex for every i in range of rows, that is for everything from 0 to n minus 1, I initialize the visited of that i to be false. And I create an empty queue. So, I am here right now. So, I have basically, I have initially set visited v to be false for all the vertices in the graph. And I have taken the queue to be empty. So, now I have done this. So, I have now finished my initialization. So, now I come to this one. So, I want to start from j, which is, confusingly called V over there. So, I set visited a V equal to true. So, I initialize my thing by saying the vertex I start my BFS from has been visited, and I put it into the queue. And now finally, I have this loop here.'},\n",
              " {'id': '39ec9906-1665-4aed-9c1e-495200ff890f',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, it says, so long as the queue is not empty, take the first vertex in the queue, initially, is going to be the vertex V that I started with. For every neighbour, remember, we had this neighbour function, which looked at all the elements in the row j, and pulled out the ones. So, that gives me a list of neighbours. So, for every K, which is a neighbour of J, if it has not been visited, then make it visited. So, basically, do this, make it visited and add it to the queue. So, notice what is happening. So, basically, this vertex is now being put into the queue for later processing. So, every vertex that I see for the first time gets put into the queue, so it will eventually be explored. But I will never put it a second time because it will be marked as visited. So, every time every vertex that is actually visited by it at some level of the search will be put into the queue. But once it is put in the queue, and I will put a second time because visited has been set to true, and I only do this for visited, where visited is false. And finally, at the end of this whole thing, my visited dictionary is set to true for every vertex I actually saw. So, if I look at that, I should get my answer. So, I just return that dictionary as the outcome of this process. And this should tell me, which are all the vertices, which are reachable from the starting vertex V. (Refer Slide Time: 12:36) So, let us see how it works on this graph, for instance. So initially, I have everything visited is false. And supposing I want to run my BFS from vertex 7. So, when I do that I initially set visited of 7 to be true, and I put 7 into the queue. So, my queue looks like this. Now, I start processing the queue. So, I pull out the 7, and it has neighbours, 4, 5, and 8, all of which are previously unvisited. So, I marked them all visited, and I removed the 7 and put 4, 5 and 8 into the queue. So, now I have to process these in this particular order.'},\n",
              " {'id': 'b68deda8-df6a-48c0-bdcd-cdd9500f89fa',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, because 4 got put into the queue, first, I will now pull out 4 and look at its neighbours. So, the neighbours of 4, if you see are 0, 3, and 7, of which 7 is already marked, so from 4, it will add two new things 0 and 3 to be true, and put them in the queue. And now 4 is done. Then I pick up the 5, because that is the next thing here, so 5 is my next element in this queue here. So, 5 is here. So, I pull out the 5. And now 5 has neighbours 3, 6, 7, and 8 of which 7 and 8 were already seen before, so only 6 gets added. So, we keep doing this, now we pull out 8, and we look at its neighbour, so the new neighbour of 8. So, the neighbours of 8 are 5, 7 and 9, of which only 9 is new. So, I will pull out 8 and put a 9 to the queue. Then I will pull out 9. And nothing I am sorry, I will put out 0, because I am doing it from the head of the few obviously. So, after 8 I pull out 0, so 0 has neighbours 1 and 4, but 4 is already been marked. So, it will only add 1 and 2. And now I will explore 3, the next thing and there is nothing new to add. I will explore 6, there is nothing new to add. I will explore 9 there is nothing new to add, explore 1, there is nothing new to add. I will explore 2. So, now I have explored everything that I put into the queue and now the queue is empty and I stop. And this visited array or dictionary tells me that every vertex was visited in the process. So, this is how breadth first search works. (Refer Slide Time: 14:48) So, as an algorithm, how much time does it take? So, let us assume that we have n vertices and m edges. So, if G is a connected graph as we said everything is connected, then it has at least n minus 1 edges, but it could have at most n into n minus square 1 by 2, so n squared edges. Now, in BFS each reachable vertex is processed exactly once this is what we said once you see it for the first time you market visited, you never see it a second time.'},\n",
              " {'id': 'a7bdf7ba-62db-4d57-9996-71ba39ac96b4',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, there are two steps you visit the vertex, which case you added to the queue and you explore the vertex, you remove it from a queue. So, each vertex enters the queue once and leaves the queue once. So, you visit and explore at most n vertices. I mean, you may not explore all of them, if not all of them are reachable if you have parts of the graph which are unreachable will not reach them. But you certainly cannot explore more than n steps, you have only n vertices and each vertex can enter and leave the queue only once. So, then the question is, how much time does it take for the second step? See, putting it into the queue is a one, is a kind of single operation, I set visited to true and put it into the queue. When I remove it and explore it, I have to explore all its neighbours. So, how long does that take? So, in this adjacency matrix representation, we have to scan all the neighbours? So, we have to look at all n entries in the row of i regardless of how many neighbours it actually has. So, I have no option but to spend time looking at all the neighbours. Whereas, if I had a cleverer version, which looked at the adjacency list, then I would only pick up the neighbours that are there I would not look at the neighbours, which are the non-existent neighbours, I would not have to look at what are the 0s and my adjacency matrix. So, it becomes faster. But of course, now the question is, if I am doing it with an adjacency list, depending on the degree, it could take more or less time to process one vertex. So, if I am doing it with an adjacency matrix, it is easier to calculate wh at happens. I have put each vertex in once, and each vertex I spend n time processing its neighbours. So, I have n vertices I put in, and each one takes time order n to scan its neighbours. So, I have n times n is n squared.'},\n",
              " {'id': 'f3d81105-abd0-450c-a859-505d47173a1a',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'Now, for adjacency list representation, I put each of these things into the queue once, but when I pull it out how much time I spend on it depends on how many vertices it is connected to. So, this is a little bit problematic. So, let us see how to analyse this. So, one thing we can observe is that even though the sum the degrees of individual vertices vary, I can tell you something about the total degrees added up across all the vertices. The claim is that if I add up the degrees of all the vertices in my graph, it must be two times the number of edges. And why is this, that is because if I take a given edge, i comma j, then it contributes the degree of i and it contributes the degree of j. So, each edge gives me contribution to two vertices degrees. So, across all the m edges, I have 2 times m added to some degree or the other. So that means the sum of the degrees across all the vertices in a graph is always going to be 2 times m, regardless of how it is individually distributed among the given vertices. (Refer Slide Time: 18:06) So now, to come back to the complexity, if I use this adjacency matrix, as we said, in the worst case, we are going to visit everything. So, we are going to n times we are going to put them into the queue. But even before that, we have to first initialize this thing. So initially, we have to set visited i equal to false for everything. So, we have to look at all the vertices and initialize each vertex. And then in the worst case, I am going to visit all of them and explore each of them. But it takes n steps to explore a vertex. So, I visit n vertices, and I take n time to explore each vertex. So, n times n is n squared. So, the overall time becomes n squared. Whereas, if I have an adjacency list, I still need to initialize the vertices. So, I will take n steps. But now, we can do something a little bit more sophisticated. So, we say that, supposing I visited it as V1, V7, then V8, then V4, and so on.'},\n",
              " {'id': '6c9049b9-683e-4b19-a5ca-01487a9829e4',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, supposing this is my sequence, so let me just write it as 1, 7, 8, 4. So, I spend a certain amount of time doing this, I spend a certain amount of time doing that I spend less time doing this, and so on. So, this is how much time the inner part of that exploration takes place. It depends on the degree. But if I add up all these red lines, what am I doing, I am adding up the degrees of all the vertices, I am adding up the degree of 1 plus a degree of 7. So, the total time I take is a sum of degree 1 plus degree 7 plus degree 8 plus degree 4. So, even though I cannot tell you which ones were fast, and which ones were slow, I can tell you that across these n vertices, the total time that I took was the sum of the degrees and that is 2m. So, this is a kind of interesting analysis. It is not saying anything about each individual thing is rather saying something about the thing as a whole and saying that if I do more work on one vertex, I must corresponding be doing less work on some other vertex. So, this is the form of accounting which so we are really doing some accounting, we are counting how many times these things happen. So, the simpler form of accounting is I do this n times, and each time I spend so much time, so n times something. Whereas here, I am not saying that. I am not saying I do, I am not telling you precisely how much I spend each time, but I am saying that across these n vertices are explored, I am going to add up totally to only 2m steps, it cannot be more than 2m, even if some of them are very large, others will have to be correspondingly small, because there is an upper bound on the sum of the degrees. So, this is what is sometimes called amortized analysis. So, rather than looking at individual operations and adding them up, I look at a cumulative set of operations. And I say across the whole thing, I do not know that individual distribution very clearly, but I know that overall, it cannot take more.'},\n",
              " {'id': '932ed3af-104b-41f9-a08c-1f1a4feca99b',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, if I lose more, if I gain something there, I will lose it here. If I lose something here, I will gain it there. So, I must take 2m steps. But still, this is not bad, because what it is saying is that I spend n steps visiting the vertices, initializing and visiting them. And the exploration step across all the n explorations takes 2m steps. So, the total time complexity now is not n squared, but n plus 2m. And of course, I can drop the 2. And I can say it is order of n plus m. So, if I use an adjacency list, my amortized complexity is n plus m rather than n squared. So, this is important for us, because in many situations, m is actually quite small compared to n square. So, most of the graphs that we actually draw and many of the graphs we encounter in practice are what are called sparse graphs. In a sparse graph, I have edges which are proportional to n usually, rather than n squared. So, if I am doing m plus n time, I am doing much better than if I am doing an n squared time. So, for graphs, typically, this is the best possible complexity order m plus n. So, this is considered to be linear time for a graph. Now, why is it the best possible time? Well, it is difficult to imagine any serious graph problem in which you do not examine the whole graph. So, if you examine all the graphs, and all the edges, all the vertices and all the edges, then you need to spend time order m plus n, you have to look at the whole graph. So, you cannot realistically do much without looking at the whole graph. But on the other hand, if we separate out m from n, and we do not just take m to be its worst case, which is n squared, then I can show that certain implementations like this adjacency list actually worked much better. (Refer Slide Time: 22:30) So now, let us see what more we can do with BFS. So, with BFS so far, we have just said that we can do reachability. But one of the things that we were interested in terms of reachability was how to get from one place to another.'},\n",
              " {'id': 'eefe49ff-11d7-47ec-8531-1315c55a56e6',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, it would be nice if we could also record the path. So, how do we recover a path from i to k? So, in order to recover a path, we need to, we know that there is a path because the visited value of k is true. But when visited value of k was set to true, it was set to true from some previous thing. It was set to true because there was an edge j comma k. So therefore, the step to reach k, last pass through j. So, I can now record that k is the parent of j. Now from k, I can follow parent links back to trace back to path to i. So, if I, every time I add an edge to the visited thing, if I record from where I added that edge, then I can follow these links back. So, I keep track of one extra dictionary called parent. So, here is a version of breadth first search which uses adjacency lists and maintains this parent information. So as before, we keep this visited dictionary and now we have a new dictionary called parent which are both initialized to empty. Now, when we did an adjacency matrix, we had to basically initialize the visited thing from based on the size of the matrix. So, we looked at the shape that is here, I can just take the keys of this. So basically, it is the same thing, but I ran through a loop from 0 to n minus 1 and I say that all the vertices are initially not visited and all of them have no calculated parents. So, remember that the vertices run from 0 to n minus 1. So, minus 1 cannot be the parent because there is no vertex called minus 1 in my thing. So, it is safe to use minus 1 as a invalid value to say that parent for this vertex has not been calculated yet. Now, this is BFS so I set up an empty queue, I mark the initial vertex to be visited. I add it to the queue. Now the initial vertex is where I start from. So, it was not visited from anywhere it was given to me to start from there. So, I will not mark its parent to be anything. So, I will leave its parent as minus 1 for now.'},\n",
              " {'id': '5604f4cd-ab98-455e-9463-5e30bb89c08c',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'We know That is if something is so it will be the only vertex that will be visited, which was not visited from somewhere. So, if something is visited, but its parent is minus 1, I know that is where I started. For every other vertex that I visit, I will set its parent as we will see. So, the rest of the thing is the same as the earlier BFS. So, while the q is not empty, I take out the head of the q. And now I am using an adjacency list. So, I do not have to go through all the rows in the matrix, I can just pick up this list of neighbours of j. And for every such k, which is a neighbour of j, if it has not been visited, I set its visited status to true, I add it to the q. And now here is the new thing that I do. I set its parent to be j because j is the reason why k got added to this list. So, if I visited k, by following an age from j to k, then I set the parent of k to be j. And now I want to keep track of both of these quantities. So, I return not just which ones are visited, but I also return this parent dictionary, which tells me if I say, you claim that vertex 7 was visited, or vertex 9 was visited, how did you get there? So, I will say parent, 9 was something the parent of that do something, and so on. So, I will kind of trace back apart from the target back to the source. (Refer Slide Time: 26:21)  So, here is the same BFS we did earlier, but keeping track of the parent information, so we want to start from 7. So, this is where we are starting our BFS. So initially, all parents are minus 1, all vertices are false. So, now when we initialize, we mark 7 to be true, we do not change its parent, but we put it in the queue. Now, each time we now pull out something from the queue and process it, we not only market visited thing, we also market spirit. So, we pull out 7, we mark 4, 5 and 8 as before, but the important thing is that we have now put the parent of 4, 5 and 8 to be 7 saying that I reached these vertices from 7, the last thing I did to reach them with 7.'},\n",
              " {'id': '70990bf4-408e-426e-8a2e-0ec54041ae5e',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'Similarly, I pick out 4, and I put 3 back and 0 back. And for each of them, I set the parent to be 4 so I not just in addition to making them true and putting them in the queue, I also set the parent vertex. So, now from 5, I get to 6. So, I say the parent of 6 is 5. From 8, I get to 9. So, I say the parent of 9 is 8. So, that is here. Then from 0, I get to 1 and 2. So, I say the parents of both 1 and 2 are both 0. So, that is here. And then after this, nothing new happens. And so I come out with this. So, now I have the fact that everything is visited. But if I asked, for instance, how do I get from 7 to say, how did I get from 7 to say 1? So, then it says I got from 1, I came back from 0, and 0, I came back from 4, and 4, I came back from 7. So, this is my path. 7, 4, 0, 1. So 7, 4, 0, 1 is my path. So, this is how I do it. So, set the path from 7 to 6, for instance, 7, 5, 6. Before I start from 6, it says 6 came from 5, and 5 says 5 came from 7. Similarly, if I say, 2, it says 7, 4, 0, 2. So, this is how I use this. (Refer Slide Time: 28:18) So, this is one thing I can do, which is recover the parent information, I can also recover how far it is? So, of course, if I trace out the path, then I get automatically how far it is, because that is telling me how many steps it took to reach there. But I can also keep this information directly. So, since I am exploring level by level, I know that my first neighbours were one step away, the neighbours of those neighbours are two steps away, and so on. So, each time I explore a vertex, I can store how many steps away it is. And with this, I can keep track of the shortest number of steps, I need to get to every vertex. So, I can keep the level and if I keep the level and the level is not 0, then it is visited. If it is not, if it is set, then it is not visited. So, I do not need to keep visited and level I can keep just the level. (Refer Slide Time: 29:04) So, instead of visited j, I maintain level of j.'},\n",
              " {'id': '8f59a8ea-54b8-4acc-93c2-ce8e5e31eb1b',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, here I change the terminology. So, it is the same BFS, but I am using level and in i. In earlier, we initialize the visited to be false. But now level is going to be a number. It is how many steps I took. So, I am going to say that level is now a number. And if it is minus 1, it means that this vertex has not been visited. So, initially, no vertex is visited, same as visited false level is minus 1. And all vertices have no parents, or parents are minus 1. So, when I now start with V, the first thing I do instead of setting visited v to true is I set the level of V to 0. So, I reached V, I started at V. So, in 0 steps, I reached v. And then I do the usual thing, I put it into the queue and I process the queue. So, while processing the queue, as before I pick out the first element in the queue, I look at all it is neighbours. And now instead of checking whether the vertex has been visited, which is what I was doing before, I will check whether the level has been assigned. So, if the level has not been assigned, then it means it has not been visited. So, if the level has not been assigned the level still minus 1. So, if the level is minus 1, then I have to set the level. So, what do I set the level to? Well, I have to set the level to be 1 more than the level from where I am looking at it. So, if I am coming from j to k, and if j was at level 7, for example, then the new vertex k must be at level 8. So, I set parent of k to j and I set level of k to be level of j plus 1. So, it is 1 more than where I came from. And at the end, now, I will return level and parent. In level if I find entries minus 1 it is false. If it is not minus 1, it is true. So, I implicitly have the visited information given back to me. And in addition, of course, I have the actual path which is given by parent. So, notice that from the parent thing, I can actually go backwards and calculate the level. But this has now given me the level in one step.'},\n",
              " {'id': '79210568-9ded-4d9b-91fd-391ab5f8e65b',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'So, this is what we said we initialize the level set it to 0 for the source vertex. And every time we visit a new vertex k, we increment the level compared to where we came from. (Refer Slide Time: 31:14) So, just our same example as before, so we start at 7, initially, all levels and all parents are minus 1, when I start with 7, now, I set this to be 0, so this is my initial thing. And now as I go ahead, I mark these to be 1, so this is now 1, because these were marked from 7, then if I go from 4, then I mark the level of 3 and 0 to be 2, because they were reachable from 4, which was level 1. And then if I go from 5 to 6, again, 5 was already at level 1, so 6 is at level 2. And now if I go to 8, for instance, 8 was at level 1, so 9 is at level 2. And now I get to level 3, because 0 is at level 2. And if I explore the neighbours of 0, then its neighbours will be at level 3. So, 1 and 2, which are going to be explored next will now be at level 3. So, it means it takes me 3 steps to get to 1 and 2. And now you can see on the left hand side that all the entries have become different from minus 1. So, I have actually reached everything. So, we are as usual just going to process the queue and find no changes. (Refer Slide Time: 32:26) So, to summarize, breadth first search is a systematic way to explore a graph. And essentially, we record which vertices have been visited and we use a queue to keep track of which vertices are yet to be explored. If we use an adjacency matrix, then we end up using n squared time regardless of how many edges that are in the graph. So, this is not optimal, because very often, you will have a smaller number of edges. So, we use adjacency lists to get m plus n because the work across all the vertices turns out to be proportional to the sum of the degrees which is bounded by 2m.'},\n",
              " {'id': '05b17f6c-8dd6-48b4-beed-11e6151eb956',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 4},\n",
              "  'source': 'Breadth First Search (BFS).pdf',\n",
              "  'content': 'We saw that we can add parent information to recover the path and we can maintain the level information to record the length of the shortest path in terms of number of edges. Now, the shortest path in terms of number of edges may not be the actual shortest path we are interested in as we will see later on. S o , l a t e r o n ,'},\n",
              " {'id': '1c8f1b6e-f20a-49e3-a558-865bbcb7662a',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': 'Programming data Structures and Algorithms using Python Professor Madhavan Mukund Longest Path in DAGs (Refer Slide Time: 00:13) The second problem for DAGs is what is called the longest path. So, we have a DAG without cycles, we saw that a topological sort will enumerate this in some feasible order such that if i depend, if j depends on i if there is an edge from i to j, then I will always be enumerated before j. Now, let us look at a typical scenario. So, for example, supposing these are courses and these edges represent prerequisites, and it takes a semester to do a course, for example, then we can see how much time it will take for us to complete all the courses and finish the program. So, each course takes a semester. And now we execute these courses or we take these courses as fast as possible. So, initially because 0 and 1 have no prerequisites, we can take them in the first semester, then having done 0 and 1, we find that we can do 2 and 4 because they only depend on 0 and 1. And we can do 3 as well. So, 2, 3, and 4 are courses we can take in the second semester. Now having done all of this, we still cannot take courses 6 and 7, because they depend on 5. So, we have to do 5 alone in the next semester, having done 5 alone in the next semester, then we can do 6, and then we can do 7. So, this set of 0 to 7, 8 courses will actually take us 5 semesters to complete, given these dependencies. (Refer Slide Time: 01:24) So, our task now is to find the longest path in a DAG. And as you can imagine, this is closely related to the order in which they are enumerated. So, this is very closely related topological sort. So, if we have a DAG, and then the indegree of a node is 0, then the longest path to that node is 0. And that in other words, it can be done on the very first day, if you only think about it. So, there is no requirement the longest path represents how many requirements I need to satisfy sequentially in order to get to the task.'},\n",
              " {'id': '27d76be3-bbe6-434a-9944-684f1ce083d9',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': 'So, if an indegree is 0, then I can do it immediately. If it is not 0, then it has some incoming edges. So, if I knew how long it took to come there, the longest path to each of the incoming edges, then I can take the maximum among these, that is the constraint now I have to wait for all of them to be completed. So, I can take the one which is going to get completed latest, the maximum among the latest longest paths to all the incoming edges. And then my incoming long, my longest path will be 1 plus path (Refer Slide Time: 02:30) So, since I need to know the maximum of these longest paths, I need to enumerate them before I enumerate this vertex. But this is precisely the point of topological sorting. So, when I am processing a vertex k and trying to compute its longest path, I need to know the longest parts of everything which has an edge pointing into k, but in a topological ordering, this would have been done. So, if I, if I compute longest path following a topological ordering, then this kind of recursive or inductive definition that I have done here can be satisfied. So, what we will do is compute longest path in the topological order. (Refer Slide Time: 03:09) So, let us assume that we have some topological ordering of our vertices, then we know that everything in the every vertex in this list has all its neighbors appearing before it. So, we can go from left to right, having fixed a topological ordering, fixed to right, and we can do this. And we do not have to first compute the topological order and then scan it again as we are going along, as we are computing the topological order at any inductive point in a topological order, we have the same scenario that is everything that I depend on has been enumerated before me. So, I can inductively compute the longest path along with the topological ordering in one single pass.'},\n",
              " {'id': 'bc1553f9-ec08-4c85-be91-d36bcea51184',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': '(Refer Slide Time: 03:48)    So, as before, we start by computing the indegree of every vertex, and now we will also simultaneously compute the longest path of every vertex. So, we initialize the longest path to be 0 by assumption. And now wherever I have indegree 0, it is 0. Now when I enumerate something, I eliminate it from the graph, I update the indegrees. But I will also update the longest path. So, now I know for instance, that it takes 2 takes me at least one step to reach 2 because I have to do something before it. So, I will update the longest path to 2 and to 7 as being 1 plus the longest path to the vertex, which I just eliminated. So, I am incrementally updating the longest path. So, now if I enumerate 0, then again, I know that its longest path to 0, but it will now contribute 1 to both 3 and 4. Notice that the longest path to 2 remains 1, because the longest path to this was 0, the longest path to this was 0. So, it is the maximum of both these quantities plus 1, so I already knew it was 1. So, having enumerated the new vertex 0 does not give me any new information about the longest path to vertex 2. Now I enumerate vertex 3. So, vertex 3 currently already has listed longest path of 1, and now 3 was pointing into vertex 5. So, now 5 must take 2 steps to be reached because it has to be reached via 3 and 3 already takes 1 step. So, the longest part of 5, which was earlier 0, as a default assumption has suddenly become 2. And I keep doing this. Next, I eliminate 2. And when I eliminate 2, nothing new happens because we had the situation like before, where 2 and 3 both had longest path 1. So, when I upgraded from 3, I already knew that 5 needed longest but 2, so I get no change. So now, if I enumerate 5, then I get that 6 requires 3 steps, if I enumerate 6, then I get that 7 requires 4 steps. Finally, when I enumerate 4, I get no new information for 7 because 4 required only 1 step and 7 we already know requires 4 steps.'},\n",
              " {'id': '9233531d-4356-4398-9d91-d2e89b2607e7',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': 'So finally, enumerate 7 and I have this sequence. So, I have the sequence below, which I computed along with a topological sort above telling me the longest path to each of these vertices in my graph. (Refer Slide Time: 06:19) So, here is a variation of our topological sort algorithm, which we are now directly giving in terms of an adjacency list, because we saw that for a topological sort, the list makes a lot more sense in adjacency matrix. So, the new thing here is that we are keeping track of this longest path metric. So, we are not really interested. So, we are implicitly doing the topological sort. But the purpose of this particular function is not to give us a topological order. So, we are not keeping track of what we were doing earlier, which was the topologically sorted list. Instead, we will keep this longest path function as a dictionary, longest path l path of I will be the longest path to i. So, we initialize the in degree and the longest path of every vertex to be 0. Now, as before, we will walk through all the vertices and update the degree in time proportional to the number of vertices. Now as before, because we are doing the same topological calculation sort calculations before we keep the 0-degree queue, and we put all the 0-degree vertices into the queue. So far, we have done nothing different from what we are doing before, except that instead of keeping track of this topological sort lists, we are keeping track of this l path longest path dictionary. So now, as long as there are 0-degree vertices to process, we take out the highest one. So now for every outgoing edge from here, for every k, which is in the adjacency list of j, we update, its indegree as before, and now is the new step. So, this is new. So, what we are doing now is we are saying the longest path of k is the maximum of what we already knew, and 1 plus the longest part of j, which we have just discovered.'},\n",
              " {'id': 'd2460c4e-06b7-44f6-a02c-85f79a1e68fd',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': 'So, this is the only difference actually between the topological sort algorithm and the longest path algorithm, except for the fact that we are, we can also keep track of the topological sort here if we wished, but we have not done so. And then, as before, if we discovered that the newly decremented degree has become 0, we put it into the queue so that it will be processed when its turn comes. And finally, instead of returning the topological sort list, we are returning this path, this longest path dictionary. So, this is basically very similar to topological sorting and so is the analysis, initializing takes n plus n time, and inside the loop, which runs n times, we do an update which is an amortized m time update, because we have to do it to the sum of the degrees and so, this is m plus n. (Refer Slide Time: 08:53) So, basically with directed acyclic graphs now, we can do both these things very efficiently we can get the feasible schedule with topological sort. And while we are doing the topological sort, we can also compute through the dependencies the longest path that is the minimum time. So, the longest path is really telling me the minimum time it will take me to finish all the tasks according to the dependencies. Now, of course, the longest path makes sense for any graph and I can take any graph which does not have cycles and say if I have cycles, then the way we have defined path, we can go round and round. So, we can basically look at paths which do not repeat vertices and ask what is the longest path in a graph, so this is often called the diameter of the graph. Now, it turns out that computing this in general is very hard. So, when DAGs we have a very efficient algorithm, we have something which is essentially linear in the size of the DAG, remember o m plus n is basically linear in the size of the graph. And if we go to a non-DAG then this problem becomes hopelessly complicated.'},\n",
              " {'id': 'f6d32114-ec01-4205-b346-93fda08fd838',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Longest Path in DAGs.pdf',\n",
              "  'content': 'So, short of trying out all paths and finding out which is the longest there is no really good strategy which works across all graphs. So, longest path in general is a very hard problem in graphs, but for DAGs it is surprisingly simple.'},\n",
              " {'id': '9e4e9f42-10e2-4778-ad84-bcbb8d270a50',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'Programming data Structures and Algorithms using Python Professor. Madhavan Mukund Topological Sorting So, we were looking at the Directed Acyclic Graphs. And we said that DAGs are useful for telling us about things like tasks and their dependencies. And one of the fundamental problems that we wanted to solve using a DAG was topological sorting. (Refer Slide Time: 00:25) So, when we have a DAG, that is a directed graph without directed cycles, what we want is to enumerate the vertices in an order which respects the order of the edges. So, we if you think of these as tasks and dependencies, this means that we execute the tasks in such a way that every task that requires another task to be done is done after that prerequisite is completed. So, we are following the dependencies when we are executing. So, given such a set of prerequisites in the form of a directed acyclic graph, how do we find such a valid schedule? So, this is like a feasible schedule. (Refer Slide Time: 01:03) So, here is a DAG on the right for example. So, we would like to say in this DAG, for instance, that I cannot do, for example, task 3, until I have d1 both task 0 and task 1. So, this is a typical example of the kind of constraint that we get. So, it should be clear that if these constraints are what are represented by the edges, and if we have a cycle, then that means that we need something, say task 1 to be done before 2, and 2 to be done before 3, and 3 to be done before 4. And then when we complete the cycle, we will find that 4 has to be done before 1. So, if we have a cyclic dependency in these task, then clearly there is no possible way to enumerate them or execute them in this order, which respects the dependencies. So therefore, a graph with directed cycles cannot be sorted topologically.'},\n",
              " {'id': 'f9ee7d8e-c234-4531-a650-72e26d7b6334',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'Now, what we are claiming is that if there are no cycles, that is if I have a directed acyclic graph, then we can always sort it topologically, we can always enumerate it in a good order such that whenever there is an edge i comma j i appears before j in the enumeration. (Refer Slide Time: 02:10) So, the strategy is straightforward. What do you begin with, well you have to begin with vertices or tasks which have no dependencies, you have to start with something that you can start a fresh. So, you begin vertices with no dependencies. And then as dependencies are completed, later tasks now become available, because all their dependencies are also empty. So, in this way, we keep exhausting the dependencies and finding new vertices to enumerate. So, we have to answer two questions to ensure that this is always possible. First of all, we have to be able to start. So, given a DAG, there must be at least 1 vertex that we can enumerate, which does not have an incoming dependency. And secondly, as we go along, we cannot get stuck. So, every time we finish executing some task, we are left with the remaining tasks. And now we must guarantee that in the remaining tasks, they will again be at least 1 task which we can execute, which has no remaining dependencies. (Refer Slide Time: 03:06) So, let us remember that in degree is what we refer to in a directed graph as the number of edges coming into a vertex. So, a task which has no incoming dependencies has indegree 0. So, our first task is to find something that we can enumerate at the beginning. So, this will be a vertex which has no incoming edges, or something which has indegree 0. So, we claim that this is always possible. This is the first thing that we need to justify to say that every DAG can be topologically sorted, that there is always a starting point for our topological enumeration. So, every DAG must have a vertex with degree 0, indegree 0. Now there could be more than 1.'},\n",
              " {'id': '031bdc6d-924c-49a6-af6b-00520f92c5b2',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, for instance, here you can see that both 0 and 1 have this property that there are no incoming edges. So, it is not a unique 1, but there must be at least 1, this is what we are claiming. So, why is this, well suppose there is none. Suppose every vertex actually has an incoming edge, then wherever we are in the DAG, we can go backwards through, through an incoming edge to a previous vertex. So, we go backwards, but by assumption, every vertex has at least 1 incoming edge. So, from the vertex, we just went to the second vertex also has an incoming edge, we go backwards to a third vertex to a fourth vertex, and so on. So, as we keep walking backwards, we keep walking to new vertices. Now, there are only a fixed number of vertices n in my graph, so after I make n moves, I must hit a vertex that I have seen before because I cannot see more than n distinct vertices. So, if there is no vertex, which has in degree 0, that is, I can always walk backwards from every vertex to a previous vertex, then there must be a cycle in the graph and so it is not right. So therefore, by the converse, if it is a DAG there must be at least 1 such vertex which is indegree 0 from where we can begin our enumeration. So now we listed out right so (Refer Slide Time: 04:57) So, now we list it out, so we list out this indegree thing. So, supposing we list out this one. Then in principle this vertex has now been enumerated, so all the dependencies that it has have been satisfied, so we can delete these 3 edges. So, now what we are left with is a smaller graph in which we have removed 1 vertex and all the edges which came out of that vertex. But since we had a DAG to begin with, this new graph must also be acyclic because we have only removed edges, we have not added any edges to create cycles. So, it is still a directed graph, it is still acyclic. So, this is again a DAG and we just showed that in every DAG, there must be some incoming indegree 0 vertex.'},\n",
              " {'id': '6004c52f-f5ec-4924-a8c6-dc4aab07b12c',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, in the new DAG also, there will be something that I can enumerate. So, in this case, for instance, of course, we know that we can enumerate 0 because it was already there. But we have also found, for instance, a new vertex here 4 that can be enumerated because 4 had only 1 incoming constraint that was from 0 and 0 has been enumerated. So, in this sense, both are conditions that we required for topological sorting are satisfied, we can always start because there is always an indegree 0 vertex, I can always continue because once we remove a vertex, we still have a DAG, and therefore, by the first condition, we will again have an indegree 0 vertex and we can finish. So, this is the algorithm for topologically sorting a DAG. (Refer Slide Time: 06:22)    So, to convert this to an algorithm, we have to first find this indegree 0 vertex. So, we have to scan our representation of the graph and compute the indegrees of each of the vertices in the graph. So, if we do that, by just looking at, for instance, in the adjacency matrix, we will look at the incoming edges, we will look at each column and count the number of edges pointing into a vertex i, if we are looking at an adjacency list, we can do it by looking at all the lists and looking for every second item in a pair, we can count the first item in the pair. So, if you see an edge u, v, then you will increment the degree of v. So however, we do it, we can come up with this numbering, so this red numbering, which tells us the indegree of every vertex in this particular DAG, so 0 and 1 have indegree 0. And as we go down the DAG, we find for instance, a vertex like 7, which has 4 incoming edges, and therefore it has degree 4. So now our algorithm says pick any vertex which has indegree 0 eliminate it, and remove it from the DAG. So in this case, let us start with 1, so we start, we could choose this or this. So, we happen to start with this, so we remove vertex 1.'},\n",
              " {'id': '91d9ae99-b6f2-4648-a456-453ddc9858b5',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'And now we have a new DAG, and in this new DAG, we have enumerated 1, so we can output this as the first element in our topologically sorted sequence. But we also have to correspondingly recompute the degrees. So, this had degree 2, but 1 of those 2 edges was coming from the vertex we just enumerated. So, for every vertex, every edge, which is coming out of the vertex that we just enumerated, we have to update the degrees, so we have to update these, so we had edges like, like this. So, if you go back, so we have we had edges from 1 to 2 and 7. So now, basically, we will replace this what used to be 4 by 3. And what used to be 2 by 1, so this is a situation after enumerating 1. So, we have been 2 things, we have enumerated 1. And we have updated the degrees of all the vertices which 1 was pointing to. So now we find another one, namely 0, for example, and we enumerate it. At this point, notice that there is only 1 vertex with indegree 0, that is the vertex 0 itself, there is no other vertex which can be currently enumerated because everything has some incoming edge. So, I enumerate 0. And now I have a graph in which I update the degrees. And I get now these 3 vertices, all 3 of these now are possible candidates for my next step. (Refer Slide Time: 09:00)  So, I can pick one of them. So maybe I picked the middle one. And now again, I enumerate, update the in degree, and now 3 was pointing into 5, so this now has degree 1. So, in this way, I can continue. So, I choose for instance 2, and then maybe I choose, so if I choose 2, then 5 becomes indegree 0s. So now I have a choice between 5 and 4. So, then I choose 5. And then I can choose to, I have to update the indegree of 6 to 0. So now I can choose between 4 and six. So maybe I do 6. So, I leave for 4 for very late, even though it was available much earlier to enumerate. But now I have to enumerate 4 before 7 because 7 depends on 4. So, I do 4 and then 7.'},\n",
              " {'id': '78b7c63c-1dfe-4281-b66e-7c85fef5f89b',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, this now is my topologically sorted sequence, one topologically sorted sequence because remember, we made some choices. Initially, we could have done 0 or 1 and we chose 1, at some point in between we had 2, 3 and 4, we could have chosen any of those and we chose 3. So, there are many situations where we had more than 1 indegree 0 vertex to choose and we picked one of them. So, the topologically sorted sequence is not unique. There are many of them, but all of them have the property that if I is enumerated before j, then if there is an edge from i to j, then i will be enumerated before j. (Refer Slide Time: 10:06)  So, let us look at an implementation of this. So, we have to remember, we have to compute the indegrees, then we have to find the degree with vertex with indegree 0, remove it and update indegrees and keep repeating this until all the vertices are listed. So, this is a topological sort function, which takes an adjacency matrix as input. So here, initially, we as usual, we find out using the NumPy shape attribute, how many vertices there are, so the number of rows and number of columns, the adjacency matrix. So, now we want to keep track of indegrees. And we want to keep track of this list of vertices in the final topological thing. So, we have an empty dictionary to keep track of the industries. And we have an empty list called topo sort list, which will accumulate the vertices in the order in which they are enumerated. So, the first thing that we have to do is compute the indegrees. So, computing the in degrees in an adjacency matrix requires me to scan all the columns. So, what I do is I pick each column for every column, I initialize the indegree to 0, and then I walk across that all the rows in that column. And wherever I see an edge of the form r comma c, I update the indegree by 1. So, I increment by 1. So, this whole thing is, is corresponds to computing the indegrees. So, I have done that.'},\n",
              " {'id': 'a6ab7a4e-c15b-4ba9-a401-311059e0d599',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, having computed the indegrees, now I can start processing these vertices. So, what I have to do is I have to find a vertex, which has n degrees 0. So, here is one way to do it, you can do it in an explicit loop or through this list comprehension. So, you find all the k in 0 to the number of vertices minus 1, such that in degree of k is 0. So, this is the list of all k for which in degrees 0. And you take, in this case, we, we said we had a choice. So, we had making now a choice to take the smallest of these vertices. So, we take the minimum over this list. Remember that a DAG will always have such a vertex, so this list will always be not empty. So, I find this list of vertices whose indegree is currently 0, I picked the minimum 1 and call it j. So, j is going to be my next vertex to be enumerated. So, what I do is I append it to this list that I had started creating. So, this is the next vertex in my enumeration. And now I have to go around updating in degrees, so for every outgoing vertex from j, so every outgoing vertex from j, so for every k in the columns, if, if I have an edge in my adjacency matrix from j to k, then I update the indegree of k to be the indegree minus 1. So, I am removing j from the thing. So, there is 1 less edge pointing into k. So, this is the step of updating the indegrees. And then I will keep doing this. Now I know that in every such iteration, I am going to come out with 1 more vertex. So effectively, I have an loop, which runs as many times as there are vertices in my graph. So, if you want to do an analysis, this says that this step of initializing the indegrees, because it requires me to walk down every column in my adjacency matrix, this is order n squared. And then I have to run this loop n times, so I am doing for i in range rows, which is 0 to n minus 1. And inside what am I doing, I am doing the scan.'},\n",
              " {'id': '3524c6df-d454-428b-8647-125000fdfc29',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, I am doing the scan to find the next vertex to enumerate, which is again a linear scan, I have to go through all the vertices, find out if they are indegree 0, collect them and then take the minimum. And then finally, I have to do another order and scan here, because when I take a vertex out of my graph, I have to update all its outgoing neighbors to have 1 less degree. So essentially, what matters is that I have an order an outside loop and inside it, I am doing various order and operations, so the whole thing is going to become order n squared. So, I have ordered n squared right up front, so this is n squared, and this is n times n because I have an outer loop which is n and I have these inner loops which are n. So, we saw before with breadth first search and depth first search that may be the way to get out of n squared is to use an adjacency list. So, let us see what happens if we use an adjacency list instead for topological sort. (Refer Slide Time: 14:28) So, we have a similar algorithm now, but except we are using not the adjacency matrix representation but the adjacency list. So as before, we will keep track of indegree and topological sort, as dictionary and a list respectively. And now we have to initialize, so here we can just go through all the vertices and initialize an indegree to be 0. And now we want to compute the indegrees. So, what we will say is that if I have like a list like this, so says 0 has an edge to 1, 0 has an edge to 2, 1 has edge to 3, 2 has an edge to 4 and so on. I will just scan each of these lists. And every time I see an edge, I will update the indegree of the target of the edge. So, for every vertex, for every vertex that it is connected to every, every edge outgoing from there, I will take the indegree of that vertex and incremented. So, that is basically the way that this works.'},\n",
              " {'id': '3fe87cb4-0dd9-4aa9-9074-2e1ff4c1f50f',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, this now becomes an indegree update, which processes all the edges, and not necessarily all the non edges, which is the big advantage of working with adjacency lists. Now, we have this other problem of keeping track of the vertices which are to be enumerated because last time we had to in the adjacency matrix version, we had to keep looking for this vertex which has degree 0. So, here instead, what we will do is we will actually explicitly keep track of these the way we do in breadth first search all the vertices which have to be enumerated which are eligible to be enumerated, we will put them into a queue and we will pick them up one by one. So, we create a queue call the 0-degree queue. So, this will hold all vertices which are having indegree 0, but which have yet to be enumerated. So, the first thing we do is we go through all the vertices, now we have to remember we have updated the indegrees. So, we know the indegrees of all the vertices. So, we go through all the vertices one more time after updating them. And every time we see a vertex with i degrees, 0, we add it to the queue. So having added it to the queue, now, what we have to do is pick up the first element of the queue and enumerate it. So, while this queue is not empty, we delete the first element and enumerate it exactly like we did in the previous case, except there, we have to explicitly scan and find this vertex here, it is available to us immediately at the head of the queue. And having enumerated it, then we have to update the outdoing, the degrees of the outgoing vertices. So that is again, proportional to the degree of this vertex. So, for every k, which is in the list of j, I will take indegree of k and reduce it by 1. And in this process, if I find that the indegree of k becomes 0, then I put it in the queue.'},\n",
              " {'id': 'a570b18e-622b-4829-96f5-118618b30d5f',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'So, every time I find that indegree becomes 0 by when, when it becomes 0 when I update it, so when I update the vertex is indegree at that point, I check has it become 0, if it has become 0, now it is a candidate for enumeration, put it into the queue. So, in this way, I have now eliminated the need to scan this thing for 0 degree things. And the updates that I am making are also now across the list of edges, both the indegree calculation and the indegree update. So therefore, in this analysis, we see that analyzing the indegree itself, initializing the indegrees m plus n. Why is it m plus n because I need to first spend order n time initializing it to 0. So even if there are no edges, I have to set this up. And then I do this once for every edge, for every edge in my represented my adjacency list, I will increment in degree once. So, this is order n plus n. Now, inside the loop, getting this vertex to enumerate is free, because I just pick it out of the queue. So, what is the complex part of the loop is updating the indegrees. But again, I am doing it in the adjacency list. So, when I take a vertex j enumerated, I am looking at all its outgoing edges. So that is proportional to degree of j. And across all the vertices, we already saw this in breadth first search, that means that I will do this, I cannot tell you how much 1 individual vertex will take. But overall, I know that it is going to be proportional to the sum of the degrees because it is going to be the sum of the number of edges, which is going to be twice the degrees. So therefore, the sum of the degrees which is twice the number of edges, and so therefore, this whole thing is in this amortized sense going to be order n. So therefore, by moving to adjacency list representation, just like we did for BFS and DFS. In topological sort, also, we move from an n squared algorithm to an m plus n algorithm. (Refer Slide Time: 19:06) So, to summarize, DAGs are a natural way to represent dependencies.'},\n",
              " {'id': 'e76da78e-a57d-42a0-acfd-9ae7b8f48f37',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 4},\n",
              "  'source': 'Topological Sorting.pdf',\n",
              "  'content': 'And what we typically need to do with a DAG is to come up with a feasible schedule, and that is what topological sort does. And we can justify the topological sort is always possible by showing that every DAG will have a vertex within degrees 0. And if we eliminate this vertex, we are left with a DAG, so we can keep repeating this until the DAG becomes empty. So, with adjacency matrices, the naive implementation takes time n squared, but if we use an adjacency list and maintain the skew for the 0-degree vertices, which have to be enumerated, we can bring it down to order m plus n. And finally, remember that this is not a deterministic process, because there may be multiple 0-degree vertices at any given point. So, there may be more than one feasible schedule. So, what we are doing is that we are using some strategy to choose amongst these and this gives us one of them. So, based on how we choose it, we might get different schedules. So, in our algorithm we will typically pick a uniform strategy like we pick the minimum vertex to, to bring out. The second algorithm, we actually put them out in the order in which they entered the queue. So, they actually got processed as they became 0 rather than in the, by their vertex order, and so on.'},\n",
              " {'id': '7939d0a3-faa6-4f07-b2e2-252458018e6a',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Chennai Mathematical Institute Introduction to graphs So, having seen functions on arrays and lists, let us move to graphs, which are very interesting objects to study in computing. (Refer Slide Time: 00:18) So, we know that a graph is a useful way to visualize a relation. So, let us imagine this relation. So, we are talking about a class in which you have or an institution which you have teachers and courses. And you have some allocation of courses to teachers. So, you have two types of objects, you have teachers who are people and you have courses which are to be taught. And you can describe this allocation as a relation. So, you can say, A the allocation is a subset of T cross C, it is a binary relation, which consists of pairs or the form T comma C, where T is a teacher, and C is a course. So, there are many possible pairs T cross C. And we are not taking all of them, but some of them, so it is a binary relation. And what we see on the right is a representation of this as a graph. So, right now, it is very clear that we have these nodes. So, we have one node for every object of interest. In this case, we have two different types of objects. But in the graph, we do not mean we label them maybe, but we represent them in the same way as black dots. So, on the left-hand side, we have the teachers on the right-hand side, we have the courses. And then we draw these edges, these arrows from a teacher to a course. So, there is a direction to this edge and this arrow indicates that this teacher is going to be teaching that course. And of course, there could be situations where two different teachers teach the same course. So, this could happen, you could also have a situation where the same teacher is teaching two different courses, which is not captured in this particular example. But of course, you could have that somebody is teaching both biology and history, for instance.'},\n",
              " {'id': 'c3185539-71dc-458e-9127-8d8cd9be09f6',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': '(Refer Slide Time: 01:54) Here is another typical relationship that we describe using graph. So, supposing you have a group of people, and some of them are friends with others, then the notion of being a friend, again, describes certain pairs, so some pairs of people are friends, and some pairs are not friends. So, those who are friends, you connect by an edge. And now, we can assume that friendship is a symmetric relation. So, if I am somebody is friend, then that person is also my friend. So, therefore, if p is a friend of q, then q should be a friend of p. So, then we have a graph like this, where now technically speaking, we have arrows representing pairs in both directions. So, we have an arrow from P to Q, saying that Q is a friend of P, and we have an arrow from Q to P, saying that P is a friend of Q. And because we have arrows always in both directions, we just ignore them and draw only a single edge. (Refer Slide Time: 02:46) So, formally, a graph is a set of vertices, those are the black dots. So, they are called vertices, or sometimes they are called nodes. And we take them and connect them using edges. So, this edge technically is a binary relation, it says some pairs of vertices, so there is a whole, all the pairs of vertices are V cross V, some pairs of vertices are connected by edges. And usually, we have the constraint that a vertex is not connected to itself. so that is usually one of the constraints that we have. So, we usually assume that this is in the relational terminology IRREFLEXIVE. (Refer Slide Time: 03:23) So, this graph can have typically will be directed. So, I will have edges from, say, P to Q, and I need not have for every such edge, an edge back from Q to P. So, a very particular example of a directed graph was the thing with teachers and courses, because the edges all point from teachers to courses, it does not make sense in such an interpretation to think of an edge starting at a course because a course cannot teach something else.'},\n",
              " {'id': '2562424a-ad22-47a7-b5f5-d164933256f6',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, these edges represent who teaches what, so the left-hand, left-hand side has to be a who the right-hand side has to be a what. (Refer Slide Time: 03:57) On the other hand, the friendship graph that we constructed, as we said, is symmetric. So, we have edges between two pairs in both directions or not at all, we never have a situation where P is a friend of Q, but Q is not a friend of P. And in such situations, we will just ignore the edges, and just draw these with these lines connecting the vertices. So, basically an undirected graph is a graph in which all the edges are symmetric. So, we can effectively think so we will think still represented by edge in both directions because we can traverse this edge in both directions. So, if you want, you can think of these as roads. So, if these are two locations served by a road, if you have a directed road, then you can only go from one direction from one to the other, you cannot come back on that road. Whereas if you have a bi-directional road, then it is an undirected road, you can go in either direction. (Refer Slide Time: 04:47) So, one of the key things that we are interested in graphs are sequences of edges, which take us from one node to another node and this is what is called a path. So, here we see two paths, we see this red path which takes us from the node representing Priya to the node representing Radhika. And the other one is a blue path which also connects Priya to Radhika but through a different sequence of intermediate nodes. So, these could represent, for instance, routes by which some information that Priya wants to convey or in some object, supposing Priya wants to send something which is precious not to be sent by courier or post by hand to Radhika, then Priya has to find some of her friends who can, in turn, find their friends and so on, who finally will be able to find someone who knows Radhika can pass it on. And here we see that there are at least two different ways to do this.'},\n",
              " {'id': '5ef0e2ce-d2b7-4554-8a37-cf4aea6bd35c',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, normally, when we are talking about paths in a graph, a path does not visit a vertex twice. So, here, for instance, we have this red path here or red sequence of edges, which certainly could be seen as a sequence where every consecutive thing is part of the edge relation. So, we start for instance here. So, we start here, and we go from Kumar to Feroze. And then Feroze to Colin, then Colin to Aziz, then Aziz to Priya, then we come back to Feroze. And then we come here. So, this is technically a sequence of edges, each of which extends the previous sequence by a valid point, but we are not going to call this apart, when we encounter such things, we will usually call it a walk. (Refer Slide Time: 06:24) So, paths in directed graphs, obviously have to respect the arrows. So, if I have, for instance, an airline route, so this is a typical thing that when sees in real life and graphs, you see these route maps of trains or planes, and they describe which pairs of cities are connected by flights. So, let us assume that these are some flights for an airline operating in India and say that V0 represents Delhi and V9 represents Madurai, then what we want to know is whether we can take flights within this airline and travel from Madurai to Delhi, which means, Can I take a sequence of edges starting at V9, and reach V0. And here, for instance, there is this red sequence of edges that you can see the ones that have been drawn here, so this red sequence of edges does represent one way of taking flights, a long sequence of flights, in this case, 1, 2, 3, 4, 5 flights to get from Madurai to Delhi. (Refer Slide Time: 07:21) So, if this happens, then we say that the target is reachable, so it could happen that things are reachable, it could happen that things are not reachable. So, if they are reachable, then one of the questions that we might ask is among all the different ways of reaching what is the shortest way. Other thing you might ask is, where all can I reach?'},\n",
              " {'id': '498700cc-f6e3-4df3-89aa-75a1398686c5',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, there are many different questions depending on the context that you might ask about reachability. And of course, one of the things that you might want to know is this graph is connected? Are all the vertices reachable from each other? Or do you get stuck somewhere? So, if we look at this graph, for instance, if you take this edge from V4 to V3, and you put it in only one direction, then I claim that you can still reach from everywhere to everywhere because earlier, you could go directly from V3 to V4. But now, you can still do the same thing by going around. So, if you want the what is now missing is the ability to go from V3 to V4, because I removed this edge red, so this edge is gone. But now, I can still go from V3 to V6, V6 to V5, V5 to V7, and V7 to V4. So, the ability to go from V3 to V4 has not changed, it has become a little more tedious, but it is not impossible. So, this graph, if it was connected earlier, remains connected, even though we have removed one edge. On the other hand, supposing we take this edge between V4 and V0 and we make it in a single direction. So, earlier, I could come from V0 to V4. Now, if I remove this, the question is, can I come? Well, if I start at V4, I can go to V1, from V1 I can go to V2, from v2 I can go back to V0, but I can go nowhere else. So, if I am now in this upper triangle of V0, V1, and V2, I stuck there, I cannot come to the lower part of the graph. So, now the graph is not connected anymore. So, the question that we want to ask is, is the graph that is presented to us connected or not? And this can have many implications. (Refer Slide Time: 09:15) So, reachability is not the only kind of question that we are interested in graphs there are many other kinds of things which can be modelled and solved as graphs. So, for instance, here is a problem which superficially does not seem to require graphs, which is to colour a map properly.'},\n",
              " {'id': 'c20484ab-1b43-401c-9954-41323304404b',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, when we colour a map properly, we want to neighbouring states or to neighbouring countries, which share a border to have different colours. So, one of the questions that you can ask is how many colours do we need. So, to transfer this to a graph problem, what we do is we first create a vertex for every state that needs to be coloured. And now we want to express this constraint that neighbouring states should have different colours. So, we have to capture this neighbouring relation. So, we do that by connecting them together. So, we attach an edge between every pair of states, which shares a border. So, now we have a graph. And now what is our constraint, our constraint is we must assign a colour to every one of these black dots such that if two black dots are connected by an edge, they must have different colours. So, we must assign colours to nodes so that the endpoints of an age have different colours, you cannot have a green on one side and the green on the other side, because that would mean two neighbouring states have the same colour. So, this is called the map colouring problem. But notice that by coming up with this graph representation, we have abstracted the problem so that we are now no longer worried about the shapes of the states, the sizes of the states, the kind of orientation of the boundary, how much of a boundary do they share? All we are interested in is this picture, which is, what are the underlying states and how are they connected? And once we have this, it does not even need to respect that true physical geography. So, I can take some of these nodes and move them aside, I can make the graph look different superficially, without changing the underlying nature that is relevant to the problem namely, which points are neighbours of which other points. (Refer Slide Time: 11:17) So, this is called the graph colouring problem. So, we are given a graph and we are given a set of colours.'},\n",
              " {'id': 'c28c1603-5455-4f54-b712-b8faa1083e02',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'And what we want to do is assign a colour to every vertex such that no pair of adjacent vertices, there is no vertices which are connected by an edge have the same colour. So, this is what is saying, if u, v are edges in a, if u, v is an edge, then the colour assigned to u should not be the colour assigned to v. So, the question that we were asking in that map colouring problem is, what is the smallest set of colours that we need? So, there is a very famous theorem in graph theory, which says that if you have what is called a planar graph, so planar graph is something which you can draw without any crossing edges. So, if you go back to the graph that we had before, you will notice that in this thing, there are no, so here we have an edge, which is crossing. But actually, you can take this edge, and you can drop it, and you can instead draw the same edge. So, remember, we said that we can modify this graph, so long as we do not change anything, which is intrinsic to the graph. So, we can take that edge and move it to the outside of the graph. Edges do not have to be straight lines, they are just something which connects vertices together. And now, I claim that we have the same graph in which no two edges cross on the paper. So, if you can draw a graph like this, this is what is called a planar graph. And it is not difficult to see that if you draw a map a physical map of countries or states, and you take the graph that we constructed based on the borders, it is going to be a planar graph. And it turns out that for planar graphs, graphs which can be drawn without having edges crossing each other, you can colour such graphs, always with four colours. If it is not planar, this is not true. And then it becomes a computationally interesting question to ask, what is the minimum number of colours? And it is one of these problems for which there is no efficient algorithm known? So, you might ask, where do we need to apply this?'},\n",
              " {'id': 'ffe65a7b-a3a0-47cc-ba7e-76018cc8be6b',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'Because colouring seems to be derived from this map question. And for the map question, we know the answer is 4 because it is planar. So, why do we need to worry about non-planar graphs? So, the thing is, there are many other questions which can be reduced to graph colouring. So, here is one. So, supposing you are running a school or a college, and you need to allocate classrooms, for the classes that are running. So, you have a timetable. So, in this timetable, this is kind of the time is shown from left to right. So, at the beginning of the day, you have a math class before the math class ends and English class starts before the English class ends, History and Science start, but notice that Science starts after Maths ends. So, the science class does not overlap with a math class. So, now we have four different classes which run during the day. But History and Science both run after Maths, so they both do not overlap with Maths. The question that we want to answer now in this particular scenario is how many classrooms will we need so that no two classes are scheduled in the same room at the same time. So, obviously, if we allocate one classroom per class, there is no problem. So, we could have four classrooms for these four classes. So, we could have an orange classroom for the Maths class and a green classroom for the History class and so on. So, clearly, if we have as many classrooms as there are classes, there is no question but in almost any institution, the number of classes or courses which are taught will obviously exceed the number of rooms that are available. So, one of the questions that is interesting is to ask, can we manage with the rooms that we have, so what is the minimum number of rooms that we need in order to schedule the classes the way we have done and not have this overlap. So, in this case, we can think of this edge relation.'},\n",
              " {'id': 'ee1b4dc0-4cc2-4183-836c-b815ec4e0fab',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, the classes that have to be taught at the vertices, the edge relation represents two classes which overlap, and therefore, they must fall into different classrooms, because they cannot share a classroom. The classrooms themselves are the colours. So, now if two edges or two classes are connected by an edge, then they cannot be assigned the same classroom. So, if classrooms are colours, they cannot be assigned the same colour. So, the edges here represent this overlap, so that we mentioned above, so these edges, so for notice that there is no overlap between these two pairs. So, there is no edge here. And there is no edge here because Maths ends before History and Science both start, but History and Science overlap with each other. Now, as for our colouring algorithm, we are allowed to use the same colour for this note as either this node or this node because they are not connected. So, one possible way of reducing the number of classrooms here is to reuse the green classroom for Maths that we are going to later use History. We could also equivalently reuse the grey classroom that we use for Science. So, this is a situation where we can make do with three colours, even though we have four classes. And of course, if you have a more complicated trade you, then it is a more interesting question to ask how many minimum classrooms you need to have in order to fit that schedule, or you have to change the schedule to adapt to the number of classes because of course, you cannot change the number of rooms very easily. So, you can change the schedule. So, you might have to change the schedule in order to make it fit within the infrastructure that you have. (Refer Slide Time: 16:36) So, let us look at another problem. So, here is a problem, which is to do with security cameras. So, as you know, security cameras are placed in such a way that they capture as much of the surroundings as they can and you want to minimize the number of security cameras.'},\n",
              " {'id': '06fc1a83-23f3-4344-9950-78a85e94b30e',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'So, supposing you are in a hotel and the hotel has corridors, you would ideally place your cameras at intersections of corridors, let us assume that the camera can rotate around and see all the corridors which meet at a point. So, a camera can monitor all the corridors. So, if supposing you have a lift here, and then after you come out of the lift you have say corridors which look like this, then if you place a camera at this point it can see all these three corridors. So, this is the kind of placement that we want. So, what is the minimum number of cameras that you need, if you know the layout of the corridors in your hotel. (Refer Slide Time: 17:26) So, here we would like to represent the floor plan as a graph. So, the intersections now become the meeting points of these corridors. So, the actual intersections of your corridors become the vertices. And now from each vertex, we said that if we put a camera here, for instance, it can monitor all these three, all these four corridors. So, the edge represents a corridor, which is incident at an intersection, and this edge says that there is a corridor, which goes from intersection V2 to intersection V3. And now the question is, where should we put our cameras? (Refer Slide Time: 18:07) So, this says, we must choose some vertices. So, supposing I choose V2, then V2 covers all of these. But I want to monitor every corridor in my hotel. So, from V2, I cannot monitor this particular corridor. So, I need to add one more camera. So, I could put like V0 or at V1. So, let me put it V1. So, if I now place my cameras at V2 and V1, then every corridor on this map is monitored. So, this is what is called a vertex cover, it is a set of vertices, such that every edge has one endpoint in the set. So, either one end or the other end of the edge must be in the set. So, every endpoint every corridor is monitored by one of these cameras. And the question here computationally is again, of course, I can put a camera everywhere.'},\n",
              " {'id': '4680552f-a39a-4751-abec-58bbd33635e2',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'And then I will obviously cover all the corridors. But what is the smallest number of intersections that I need to place cameras at or in other words, what is the smallest vertex cover that I can achieve? (Refer Slide Time: 19:03) A related problem is what is called an independent set. So, here, let us look at this scenario. So, we have a kind of Fine Arts Academy, which can put up various cultural programs including some group dances. Now, in a group dance, of course, a number of people are involved. And it could be that some dancers are trained to dance and more than one group dance. So, suppose that this academy wants to put up a cultural program, each dance requires the person performing to wear some elaborate costume. So, it is not feasible for a dancer to take part in two dances in the same evening because he or she will have to change their costume and that will be very problematic. So, now, you want to know, given the performers who are available and the dances that are available, what is the maximum number of dances you can schedule in this program, so that you never have to take a dance from one dance, and have them change their costume and take part in another dance. So, in this case, again, you can represent each of the dances as a vertex. And you can say that two dancers are connected if they need one person to participate in both, so if they have a common member in that group, then they are connected. So, here it says that V3 and V7 for instance, somebody will is required, who will participate in both of them. Similarly, say V5 and V6, but there is no connection between V6 and V8. So, anybody who takes part in V6 is guaranteed not to be part of V8 and so on. So, in this case, we want to find out what is the maximum number of dances we can simultaneously schedule on a day so that there is no such overlap.'},\n",
              " {'id': 'e3f3efdb-5137-4881-b719-a0a954738780',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': '(Refer Slide Time: 20:44) So, here is one such allocation, supposing we choose V4, then we cannot schedule these dances because they have some overlap. On the other hand, there is no overlap with V7, so we can take V7, so V7 will rule out V6, but it does not rule out V5. And if we have done V4, V5, and V7, we still have the possibility of V2 because V2 is not connected to any of these. So, these vertices, now unlike our previous situation, where the vertices are trying to cover all the edges, here, we are saying these vertices should be mutually disjoint from each other, no two vertices that we choose, no two dances that we choose must be connected because they should not share a participant. So, in this case, we have found four such and this is called an independent set. So, we want the largest number of dances earlier, we wanted the smallest number of cameras. So, that was the smallest vertex cover. Here, we want the largest number of dances that we can choose simultaneously in a day is program. So, this is the maximum independent set. (Refer Slide Time: 21:44) Finally, let us look at a very important problem, which is called Matching. And suppose we have a class project we are going to assign. And we allow groups of two students to do this project together. But we know that for two students to do this together, they must be friends, if they do not get along with each other, there is no point assigning them to a group. So, we have a graph like the one on the right describing the friends. So, it says that V2 is a friend of V4, but for instance, V2 is not a friend of V3, because there is no edge there. What we want to know is how many groups can be formed from this, so that we only pick pairs, which are friends. So, we want to find a good allocation. So, this is a subset. So, the allocation that we pick will be a subset of the edges.'},\n",
              " {'id': 'abfbc9b2-5cdd-44ee-98bd-e8a4dbce3d2f',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'And once we pick this subset, those will form the groups and if we have any person who is not paired up with another person, that person will have to work on their own. So, maybe this is a complicated project. And we would ideally like most people to work in groups, we want to find the largest such. So here, for instance, is what is called a maximal matching. So, I have paired up these two, I have paired up these two, I cannot pair up now 5 with anybody because the only thing five can be paired up with this 4 who is already part of another pair and same with 3, 3 can only be paired up with 1 and 4. So, this matching is maximal in the sense that I cannot add any more edges and make it a larger matching from what I have right now. But it turns out that this is not a maximum matching the sense that this says we can form two groups of two and then two people are left alone, but maybe in the six people we can form three groups of two so that everybody has a pair and yes, indeed it is possible. (Refer Slide Time: 23:26) So, we can pair up 5 to 4, 3 to 1, and 0 to 2 as we see here. Now everybody is assigned a pair and now everybody is covered. So, in this particular case is quite easy to say that if you have an even number of vertices and you have half as many edges in the matching that everybody is covered. So, in general, this could be a question which is if I give you such a graph, and I asked you to match up pairs with this constraint, then what is the largest matching that you can construct. (Refer Slide Time: 23:56) So, to summarize, graphs are very interesting because they express relationships, many flexible types of relationships in a very concise way. Graphs can be directed as we saw or undirected. So, when an undirected graph really every edge has a matching edge in the reverse direction, so we can think of it as a single edge which has two directions associated with it. One of the most fundamental questions that we need to ask about graphs is reachability.'},\n",
              " {'id': 'ef96f6df-8e6d-4ba6-a608-529151187e03',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 4},\n",
              "  'source': 'Introduction to graphs.pdf',\n",
              "  'content': 'Can I get from one node to another node by a path which is a sequence of connected edges? But there are a wide range of other problems also, which are not directly called connected to reachability which are also interesting. So, we saw graph colouring, we saw vertex cover, we saw the independent set problem, we saw matching. So, these are all a variety of problems, which are interesting computation problems on graphs and which have a number of practical implications. So, we will study some of these as we go along.'},\n",
              " {'id': 'a7d77a35-0022-429f-ba58-1ccdd12a3f1c',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Depth First Search (DFS) We said that there were two fundamental strategies to search and find reachability in graphs. One was breadth first. And now we will look at depth first search. (Refer Slide Time: 0:18) So, in depth first search, we start from some vertex, and we look at any one unexplored neighbour. And then from that explored neighbour, we will not come back and look at more unexplored neighbours with the original vertex, but rather we will go to j and then proceed from j. So, we kind of follow along path until we get stuck, and then we come back. So, unlike breadth first search, where we explore all the neighbours of the first level, and all the neighbours second level, here, we will kind of follow exploration. So, it is sort of like, if you are reading a web page and the first link that you come across, instead of reading the rest of the text, you click on that link, and you go to a new web page. And now the first link that you see on that, you click on that and go to a new web page. So, you are kind of getting infinitely distracted from what you were doing earlier, but this in a finite graph will actually work. So, we will continue until we reach a vertex which has no unexplored neighbours. And when we hit that point, because we did not look at all the neighbours before, when we looked at a vertex, we explored the first neighbour that we found, maybe the second neighbour has not been explored yet, it could have been explored also, because we could have reached it in around about way after going through that first neighbour. But if we find a neighbour, which is not explored, we will visit that so we kind of backtrack, but we backtrack systematically. So, we take the last place, we got stuck, come back to the previous step, backtrack there, then come back to this backtrack there and so on.'},\n",
              " {'id': '5b44c598-3f10-49ca-b6b5-4e2e3a90e2b1',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'So, in order to do the systematic backtracking, we will use a different data structure from what we did for the breadth first search and the breadth first search, in order to process vertices, we put them in a queue. So, every time we saw a new visited vertex, which is unexplored, we put it in a queue, and they all came up for exploration in the order in which they went into the queue. So, earlier vertices got (pro) processed before later vertices. Whereas in depth first search, this is not going to happen in general, because the earlier vertices, which are not, which are neighbours of which could have been seen later, will be processed only later. But we want to make sure that when we backtrack, we go back to the most recent vertices from which we made a choice. That is why we use a stack. So, a stack is a last in first out. So, just imagine a stack, like a stack of books, you put a book on top of the stack, the book that you take out will be the one you put last, if you want the second last book, you have to take out the top book and then take out, you can pull out something from in between. (Refer Slide Time: 2:36) So, if we run depth first search, for example, from vertex 4 here, then we can take any one of its neighbours, so it has 0, and 3, and 7. So, if we take the smallest one, for instance, 0, so we mark it, and then we explore, say, the strategy is that we will explore the smallest outgoing vertex from a given vertex, so 4 now. So, now I basically put 4 on the stack saying that I am not finished with 4. The thing that I had done with 4 was incomplete. But I am now processing 0. So, now I look at the neighbours of 0, so the neighbours of 0 are 1 and 2. And if I look at them in this order, then I will find that 1 is not yet visited. So, I will suspend 0 and look at 1 instead. So, now 0 has been put on top of 4 on the stack. So, the stack is growing from left to right. So, the stack is growing in this direction.'},\n",
              " {'id': 'e50556c7-303b-46f0-a383-2c09653a3661',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'So, now I have suspended 4 in the process of exploring neighbours of 4, I have suspended 0. And now I am looking at neighbours of 1. So, when I look at the neighbours of 1, I find that it has 2 neighbours 0 and 2, but only 1 of them 2 is unexplored. So, I will suspend the 1 and explore 2 instead. But 2 has neighbours, which are already explored. So, the exploration of 2 does not get me anywhere. So, this is a situation where I get stuck. So, I get stuck here. So, since I get stuck, I have to go back and say, I have nothing I can do here. So, I go back to 1 and I ask was there anything left pending in 1 which I could do now that I know that 2 is a dead end? So, I go back to 1. So, I pull 1 off the stack and I say, now let me look at its neighbours again and see if there is anything left to do. And I find there is nothing left to do because there are only to neighbours 0 and 2, 0 anyway, I had ignored because 0 was marked right at the beginning and I just marked 2. So, now I say this 1 is also a dead end for me. So, I backtrack to 0. Now I look at what was left with 0 so when I went from 0 to 1; 2 was unexplored, but notice that I came through this roundabout route and explored 2. So, now if I look at 0 to 2 directly, I find that it is already been visited by some other exploration which I initiated earlier. So, though 0 has never explored 2 itself by the time 0 looks at 2; 2 is visited. So, this is unlike breadth first search, where 2 would be visited only from 0, it will not be visited somewhere else. So, 0 visits cannot visit 2 so now I backtrack back to 4. (Refer Slide Time: 5:17) So, now I basically come back to the first vertex, I started with saying that whatever I did in the direction 0 has now completed and there is nothing more to search below 0. So, I look at my next neighbour, which is 3. So, I say let me suspend 4 again, and explore 3.'},\n",
              " {'id': '7383c8c5-fc91-458d-b293-5b3a0a4a9f28',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'Now 3 will suspend itself and explore 5 because that is a smaller of the two unvisited neighbours, 5 will suspend itself and explore 6 because that is the smaller of the two unvisited neighbours. But 6 has no new neighbours to visit, because 6 has neighbours 3 and 5, which are both marked as visited. So, 6 will say backtrack to 5. So, now I have something interesting because I went that way but I still have these two pending. So, backtracking to 5 allows me to do more things, I do not have to go all the way back to 4, like we did last time. So, 5 will say, let me now explore 7 instead. So, I spend 5 one more time and this time I explore 7. From 7, I can explore 8, because 4 and 5 are known, but 8 is not. So, I suspend 7, explorer 8. From 8, I explored 9. Now 9 is a dead end. So, having explored 9, I will come back and backtrack to 8. But there is nothing more to see from 8, so I backtrack to 7. There is nothing more to see from 7, so I backtrack 5. There is nothing more to see from 5, so I backtrack to 3. There is nothing more to see from 3, so I backtrack to 4. And now the stack is empty. So, there is nothing pending. I have seen everything. And as you can see, because it is the same graph. And hopefully, you get to get the same result everything is actually reachable from 4. So, this is how depth first search works with a stack. Now, when we implemented breadth first search, we actually had to construct that queue. If you remember, we created a class called queue, and we created a queue object and kept track of it. So, you might imagine that when you do depth first search, you have to do the same thing, you have to create a stack object and keep track of it. (Refer Slide Time: 7:15) However, it turns out that you can actually implement depth first search using recursion, and recursion implicitly keeps a stack.'},\n",
              " {'id': '9744f5ee-06c7-48a6-9fcc-2e8e7ee4f301',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'So, the thing about depth first search is if you want to do it, recursively, then you must separate out the initialization from the recursive call, because otherwise, every time you initial you call depth first search it initializes all the visited to be false in all that it is going to be something that will not work. So, there is an initialization phase where you as always set in this case say let us do it in one shot, this is visited and parent. So, we set visited to be the empty dictionary, the parent to be the empty dictionary. And then for having extracted the number of so this is with an adjacency matrix, having extracted the number of rows, we can set all the vertices to not be visited and all of them to have parent minus 1. So, this is an initialization phase. So, what it does is it returns an initialized version of parent and visited. So, we now have to take this initialized version of parent and visit and pass it through to DFS. So, the actual DFS is here. So, DFS now the way I have written it takes 4 parameters, it takes the matrix of the graph, it takes the vertex to start and it also takes the current status of visited and parent. So, what does it do? (Refer Slide Time: 8:44) So, it basically calls itself, so it does the obvious thing that it visits the vertex, which it has been told to visit, so it sets visited of V to be true. And for every neighbour, if it is not visited, it sets the parents so all this is familiar to us, this is what we do, whether you are doing breadth first search or depth first search. But now this is the new thing, which is I do not proceed with the next neighbour. So, I am looking at all the neighbours but I am not if I see one neighbour, which is not visited, I kind of suspend myself and this is done now implicitly through recursion, I restart DFS from the new vertex k which I just saw, and I passed the current value visited and parent is going to return back to me an updated value of visited and parent.'},\n",
              " {'id': '04c4b96f-9676-4c72-8074-8b492c26a828',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'So finally, DFS when it concludes, gives back so basically each time I visit something it update visited and parent gives it back to me. So, this is the recursive nature of DFS. Now, here, because we have essentially made visited and parent kind of internal to DFS, we have to keep passing it around. We have to call DFS with visited and parent and get it back. There is a little bit cumbersome and this is not how you would normally see it presented if you see an implementation in a language other than Python. So, usually in presentations of DFS, they will assume that this visited and parent, in this case dictionaries, or lists or arrays or whatever you are using to store that are actually global values which you can access from inside DFS. And so you can update them from inside DFS without having to pass them around. So, we can also do that. (Refer Slide Time: 10:25) So, we can make visited and parent global. So, we declared outside all our functions, these two empty dictionaries called visited and parent. And now because Python has this convention that all mutable values, a mutable value is either a list or a dictionary. A mutable value can be globally referenced from inside a function. So, this is one of the decisions that Python has made, which is basically to take care of the situation. So, you do not have to keep passing lists and dictionaries in and out of functions. I still have to initialize it, but the initialization happens now in this case, without having to pass the dictionary. So, remember, in the earlier thing, the initialization actually returned back the initialized visited and parent dictionary here, it is just going to initialize it. So, it is going to do the same thing, it is going to run through a loop. But now it is initializing it outside. So, this visited and parent dictionary are outside the function is just setting them up. So, I will have to first call this function to initialize it.'},\n",
              " {'id': 'f1e84bac-3676-4571-94a9-5191ef82a5d1',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'And now when I do the DFS itself, I only have to pass the functions, the parameters, which I would expect to pass, which is the matrix and the vertex to start with. So, I set visited of this vertex to be true. And then for every neighbour, if it is not visited, I will call DFS with that. Earlier, I had to call it with the updated value of visited and parent and get it back. So, this is a simpler, easier to understand version. And this recursive function is implicitly manipulating this visited and parent which are sitting outside. So, that is something so these updates here, setting visited v equal to true and setting parent k equal to V, these are happening to this global data structure, which is outside, those are something to keep in mind. (Refer Slide Time: 12:11) Now, you can do the same thing with an adjacency list. So first, let us look at the version where we have this thing passing around. So as before, except that we have a list, you will initialize these dictionaries to be empty. So, this is the version where visited and parent are maintained inside the function, so we have to keep passing them around. So, I will initialize them using the list of keys of the adjacency list. But otherwise is the same thing I updated to false and minus 1 and I return the initialized arrays. And now when I call will DFS list, I have to pass visited and parents. So again, I do the same thing I set visited to true. And instead of checking the neighbouring function, which looks at all the rows in the matrix, I just check the adjacency list of the current vertex. And for every such thing, if it is not visited, I will set its parent to be the current vertex. And I will call recursively DFS with the current value of visited and parent and get back an updated value of visited parent. And finally, DFS when it returns has to return this updated value. So, this is the non-global version of DFS using an adjacency list.'},\n",
              " {'id': '176b3136-6796-4f97-87d5-268640b7f297',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': '(Refer Slide Time: 13:22) And here is the global version of DFS using an adjacency list. So, as before, now, we make visited and parent dictionaries which are outside. So, when we initialize, we only initialize but we do not return the initialized values because they are being globally updated. And the only change is that the loop for initialization runs over the keys of the adjacency list rather than the columns of the matrix. And then the DFS itself is the same thing. The only difference is that instead of looking for the neighbours of k, v, which looks at all the columns in the matrix, I just look at the list of neighbours of V. And if it is not visited, I set the parent and I call it again. So, these are now 4 different implementations of DFS, which I have shown you, with and without, with an adjacency matrix, with an adjacency list, and maintaining visited and parent globally and locally. So, all 4 of them work. And it is a question of which one you find most convenient. Generally, if these are large values, large dictionaries this is generally desirable, not to pass them around. So, global things make it easier to write because you do not have to pass these dictionaries and remember to call them and get the updated value. So usually, for such things, the global version is preferred. (Refer Slide Time: 14:39) So, what about the complexity? Well, the complexity despite all this recursion is very similar to BFS. So, every vertex is visited once so it is marked as visited once and it is explored once. So, you call DFS from that vertex only once. So, at that sense, we do order n processing in terms of vertices, we never see a vertex twice. And as before, in order to explore a vertex, to decide which of its neighbours to process, we have to check which are all the neighbours. And an adjacency matrix that takes order n time. And in an adjacency list is take degree v time.'},\n",
              " {'id': '9614f917-2f18-4e11-a1fe-3dcc8ecaecbf',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Depth First Search (DFS).pdf',\n",
              "  'content': 'So, as exactly as in the BFS, if we do this thing with an adjacency matrix, we get an order n squared algorithm. If we do this adjacency list, we get an order m plus n algorithm. So, there is no difference in the worst-case running time, of course, there will be a cost due to recursion, which we are not going to worry about right now. But otherwise, in terms of the number of times you process these vertices and edges, we have this usual scaling of going from n squared to m plus n. (Refer Slide Time: 15:49) So, DFS is a different strategy from BFS. And it uses a stack rather than a queue and the stack is implicit when we do it recursively. Now, BFS we said gives us some extra bonus, which is that you get shortest paths, so we did two things would BFS we first recovered the path using parent and then we use this level function to discover the distance. Now, in the BFS, we only did the parent part, we did not do the level part. And that is because as we saw, even in that example, we saw that, like if I had 0, 1, and 2 connected like this, it could be that DFS finds this path because it suspends 0 then suspends 1. Whereas of course, there is a shorter path. So, DFS is not generally going to find the shortest path. So, even though the parent thing tells us something about one way of getting there, it is not really the fastest way so DFS does not have that advantage. So, you might ask, I mean, you have to do all this work, you have to do recursion you have to do all this. And at the end, you do not even get this information what is the point of DFS? It turns out that for other things, DFS is very useful. So, actually, if on the balance of things DFS is a more informative search than breadth first search. So, we will look at examples in a subsequent lecture as to why DFS gives you very useful information about the structure of a graph and therefore very often DFS is the primary way of exploring a graph rather than BFS.'},\n",
              " {'id': 'a9bc6a94-cfc1-42cb-884b-e596386a5694',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Applications of BFS and DFS So, we have been looking at graph exploration using breadth first and depth first search and we have primarily focused on reachability and recovering a path and also in the case of breadth first search, we said you also recover the distance in terms of number of edges. (Refer Slide Time: 00:24) So, both of them systematically compute reachability and since breadth first search works level by level, it also discovers shortest paths. So, the question now we are going to ask is beyond reachability, what can we do with breadth first and depth first search. (Refer Slide Time: 00:39) So, one of the important things that we want to know about a graph remember that a graph is not something that we can see. So, if we see this graph visually, we can see that there is something here which is not connected to the rest of the graph, we can also see that the graph as a whole consists of multiple parts which are not connected to each other. So, we can see that this graph is connected, disconnected whereas this graph is connected. So in a connected graph, we are talking say here about undirected graphs, in a connected direct undirected graph, you can get from everywhere to everywhere, in a disconnected directed graph, there are parts which cannot be reached from other parts. So now, the question really is how do we discover this algorithmically. So, one way is to find out what are the connected parts, so these are called connected components. So, if the graph is connected, then there will be only one connected component which collects all the vertices, if the graph is disconnected, we will find more than one connected component. So, connected component is a maximal set of vertices that are connected, that is everything in that set of vertices connected and I cannot add any more vertices to make it connected.'},\n",
              " {'id': '36b5c85c-d97e-4d19-9dbf-71bc6057c91d',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, for example, in the bottom graph this 0, 1, 4, 8, 9, so I cannot add any more vertices and keep it connected, any vertex in that graph below which I add to it will be disconnected from these. So, these 5 vertices form one connected component. These 6 vertices on the right 2, 3, 6, 7, 10 and 11 are also one connected component and we could also have a trivial connected component which has only one vertex because it has no edges. So, 5 is a connected component, because 5 is not connected to anything else. So it is connected to in a vacuous way because it is, it is connected to itself even though there is no explicit edge, but there are no edges anyways. So, what we want to know is how to find this. (Refer Slide Time: 02:31) So, the way to do this is to identify these components through reachability. So, we assign each vertex a component number. So, we start from, so now we are really trying to find the number of components, so we are not using BFS or DFS in the usual sense, where we start from one vertex and want to know what all we can reach. We are asking something about the graph as a whole, it does not really matter where we start, so you might as well start with 0. So, we start with vertex 0 and what it will do is it will explore everything that is reachable from vertex 0. So, we will call all of this to be the 0th component. So, we start our component numbering from 0, so we say everything that we can reach from vertex 0 is an component 0. So, in the graph on the top which is connected this spans everything, because all the 5 vertices are actually reachable from 0 including 0 itself, whereas in the bottom, it does not span everything. So, we reach 5 out of the vertices, 5 out of the 12 vertices, the remaining 7 vertices are not yet reached. So, now we assign all these things which you can reach in the first round of BFS or DFS, we assign them the value 0 that is these are all in the 0th component. Now, I look for something which has not been visited.'},\n",
              " {'id': '78e41c74-969e-4646-a4c4-9a797139fa3e',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, I pick the smallest unvisited node. So, in the first graph, there is no such node, in the second graph I pick 2 for instance and I starting with the smallest unvisited graph, unvisited node I run again DFS or BFS and it will now explore all of its reachable vertices, but I have to now record that this belongs to a different component. So, before I start this, I will increment the component number. So, I will say now whatever I can reach from 2 belongs to component number 1 not 0, because 0 was whatever I could reach in the first DFS. So, I ran BFS or DFS from component 2 in this case and I will get all the 6 vertices and they will now all have a component number 1 attached to them. Now, I see if there are still, so I repeat this I see if there are still unvisited vertices, I pick the smallest one namely in this case 5 and then I will explore everything I can reach from 5 which is just 5 itself and I will give it a new component number 2 and now after these 3 BFSs or 3 DFSs, I have now visited all the vertices, so there is nothing left to do. So, this is how I can discover the components in my graph and if there are more than one component in my graph, it means the graph is disconnected, if there is only one component means it is connected. So, here is a BFS version of this connected thing. So, I want to find the components in a graph which is given by an adjacency list. So, what I do is I initialise that, so this is going to be a dictionary which tells me the component number of each vertex, so component of I is going to tell me which component vertex I belongs to. So, I initialise the component of every vertex to minus 1. So, if a vertex has component minus 1, it means that it has not been visited yet. So, this is like saying that it is level was minus 1 and BFS. So, this is an implicit way of saying something has not been visited.'},\n",
              " {'id': '8aeddb26-a44e-4c99-8304-a9a40810f4e8',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, I have a current component ID, which I will compid, and I will mean this is one way to see whether we have finished or not, I will keep track of how many vertices I have actually visited. So, this is the number seen. So, seen is just a number of vertices which I have been visited. So, when I have seen n vertices, I visited them. So, if I look at the keys of my adjacency list, they run from 0 to n minus 1. So, so long as I have not reached n minus 1. So, so long as the number of vertices I have seen is less than n minus 1, there is still some work to do. So, what is this work, I must find the smallest vertex which has not been seen. So, I take all those vertices whose component is minus 1 and I take the minimum, so this is a kind of short form for saying, I construct a list of all i whose component value is minus 1 and I take the minimum of that, which is a built in list function in Python and I say that that is going to be my start vertex for my next round of BFS. I am going to start a BFS from here. So, now I will visit everything from here. Now, notice that when I visit from here, I am not visiting the things in the other components, but I do not care, I just want to know which ones I visited from this start vertex and for each of the nodes that I visit from, so now I am, I have got visited is set to true because I have kind of explored those things which have been visited from here. So, if visited is true, then I set this seen to be seen plus 1 and I said the component to be the current component and then eventually I update this. So, the point to note is that this BFS list, is going to start with visited to false for everything. So, each time I start a BFS list with start v, it is as though I start a fresh BFS on that list. So, the earlier BFS values are not there. So, each time the visited dictionary that I get back from BFS is only true for those things which are reachable from that particular node.'},\n",
              " {'id': '2cddeb9e-b197-40f9-9715-9c7d0acda188',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, that is why I am not going to be overwriting the component of anything that I have seen from an earliest starting node. So, at this point, I go back and I check, have I finished seeing everything, in the process of this has seen reached n minus 1? If not, there is still something to be seen. I look for the smallest unseen thing run another BFS and so on and finally, this component dictionary is the one that I want, it tells me the component ID for every vertex in my graph and by finding out how many values there are in the dictionary and know how many components there are. So, this is a BFS version of this component function. (Refer Slide Time: 08:53) So, what else can we do with BFS and DFS? Well, one thing we can do is to detect cycles. So, a cycle is a path or technically a walk that starts and ends in the same vertex. We said, remember that a path is something which does not repeat vertices. So, let us look at this, so for example, 4, 8, 9, 4, if I go around counter clockwise, so this is a path which starts at 4, it ends at 4. So, by definition, the cycle will start and end at the same vertex. So, in that sense, it will not be a path because it will repeat a vertex, so it will be a walk. Another cycle is this one. So, in this cycle, you might also repeat an intermediate vertex. So, we start with 2 go to 3, 7, then we go down here, come back here, come back here and come back here. So obviously, the first and the last vertex in a cycle must repeat. But in this case, also there is cycle vertex in between that repeats. So, this is allowed. But what we do not want is that we go round and round the same edge again and again. So, we do not want cycles to repeat stages. So, in particular, we will not say that, for instance this is a cycle. Otherwise every edge will become a cycle and then it will be a bit problematic, because that is not what we want to really look at. So, we want to really look at non-trivial cycles, which at least go around 3 vertices.'},\n",
              " {'id': 'e3a02963-b70b-48a7-bc4d-167cbe4d0104',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, if I go from i to j and back, that is not a cycle. So, I should never get the same edge in both directions in a cycle. I can repeat vertices, of course, I will repeat the start and end I can also repeat intermediate things, but if I just pass through like this, you know, so like this ray pass through this vertex twice, if I pass through this vertex twice, but I go through it through different edges. (Refer Slide Time: 10:44) So, if I do not have this situation, then it is called a simple cycle, because this cycle that I drew earlier decomposes into 2 cycles. So, there is this cycle and there is this cycle. So, this are 2 simple cycles and if I combine them by attaching them at the 7, I get a complex cycle which involves both of them. So, in many situations, we are interested in acyclic graph. So, a graph is acyclic which has no cycles. So, the graph at the bottom is actually cyclic as we saw, because there are cycles in the graph. Whereas this graph on the top is a cyclic because if I start anywhere, I cannot come back following the edges in the graph. (Refer Slide Time: 11:24) So, the way to explore this using BFS is to observe that if I start with BFS and I record the edges that are used for BFS, remember when I use BFS when I find an unvisited neighbour, I add it to the queue and I record this parent thing. So, in some sense, I am recording this edge from j to k was used in BFS. So, these used edges actually will form a tree because I will never come back to a node which I have already seen and so, BFS guarantees that I will never add an edge to a vertex which has already been visited. So, I will never that, so if there is a cycle it must be of that form, I have I have already seen it from an earlier node and then through some long path I tried to come back and visit it again. So, this cannot happen, because BFS does not allow you to visit the same vertex twice. So, BFS will always visit edges that form a tree.'},\n",
              " {'id': 'e8f0c72b-7aba-4a89-8876-365e3bc2cc90',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'Now, of course technically, it may not be a tree because a tree is connected graph on n with n vertices and this may not be a tree in that sense, but it might be a collection of trees like on the bottom here, these red edges are what maybe BFS does and it forms a collection of trees and a collection of trees from the English term is called a forest. So, the point we want to observe is that if there is an edge which is not used by BFS, then if I add that edge, it must correspond to an edge which I discarded because I, the target of the edge was already visited and that means that there must have been a cycle. So, any non-tree edge must cause a cycle. So, if I look at these blue, green edges, the bottom, all of these actually correspond to cycles. So, this is a cycle, this is a cycle, this is a cycle, this is a cycle. So, any of these if I add through the cycle. Now here, for instance, there are no such edges, because the tree itself exhausts all the edges and there is no other way of reaching any of the vertices except for following the BFS. So, if I have a non-tree edge in a BFS tree, that is the cycle. (Refer Slide Time: 13:31) Now, we can do the same thing with the DFS tree. So, let us see what happens to DFS tree. So, in a DFS tree, I will do something more complicated. So, I will maintain a counter which tells me when I explored a vertex and when I finished exploring a vertex, so in the BFS tree, I just kept track of which edges I explored. In a DFS tree, I will actually keep track of a more complicated thing I will say, I will count in some sense that time that I started exploration and finished exploring the vertex. So, here is what we will do, so we will assign a pre and a post. So, let us look at an example. So, supposing I start with 0. So, I start exploring it at step 0. So, I assign the vertex 0 the prenumber 0 and now I this counter will keep incrementing. So, from 0 I will go to 1. So, I enter 1 at step 1.'},\n",
              " {'id': '415fc056-7683-417b-9844-cdb70a210a5d',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, this is the pre-number for 1, but 1 is a dead end. So, I will finish with 1 and now I will say I finished with 1 at step 2. So, each vertex has a purple number, a pre-number and a green number a post number. So, the pre-number is when I started it and the post number is when I finished it and the post number will be after the pre-number and I keep incrementing. So, now I will come back to 0. So, I have gone here and I have come back to 0 but there is I have not finished with 0 yet. There is more work to do. So, I will now enter 4 at step 3. Because I have not finished with 0, I will not mark it as done. Instead, I will say that I increment a counter which I left it 2. I will increment to 3 and I will start with vertex 4, 4 will now explore its smallest neighbour which is 8. So, 8 will now be explore at step 4, 8, will explore 9, so 9 will be explored in step 5. But these are all slippery numbers because I have not finished any of these, I just going forward. When I come backwards, I close. So, now 9 is a dead end, because from 9 I can only come back to 4 which is already marked or 8, which is already marked. So, now I say 9 is finished. So, I mark 9 as closed and say its post number is step 6, I come back to 8, 8 is done. So, it is post number is 7, 4 is done, it is post number is 8. Now, I come back to 0, I have no further things to do 0. So, it is post number is 9. So, I finished exploring this component. Now remember, what happens next is I look for the smallest unmarked vertex, so the smallest unmarked vertex is 2. So, I start with 2, but I continue my numbering. So, I want to record the fact that I reached 2 only after finishing everything that I saw from 0. So, when I finished 0, I was at step 9. So, I start 2 at step 10.'},\n",
              " {'id': '2a7f8b50-c62d-4236-80bf-25807520b14c',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, from 2, I go to 3, which is exclude smallest neighbour, which is the step 11, from 3, I go to 7 which is its smallest neighbour, from 7, I go to 6 which is its smallest neighbour at each point, I keep incrementing this counter and setting it as the starting point. So, I started 2 at 10, I started 3 at 11, started 7 at 12, I started 6 at 13, from 6 I go to 10, I started 10, at 14. Now, from 10, I cannot go anywhere, because 6 and 7 are done. So, I finished 10 at 15, I come back to 6, 6 has only 2 and 7. So, 6 is also done at 16, I come back to 7, now 7 is not done, because 7 went this way and came down here and then down come back. So, now I can still go to 11, I can still go to 11. So, from 7 I can increment the counter to 17 and go to 11. But 11 is a dead end, so I finish 11 and 18, now I come back to 7, 7 is a dead end, so I finish 7 at 19. Now I finish 3 at 20 and I finish 2 at 21. (Refer Slide Time: 17:22) So, now starting at step 10 and going up to step 21 I finished these 6 vertices, now I have to do, again look for the smallest unmarked vertex which is actually 5. So, I have to look at it and continue in the sequence. So, 5 will now start at step 22. So, I start at step 22 and then I come back at step 23. Because that is it and now there are no unmarked vertices and I will stop. So, this way when I am doing my depth first search, not only am I keeping track of this tree, so I have this tree here, this forest, but I am also keeping track of this order in which I went in and came out. So, it said that I did 6 after I 7 and so on. So, as before any edge which is there in the graph, but which is not in my tree creates a cycle. So, for instance I have this edge from 4 to 9 for instance. So, the way I did my depth first search I came this way and then I backtrack that is because 9 to 4 would have created a cycle. So, in this if I look at my picture on the right, it creates a non-tree, so any non-tree generates a cycle.'},\n",
              " {'id': '9f840f3f-8e4c-4c80-972b-7849f3ff0822',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': '(Refer Slide Time: 18:34) So, here is an implementation of this DFS with this pre and post. So, we will use this global implementation, remember in DFS, we had this implementation we passed around these dictionaries and the implementation where we did not pass around. So, here I will dispense with the parent information, I am just in keeping the pre and the post. So, I have this visited dictionary and I have now these two things which record pre of i and post of i, so all of them are empty. So, I initialise them by saying that for every vertex visited is false. So, there should be a v here and pre and post are both set to minus 1 and now, the way it works is that I start DFS, so I have to keep this counter going, remember this counter is a running counter, which starts at 0 and as I go on it keeps getting incremented. So, this running counter is initially 0. So, we will have to call this thing 0 which I have not shown, but now when I come to visit a vertex I set pre to be the current value of the counter and then I increment the counter, then having incremented the counter I do the usual thing which is I look at all the neighbours and if they are not visited I set, so this parent should not be there. So, let me just remove this, so this parent is not there. So, I do not need to keep the parent, but what I will do is I will recursively call this DFS for k, with the new value of counter. Remember I have currently I have just incremented the counter, so I am now going to start this.'},\n",
              " {'id': 'ee168cd8-156b-48c2-865f-7e0185bdea30',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, when I reach that counter, its pre value will be the current count plus 1 and when I finish all these recursive calls and I am about to exit, I will set my post value to be the current value of the counter and I will again, increment the counter and leave so that the next day will get the, so every vertex which is explored, can work with the current value of the counter that is returned by this DFS, then assign it as pre or post, depending on whether its call is coming from pre or post and then incremented to pass it on to the next round. So, this is the only work that we have to do is to maintain this extra pre and post dictionary as a global dictionary and update it before and after each vertex, each DFS call and pass this counter around and increment it every time we assign it. So, we assign pre-increment, call it with a new value of count, I have to get back the value of count. So, I have to keep this count running through all these DFS calls, because the same count is being used in this whole sequence. So, this is showing you for one particular search. Now, if you want to do the earlier thing, you have to overlap it with the component thing and do this counting on one component, then do it on the next component, do it on the next component and keep the count value circulating through all of these, so that I have not shown in this. (Refer Slide Time: 21:32)  So, what can we do with these numbers? I mean, in the undirected case, we just said this nontree edges will give us the cycles. But in a directed case, the natural nature of a cycle is a little bit more complicated. So, for instance, if you look at this, if we look at 0 to 3 and 0, then this is a cycle because it goes around following the arrows. Whereas what looks like a cycle in the picture, if I say 0 5 1 0, this is not a cycle, because this last arrow is going the wrong direction, I can go from 0 to 5 I can go to 5 to 1, but I cannot come back from 1 to 0. So, it is not so straightforward.'},\n",
              " {'id': '1d76198e-44d0-4c11-b61a-62fe1f1494b6',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, a cycle must follow the edges. So, if I do the same DFS, remember the DFS will work whether the graph is directed or undirected. So, if I do the same DFS on this particular graph, then these are the pre and post numbers. As usual, the purple numbers are the pre-numbers and the greenish numbers are the post numbers. So, I start with 0, then I will go from 0, I will explore 1 from 1, I will explore 4, from 4 I will explore 5, from 5 I will get stuck. So, from 4 I will then explore. So, from 5 I will explore 7 and 7 has no outgoing edges. So, I get stuck, then I come back and then I come back to 4 and I can explore 6. So, this is the left-hand side. Then after that I am done with that side. So, then I will go from 0 to 2 and from 2, I will come to 3 and that is the right answer. So, this is how this DFS actually was explored and these are the numbers. So now if I look at this, of course, I have the tree edges. So, I have these edges, which are edges from the original graph, which I have used in my DFS. So, all these mark edges are tree edges. But I have many different types of non-tree edges. So first of all, I have edges which follow along the paths in this tree, so I have edges that go forward go down a path in the tree. So, 0 is connected to 5 in the tree via 1 and 4. But there is also a direct edge from 0 to 5 in my original graph, which is not there in my DFS tree. So, this is a forward edge, so it goes in the tree, from a node to something below it in the tree. (Refer Slide Time: 23:46) And of course, symmetrically, there is a backward edge, something which comes from below to above. So, we have an edge, for example, from 5 to 1, so 5 to 1 is not there in my tree, but it is an edge from a node below to a node above in the tree. So, it is connected by path in the opposite direction, this is a backward edge and finally there will be edges which go across branches in the tree. So, if I look at 3 to 6 for example.'},\n",
              " {'id': '15a9ed8c-07a4-4bd0-b753-f0b861bbfe9e',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, this part, if you remember, was explored from this side and this part was explored from the side. So, these were part of two different explorations. So, the edge from 3 to 6 is an edge from the right-hand side to the left-hand side. Similarly, the edge from 6 to 7 is also something which we will explore because these were explored in two different settings. So therefore, 6 to 7 is also different edge. So, these are called cross edges. So, they do not go from ancestor to child or child to ancestor but they go across. So, we have these non-tree edges come in these three flavours either they go down the tree, they are forward edges or they go up the tree, they are back edges or the across the tree. They are cross edges. Now, which of these are cycles? So, remember we said that if you look at this edge, for instance, 8, 5, 2, 1. So, the thing is that we have an edge from 1 to 4 we have an edge from 4 to 5 and we have an edge from 5 to 1, which goes back and this is indeed a cycle, what about the 0 to 5 edge. So, we have an edge from 0 to 1 and then we have an edge from 1 to 4 and then we have an edge from 4 to 5. But now 0 to 5 in the wrong direction, it does not complete a cycle, I need us I need an edge from 5 to 0. So, 0 to 5 does not make, I mean 1 to 5 does not make a cycle here. So, it turns out that only the back edges will give you cycles, the cross edges and the forward edges will not give you the cycle. So, unlike in the undirected case, in the undirected case, we said that you look at any edge which is not there in the DFS tree or in the BFS tree and you are guaranteed to get a cycle. But in the directed case, you have to be bit careful because you will get a DFS tree which you will get, which will give you the reachability but not every edge which is absent from the DFS tree actually gives you a cycle. So, you need to classify these edges as forward backwards across and look at only those cycles which correspond to these back edges.'},\n",
              " {'id': 'df6b9461-cb5b-4d51-b12c-2cac85c76ae6',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, how do we find this out, this is where our numbering helps us. (Refer Slide Time: 26:23) So, we can find out whether an edge is a back edge or a forward edge or a tree edge by using this pre and post number. So, if a tree edge is a forward edge, so let us look at this edge for instance. So, it is going from some u to some v, then what this means is that v was processed under u. So, the starting of v happened after u started and the ending of v happened before u finished. So, if I look at the interval 0 to 15, which is the entire time that I was processing u and I look at the time that I was processing v, so this is the u interval and this is the v interval, the v interval is included in the u interval. So, if I see an edge from u to v and the interval associated with the target is subsumed by the interval of the top, it means that this entire node happened below this node. So, that means it is a forward edge. The backward edges the reverse, this basically tells me this ancestor child relationship, a node is an ancestor of another node, if its interval spans the interval of the lower node, so a back edge will go from something which is sitting inside. So, if I want to look at, for example the edge from 5 to 1, I will say that 3 to 6 you are sitting inside 1 to 10. So, if the starting interval is sitting inside the ending interval, then it is a back edge if the starting interval is subsuming the other intervals, so it is a forward edge and finally, if the 2 edges, 2 intervals are kind of overlapping or actually if they are disjoint, so if I look at this, I have 4, 5 and I have 7 8 there is no, they will not overlap, they will just be disjoined. So, I have 7 8 and I have 12 13. So, they do not. So basically, while I was doing 6 I was not doing 7, while I was doing 7 I was not doing 6, while I was doing 6 I was not doing 3, while I was.'},\n",
              " {'id': '286d1e7f-0b31-4f3b-a906-87eb6cf5ad6e',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'So, this interval basically tells me I started doing it and I ended up doing it and anything which has a number in between happened while I was suspended or working on this node. So, forward edge will say the entire node below was processed while I was working, the back edge will say that I was done, I was done before that and so on and the cross edge will say we happen to disjoined them. So, by looking at these pre and post numbers, I can do this classification. So, therefore calculating this pre and post numbers gives us extra information. (Refer Slide Time: 28:47) We can also do other stuff with this. So, for instance, this notion of connectivity, which we talked about. So, connectivity, we said was something which tells us whether everything is reachable from everything. But in case of a directed graph. reachability also has to take into account the direction. So, if I look at direction, then I will say that i and j are strongly connected. If I can go from i to j and I can go from j to i it is not enough to go in just one direction. So, these are what are called strongly connected components. So, in the graph that we have on the right, the red marked portions are strongly connected, because I can go around this from anywhere to anywhere I can go around this from anywhere, anywhere. Though I can go from 0 to 1, I cannot come back, there is no way to come back from this component to that. So, this does not help me, the fact that it is connected in one direction. So, strongly connected components correspond to subsets of vertices where I can go from anywhere to anywhere and it turns out that we can also use this DFS numbering to compute strongly connected components, but we would not do that now in this course. (Refer Slide Time: 29:48) So, to conclude, BFS and DFS are first can do reachability they can talk about parents and paths and BFS can also tell you about distance. But in addition to that, we can find connected components.'},\n",
              " {'id': '55784212-742e-49ec-9fed-9976665bda06',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 4},\n",
              "  'source': 'Applications of BFS and DFS.pdf',\n",
              "  'content': 'We can also find these cycles by looking for non-tree edges and by using this numbering scheme for DFS, we can also find forward back and cross edges because only back edges generate cycles. So, DFS numbering is a very powerful way of recording the progress of DFS and uncovering the structure of the cloud of the graph and then connectivity also is more complicated for directed graphs, it is not enough to just be part of the same component as in terms of one directional edges you need bi directional connect, we need to go from i to j and j to i. So, the DFS numbering can also be used for strongly connected components. There are also other structural features. So, you might ask can I if I did, if I remove this vertex will the graph fall apart? If I remove this edge will the graph fall apart? So, these are important things. So, these are like if you have a telephone or internet cable and if one cable is snapped, when the whole network can disconnected or if this router breaks, will our network become disconnected. So, these are important points to find out in a graph and these DFS numbering can also help to find these. So, DFS numbering is actually a very powerful tool and that is why as I mentioned before, though, it seems to be more complicated as a concept because it uses recursion. It seems more complicated as a concept than BFS. Actually, DFS gives you a lot more information about the graph and therefore it is really the preferred tool normally for exploring a graph.'},\n",
              " {'id': '5c39f558-fa71-436e-91d2-6cc8b20d31af',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Representing Graphs So, we saw that graphs are going to be interesting objects to represent relations, and we have to do some computations on the graph. So, in order to compute on a graph, we need to represent the graph. (Refer Slide Time: 00:19) So, to work with the graph, we need to transform this picture into something that we can manipulate in an algorithm. So, we said, for instance, we have these vertices and edges, and then we have paths and we have reachability. Now, if I look at this as a picture as a human being, I can kind of see that this thing is connected. Or maybe if there are edges, I may not be able to see, I may have to calculate that is connected. Now, if it is a much larger graph, even visually, it may not be possible. And certainly, I cannot use this kind of visual analogy to write an algorithm. So, the algorithm has to represent this graph in a more kind of concrete way, and manipulate this representation in such a way that it can solve these problems without relying on this intuition of what the picture looks like. So, the picture is helpful for us. But it is not helpful for an algorithm. (Refer Slide Time: 01:12) So, we need to represent this graph in some simple way that we can manipulate. So, the first thing is we need to represent the sets of underlying objects. So, let us start with the vertices. So, we saw that the vertices could be named in many different ways. So, in one of the early examples, we saw names of teachers and courses, so each vertex had a name of either a person or a course, we also saw another graph in which the names corresponded to friends in a group. So, we could have many different ways of representing the vertices. So, we will choose typically a very simple representation, we will just say that we have n vertices that we will label them 0, 1, 2 up to n minus 1.'},\n",
              " {'id': 'f7709597-e042-4a8e-ba1d-2c1dc4d797b5',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'So, we will always think of the vertex names as numbers between 0 and n minus 1, when there are n vertices. So, essentially, we have to have some way of taking the original graph and mapping them. So, here, for instance, we have a graph, we will call them V0, V1 up to V9, so I will replace this by 0 up to 9. So, now, if I look at an edge, an edge now, because vertices are all numbers between 0 and n minus 1, an edge is also a pair of numbers I comma j but, as we said before, we will not allow a reflexive relation in graph, so we will not allow an edge to start at i and end at i. So, when we say that i comma j is an edge, i and j must both be between 0 and n minus 1 that is in the allowed range of vertices, but i should not be equal to j. So, with this representation, or this way of representing edges, and matrices, and vertices, the most obvious way to represent the graph as a whole is to record which edges are present and which edges are absent in a table, or a matrix called an adjacency matrix. So, we have rows and columns, 0 to n minus 1, and we put an entry at i comma j. So, I have a matrix that looks like this, so I put an entry at row i and column j, I put this to 1 if there is an edge from i to j. So, this is now a 0, 1 matrix. And the ones represent those edges which are present in my graph everywhere else, they will be zeros. (Refer Slide Time: 03:25) So, here, for instance, is how you would possibly do it in Python, if you use a NumPy array. So, first of all, the actual graph could be represented explicitly by a set of edges. So, here, for instance, if I as I said if we replace this by 0, and this by 1, and this by 2 and so on, we have an edge from 0 to 1, we have an edge from 0 to 4, we also have an edge from 4 to 0, which is a separate edge and so on. So, the edge list here represents, in a literal way, the arrows which are drawn in the graph there. But we want to translate this into the 0, 1 matrix.'},\n",
              " {'id': '5bbcaa60-1379-40bb-be89-615d64ba0233',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'So, what we do is we know that we have 0 to 9 in this case, so we create a 10 by 10, matrix 0 to 9, 0 to 9, initialize it to zeros. So, this is a NumPy initialisation, which has give me a matrix of all zeros of size 10 by 10. And now, we scan through this list. So, for every i comma j pair, which we see in this list of edges, we set the ij’th entry of the matrix to 1, so we already initialized to 0, so wherever we did not find an edge, it is implicitly 0. (Refer Slide Time: 04:36) So, if we do this, then we get a matrix which we can visualize like this. So, for instance, here it says that 3 comma 4 is 1, which means that in my graph, I should have an edge from 3 to 4, or it says that 7 comma 6 is 0. So, if I look at 7 and 6, I should not find an edge between them. So, the zeros represent edges which are not there. The ones represent edges which are there. (Refer Slide Time: 05:01) So, if this is a undirected graph then remember that if i, j is an edge j, i will also be an edge. So, if I take the same graph that I had before. And I removed the directions, I get this undirected graph. Now, the matrix will be symmetric. So, if I see 0 comma 1, I must see 1 comma 0. So, if I take this diagonal here. If I take any entry, which is above the diagonal, then if I go the same distance below the diagonal, I will find, so if I have 4 comma 7, I will have 7 comma 4. So, there is a kind of symmetry over this main diagonal. So, everything above and everything below the main diagonal, there will be a symmetric either it is both one or both zeros. (Refer Slide Time: 05:43) So now, we need to manipulate in terms of this matrix. So, earlier, when we had this picture, if we wanted to go to the neighbour of a vertex, if we want to extend a path, for instance, then we just look at all the outgoing arrows and see which are all the neighbour, which are connected to outgoing arrows. So, in an adjacency matrix representation, this corresponds to looking along a row.'},\n",
              " {'id': '45ca8b37-a301-4a3b-8323-6a6e44ff4968',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'So, basically what we have is, say the neighbour of 6 are 3 and 5. This means that if I look at the rows 6, in my matrix, if I look at all the entries here, these are all entries of the form 6 comma j. So, some of these 6 comma j pairs are in my edge relation, some are not. So, it turns out that 6 comma 3 is there at 6 comma 5 is there. So, here is a simple Python function, which computes the neighbour of a given vertex and returns it as a list. So, it takes an adjacency matrix, which is given and it takes i as this vertex for whom you want to find the neighbours. So, what you do is you initialize the list of neighbours to be empty. Now, you need to find out the size of this matrix. So, NumPy has this shape attribute, which tells you the number of rows and columns. In this case, remember, it is going to be a square matrix, or rows and columns are going to be the same, we need either of them, we will use columns. So, because we are going to scan through the columns, so for every j in the range columns, which is for every j from 0 to n minus 1. If I see that the vertex i, that I am looking at, has a neighbour j, then I append j to the list of neighbours, and I return it. And so, if I run this function on this particular example, if I run it on 7, for instance, if I start in row 7, then I will pick up 4, I will pick up 5, and I will pick up 8, so this should be 4, 5, and 8. So, this is what the function should return on this graph. (Refer Slide Time: 07:41) So, if I have a directed graph, the rows represent outgoing edges. And these are not the same as the incoming edges, because not every outgoing edge has a matching incoming edge. So, the columns represent the incoming edges. So, if I look at row 6, then this entry represents an edge from 6 to 5.'},\n",
              " {'id': '15176a3e-8b05-47b3-814a-f943d7f12e3e',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'If I look at column 6, this represent an entry from 3 to 6, so from 3 there is an edge in to 6, and from 6, there is an edge out 5, and there is no correspondence between these two in general, they could be but they need not be. (Refer Slide Time: 08:15) So, the degree of a vertex, as you know is a number of edges which are incident on it. So, if I am just looking at an undirected graph, then I know that if you look at this thing, for instance, so the column 6. So, this has two ones, and this has two ones because if I have an edge from 6 to 3 and 6 to 5, I also have incoming edges from 3 to 6 and 3 to 5 because there are no directions. So, I just need to see how many edges are connected to six and I get the degree which is 2. (Refer Slide Time: 08:42) But if I have not directed graph, this is no longer the same. So, I look at the outgoing degree. So, it says that the out-degree of 6 is 1 and it says the in degree of 6 is 1. So, there is one edge coming in and one is going out, in general, these is not be the same. So, if you look at, for instance, let us see an example. Supposing you look at 1 for example, overtake 0, it has two edges going out, it has 0 to 1 and 0 to 4. So, it has an out-degree of two, but if I look at the in-degree of here, oh it has 2 here also, it has 4 to 0 and 2 to 0. So, this is not anyway so you can have situations where you have unequal numbers of edges coming in and out. So, let me see if I can spot one here. So, I think in this particular graph, they are more or less all equal, but they need not be equal. (Refer Slide Time: 09:41) So now, one of the questions that we asked was reachability. So, we said given these edges, we want to find out if there is a path from one vertex to another vertex. So, concretely, if we interpret these as airline routes, we said that Delhi is city 0, Madurai is city 9. Is there a route from 0 to 0 from 9?'},\n",
              " {'id': '0596b247-a001-46e5-8cc0-bb0816ac05b5',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'So, the way we do this is a straightforward exploration, we start at 9 and we say, Okay, if we start at 9, certainly we can reach 9 from there because we have to do nothing. Then we look at all the places we can reach from 9. So, where we can go from 9? Well, if we look at the row of 9, it says that I can reach 8. So, if I start from 9, then I can reach all its neighbours. In this case, it is 8. So, now I say that 8 is also reachable from 9. Now, I see where all I can go from 8. So, 8 can go to 5, 8 can go to 7, of course, 8 can come back to 9. But that is where I started. So, that does not add any new information. So, I can now look at 5 and 7 and add them to my list, 9 is already there. But I have already, in some sense, I know that 9 is reachable, so there is no point in marking it again. So, then I pick these up in turn, so I pick 5 and 7, for instance. And then I can extend my thing, saying that from 5, I can reach, for instance, 3 and 6. And then from maybe 7, I can reach 4, then from 4, I can reach 0. And then I can stop in this case, because my target was to meet 0. So, I may be able to reach more things from 9, but I am not interested in knowing whether I can reach 1 or 2. So, this is the type of calculation that we want to do with the graph, checking reachability. (Refer Slide Time: 11:24) So, abstractly, we mark the sources reachable. And then we systematically mark neighbours, of marked nodes. So, we say that 9 is reachable. So, what are the neighbours of 9, 8, mark 8, what are the neighbours of 8, 5, and 7, mark them, and so on. And we stop when the target is marked. But the main thing that we need to do is recognize that we do not go back and get into a loop. So, we do not want to go back and say that from 8, we know that 9 is a neighbour. So, let us go back and explore what we can reach from 9 because we already started with 9. So, if we go back from 9, then we will come back to 8, and then 8 will take us back to 9.'},\n",
              " {'id': 'f3adebf2-6e33-44a1-9a1e-a47c1ab3d6e1',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'And then we go round and round in circles without making progress. So, we need a systematic way of making sure that we do not get into this loop. And as you should remember, there are two ways to do this, one is called breadth-first search, which does it layer by layer, how many things can reach in one step two steps, three steps, and so on. And then there is depth-first search, which keeps following a path until you get stuck, then it comes back and takes an alternative path, and so on. So, we will look at these in detail. (Refer Slide Time: 12:25) So, what we are talking about now, right now is representing the matrix, representing the graph. So, we want to make sure that the graph that we are dealing with is not a picture but something that our algorithm can manipulate. So, we came up with this straightforward representation, which was this adjacency matrix, which had zeros everywhere, except where there are edges and these are ones. Now, as you can imagine, very often, they will be mostly zeros and very few ones. So, we could look for a more compact representation. And this is what is called an adjacency list. So, notice that if I have in an undirected graph, if I have n vertices, then each of these can be connected to n minus 1 others and then they are symmetric, so we do not count them. So, n into n minus 1 by 2 is the number of edges that I could have. So, if I have n vertices, I have order n squared edges possible, but very often I do not. Similarly, in a directed graph, that factor of two goes away, we have at most n into n minus 1 different edge. But in many realistic situations, the number of edges is not n squared is not order of n squared, but it is closer to order of n. So, in such situations, most of my matrix will be zeros. So, here, a better representation is what is called an adjacency list.'},\n",
              " {'id': 'ebc0ff96-5c69-4bcf-afb0-db7594d5da86',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'That is, for every vertex, I explicitly record instead of that whole sequence of zeros and ones in the matrix, I only record the ones, I only record those vertices that are connected. So, here I say that 0, vertex 0 is connected to 1 and 4, I do not have to put the other zeros we say is not connected to 2, is not connected to 3, is not connected to 5, and so on. So, this is the adjacency list representation. So, for every vertex, it just gives you the list. So, it says 8 is connected to 5, and to 9, we do not have to say it is not connected to 7, is not connected to 0, is not connected to 6, and so on. (Refer Slide Time: 14:15) So, the simplest way to build up such a list in Python is to use a dictionary. So, we use the names of these vertices, in this case, 0 to n minus 1, 0 to 9 as the keys. And then for every edge that is there in my edge list, I just append it to this. So, if I assume that the edges are given an ascending order, then I will get these adjacency lists also in ascending order. So, very often convenient to assume that the lists are enumerated the same way that the columns are enumerated. So, if I have an edge from say, 8 to 5, and 8 to 9, then the adjacency list of 8 will be 5 comma 9, it is not 9 comma 5. So, if I construct the adjacency list for this graph, for instance, it shows this dictionary. So, 0 is connected to 1 and 4, 1 is connected to 2, and so on. So, for example, 8 is connected to 5 and 9. So, this is a more compact representation in general, especially if the number of edges is small with respect to n square. (Refer Slide Time: 15:13) So, one of the advantages that an adjacency list requires less space. Now, there are some disadvantages. So, if I want to check whether i is connected to j. Now, I want to check whether is connected to J in an adjacency matrix, I just have to look at the ij’th row. So, supposing I want to know whether 5 is connected to 7, then I just have to look at the entry 5 comma 7 and say, yes, 5 is connected to 7.'},\n",
              " {'id': '41c27c18-047a-453c-9a40-580ca833f00d',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'On the other hand, if I want to check in the adjacency list with a 5 is connected to 7, I have to take this list and walk down the entire list. So, this, in this case, it is a small graph. So, it is a small list. But in general, you can imagine that takes more time because you have to look at that entire list and go from one end to the other and find it. (Refer Slide Time: 15:58) On the other hand, if I want to add together all the neighbours, then it is more efficient use the list because the non-neighbours are absent. So, if I have a small number of neighbours, I will pick them up from the list, but I have a large number of neighbours also I will need to anyway scan them. But in an adjacency matrix, there is no way to find the neighbours except to scan all these n minus 1 entries in the columns. So, even if I have only one neighbour, I will not know it without looking at all the entries. So, in an adjacency matrix, finding all the neighbours always takes time proportional to the number of vertices, there is an adjacency list, it takes time proportional to the degree or more precisely if you are doing a directed graph the out-degree. So, depending on what we are planning to do one representation or the other can be more efficient. Very often, we will see that adjacency list is better than adjacency matrix for the kinds of problems that we are looking at. (Refer Slide Time: 16:52) So, to summarize, what we said is that we ca not just think of graphs as pictures, and then try to compute on them because this picture cannot be read by our algorithm. So, we need to represent this graph in some concrete form. So, we have two representations that we propose one is 0, 1 matrix, this adjacency matrix, where i, j represents Aij represents the edge from i to j, so it is 1 if there is an edge, 0 if there is no edge, so I canonically change my vertex names from whatever they are to 0 to n minus 1.'},\n",
              " {'id': '51915b0e-5ba8-4c32-8b06-36247f0c93fa',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 4},\n",
              "  'source': 'Representing Graphs.pdf',\n",
              "  'content': 'And then an adjacency list, think of it as a dictionary, where the keys are 0 to n minus 1. And each key is attached to a list the list of vertices which are connected to vertex. So, if I look up A, A square bracket i in an adjacency list, it gives me a list of values, precisely those neighbours. So, if I want to scan this list, it is only proportional to the degree of that vertex. So, with both of these, we can systematically explore the graph. So, we saw that we want to do the reachability kind of analysis and we have to do it, start into a graph and mark all the vertices are reachable. So, we will now describe how to do these computations using these representations.'},\n",
              " {'id': '4de98935-c1be-4f3a-82c7-6c8e78f8ede4',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees.pdf',\n",
              "  'content': 'Programing, Data Structures and Algorithms using Python Professor Madhavan Mukund Minimum Cost Spanning Trees (Refer Slide Time: 00:10) So, having looked at shortest path problems with weighted graphs, we now look at a different problem, which is a Minimum Cost Spanning Tree. So, minimum cost spanning tree is something that comes into play when we want to analyze a graph, and see, how to recover a connected part of it. So, for example, you could have a district where roads have been damaged by a cyclone and you want to restore the roads. And your first priority is to make sure that you can go from everywhere to everywhere. So, you want to make sure that the roads that are restored, connect all the vertices, so, you could have some roads which are not restored, but you want to choose the roads to restore which makes sure that after you have restored these roads, every town in that district is reachable from every other town. So, which set of roads should you restore first? And similar situation could appear in telecom. So, you have some fiber optic network or some other kind of network, which serves a bunch of customers. And now, you may want to provide some kind of guarantee to the customers that their service will not get disrupted, even if there is some kind of road digging along some of these roads. So, these cables are all laid along the road. So, what you do is you lay a second set of cables, which is on the opposite side of the road, for example, so that you make sure that if one set is done, the other set is not done. Now, of course, you could duplicate all the cables on all the roads, but is there a smaller set of cables that you can duplicate, so that if you lay these cables, still everybody will be connected even if the original cables get cut. (Refer Slide Time: 01:51) So, this is a problem that is called a spanning tree. So, you want to take a graph, which has a certain number of edges.'},\n",
              " {'id': 'ab085d4a-c55b-4a06-81fb-c851950a5faf',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees.pdf',\n",
              "  'content': 'They could be roads in the network or they could be these telecom fiber connections, and you want to retain a minimal set of edges so that the graph remains connected. So, a minimally connected graph is a tree. So, a tree is something which on n vertices is n minus 1 edges. So, if you add an edge to a tree, you get a loop. And if you remove an edge from a tree you get a disconnected graph. So, we want a tree, which is a sort of smallest possible connected graph on n vertices, and we wanted to cover all the vertices in the graph that we have. So, it spans that graph, so it is called a spanning tree. Now, in general, you could have more than one spanning tree. So here for instance, the red graph that we have drawn is a spanning tree it connects 0, 1, 3, 4, 2. So, it leaves out these two edges and still it connects everything, so this is one spanning tree. We could also have a spanning tree with these four edges 03, 31, 12 and 24, which leaves out two different edges. (Refer Slide Time: 02:57) So, when we are doing some practical problems, like the one we discussed about relaying roads, or restoring or placing duplicate fiber optic cables, there is also a cost associated with either relaying that road or maintaining a cable of that point. So, it is interesting to look at the spanning tree problem in the context of a weighted graph. So, we do not have just an underlying graph, we have a weighted graph. And now we have a criterion to choose between the different spanning trees. So, we said that, we could have multiple spanning trees. But now, we can say that one spanning tree is better than another spanning tree, if it costs less. So, this is a natural interpretation, where for instance, if I want to restore the roads as fast as possible, and as cheaply as possible. I would like to restore a spanning tree of roads, which costs the minimum and which takes least time to do.'},\n",
              " {'id': '77a34cde-31c7-4202-9881-3b12ce3ff971',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees.pdf',\n",
              "  'content': 'Same way, if I have to maintain redundant links to my customers from a point of view of a service provider, they want to spend as little as possible for this extra service that they are providing. So, we are looking at these different possible spanning trees, and we are assessing their value based on their weight. And what is the weight? Is just the sum of the costs of all the edges that form that tree. So, just like we saw for a path, the weight of a path is the sum of the weights along the path, here we have a tree, so these are all the edges that we need. Now, what tree gives us the total cost which is minimum? And this is what we call a minimum cost spanning tree. So here for instance, we have this weighted graph when we construct one spanning tree, the red one, then this will have its weight which is 18 plus 70 plus 6 plus 20 which is 114. But this is as we saw, not the only spanning tree, so we could also consider another spanning tree which has 10 plus 6 plus 20 plus 8, and we get a drastic reduction in the cost from 114 to 44. So, our goal is now to give given such a graph to identify the minimum cost spanning tree. (Refer Slide Time: 05:01) So, it is useful to remind ourselves of some basic things about tree. So, first of all, a tree, as we said, is the smallest connected graph that connection that you can draw on a graph. So, if you have n vertices, then a tree on those n vertices will have exactly n minus 1 edges. So, one way of thinking about this is that, if you have a connected graph then everything is in one component. Now, a tree is connected and acyclic, which means that, between any two vertices, there is no cycle, so there is only one way to go from one vertex to another. So, if you take a graph like this a tree and you cut it, at some point, it must fall apart into two components. Because if it does not fall apart in two components, it means there is a way to go around.'},\n",
              " {'id': 'a48cce34-685e-4a29-99a4-609fbdfbd6e9',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees.pdf',\n",
              "  'content': 'So, basically, if I have this and if I cut this vertex here, and I find that I can still go from here to here, then originally there must have been a cycle. So, therefore, if I delete an edge from a tree, I will add from one component I will go to two components. Now, each of these is connected. Now again, there they are also trees. If I cut thing I will go one more time and I have n vertices originally. So, I have one component, if I cut one vertex, I have two components, if I do this n minus 1 times I have n components, but n components means I have n isolated vertices, no more edges, so I must have started with n minus 1 edges. So, this is one argument saying that if we delete n minus 1 edges we will come to singleton vertices, so, the original tree had only n minus 1 edges. Conversely, if I take a tree and add an edge, it must create a cycle for the same reason we said before because you could already go from anywhere to anywhere. Now, if you add an edge, you are adding another way to go. So, therefore, the new edge plus the old path, so I have already a way to go from i to j and now I have added an edge directly from i to j together this must form a cycle. So, adding an edge to a tree will form a cycle. (Refer Slide Time: 07:03) So, the third property is that if I take any two elements into vertices in a tree, then there will be only one path between them. And they will be exactly one because they are connected, but there will be only one. Because if there were two then you can see like this, you can go two different ways, then there must be some place where these paths diverged and join. So, it could be at i and j or some intermediate point, so there must be a cycle somewhere in the graph. So, if you add multiple paths between two vertices, you cannot be having a tree. So, any of these facts, basically, any two of these tell you that something is a tree.'},\n",
              " {'id': 'dc3dc3ab-b9db-4caa-988d-8af935d1fcaf',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees.pdf',\n",
              "  'content': 'So, if it is connected an acyclic then it must be a tree, if it is connected and it has n minus 1 edges, it must be a tree, if it is acyclic and it has n minus 1 edges it must be a tree. So, all of these basically any two of these tell you that you have a tree and I imply the third. (Refer Slide Time: 08:00) So, why do we need this? Well, we actually use these facts when we are building spanning tree. So, we will have two strategies to build spanning tree. So, one will basically start with a small edge and try to grow a tree by adding smaller edges. So, this will be similar to our shortest path kind of strategy, where we try to add edges as minimally as possible. And the other one we will basically try to create a small graph, which is not a tree but in between I may not have yet a tree. So, I might have components, but I will try to avoid cycles, but I will pick up, since I know I want the smallest possible spanning tree I will pick up the edges in ascending order of cost. And if I can add it to my tree, I will add it if I cannot add it, I will throw it away. So, the first algorithm is called Prim’s algorithm and the second one is called Kruskal’s algorithm, and we will study both of them.'},\n",
              " {'id': 'ac54a743-050e-4d13-abf2-e696af73df7b',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': \"Programing, Data Structures and Algorithms using Python Professor Madhavan Mukund Minimum Cost Spanning Trees: Prim's Algorithm (Refer Slide Time: 00:26) So, we are looking at Minimum Costs Spanning Trees. And we said that there are two broad strategies. And the first one of them is called Prim’s algorithm. So, in Prim's algorithm, we will try to find a minimum cost spanning tree by incrementally growing it from a small edge. So, in particular, we could start with the smallest edge, and then we will extend the tree by adding the smallest edge from the tree. So, we already have a tree in Prim’s algorithm, so starting with 1 edge, so 1 edge is a tree. So, we will start with something that is already a tree, and we will see in what way we can grow this graph. Within the graph that we are looking at, what way we can extend this tree while keeping it a tree in the minimal way. So, we want to add the smallest edge to the existing tree to get a larger tree until we can grow no more. So that is broadly Prim’s algorithm. So here is Prim's algorithm at work, so I first start with the smallest edge. In this case, the smallest edge weight is 6, so I start with the edge 1, 3. Now, I have to grow this tree. So, I can either add this edge, or I can add this edge or I can add this edge or I can add this edge because I cannot add, I cannot add edge 8. Because if I add the edge 8, then I would have essentially. If I add the edge 8, then I will have two disconnected components. I would have two 4 and one 3. So, I cannot add the edge 8, so I have to add among the others the smallest edge to extend the tree. So, for instance, I would pick the edge 10 because 10 is the smallest of these. So, now I have this. Now, among those which are connected, the smallest is 18. But if I add 18, I will end up creating a cycle. So, this adding 18 will not allow me to keep this a tree.\"},\n",
              " {'id': 'cdeb40ea-423f-4536-98f4-5621bd470349',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': \"So, the smallest edge I can add, which keeps it a tree is, so I cannot add that edge, so the smallest edge I can add to keep it a tree is 20. And finally, now I have reached the point where I can add the edge 8, so I can add that and now I get this spanning tree. So, this is Prim’s algorithm. We have started with the smallest edge 6, take the smallest edge which is connected to the current tree 10, smallest edge is connected but does not form a cycle that is 20 and then 8 and I get a spanning tree. So, we have to argue that this is correct, of course, but this is Prim's algorithm. (Refer Slide Time: 02:28) So, Prim’s algorithm says, incrementally build a spanning tree. So, we have a set of vertices, which are already in the tree, we have a set of edges, which are already in the tree. So, the set TE forms a tree over the set TV, both of which are parts of the existing graph. So, initially, I have nothing. So, one strategy that we saw now is to start with a minimum cost edge. So, we start with the smallest cost edge i comma j and make that our initial set of tree vertices i comma j and our initial edge is this E. And now, I will choose the smallest edge, which starts in the tree and ends outside the tree. So, U is in the tree, V is outside the tree, and I will add this edge, and I will add the corresponding vertex V to the set of tree vertices. So, I will add U, V to the U comma V edge to TE and I will add the vertex V to TV. So, just to run it by hand, so initially here TV is empty and TE is empty. Then we pick that 1 comma 3 edge, so we say that the set of tree vertices 1 and 3 and the edge 1 comma 3 is there. Now, I pick 1 comma 0 or 0 comma 1. So, I add the vertex 0 to my tree vertices and I add the edge 1 comma 0 to my tree edges. Next thing to add is1 comma 2. So, I will add the vertex 2 to my set of vertices and the edge 1 comma 2 to my tree. And finally, I will add the vertex 4 and the edge 2, comma 4.\"},\n",
              " {'id': '44e2bc7c-a27c-44d4-a0b9-7cdfad38ff5d',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': \"So, this is the representation of Prim's algorithm as we call. (Refer Slide Time: 04:12)  So, let us see, why this is correct. So, the correctness of this algorithm is based on what is called the Minimum Separator Lemma. So, supposing I can take any set of vertices, and partition it into two parts. So, I have U and I have W. And now, let me look at any edge, which crosses the partition. So U is on, so this is small u and w. So, small u is on the left hand side small w is on the right hand side. Then the claim is that, no matter how I construct a spanning tree on, so this is part of the entire graph. So, no matter how I construct the spanning tree on this entire graph, it must include this red edge. So, that is what the minimum separator lemma says. So, let us assume for simplicity that all the edges have different weights. So, all edge weights are distinct. So, this is the picture now. So, my entire set of vertices, so, this whole thing is meant as set V, so this is capital V, this is my set of vertices, and it is split into two parts, one part capital U one part W, and I have an edge of going from capital U to capital W of minimum weights. So, I might have many different edges here. And among these this is the smallest one. So, the claim is that, this must be part of every minimum cost spanning tree. So, supposing we have a tree, which does not include this edge. So, the tree that does not include this edge because it is a spanning tree it connects the entire graph. So, in particular in this tree, I have this path. So, I have a path from U to W in this tree. So, the blue dashed lines represents a path from u to w. So, this P this path P starts in capital U. So, this is the path from U to W. So, this path starts on the left-hand side in the red set, and it ends on the right-hand side in the purple set, so it must cross at some point, it has to go from here to there. So, this must be some edge, which takes it from capital U to capital W.\"},\n",
              " {'id': 'c1387e8d-7c0a-42f1-ba0d-c3a5d84bf354',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'So, let this be the first such edge. So, what do we know? We know that this is the smallest edge which crosses. And here is another edge which crosses, and we have assumed that all the weights are distinct. So, we know that the weight of e is strictly less than the weight of f. So, what we can do now is remove f and construct the path the other way. So, we get something which excludes f and includes e, so it will certainly have less cost because we have taken a bigger edge and we have dropped it to a smaller edge, and now, you have to argue that is a spanning tree. So, that is easy to check. Because you now just have to check that everything is still connected. So, anything that was connected through f is now connected through e. Because if I can go from here to here, and so, supposing I let me assume that I had some other path which followed this thing, so now I can instead go from here to here go all the way around come here and then go there. So, every path which was there before through f can now be rerouted via e. And in particular, because e is smaller than f, the new tree that I have constructed is strictly smaller. So, this minimum separator lemma is very powerful. It says that, if I take every way of partitioning my thing then the smallest edge connecting these two partitions has to be there in every particular tree. So, what if two edges have the same weight? Well, then we will kind of break ties by assigning some index. And we will say that, one edge is smaller than another edge either if it has a lesser weight or it has a smaller index. And then in that sense, we will say that the smallest edge must always be there. (Refer Slide Time: 08:25) So, what we have in Prim’s algorithm remember, is that, at every point we have constructed the tree so far, this is TV, with a set of edges, and then we have the remaining part of the graph. And what we are doing is we are trying to find the smallest edge e such that u belongs to TV, and V belongs to the rest.'},\n",
              " {'id': '4cd45ca0-32c1-42fc-829a-a65f13f8a18f',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'So, the smallest edge which takes me from the tree outside the tree. But this is like saying that I have a partition, and I am looking for an edge which goes from this partition to that partition. And therefore, from the separator lemma it says that this edge that I have picked in Prim’s algorithm must be part of every spanning tree, so it is always making a correct choice. So, actually, we started with the smallest edge, and the smallest edge must be there in every spanning tree. Because if I take, if i, j is the minimum edge in the graph. If I take my partition as i and V minus i, then clearly since i to j is the smallest edge anywhere in the graph, this must be the smallest edge connecting i to the rest, so it has to be there. So, that is for starting with the smallest edge is a safe thing to do. But if you look at this minimum separator lemma, it actually tells you something a little bit more general than that. It says that, for any vertex if you take. So, it is not necessary, in this case, we are looking at that particular minimum edge i comma j and saying i and the rest or j and the rest must be connected by this edge. But if I take any vertex V, and I take the rest of the vertices, then this vertex because original graph is assumed to be connected, this vertex must be connected to the other vertices somehow. So, if I take the smallest edge coming out of this vertex, it goes to the other side. So, I am sitting here in V, and here I have V minus v. So, here I have a bunch of edges, because it is connected I have at least one. So, if I take the minimum among these it is going to be the smallest edge connecting the partition small v to the partition V minus v. So, therefore, the smallest weight edge leaving any vertex has to be part of every MCST. So, instead of starting with this minimum cost edge, we can actually start with any vertex and just start with a trivial tree consisting of one point.'},\n",
              " {'id': 'd3eaa260-c069-4455-a6ad-1323c3a12e42',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'And then say, okay, from this one point I want to grow this tree, so I want to pick the smallest edge which leaves this tree, this one point. And then I get 1 edge which may not be the overall smallest edge, but it will be the smallest edge from this V, but still we will get a correct answer because that will be still by the minimum separator lemma part of every spanning tree. (Refer Slide Time: 11:29)   So, here is an implementation. So, we are going to keep track of the tree that we have built, and we are going to keep track of the distance of every vertex. Because, remember, that we have to take the shortest distance for every vertex from the tree that we have built and pick the vertex, which is at the shortest distance, and we are going to keep track of the tree edges. So, remember, so this is basically going to tell us what we call TV, and this is going to tell us what we call TE. And this is going to, distance V is going to tell us the distance from every vertex to TV, that is the smallest edge connecting V to the tree. And we are going to pick the minimum among those at every step. So, these are the three things that we are going to keep track of. So, initially, as usual, we define infinity. So, we are directly working with a list because we do not, we know that it cannot be worse than working with a matrix, so we work with a list. So, we initialize visited to be a dictionary, distance to a dictionary and tree edges to be a list, to empty dictionary empty list. Now, the first thing what we do, is that, for every vertex we initialize visited to false, and we initialize distance to infinity. So, initially, there is no tree, so every vertex is unvisited and every vertex is infinitely far from the tree, because there is no vertex in the tree. Now, we set some vertex to be in the tree. So, we said before that it is not necessary to start with the minimum vertex, so we can just pick vertex 0 and say start the tree at 0.'},\n",
              " {'id': '3df20b38-2320-4463-827a-def31dddca55',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'So, eventually, in the tree 0 has to be there. And we said that the smallest edge connected to 0 will actually form part of the tree. So, once we have thrown 0 into the tree, then 0 is part of TV. So, if I have 0 there, vertex 0, and if I look at all its neighbors, they are now connected to the tree by edges to 0, and that now defines their minimum distance to the tree. So, for every vertex in the adjacency list of 0, I set the distance to that vertex to be the distance of that edge, the weight of that edge. So, if I have u, 0 to v, with distance d, it will be a pair v comma d in the adjacency list of 0, so I will set the distance of it v to be d. So, this is my initialization. So, I have set up a trivial tree with one vertex and I have initialized the distance of all its neighbors to be the distance of the edge from 0 to its neighbor. And now, I do the usual thing, which is I will try to calculate the minimum distance. So, I initialized to, I will do the traditional minimum distance thing. I will set minimum distance to be infinity, and then I will find it. And I will set the next vertex that I want to add to be none. So, essentially, I will go through all the vertices and I will check whether it is visited. And if its minimum distance is currently smaller than the minimum distance of the, so this is. So, basically, I take an edge u comma v, I check that u is visited, v is not visited and this is the distance. And I check whether this distance is actually smaller than the minimum distance that I am trying to calculate, and if so, I update it. So, at the end of this loop, I have found the distance which is minimum, and I found the next vertex, and I found the next edge also, which is the UV, this is the next edge to be added. So, if I have not found a vertex to add, I come out of it. So, this will be a paradox, this will be a pathological case when the graph is not connected, which we need not worry about, but I have put it for completeness.'},\n",
              " {'id': 'f1682ef5-191f-4f60-bc9c-08fc78b59426',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'But in general, I will come to this point, where we have now identified this minimum vertex. So, we have identified this minimum vertex. And now, we set it to be true, we append the edge that we have discovered from u to v, so, there is bracket u to v to be there. And now, for every neighbor of the vertex that we have just discovered, for every v comma d, which is in the list of next v, if it is not visited, we update its distance. So, this is how Prim’s algorithm works. But if we look at the complexity of this algorithm, the initialization takes order n, the loop runs n times as usual. And now in this loop, we have this, which is actually looking at all the edges. So, it is saying for every edge, if the starting point is inside and the ending point is outside then consider the distance and add it. So, this is terrible, because this thing is actually n times m, which could be n cubed. And we would hope that this would be no worse than n square, similar to say, Dijkstra’s algorithm. So, we have to do something a little more clever to get this algorithm to actually come down to a reasonable complexity. So, we will do the same thing, but we will maintain our information a little differently. So, as before, we will keep this visited and this distance. But remember, that every time we update the distance, we have a reason to update it. So, we updated the distance because we discovered the distance from the most recently added vertex was smaller than the distance we had before. And therefore, this edge represents the edge, which connects this unvisited vertex to the tree in the smallest way. So, supposing I had this thing, and I had a vertex v there, and now I grow my tree to this point. And now, I discover that this distance is smaller than whatever distance I had from here to here. So, I throw away that and I say that this is my new distance, but I also know that this edge is the reason why this vertex has its distance.'},\n",
              " {'id': '71e29813-9f79-413d-8787-736dac31ae05',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': \"So, when I compute the minimum distance, I will also recognize which edge I added, so I will keep track of this neighbor. So, distance v tells me how far away v is from the tree, but neighbor v tells me which vertex in the tree is actually connected at that distance. So, remember that this is not the shortest path distance this is a direct distance. This is an edge connecting v to the tree. So, where is the other endpoint of the edge that is that neighbor of v. So, now this becomes my new thing. I no longer need the tree edges, because I will discover them from the neighbors. Eventually, the neighbors will form the tree. So, what I will do is, instead of keeping the three edges I will keep the neighbor, and initially, the neighbor is also a dictionary, which is empty. Now, I initialize visited to false as before distance to infinity as before, and the neighbor of every vertex, I initialized to minus 1. Minus 1, it should be a name of a vertex between 0 and n minus 1, so since it is minus 1 there it is not defined. Now, as before, I start by setting visited of 0 to be true. And now I do two things. For every neighbor of 0, I set its distance to be d, and I set its neighbor to be 0. So, now when I connect 0 to all its neighbors I make them all point back saying the reason my distance is d is because my neighbor is 0. So, now that I have this list of neighbors, I am in a little better shape. I can do what we did in Dijkstra's algorithm, which is among now all the vertices which are not visited I find the distance. And among these, I find which ones are the, I mean, I find the minimum distance and then I find the vertices corresponding. So, this is exactly the same code that was there in Dijkstra's algorithm. We first find the minimum distance among the unvisited vertices, and among those, we find out which vertex actually has that distance, so we get a candidate to add.\"},\n",
              " {'id': '0c41ec78-13ad-4689-8a4e-8dacc3a34fe5',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'So, we get this candidate next v, and we make it true, but we do not need to know how it was connected because it was already connected by its neighbor, so that is the critical thing. So, we do not need to know, which edge to add because that edge is implicitly there, in neighbor of next v. And now we go through all the neighbors of this newly added nodes. And when we update the distance, we also update the neighbor. So, if the distance reduces because of next v, then the neighbor becomes next v. So, in this way, as we update the distance, we also keep track of this witness, as it was saying, why did my distance come down to d because it came down through this vertex u. So, when I pick up this vertex to add to my tree, I already know which edge I add. I add the edge from my neighbor to myself. So, if I return the set of neighbors at the end, then from the neighborhood relation, I know exactly I can reconstruct the tree. It will tell me, the neighbor of 3 is 2, it means that the edge 2, 3 is in the tree, and so on. So, this is an alternative way of implementing the same idea of Prim’s algorithm, except that instead of keeping the tree edges, we keep track of this neighbor for every unvisited vertex saying this is my current best way of joining the tree. (Refer Slide Time: 21:05) So, in this case, we scan all the non-tree edges. And after we do all this, then the complexity improves because the scan to find the next vertex is now order n. Earlier it was order m. So, this is very similar to Dijkstra’s algorithm, in fact, so Prim’s algorithm and Dijkstra’s algorithm are really more or less the same except for this update operation. In one case, you update the distance to be the distance to the burnt vertex plus the new edge, here we just contain, construct the distance from the burnt vertex do not plus it. We just take the distance from the most recently burnt vertex to the tree, to this vertex.'},\n",
              " {'id': '55e213d4-feca-4bd9-848f-172e721a4ef8',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': \"So, we want to know the distance to the tree and not the distance to the source, that is the only difference. So, Prim’s algorithm is really a kind of variant in some sense of Dijkstra’s algorithm. So, like Dijkstra’s algorithm, though it remains order n squared for adjacency lists we will see that if we can do this more cleverly of finding the minimum in a collection then we can do better. (Refer Slide Time: 22:06) So, Prim's algorithm is our first algorithm to build a minimum cost spanning tree. So, it starts with an arbitrary vertex or the minimum edge as the case may be, and it grows this tree by adding to this existing tree, the smallest edge that can be added. So, we had this very powerful minimum separator lemma, which showed that every edge that Prim’s algorithm adds to the tree is necessary for the tree to grow. So, it basically justifies why this strategy of, again, it is a greedy strategy like Dijkstra’s algorithm, where I look around the tree and I find the smallest edge which is connected out of the tree, and I add that to the tree. So, I am not going to look further down and see, whether I can take the second minimum now and maybe later on it might help me. So, every time we have a greedy strategy, we have to justify that this choice, which is frozen now is going to work in the future. So, just like in Dijkstra’s algorithm, we had that argument telling us that later on we cannot find a shorter path here it says that every such greedy choice we make is guaranteed to be correct, because it must form a part of every minimum cost spanning tree. So, the implementation is also very similar to Dijkstra’s algorithm. And like in Dijkstra’s algorithm the complexity is n squared even with adjacency lists, but the bottleneck is really this thing of among the unvisited vertices find the minimum.\"},\n",
              " {'id': '8882daeb-82c1-433e-bcbc-6eb4fcefd902',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Prim_s Algorithm).pdf',\n",
              "  'content': 'And so, this is a common problem that arises and a lot of things that I have a collection of objects and I want to find the minimum and remove it or I want to find the maximum and remove it, they are kind of symmetric questions. So, we will look at a data structure for this later on, which will allow us also to reduce the complexity of Dijkstra and Prim’s algorithm to something from n squared down to something more like n log m.'},\n",
              " {'id': 'a189bf02-2fd5-4ab1-b35c-707b10e36e06',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': \"Programming, Data Structures and Algorithms using Python Professor. Madhavan Mukund Single Source Shortest Paths (Dijkstra's algorithm) We are looking at shortest paths and weighted graphs. And we said that there are two types of shortest paths single source, and all pairs. So, let us look at single source shortest paths. (Refer Slide Time: 0:20) So, we want to find the shortest path from a fixed vertex to every other vertex in a weighted graph where we have this weight function, which assigns the real number weight to every edge. And for now, we will assume that these weights are all non negative. So, there are no negative weights in the graph. (Refer Slide Time: 0:37) So, the shortest path problem now is to compute the shortest path from some fixed vertex. So, let us say that fixed vertex happens to be 0. So, we want to compute the shortest path from 0 to every other vertex. So, here is a way to think about it, you think about the time it will take to reach every other vertex from 0. So, you start off some process, which takes a uniform time according to the cost of the path. So, you can imagine that if these are roads that you are walking on and these paths are these weights are actually lengths of roads, and if you walk at a uniform pace, then the time it will take you to reach a given place would be proportional to the length of the road up to that place. So, to graphically illustrate that, let us assume that these are oil depots, and these edges are actually oil pipelines. And now we set fire to this depot at vertex 0. So, the fire will now travel along all the pipelines at some uniform rate in all directions. So, you wait to see where this fire will reach first, and that will set fire to the next depot. So, for instance, if the fire travels at a uniform speed, the first oil depot to catch fire after 0 is going to be 1 because if I start a fire from 0 and go in both directions, then after 10 units of time the fire is going to reach vertex 1.\"},\n",
              " {'id': '66424bfd-8820-4b6b-94d8-ed8a9a0444be',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'Then if the fire continues to flow, then the fire will propagate from here in both these directions, and then the next vertex to burn will be the one which is second closest to 0. So, it is not measured in terms of the number of edges, but in terms of the time it takes for the fire to reach. So, we can make this systematic in the form of an algorithm. (Refer Slide Time: 2:19) So, for instance, we start at 0 and then after 10 units of time, we can see that vertex 1 is going to burn. And now at this point, this fire has started here. So, it has reached up to some point. So, this is not that there is no fire going towards 2 there is a fire, but it is only reached 1 eighth of the distance because this traveled 10 by 80 of the distance. Now after 6 more units, this fire will reach here. So, there will be a fire going in both directions, but the fire will reach 2 before it reaches 4. So, after 16 units of time from the beginning, that is 6 units of time after vertex 1 burned, I have a fire, which is kind of heading in this direction, I have this fire which has made some progress, but I have burned vertex 2. Now I am at time 16. So, if I continue for some time, then when I am 20 units away from this time, so, at time 30 I will expect that vertex 4 will have burned. Now at this point there is a fire which has started from 2 towards 70. Towards 3, there is this fire, which is still running from 0 towards 2. So, there are these two ongoing fires which will eventually get there. But we may not need to, they may not need for example, this fire certainly does not have any purpose to serve because once it reaches 2, it is going to find the 2 is already burned. So, now proceeding after 30, the next thing that is going to happen is that I am going to have two fire going in this direction. So, after 5 units of time, something is going to happen. So, at 35 node 6 will burn at 45 node 10 will burn.'},\n",
              " {'id': 'e74e8b7a-6a9d-4f48-bd8c-d771355aa553',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'And finally, the only unburned vertex at this point is 3 and the only way it can burn is from 2. So, since 2 burned at 16, and it takes 70 units of time from 2 to reach 3, at time 86 I will finally see vertex 3 burn. So, this is the way we want to track this process and compute these distances. So, from this what we will conclude is the shortest distance from say, 0 to 3 is 86 from 0 to 5 is 45, and so on. (Refer Slide Time: 4:23)  So, to make this actually effectively computable we need to calculate these times and decide which will burn left what we did by kind of examining graph by hand. So, we compute what is called an expected time to burn. So, initially, we know nothing about the graph. So, all the vertices will, may never burn. So, we can just assume them to be some arbitrary value infinity. So, all vertices as far as we know will burn will never burn. Now, we set fire to the start vertex. Remember this is a single source shortest path problem. So, they will be a well defined start vertex which is we start at. So, we know that at time 0, we will burn but 0. So, that expected burn time of 0 gets upgraded from infinity to 0. But once we know when vertex 0 burns, we can look at its outgoing edges. And from these outgoing edges, we have a better estimate of when its neighbors will burn. So, I know now that vertex 1 will burn not later than 10. And vertex 2 will burn not later than 80. So, I can update their times from infinity to 80. Now I look at all these expected times. And hopefully there is something which is finite. And among the finite one, whichever one is smallest is the 1 that is going to burn next. So, I can now see that there are only two finite 1, 80 and 10. But 10 is smaller than 80. So, in 10 units of time, vertex, 1 will burn. And once vertex 1 burns, again, I can look at its neighbors. So, one of its neighbors is already burned. So, it is not useful to me, but neighbors which are not burned, namely 2 and 4, I can update my estimate.'},\n",
              " {'id': '9f497a70-75b3-4260-beb5-538bf7c17120',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, earlier, I believe that vertex 2 would burn at time 80. But now I know that it is actually going to burn not later than 16 because the vertex the 1 fire is going to reach it at time 16. Similarly, the vertex 4, which was earlier assumed to never burn is now going to definitely burn within 30 times units. So, I can update now those to 16 and 30. So, at each point, when I burn a vertex, I will update the expected time of its neighbors and I will keep them picking up the smallest unburned vertex to burn next. So, after this now, I can see that I have two things which are unburned and which have a finite time to burn. So, of these 2, at 16 is the earlier one. (Refer Slide Time: 6:38)  So, I will burn that this will update only one thing, namely the time at 3 from infinity to 86, because it was earlier believed to never burn now I know it will burn within 70 time units of 2 having burned. So, now I have these two which are unburned, 86 and 30. So, I pick the smaller one of these two and burn it. And now I have to look at the new things that I learned from that. So, from this, I learned that vertex 5 will burn at time 80 and vertex 6 will burn a time 35. So, now I have all my unburned vertices have a finite estimate. So, the smallest estimate among those is actually going to be the one that will burn next. So, I burn at 35 vertex 6. And with this, I get a new update saying this is not 80 but 45. So, now I am left with these two. So, I burned the 1 at 45 and finally I burn the one at 86. So, this algorithm that we have just described is a very well known algorithm due to a very famous computer scientists called Edsger Dikjstra. So, this is often called Dikjstra algorithm. (Refer Slide Time: 7:56) So, in Diksjtra algorithm, we are choosing a strategy to burn vertices, based on what we have seen now and what is the best at the next step. So, we have made these estimates about when things are going to burn.'},\n",
              " {'id': '3da3c301-ded9-4c6b-a15d-c8abbc50037c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'And we are using these estimates to guide us to tell us which vertex burns next. So, this kind of short sighted strategy where we are ignoring anything else, we are just saying, so far this looks like the best thing to do next, we will not look two steps ahead, for example. We will just directly burn it and proceed. We never go back and make a choice that we had to take a choice we had taken before and undo it. So, this kind of forward thinking strategy where we keep making a local decision and never go back on it is called a greedy strategy. So, greedy for the obvious reason, you have a number of choices given to you, you take the one that looks best, and then you jump into it, and you never undo that. So, whenever we have a greedy strategy, we have to argue that this short sighted strategy of doing whatever looks good next, is better than having to wait and do something later on. So, very often, this is something that happens. So, you are, looking for some to buy something and you go to a shop, and in that particular shop, there are 5 things on display which are of interest to you, and one of them is the best. Now, if you had a greedy strategy, you would buy that. But if you were not greedy, then what you would say is, there are 3 other shops which are selling similar things, maybe they have different products, which might be more interesting. So, you would say, well in this shop, this is the best, but I am not going to immediately go for this. I am going to go to the other shop and check and so on. So, then your strategy is less greedy, you are not looking at the very first option that comes your way. Whereas here in Dijkstra algorithm we are doing that. So, whenever you have greedy strategy, you have to argue that it is correct. So, each new shortest path extends an earlier one. And by induction, we have found paths to all the vertices already burned. So, this is now the strategy that we have at some given point. So, initially, we burned 0.'},\n",
              " {'id': 'e0bdf94d-2272-40a8-bcc9-4d388bd92429',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, that is clearly the shortest path. And now when we burn one more vertex what we are inductively assuming is that we have found the shortest path. This is our justification for the greedy strategy that every time when we choose the minimum expected time to burn, it is actually the minimum time among all the vertices which is going to burn. So, at any given point we have burned everything and whatever we have burned has got the correct answer. Now, we have to check why the extension is going to continue to give us the correct answer. So, supposing we choose to burn a vertex v which is connected to so, now it must be connected the fire has to reach v. So, let us assume the fire is going to reach v from x. What we have to argue is that the fire that reaches v from x because right now we believe is the next one is not going to be wrong, because later on there is a fire from some other vertex which comes indirectly, which should have arrived earlier. So, at the time when we decide to burn v, we should rule out the possibility that we can get this kind of a fire which is coming from see it cannot come directly from y to v because if it was y to v, I would have chosen y instead of x to update the time of y. So, there must be some indirect path, which I do not know of now, which causes v to burn faster than if I burn it from x. And we have to say that this cannot happen. Well, so now, y can this not happen? Well, first of all, let me say that this is some m, and this is some n. So, the fact that I chose to burn v before w means that m must be smaller than n. It cannot be bigger than n. So, n is at least as big as m. And now I have to come and do something here. So, this has to be at least 0. So, I have n plus some q. So, I am comparing m to n plus q, the two steps on the other direction. And q is, n is at least as big as m and q cannot be negative. So, q cannot reduce m.'},\n",
              " {'id': '330e5727-b617-4c7c-af4f-b88f773d4e9e',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, this basically tells us that this going around these two things cannot be shorter than coming this way. Now, if q could be negative, if I have this and if q is negative, then I have a problem. So, supposing this is 5, and this is 7, but this is minus 3. Then by choosing 5 right now, it looks good. But actually, if I chose the 7, then it would allow me to come back and finish this move in 4, 7 minus 3. So, this is why we had assumed initially that there are no negative edges. So, it is not a question of negative cycles, no negative edges at all. Only if we have no negative edges, can we guarantee that this particular greedy rule of Dijkstra to pick the shortest vertex shortest time to burn available to be right now, as the next vertex to burn is correct. So, we cannot use the extra algorithm with negative edge weights because this argument will not work anymore. (Refer Slide Time: 12:52) So, let us look at an implementation of this. So, we need to maintain two things we need to maintain this burning. So, burning is the same as visiting. So, we need to make keep track of which vertices have been burned or visited. And we need to keep track of when a vertex is going to burn which is basically the distance. So, the distance or time is the cost. So, we want to keep track of the visited status true or false and the distance. So, initially, we said that everything is going to be assumed to burn at infinity is not going to burn at all. So, what is infinity? So, we need a value for infinity because we cannot put infinity into our Python code. So, one way of thinking of infinity is that something which is larger than any part that you could find. So, we have a weighted graph. So, there are some weights in our particular weighted graph matrix. So, supposing I take the largest weight amongst these. Now, the longest sequence of edges that connects two vertices in the path has at most n minus 1 edges.'},\n",
              " {'id': '826cfaf7-2652-4116-9485-8af6c5bcc431',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, if I take the longest weight largest weight and multiply it by the number of vertices, then this must be bigger than the length of any path which is actually there in the graph. So, this is what we are doing here. So, initially, we are finding out so, this s is our source matrix and W mat is the weighted adjacency matrix that we are working with. So, now, we take this weighted adjacency matrix and we use the as usual the shape attribute to find out how many rows and columns that are, remember now, this is going to be we are going to assume this is actually a two dimensional matrix. So, the zeroth entry, so, this x here, x is 0 and x is 1. So, this will be the edge information and this will be the weight information. So, ij 0 will be whether there is edge from i to j, ij 1 will be what is the weight of the edge from i to j and ij 1 will be nonzero only if there is an edge. So, if there is no edge, there will be no weight. So, we get the shape so, we get the rows and columns. So, now from this we make a crude estimate of infinity, we say take the maximum value that appears anywhere in this weighted matrix, multiplied by the number of rows, this is the length of the longest path you can get by having the longest largest weight repeated again and again. And then just for good measure rows plus 1. So, this is a very conservative estimate, but it is certainly larger than any real path distance that you will find. That is all we want. Infinity should be such that if I compare it to a real path, infinity must be bigger. So, now I have set my, so I have this visited dictionary and the distance dictionary. And for every vertex, initially, I said visited to false and I said distance to infinity. So, this is my initialization. That is what I have done so far. Now I am begin now I am ready to get started. So, the way I get started is by saying, let me set the distance of the source vertex to be 0. But I have not yet visited it, because I am going to put that into this loop.'},\n",
              " {'id': '3701e418-4824-4a37-b997-eea356b153b7',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, what I will typically do is I will look for the smallest distance vertex which is not visited, that is what I will look for. That is my strategy I look for the expected burn time of this unburned vertices, which is smallest. So, in our current terminology, there is the distance to vertices among the unvisited vertices that the smallest. So, at the moment, everything was infinity, and I said distance s to 0, but I did not set it to be visited. So, the first vertex I am going to explore is in fact 0, but formally, I will do it in a loop. (Refer Slide Time: 16:31) So, what I will do is, I will first find out, what are all the minimum distances that are there. So, I will take this min function, I will compute the distance for every vertex in my graph, which is not visited, and I will take the minimum of these now there could be multiple such vertices at the beginning, there will be only one but in given point there could be multiple such vertices. So, I will say that the next distance that I am going to explode, next, D is the next distance is the minimum distance among the unburned vertices. Now, I have to find the vertex for which that is the minimum distance. So, that is my next thing I will say. The next vertex list is a list of all vertices which are not visited and which have this actual distance. So, there is a two passes on the list. So, you can do it in an explicit loop, but I felt that this was the fastest way to do it. So, you first scan it once, to find what is the minimum distance among the unvisited vertices, then you scan it a second time to find out which are all the unvisited vertices, which have that distance, and that is my next v list. Now, it is conceivable that there are some vertices which are never going to be visited. So, basically, if I do not have any vertices in this list, I will just get out and I will not continue. So, this is just a kind of to handle a boundary condition.'},\n",
              " {'id': '3962d11b-5175-4215-9ebb-444289564906',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, now, the real situation is going to happen when I find these things. So, I need to now choose one. So, I choose the smallest vertex among the minimum vertex list to be the one to burn next. So, these are all vertices, which have the minimum distance and are unvisited. So, I pick one of them and I make them visited, and I will call that next week. So, I how do I visited, I will first set its value to be true. And then I have to take all its outgoing. So, I have next v here, which I am going to and then I look at all its outgoing edges. And I will update for all these vertices, the time that they burn, which is their distance, to be the minimum of their known time, I already know they are supposed to burn at some point, or the time I will get where burn it through next week. So, I take the minimum of the existing distance of v, and the distance from the new vertex next v, plus the edge weight. So, the edge weight remember is ij comma 1. So, this is just saying that for each vertex, update, its distance to be either the current distance or the distance from the vertex I just burned, plus 1. And now I kind of come back and do this again. So, this is my implementation. So, I said the distance s to 0. And I repeat until all the reachable vertices are visited, find the next vertex v to visit, set it to true and recompute the distance for every neighbor. So, this is with an adjacency matrix. And as you might guess, this is not likely to be ideal in terms of computational complexity. (Refer Slide Time: 19:23) So, this itself requires n squared time, because I have to go through this entire weighted matrix, find the maximum value, that is an n squared size matrix, and then add something to it. Now, this main loop, runs n times because in every iteration of the loop, I find one more shortest distance. So, I keep a numerating these vertices one by one. So, I need order n time to find the next vertex to visit and I need order n time.'},\n",
              " {'id': 'a69c61f2-697d-4ea1-849a-6a426ebf9d36',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'So, that is this part, I need to scan through the entire thing to find the minimum distance and then scan through it again to find which vertex has a minimum distance. And then I need order n time, because I have an adjacency matrix, I need order n time to update these distances, I have got to go through all the columns in a matrix. And wherever a vertex is not visited, I have to check its distance and update it. And regardless of the degree of this vertex, I have to go through all the columns. So, I have an order n outside and I have these two order n steps inside. So, this is going to be order n square. So, the usual thing is what happens if we move to an adjacency list. (Refer Slide Time: 20:31) So, here is the same thing with an adjacency list. So, now I am passing a weighted adjacency list, which I will call w list. So, again, I will compute infinity in a similar way, I mean, so, basically I take the length, this is the number of vertices, the length of the number of keys of my weighted adjacency list, and multiplied by the maximum value of all the weights in my adjacency list. So, I every entry in my adjacency list has a is of the form v comma d. So, u to v with distance d, So, I take the maximum among these d and multiplied by the number of vertices add 1. So, this is the same infinity as the earlier 1 except that I am doing this in order m time. Why am I doing in order m time because this process of finding the maximum across all the adjacency lists is proportional to the number of all the edges in my adjacency list. So, it is not order n squared, but order n. So, that is a good piece, I have done one thing, which was order n squared before I have replaced by something which is order m. So, that is good. Now the rest is similar. I kind of initialize the visited and distance to empty then I update these things. And then I start with distance s 0. So, so far, so good. Now, again, I have this outer loop, which is order n.'},\n",
              " {'id': 'e71c8e7b-2db2-4a86-af03-12a2da92890a',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'In this outer loop, we had two nested things which are problematic 1 is solved because now if I want to update the degrees, this is order m. Because once I get a vertex, and I want to update all its neighbors, I only have to go proportional to the degree of that vertex, which amortized as usual across all of them is going to be the sum of the degrees which will be proportional to the number of edges. But the problem is that this remains the problem of identifying the minimum vertex remains I need to find the minimum vertex, which is there. And therefore, I need to spend order n time to scan through all the vertices which are unvisited and find the minimum. Because these are in no particular order. So, if we do not have a better way of doing that, even with an adjacency list, the second loop is going to take order n squared time. So, we will see later that there is actually this is therefore a useful thing to be able to do to take a collection of values, and to be able to extract the minimum. If it is sorted, of course, I can extract the minimum. But if it is not sorted, we can still do something clever and extract the minimum fast and that is what we will need to do. But till we have that, we are stuck with this problem that Dijkstra’s algorithm takes order n squared time whether you use an adjacency matrix or a list. (Refer Slide Time: 23:09) So, Dijkstra’s algorithm basically computes single source shortest paths, when we do not have negative edges. It uses a greedy strategy to identify which vertex to visit next mainly picks up the vertex with the expected burning time which is minimum among those who are unburned or unvisited vertex with the smallest distance. And whenever we use such a greedy strategy, where we are ignoring everything and we are not looking multiple steps ahead, but choosing the optimum one right now, we have to justify why it is true.'},\n",
              " {'id': 'b4313c64-f752-48b4-a1f1-59b5a1adc4ea',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths (Dijkstra_s algorithm).pdf',\n",
              "  'content': 'And we prove that if the edges indeed have non negative weights, then you can argue that you will not find the later part which will become shorter than the path that you have just found. So, in that sense, Dijkstra’s algorithm is correct, without if you assume that there are no negative edge weights. But one of the bottlenecks is that because of this problem of having to identify the next vertex as the one with the minimum distance among the unvisited ones, that takes order n time inside an order and loop. So, we have this complexity bottleneck of order n squared whether we use adjacency matrix or we use an adjacency list.'},\n",
              " {'id': '8d168676-d18c-44e6-9c8c-04967eefcf91',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': \"Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Single Source Shortest Paths with Negative Weights So, we are looking at shortest path problems on weighted graphs. And we started with the single source shortest path problem. And we looked at Dijkstra's algorithm, which works whenever there are no negative edge weights. So, what happens if you do have negative edge weights? So, let us look at the single source shortest path problem where there are negative weights. (Refer Slide Time: 00:29) So, let us revisit the proof of Dijkstra's algorithm and see what the problem is. So, we had this burning pipeline. And what we said was that we will keep track of the burnt vertices, and how long it is going to take for the remaining vertices to burnt the expected burn time. So, initially, no vertex is burnt. And initially, all vertices have an initial burn time of infinity, and then you burn one vertex, so the source vertex now acquires a burnt time of zero. And after this, what we do is we pick the unburnt vertex, which has the smallest burnt time or the smallest distance among the unvisited vertices, we burn it, we make it status change from unburnt to burnt or from unvisited to visited. And then we update, we update the time or the distance to all the neighbours. For every j, every k, which is a neighbours of the vertex j that we just chose, we update, its burning time to be the minimum of its already known burning time, or the burning time of j plus the distance from j to k. (Refer Slide Time: 1:34) So, for this to work, we had this proof, which said that if I extend my thing in this direction, then coming from here, cannot produce a shorter path. And this required all my edge weights to be non negative, because we said, for instance, that if we had negative weight, like 5, 7 and minus 3, then we could, in fact, discovered later on through w a shorter path in terms of the costs to v, then we can discover right now.\"},\n",
              " {'id': '4fe84307-17a3-4bef-b607-12076774d7c1',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'And so we could not apply this anymore. And one of the reasons that this happens is because we are kind of freezing this choice. So, we are saying this is the shortest path, we have discovered to v, and we have frozen it once we move it into this red part, burnt vertices are the visited vertices, we are never going to update its thing again. (Refer Slide Time: 2:24) So, this is a kind of one shot operation, I discovered something is the current shortest if I make a decision to burn it, or to visit it, and I will never reconsider that distance again. So, even if I am presented with different information, I will not reconsider it. So, if we have non-negative edge weights, I am guaranteed that I am not going to get contradictory information later on. So, it is fine. So, the obvious solution is to say, well, maybe if I find the later path, which is better, maybe I allow myself to change it. And the problem with this strategies How many times will I do this? I mean, what will happen if I keep setting it, changing it setting it changing it? How do I guarantee that this does not keep going on forever? So, we are going to allow negative edge weights because that is the only scenario in which the Dijkstra’s algorithm will no longer work in the way that we said before. But remember that we do not allow negative cycles. So, if you do not allow negative cycles, then we cannot have arbitrarily small path links. So, if you go around the cycle, we cannot reduce the overall length of the path. So, this means that there is no advantage to going around the cycle. And if I go around the cycle, either I incur zero cost which is fine, or incur some positive costs. So if I go around it, I might as well not go around.'},\n",
              " {'id': 'f165e85d-5f92-40b8-83de-af131d6c9c72',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'So, with this justification, we can easily argue that if I do not have negative cycles, then the shortest way to go from anywhere to anywhere must be a path in the sense that we discussed before a path is a sequence of edges which are connected to each other which do not repeat a vertex. A cycle is a situation where you will repeat a vertex but going around the cycle has no advantage. So, either it has zero disadvantage, in which case you can still drop it because it only has cost zero. Or you could have a real disadvantage because it has a positive weight, but it can never have a negative weight and give you a bonus. So, with this in mind. Now we have some idea that among all the so remember the single source pathway. So, I am starting from S and I am trying to go to some t and I might have many different ways of going from s to t. And I want the minimum among all this, but what I am guaranteed is that the shortest route from s to t is going to be a path. So, it cannot pass through more than n minus 1 edge, because if it passes through n edges, then it will have to repeat a vertex and then it will no longer be a path. So, there is an upper bound in terms of number of edges which are seen going from s to t and this will help us to derive an algorithm for the case where we have negative edge weights. (Refer Slide Time: 05:06)  So, supposing 0 is our starting vertex and we want to get to vertex k and the shortest path follows these l edges. So, I have w1 to j1 then w2 to j2 and so on. And finally, wl minus 1 to j l minus 1 and wl to k. So, I follow l edges. So, this as we discussed need not be the shortest in terms of number of edges, but the shortest in terms of number of weight if I add up w1 plus w2 to wl I can find no shorter path. So, the first observation is that each of the paths before it must be themselves shorter. So, supposing for instance, I could find a better path from 0 to j2.'},\n",
              " {'id': 'e271bd4a-7c80-4ac9-98b2-59afc945b934',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'So, supposing there is something which with weight w prime, which is less than w1 plus w2. Well, then I know that from j2 I can get to k. So, I could choose the shorter path w prime and get a shorter path from 0 to k which contradicts my assumption that 0 to k as stated is the shortest path. So, therefore, if this is the shortest path, then 0 to j l minus 1 is also shortest path, 0 to j l minus 2 is also shortest path and so on. So, every one of the paths on the way to this k is also a shortest path up to that intermediate vertex. So, that means that if I can find shortest paths of length l minus 1 and if I know that k has a shortest path of length l, then for every vertex if I know the l minus 1 path or shortest length to that vertex. Then among those, I will get the shortest path of length l to k. So, this is the idea now that we will keep updating the shortest path, and each time we update the shortest path, we would have discovered shortest paths up to a longer length. So, initially when we update shortest paths, we will only know shortest paths of length 1 path length to 1. So, that is if you remember our original picture. So, we had this 0 1 2. And we had these weights, 10, 6 and 80. So, if I look at path length 1 starting from here, then I have this edge and this edge. So, this gives me this estimate of 80, and 10. Now, if I look at path links 2. So, this is 80, and this is 10. If I look at path links 2 then this 80 becomes 16. So, the fact that there is a shortest path of length 2 to vertex to gets discovered after I find the shortest paths of length 1. So, in general, if I have a shortest path of length l, this will be discovered after I find the shortest paths of length l minus 1. And that is what we are going to do by iteratively updating. So, we are not going to freeze the update, but we are going to keep up every time we make a change, we will update all the neighbours of that changed vertex.'},\n",
              " {'id': 'b95d8223-105d-485a-aadc-4abecbd5120c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'So after l updates, the claim is that all the shortest paths using at most l edges would have reached their optimum. And since, as we discussed before, the shortest path to any vertex must be a path and it can have at most n minus 1 edge. If we do this n minus 1 times, we would have stabilized all the paths. (Refer Slide Time: 08:28) So, this actually turns out to be a much simpler algorithm, then the Dijkstra’s algorithm in some sense to implement you, you first set the source vertex to be let us assume the source vertex is 0. So, initially, you assume the distance to 0 is 0, and for everything else is infinity. And now, every time you find a minimum vertex. So, for each edge, for every vertex, you know its estimate, you look at its outgoing edges, and you reestablish the minimum for the outgoing thing. So, supposing I am currently at j, and I have some outgoing things, and I have some w1, w2, w3 as my estimates there. And I know my w here. So, I have d1, d2, and d3. So, at every point, I will compare w1 with w plus d1. I will compare w2 with w plus d2. So, I will update everything, and I will update all its neighbors. And I will keep doing this again and again. So, that is the algorithm. So, this works for both directed and undirected graphs. So, here is the algorithm we will come back and look at it a little more carefully after we do kind of run through of it. So, what we do is we take an adjacent c matrix and a source vertex. As before we figure out how many rows we have, and we set this infinity in terms of the number of rows and the maximum value in the matrix. And now we have this distance dictionary, which will give us a distance everything. So, initialize all the distances to infinity, and then set the distance of the source vertex to 0. And now we are going to do something n times, n minus 1 time suffices. But we will do n times. So, n times, what do we do for every vertex for every v.'},\n",
              " {'id': '6f07e3b6-e0c8-483d-a1c1-773faf429fe1',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': \"So, for every u for every v, if u v is an edge, then we update the distance of v to be the minimum of the existing distance, plus the distance of the starting point of the edge and the weight of the edge. So, we say d of v, is going to be min of the existing distance plus or d u plus the weight of this edge. And we just keep doing this blindly. And an analysis says if there are no negative cycles, and this is going to settle down. So, this is, as you can see, in terms of code, this is just a bunch of nested loops. And it is significantly simpler than Dijkstra's algorithm to implement. (Refer Slide Time: 11:14) So, here is an example of a graph which has negative weights. So, you have these minus 4 and minus 1 and so on. But there are no negative cycles. So, for instance, if you go around this, there is a cycle. But the along this path, the cycle, you have minus 2 plus1, plus 2, so the total of this cycle is plus 1. So, in the same way, you can look at other cycles. So, there is a cycle here, for instance, 6 to 1, 1 to 5, no, that is not the cycle, sorry. So, you have a cycle here, for instance, 5 to 2, 2 to 3, 3 to 4, so here this minus 1, minus 2 plus 1 plus 3 minus 1. So, this again, has a net of plus 1 because it is minus 3 plus 4. So, in this graph, we have negative edges, but no negative cycles. And in principle, if I take 0 as my canonical starting vertex, this algorithm that we just discovered, called the Bellman Ford algorithm, after two gentlemen, Bellman and Ford. This algorithm should produce the shortest paths from 0 to every other vertex. (Refer Slide Time: 12:24) So, the way it works, as we will keep this stage wasting. So, initially, we want at the 0 at stage, the distance to 0 is 0 and the distance, so everything else is infinity. That is our initialization. Now, what I do is I take everything that is known, and I update things which are unknown. So, what do I know? 0. So, if I know that 0 is 0, then this must be 8.\"},\n",
              " {'id': '0f9c37d1-2b6f-4e97-b2ae-8e65188dd2a0',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'And if I know 0 is 0, this must be 10. For everything else, my current estimate is infinity. So, infinity plus whatever is going to be infinity. So, I get a new distance for 7 and 1. So, vertex 1 is going to be 10. And vertex 7 is going to be 8. So, this is what I get. So, I get new entries here, 7 and 8. Now from this, I can get the distance to 6. So, now I know something about 7, I know something about 0, I know something about 1. So, now I can update to 6, for example, 7. I can update from 1 to 5. And so I get two new entries for 6 and 5. So, I keep doing this. So, now that I know all these things, I can update once more. And I will get now information about 2, I get information number 2, because I know 5. So I know from 5 to 2. So, 5 was at distance 12 before and 12 minus 2 is going to be 2. So, I keep doing this. And I do this blindly. And after the seventh iteration of this, I stabilize. And this is going to be the shortest path from 0 to everything else. So, this is the Bellman Ford algorithm. (Refer Slide Time: 14:02) Now, if there was a negative cycle, then what would happen is that if I computed one more column of this, then I would find a longer path which has gone around the cycle, and then something will reduce one more time. So, this Bellman Ford algorithm will work correctly if there are no negative cycles, but if there are negative edge weights, but also if I run it one more time, then it should stabilize and it has not stabilized. So, I had run it an eighth time for seven, rather a ninth time for these eight vertices. And if it does not stabilize, that means actually there is something wrong. So, I can also detect negative cycles using Bellman Ford. So, I just check up the n eth update will reduce the distance to any vertex. If it does, then there is some negative cycle. (Refer Slide Time: 14:48)   So, here is the adjacency matrix version of it that we saw before. So, this is going to take now n squared time. And then this loop runs n times.'},\n",
              " {'id': 'eeeb2dfe-d25d-4e6d-8ea0-8848fb22ad3a',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': \"And inside this loop, we have this, which is another n squared time. And so this whole thing is going to take, it should be an n cube I think. We should take n cube. So, if we take adjacency lists instead, again, this is an order m algorithm. And now, this particular update inside, which is earlier taking n squared time. So, remember this is n cubed, this earlier thing, which is taking n squared time is now going to take order m time. I am going to take and update the distance across all the edges in my graph. I am not going to look at anything except an edge. So, across all the vertices, I am going to look at all the neighbors and that is going to be the sum of all the edges in my graph. So, I will do that update, and that will be ordering. So, therefore, I have an outer loop, which is n. And inside that outer loop of n, I am doing order m operations. So, I am going to do m times n. So, remember that m is at most n squared. So, m times n could be n cubed. But if m is small, for example, as we said, If m is linear in the number of vertices, then m times n could be an n squared. So, it could be one order of magnitude smaller than n cube. So, again, adjacency lists give us a much better complexity, then adjacency matrices for Bellman Ford. (Refer Slide Time: 16:32) So, Bellman Ford allows us to do single source shortest path, even in the presence of negative weights. So, the point of Dijkstra's algorithm was that because of this assumption about non negative weights, you could do this greedy update. Whenever you see the minimum distance to an unburnt vertex, or unvisited vertex, you could freeze that and say, forever, this is the minimum distance. But if you have negative edges, you could later discover a shorter path to that vertex, and you have no way to update it in Dijkstra's algorithm. So, what Bellman Ford says is, well, you just go ahead and do it, we just every time you see an update to a vertex, you update all its neighbors if they can be updated.\"},\n",
              " {'id': '6638c4ad-d237-43d8-a70b-7727ec6976c9',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 5},\n",
              "  'source': 'Single Source Shortest Paths with Negative Weights (Bellman-Ford Algorithm).pdf',\n",
              "  'content': 'And you are guaranteed if there are no negative cycles that this process of updating will stabilize after n minus 1 rounds. So, after k rounds, it will stabilize all parts of length k. So, after n minus 1 rounds, you will have all paths of length n minus 1, which will cover all the shortest paths. So, by repeatedly doing this blind update, in some sense, you uncover all shortest paths in the presence of negative edges. So, this takes n cube time if you use an adjacency matrix, but if you use an adjacency list is m times n, which in the worst case could be n cubed, but it could be much better if m is small. And we can also exploit the fact that Bellman Ford should converge. But if it does not converge, it tells us something interesting, which is that the distances continued to reduce and therefore my assumption that there were no negative cycles is false. There are negative cycles in the graph. So, by running Bellman Ford one more time, and checking if things change again, they can only change by going down of course, if they go down, then it means that you have a negative cycle in your graph and therefore this was not a valid graph to compute shortest paths at all.'},\n",
              " {'id': 'aeaa903c-2a31-4f51-9f28-c9e87ea84f3a',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Shortest Paths in Weighted Graphs.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor. Madhavan Mukund Shortest Path in Weighted Graphs So, far we have been looking at graphs, which consists of nodes, and edges. Now we will look at weighted graphs where there are also some values or costs attached to the edges. (Refer Slide Time: 0:22) So, remember that BFS has the property that it will find out level by level the shortest path from an initial node to every other vertex in the graph. But if we have some costs associated with the edges, then this shortest path in terms of the number of edges is not necessarily the shortest path that we are interested in. So, in a weighted graph, we assign costs or weights to each of the edges. So, this is expressed as this function which we can sometimes write as W, which takes every edge and assigns a real number to it. So, this could be any number, it could even be negative, we will see what it means to have a negative cost in the graph. But here is an example of a graph on the top right, which has some numbers. So, we have a cost of 80 for going to 0 to 2, we have a cost of 10 for going 0 to 1 and so on. (Refer Slide Time: 1:16) So, now, if you take a weighted graph like this, we have to now represent it. So, it is no longer enough to say whether there is an edge or not, we also have to say what is the weight of the edge? Now, hypothetically, you could have an edge with weight 0, so it is best to keep both quantities. So, if we have an adjacency matrix, then in each cell of the matrix, we keep two quantities, we keep a pair with the first item, and the pair is the edge information. So, this tells me there is no edge here. And of course, if there is no edge, the weight must be 0. But here it tells me there is an edge, and the weight of that edge is 10. So, if I look at 6 to 5, that this edge, then it tells me that the weight of the edge from 6 to 5 is 10. So, this is one way of representing a weighted graph as a kind of weighted adjacency matrix.'},\n",
              " {'id': 'd261c160-4218-403f-812d-1c6b0452a551',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Shortest Paths in Weighted Graphs.pdf',\n",
              "  'content': 'So, in each cell of the matrix, I have a pair of numbers, or you can think of it as a three dimensional matrix if you want. So, each i comma j has coordinates 0, which is the edge information and coordinate 1, which is the weight information. And we know that if there is no edge, there is no weight. So, if there is no edge, the weight must be 0, if there is an edge, the weight may still be 0. So, just looking at the weight, I cannot tell if it is 0 or not. So, you need to know both the edge and the weight. And if I want to have it in an adjacency list, then with each vertex, I will keep track of not just the outgoing vertices is connected to but the weight of each of these edges. So, this says that for instance, 1 is connected to 0, with weight 10 is connected 2 with weight 6, and is connected to 4 with weight 20. So, the normal adjacency list would just have been 0 to 4. So, instead of just 0 to 4, I keep 0 comma 10, 2 comma 6, 4 comma 20 to keep track of the weights. So, this is how we represent a weighted graph. (Refer Slide Time: 3:00) The first problem that we will look at in the context of weighted graphs is the shortest path problem. So, as we said before, BFS computes the shortest path in number of edges. But this is not necessarily the shortest path in a weighted sense. So, if we look at a weighted graph, we want to add up the costs as we go. So, these costs could represent different things, it could represent time, it could represent distance, and so on. So, as I follow an edge, I incur a cost. And if I follow two edges, I incur the sum of the costs. So, I add up all the weights along the path. So, here if we look at this example, the shortest path from 0 to 2 is actually of weight 16 via 1. So, it is a two stop path. Whereas if I take the shortest path in terms of number of edges, I can go directly from 0 to 2, but it will cost me 80.'},\n",
              " {'id': '222398be-558f-46d6-bff1-79696237a3ae',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Shortest Paths in Weighted Graphs.pdf',\n",
              "  'content': \"So, you can imagine if you bought flight tickets, sometimes this happens, a direct flight is more expensive than the flight which hops because it is more inconvenient for the passenger and possibly goes through less serviced destinations. So, this is not something which is unusual the something that we have we do encounter in real life. So, we can just say that there is no direct way of correlating the number of edges on the path with the total weight of the path. So, we need a separate strategy in order to find shortest paths in weighted graphs. (Refer Slide Time: 4:22) So, there are multiple problems that we will look at. So, the first problem is one where we are starting from a fixed vertex, which is called the source and we are looking at the shortest path to every other vertex in the graph. So, this is called a single source shortest path problem. So, this could be of interest. For instance, if you have a centralized facility, like a warehouse or a factory, from where you have to ship out things. So, what happens, for instance, when you get something by courier, is that it arrives on a flight from a city and it all gets dumped in some central office, inside that city. Then from there the courier company has to decide for each of the parcels how to send it out to your home. So, this now amounts to computing the shortest path from that career facility to everybody's house. So, this is the single source shortest path problem. Another problem is to find the shortest path between all pairs of vertices. So, for instance, if you are running a travel website, then at any given point, a customer might ask to go from any city on your website to any other city, and you must be able to find out in terms of different quantities time, price, whatever, the best way. So, you need to quickly compute for every pair of cities, you need to be able to answer this question. So, this is an all pairs shortest path problem.\"},\n",
              " {'id': '0ea9f73f-b40d-4dc3-a4b4-4206df272e97',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Shortest Paths in Weighted Graphs.pdf',\n",
              "  'content': '(Refer Slide Time: 5:44) So, we mentioned that the weights can be arbitrary real numbers, so you can try to imagine what it would mean to have a negative weight or a negative cost. I suppose you have, you are running a taxi, so you are a driver of Ola or an Uber. And now, towards the end of your shift, you would ideally like to finish your run close to home, so that you have less time to go before you reach your destination. So, at this point, if you travel without a passenger, you are incurring a cost it because you are spending fuel and you are not gaining any income. So, a road, which takes you towards your home, but along which you are not likely to find any passengers, because it is a very quiet road, is likely to have one with positive cost. Whereas, if you follow a road, which may be more long winded in terms of the time it takes you to go home, but it passes through a busy area where there are likely to be customers, then this you could incur as think of as a negative cost, because you will actually earn money going there. So, now if you want to find a route that minimizes the cost, you might want to choose a combination of these busy roads and these quiet roads so that you kind of optimize on the distance and the and the money that you earn. Now, when you have negative edges in the graph, one of the dangers is that you could have a situation like this where you, for example, have a cycle where you have some minus 5, 6, minus 2 and 2, so minus 5 minus 4. So, if I look at this cycle, it has two negative edges minus 5 minus 4, and it has two positive edges 2 and 6. But if I go around the cycle, and come back to the same point, then I have plus 8 minus 9, so my net gain is minus 1. So, by going around the cycle, my cost reduces. So, if I go around it one more time, my cost reduces again.'},\n",
              " {'id': '1aa946b6-b598-4d1b-9d04-2efcf0d85ce6',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Shortest Paths in Weighted Graphs.pdf',\n",
              "  'content': 'So, now if I have a path, which comes from somewhere and go somewhere else, if I look at if this is my source, and this is my target, and I am asking you the shortest path from s to t. Then this quantity is not defined, because I could go around the cycle as many times as I wanted to reduce the cost as much as I want. So, if I have negative cycles, then this notion of shortest path does not exist. It does not make sense. But if I have negative edges and no negative cycles, we could still think of shortest paths and try to solve. (Refer Slide Time: 8:07) So, to summarize, in a weighted graph, each edge has a cost. And we can now represent this in an adjacency matrix or an adjacency list by explicitly representing the cost of every edge. Now, when we follow a path in such a weighted graph, we add up the costs. So, the length of a path is not the length of the number of edges as we were doing for BFS, but it is the sum of the weights along these edges. And we want to solve the shortest path problem with respect to this notion of path length, which is the number of the cost added up across the different things and there are two different problems that we will look at the single source problem where I want to find the shortest path from a fixed vertex to every other vertex. And the all pair shortest path problem which asked me to find the shortest path from any vertex to any other vertex.'},\n",
              " {'id': 'a1a9dd1a-c49a-484c-9fe4-ce162de2d248',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': \"Programming, Data Structures and Algorithms using Python Professor. Madhavan Mukund Minimum Cost Spanning Trees: Kruskal's Algorithm So, we have seen Prim’s algorithm for the minimum cost spanning tree which starts with a vertex or with a minimum cost edge and grows the tree gradually. The second strategy to build a minimum cost spanning tree is the one due to Kruskal. (Refer Slide Time: 0:23) So, in Kruskal's algorithm what we do is we start with n components, so we think of each vertex as an isolated component and then we try to combine components with the shortest edge available to us. So, we process the edges in ascending order and each time when we come across an edge we include it, in other words it grows the tree by merging two components provided it does not create cycle. So, for instance if we have this graph, so if we sort the edges then we start with the smallest edge which is the edge 1 to 3, so this is the smallest edge because it has weight 6. So, when we add this edge we get a one component, initially we start with 5 components, the four vertices, 0 to 4 when we add this edge it merges the components 1 and 3 and it gives us now 4 components. (Refer Slide Time: 1:16) Now the next smallest edge is this one, so notice that unlike Prim’s algorithm, so Prim’s algorithm would take this tree which we had already started with and try to extend it in some direction so we are not doing that. (Refer Slide Time: 1:28) So, instead what we are doing is we are saying we will build up this tree in disjoint parts, so as and when we can add a small edge we add it. So, when we add 2, 4 we create yet another component but there is no cycle yet, so it is still potentially a tree. So, now we have added 1, 3 and 2, 4, now we add 0, 1 which is also the next smallest edge, so we get to 10. And then finally we add, we try to add 0, 3 because 18 is the next smallest edge but if I add 0, 3 then we get a cycle here because we already added 1, 3 and 0, 1.\"},\n",
              " {'id': 'febe18d2-87c2-4793-8dca-6039adf2ed90',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': \"So, we cannot add this edge so we skip it. (Refer Slide Time: 2:08) So, we go to the next biggest edge, next biggest edge is 20 and then when we add that we are done because now we have connected these 5 vertices with 4 edges, so this is Kruskal's algorithm so the idea is that instead of creating a tree from scratch and growing it as a tree all the time you use the edges in a kind of greedy fashion anytime you can use a small edge you draw it, throw it in, if you cannot use an edge you skip it. (Refer Slide Time: 2:31) So, to start with we order the edges in ascending order because we want to start with the smallest weight edge and grow this tree by combining components. So, we have a set of tree edges that we have currently used and initially this set of tree edges is empty so we do not have a tree to begin with. Now, we scan the edges from the first to the last from 0 to m minus 1 and we try to add it to the tree. So, if we try to add it to the set of tree edges either it will create a new component by combining two components or it will try to connect two parts which are already in the same component. So, if it is going to create a loop we skip it, loop or a cycle otherwise, we add it. So, let us look at a more complicated graph, it is little easier to visualize. So, here are all these edges. Now, we sort them, so when we sort them we find that this is the smallest one because it has weight 5, then this is the next smallest one because it is weight 6, then we have two at, at three weight 10, so we have these three edges all at weight 10, so we put them in some order so say 0, 1, 4, 5 and 4, 6, so we are ordering them by the vertex number. Then we have this 18 weight edge which is 0, 2. And then finally we have this 20 and then the 70. So, this is the order of edges that we are going to process in ascending order of weight. So, initially our tree is empty.\"},\n",
              " {'id': '6b443ca7-1588-4319-9813-825f80bc1096',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, we can always add the smallest edge, so this is what we saw with Prim’s algorithm also that we could always start with the smallest edge or we could start there with any vertex and pick its smallest neighboring edge. So, here we can always start with the smallest edge because to begin with everything is an isolated component, so the first edge will merely merge two vertices into a single component with two vertices. (Refer Slide Time: 4:20) So, after I do this, now I go to the next smallest edge which is this one and notice that this is going to create another component far away which has no problem, so we add that. Now, we look at 0, 1 so that is this one. So, when we add 0, 1 again it does not create any problem so we add it. (Refer Slide Time: 4:38) Now, we look at 4, 5 this is our next one. So, 4, 5 is going to be this edge so again we can add it without a problem. Now, we look at 1, 4 sorry, we look at 4, 6. Now, if you look at 4, 6 the problem is that we already have these two edges so 4, 6 if we add it is going to create a cycle so we have to skip it. So, we skip over 4, 6 doing nothing. (Refer Slide Time: 5:03) And now we look at the next one which is going to be 0, 2 because we move from 10 in the next smallest in the ascending order the next is 18. Again we have a problem because we have already added these two edges, so we cannot add that edge, so we will have to skip 0, 2 as well. So, we have skipped these two edges 4, 6 and 0, 2. (Refer Slide Time: 5:27) Now, we find 1, 4 is our next possible edge and adding that does not create a cycle so we add it. And finally we have only one edge left which is this 2, 3 the largest edge but that is the only edge which connects to 3, so eventually it makes sense it has to be in the spanning tree so we add it and we are done. So, this is how Kruskal’s algorithm works. (Refer Slide Time: 5:44) So, the correctness of Kruskal’s algorithm follows from the same observation that we use to justify Prim’s algorithm.'},\n",
              " {'id': 'de53e0b5-7d8e-46c8-91e7-4609885048a9',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, remember we had this minimum separator lemma, so we said that if you take the set of vertices v and you partition it in any way as u and w. Now, you take any edge which crosses this of minimum cost, so this is the smallest edge which crosses this boundary between u and w, then the minimum separator lemma said that no matter what minimum cost spanning tree you construct this particular edge e must belong to that tree. And the argument that we said was if it was not in the tree, then there must be some other way which connects u to w and we could bypass that and in fact instead make the tree go through this and create a smaller tree, so this was the lemma that we proved in the context of Prim’s algorithm. And now we can just use the same result to claim that Kruskal’s algorithm is correct. (Refer Slide Time: 6:44) Notice that at any point the set of edges that we have create some components and these components are disjoint. So, initially we have all vertices separately so these are the components. Now, whenever I add an edge to my tree some of the components get merged but each component is different from every other component, there is no vertex which belongs to two components. So, now when we merge these two things if they are in the same cycle we discard the edge but otherwise we have to combine them. So, what are we doing we are taking two different components so we can think of the two components as a partition, we can ignore the rest, we can put them in one of the two partitions. So, we have essentially, we are trying to add u and v, so u is connected to some things, v is connected to some things but we know that these two things are disjoint because otherwise we would have a cycle when we added this edge. So, we are not getting a cycle so we are allowed to add this edge. And of course, we have some other components sitting around somewhere which are not connected to either of these.'},\n",
              " {'id': '4e8b8de5-c182-4940-85e3-026cc78c33fa',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, what we can do is we can basically think of this as some kind of a partition saying this is my capital U and this is my capital W. And now clearly because all the edges which are yet to be added are bigger than the edge that I have now. So, this is the smallest edge which connects, if it were, if I had seen a smaller edge before I would have added it. So, so far I have not seen any edge which connects capital U to capital W and this is the smallest edge of that. So, therefore by this minimum separator lemma this particular edge must be there in the spanning tree. So, we are using essentially the same formal argument as Prim’s algorithm to justify that every time Kruskal’s algorithm adds an edge to the tree that edge is a valid edge to be added. (Refer Slide Time: 8:33) So, the implementation of Kruskal’s algorithm is reasonably straight forward, so what we have to keep track of are the edges in my tree, so that is this tree edges and we have to now keep track of two things we have to keep track of the vertices and sort the edges in sorted order to process. And we have to keep track of these components, so this is something which is new compared to the other things because we did not have to keep these which group of vertices are connected to each other in any of our earlier algorithms. So, the first thing we do is we sort the edges. So, what we do is we take the edges, the edges are all the form typically u comma v comma distance or weight. So, what we will do is we will represent them instead as d comma u comma v, so we put the weight in the front because we want to sort by the weight. So, we create this list of edges by putting in d comma u comma v for every edge v comma d in the list associated with u. So, we process the adjacency list starting vertex by starting vertex and for every vertex in that list we just throw it in. Now, having done this, we sort this and of course this was just something which I had used to check my code.'},\n",
              " {'id': '5441b890-6817-4f6d-8718-b0cab07ad39c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': \"So, we sort this and now we have the list of vertices in ascending order. Now, we just process them in that order. So, we take this d comma u comma v in this edge list and we check whether the component of u is the same as a component of v. Now, initially we said that every component must be a singleton, so that we have done in this list. So, whenever, while we are processing the adjacency list, we are also assigning to every component, every vertex it is a name, so we are creating n components 0 to n minus 1 by saying component i is i. So, the vertex 0 belongs to component 0, the vertex. So, we are naming these components, so the initial names are 0 to n minus 1. So, each vertex belongs to a component whose name is the same as the vertex. Now, what happens when I merge 2 vertices? So, I have a vertex u and I have a vertex v and these belong to some component, so this component over time has some name, so this is some i and this is some j because they were originally the vertices, the components which belong to vertex i and vertex j and as we merged we have to, so when we merge two components we keep one of them, the name of one of them. So, if component 1 merges with component 7, then we get a new component, now we call that new component either 1 or 7, it does not matter but we do not have to create a new name for it so either we rename all the 1’s which were called 1 as 7 or all the 7's as 1, so that is basically what is happening here. So, what we do is we remember the name of the component that we are at one end point of the edge and for every other vertex which has that same component we change its component to the other point. So, basically everything from here is now going to get called j instead of i, so this is going to extend this component. So, when I take two components and I merge them if this is component i and this is component j either I make everything in i and call it j or I make everything in j and call it i.\"},\n",
              " {'id': 'ba256340-1708-4b6c-a05c-41f17086d0c2',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, what I am saying here is if the edge is going from i to j I am going to convert everything in i to j so that is what this loop is doing. So, it is saying that let c be the component of the starting vertex and for every other vertex which has the same component name change it to be that of the target vertex. The reason that we have to save this name is because once we process u that name will change, so that if there are vertices after you which have the same component as u then I will not know what their component should be. So, that is why I keep a copy this small c to remember the component name of u before I changed it because in the process of changing it u may change before some of the other vertices in that component. So, this is a very straightforward thing now basically we process, see we initially we start sort the edges by weight, we assign the component of each vertex to be itself and we have an empty set of tree edges and whenever we find an edge which spans two components, we add it to the tree and we relabel the components associated with it. (Refer Slide Time: 12:53) Now, technically if you look at this initialization so one thing you will see is that if I have an undirected graph if I have u v d, I will also have v u d, so technically every edge is going to come twice in this list of edges. But remember the first time I add an edge it will merge the components, so if I see it again it will just ignore it so it is not a real problem if I keep the edges in both directions in an undirected graph in the sorted list because the first time it will get processed in whichever direction I pick it up and when it comes in the reverse direction I will just ignore it. (Refer Slide Time: 13:20) So, this is how the algorithm works. So, the main problem with this algorithm is really this loop. The problem is that I need to every time I merge two components I need to scan through all the vertices to decide which one’s components change.'},\n",
              " {'id': 'a5a5c53d-ce49-4fa0-94a7-21d0c3fde5cd',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, we have to merge all these vertices there is no way to avoid this because although we know that u is joining v, we do not know which other vertices are also connected to u. So, we have to go through and explicitly make all of those vertices which are connected to u also connect to v. (Refer Slide Time: 13:52) So, the first step which is sorting the edges we know can be done in m log m time if you use a efficient sorting algorithm. Now, it is a simple thing to observe that because m is at most n squared log m is at most log of n squared but if you know how logs work this is the same as 2 times log of n, so this is big O of log of n. So, log of m and log of n are really the same when I take logs the fact that m is n square does not matter. So, I can also write m log m as m log n it does not really matter and the reason it is useful to write m log n is because later on we will see that some of the complexity depends on n and not m. So, sorting is m log m which we can also think of as m log n if we wish. (Refer Slide Time: 14:44) Now, this outer loop runs m times for every edge in my sorted list I have to do something, but I do something only when the edge is going to contribute to the tree. So, I actually do this part only n times or n minus 1 times because the tree on my n vertices is going to have n minus 1 edges. So, most of the times this if condition is going to say the component of u is equal to the component v and I am going to just skip over to the next edge. The only time I have to do some work on the edge is when it is actually a tree edge and that happens n minus 1 times, but what I have to do in those n minus 1 times is to update all the components potentially which are there, it could be 1, it could be a lot but I have to scan all the vertices and wherever I see a vertex whose component is the one getting merged I have to update it. So, this is an order n scan.'},\n",
              " {'id': 'f7b58612-f7de-461e-84f5-35d3b3a91451',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, n into n minus 1 says inside this loop even though the loop happens m times there is actually an n squared amount of work being done because n minus 1 times I am doing a scan of n vertices. So, this gives us an overall complexity of n squared. Now, n squared is not something that we would like but this is the same problem that we faced it is a different problem in the sense that this is not due to the data structure that we were facing with Prim’s algorithm and with Dijkstra’s algorithm. So, there we were getting an n squared factor because every time we had to scan all the vertices to find the minimum one to be added next, and we did not have a clever way to extract the minimum if the vertices are not in any particular sorted order. Here the problem is different, here our problem is that when we update a component we have to scan all the component names and update all the ones which have the same component as the one that is being merged and we have no clever way of understanding which ones need to be merged now. (Refer Slide Time: 16:37) So, the bottleneck is really in this naive strategy to label and merge components. So, remember that these components actually represent a partition of the vertices. So, a partition of a set is if I take the set and I actually break it up into disjoint subsets such that the sum of all the subsets, the union of all the subsets is the whole set and no two sets in that partition will intersect with user, so we have a collection of disjoint sets. So, what we really need is some way of maintaining this collection and performing some operations on these. (Refer Slide Time: 17:09) So, what are the operations that we need given one element of the whole set, we need to find out which partition it lies in, so this is this find of v, this is what we call component of w here. So, component is basically telling us which is the current component in which I will find this vertex.'},\n",
              " {'id': 'c9078ff6-f442-410e-9750-2a025e63dfd6',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, abstractly it is a function called find, given a vertex it tells me the component name or the set name in the current partition that that vertex belongs to. And the more important thing which is really the one which got us into trouble in terms of complexity is this one. I need to take two vertices, I know so I can use find to check this, this is the same as asking whether find of u is equal to find of v. So, if I have the function find I can check whether or not two vertices belong to the same component by just finding the name of their components. But if they are not the same then I need to merge them so I have to take the union of these two partitions and make it a bigger partition so this is the union operation the usual set theoretic union operation. So, in the set theoretic union operation I take two sets and I combine it into a bigger set containing all the elements and since it was a partition all these elements will come together with no duplicates, so nothing is going to get overwritten or something like that it is going to be two disjoint sets merging and these two will remain disjoint from everything else. So, the fact that I have a partition remains. So, I have this find operation and this union operation and this is what I need to perform efficiently in order to break this complexity block here of doing an order n scan. (Refer Slide Time: 18:41) So, we will see that there is an efficient way to do what is called union find, this is essentially a way of maintaining disjoint sets which form a partition such that the union and the find operation together give us something which is logarithmic rather than order n so across operations. So, basically we want to make sure that the cost per operation is not order n. So, every union for us is technically taking order n time because each time we need to take the union you have to scan all the vertices, we would like it to take logarithmic time.'},\n",
              " {'id': 'ffb6b8ea-4055-4d81-8356-8a5d0815726e',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'And if that is possible, if it takes log n time then if we do this we will get n log n and that is what we really want. So, we will see later how to do this efficient union find. (Refer Slide Time: 19:23) So, you can think of Kruskal’s algorithm as a kind of bottom up algorithm for a maximum, minimum cost spanning tree, you start with all these broken bits and you try to glue them together to form a tree. So, you start with these n components and then every time you can connect something by a small edge you do so making sure that you never create a cycle in the process. The reason that this is correct is that whenever we find an edge that we can add using this bottom up approach we are guaranteed at that point that it is the minimum edge connecting two disjoint partitions of the vertex set. So, from the minimum separator lemma, it will always be part of every minimum cost planning tree, so it is legal to add that edge. Now, this algorithm that we implemented turns out to have cost n squared because of this component merging problem, every time we need to merge two components we said we have to scan through all the vertices and update the component of every vertex because we cannot directly say given a vertex u which are all the other vertices in the same component as u. And we will see later how to come out of this n squared and make it m log n. Now, one thing to remember is that we have now two algorithms so you might ask yourself does Kruskal’s algorithm give the same answer as Prim’s algorithm, if not when do they not give the same.'},\n",
              " {'id': '0c1e19eb-2e45-4f0f-9887-9115707c707a',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, if the edge weights in our graph are all different, then you can prove using this minimum separator lemma among other things that the minimum cost spanning tree is unique because the Kruskal’s algorithm will pick up these things for example if they are all different weights you take them in sorted order, every edge that Kruskal’s algorithm adds has to be there in the tree, there is no edge that Kruskal’s algorithm adds that cannot be there in the tree and there are only n minus 1 edges that you can add. So, there are n minus 1 must add edges according to Kruskal, so there is only one possible tree. And Prim’s algorithm finds the correct tree so it must find the same tree. But if edge weights repeat, then this is no longer true because then it depends on the strategy because in both Prim’s algorithm and in Kruskal’s algorithm there is a step which says extend the tree or in this case combine components using a minimum cost edge. If there are multiple edges which are currently minimum, so in prims algorithm with a multiple ways to grow the tree in different directions but each of them has the same weight or in Kruskal’s algorithm there are multiple components which can be merged using the same weight edge, we have a choice of which one to do and therefore we might actually construct different trees. So, if you want to think of an example here is a very simple example. So, supposing I construct a tree, a triangle which has some equal weight. So, supposing I say the weight is 10, 10, 10; then you can see that any two edges of this will form a minimum cost spanning tree I could find this as a spanning tree I could find this as a spanning tree and I could find this as a spanning tree. So, depending on which I choose as my first minimum edge to add and which I choose as my second minimum image to add I could construct three different spanning trees on this triangular graph.'},\n",
              " {'id': '5814c96b-dda8-41ad-b92e-742a50dbd44a',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 5},\n",
              "  'source': 'Minimum Cost Spanning Trees (Kruskal_s Algorithm).pdf',\n",
              "  'content': 'So, it is not difficult to come up with examples where the choice of the minimum cost spanning tree is not the same across iterations of the algorithm and therefore Kruskal’s algorithm and Prim’s algorithm may find different spanning trees. But if they are all distinct then the minimum separator lemma will tell you there is only one. (Refer Slide Time: 22:45) So, thanks to these different choices there are different spanning trees and in fact there are a very large number of minimum cost spanning trees. So, what we have shown is how to construct one, if the question is how to enumerate all the possible minimum cost spanning trees, then that could be a very large number, so that could be an expensive thing to calculate simply because we have to enumerate a large number of things and there is no way to enumerate a large number of things except to you know list them out one by one. But what we have is this kind of as in Dijkstra’s algorithm a greedy strategy, remember a greedy strategy says here is something that I think looks good the minimum cost edge to add, let me add it and proceed without going back and ever changing my mind. And this greedy strategy helps you to find one good solution among these various different possible solutions that exist.'},\n",
              " {'id': '777bfd1e-83a1-4a31-9867-58b87a574fa0',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': \"Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund All Pairs Shortest Path We have seen two algorithms concerning single source shortest paths where we fix the starting point and we look at all the paths which are from that point to every other vertex, we said the other class of shortest path problems are all pairs. (Refer Slide Time: 00:24) In the single source shortest path, you can imagine that you have a warehouse or a factory from which you are shipping things and you want to find the shortest way to reach every destination from that fixed place. Whereas if you are running this travel service kind of thing, you need to be able to answer questions over every pair of distance, you have no preference for a given source and a given destination. So, your customer will say I want to go from point A to point B, what is the best route, and you have to be able to solve this for any A and any B. So, what we are going to look at now is this all pairs shortest path problem? So, Dijkstra's algorithm works for the single source problem for non-negative edge weights and the Bellman-Ford algorithm, which is more expensive, works with negative edge weights. But, of course, remember that you cannot have negative cycles with negative cycles; shortest paths are not defined, because you can go around the cycle and make paths arbitrarily small. So, one way to solve the shortest path problem, of course, is to fix each vertex as a potential source. I know, if I fix a source, I know how to solve a single source, shortest path problem. So, there are n sources, n possible sources. So, I start from zero and I solve by using the Dijkstra or Bellman Ford, then I started from one and I use it use Dijkstra's Bellman Ford, and I keep doing this. And of course, that will give me a solution.\"},\n",
              " {'id': 'e98dc766-cec5-4086-bd76-0e05d928a7b0',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'What we are trying to find is a more direct way of addressing this all pair shortest path problem, it does not require me to repeatedly rerun Dijkstra or Bellman Ford from every starting point. (Refer Slide Time: 01:52) So, remember, the transitive closure of a relation. So, the transitive closure of a relation consists of repeatedly applying this relation. So, in the case of a graph, the edge remembers captures are relation. There was a motivation for studying graphs or graphs or visual representations of relations, relationships between vertices, whenever vertex 1 is related to vertex 2, there is an edge from v 1 to v 2. So, now, if we represent this in an adjacency matrix, as we said before, then if we multiply the matrices, then we get all pairs which are connected by an intermediate value in between. So, if I have A comma B is in my relation and B comma C is in my relation, then A B and B C would have been one in the original matrix, and AC will now become one in the square of the original adjacency matrix. So, in this way, if I have the kth power of my adjacency matrix, it will tell me all word pairs, which are related by a sequence of k intermediate relationships. And since there are only n overall, I can look for everything connected directly or connected by a pair or connected by three and so on. So, if I add up these things, then I will get any pair of vertices which are connected by a path of any length. And this is what we call the transitive closure A plus. So, an alternative way to do this is to look at how these paths evolve. So, we look at paths through a limited set of vertices. So, we say that B superscript k. So, the superscript indicates that I am going from i to j. So, here I was talking about the length of the path. So, I was saying that, if I look at A to the power k, it is saying that there are k intermediate steps.'},\n",
              " {'id': 'f49f568a-8d13-4a26-8576-1a21d376c416',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'So, I go from A1 to A2, A2 to A3, and so on, I do this k times, and I will get so if I do it one time, I have A if I do two times k squared. If I route k times i v, e to the power k. So, the superscript the power of k in this transitive closure computation captures the length of the path. Here, I am going to capture not the length of the path, but the names of the vertices, which I can see on the way. So, here, if I look at B k of Aj. What it is describing is that I can go from i to j, by using vertices only from 0 to k minus 1. It is not telling me how many of these vertices are used, what is the length of the part, it is just constraining the choice of vertices. And of course, i and j are not constrained by this. So, this tells me from any i to any j, if I am only allowed to pass through 0 to k minus 1. So, i and j need not themselves be between 0 and k minus 1, they could be arbitrary. But I am only allowed to visit these vertices in between. So it is a constraint on where I am allowed to stop on my way from i to j. So, if I say that k is 0 then this set 0 to k minus 1 because k minus 1 becomes now minus 1, that becomes an empty set, or in Python notation an empty list, if I take in Python a list of the form 0 to minus 1, it is going to be an empty list. So, this says that I can go in B 0, if I go from i to j, it means I am not allowed to use any intermediate vertices. That means the only way I can go from i to j is that I can directly follow an edge. So, there must be a direct edge. So, B 0, in other words, is the same as this matrix A. So, B 0 is just the adjacency matrix, which captures all the direct edges. Now, I can calculate B k plus 1 from Bk as follows. In Bk, I already know that I can use the vertices 0 to k minus 1. Now, and B k plus 1, I am additionally allowed to use the vertex k. So, remember that if I use k, then I am allowed to use 0 to k minus 1. So, k itself is not allowed. So, I am allowed to use the vertices from 0 to 1 minus the exponent.'},\n",
              " {'id': 'a54f0d6f-0d76-4798-8571-30b362d4b4c8',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'So, if I am doing B k plus 1, I am allowed to use 0 to k. But in Bk could always use already use 0 to k minus 1. So, there is a possibility is that when I am allowed to use one more vertex, I do not need it. So, I can already reach from i to j without using this new vertex k. That means that B i j to the power k is already 1, or I need this vertex. So, if I need this vertex, I start from i, and I cannot get to j without using k. But if I go from i to k, then remember that these are all going to be shortest paths. If I am going from i to k, I should not need to visit anything in between. And if I go from k to j, I mean, I should not need to visit k again in between and because overall, I am constrained to go between 0 and K. And I have already seen the k then it must be that I go from i to k using only 0 to k minus 1. And I go from k to j using only 0 to k minus 1. So, either I use the fact that B k ij is already 1, or I can go from i to k using 0 to k minus 1 and k to j using 0 to k minus 1. (Refer Slide Time: 07:20)  So, this gives me this way of updating. So, Bk ij is 1. If either B k plus 1 ij is 1, either Bk ij is 1 or Bk ik is 1 and Bk kj is 1. So, this gives me a way of updating these things. So, what is the connection to transitive closure? Well, clearly B n, now tells me that B n of ij is 1 if there is some path from i to j that passes through only the vertices 0 to n minus 1. But only the vertices 0 to n minus 1 is all the vertices. So, it tells me that if I am computing this correctly, then B n i comma j, will tell me that there is some path from i to j. And this is precisely what transitive closure also says that there is some sequence of intermediate vertices in this relation, which takes me from i to j. So, B n is the same as A plus. So, this way of calculating by restricting not the length of the path, but the vertices are allowed to pass through on the path gives me an alternative way of computing, transitive closure.'},\n",
              " {'id': 'af6e2f37-b37b-4325-af96-c78fcf4f5f2e',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': \"And this transitive closure algorithm is called warshall's algorithm. So, notice that it is not the same as the previous algorithm. They are both computing eventually the same thing. But one is doing this matrix multiplication. It is concatenating paths, it is saying there is a path from A to B, and then the path of B to C. So, there will be a path from A to C whereas here, you are doing something more clever, you are saying that I am passing through a new vertex, which I have not seen before. And I am combining those two paths to form a longer path. So, the two paths I am combining might already be long paths, they are not just adding one. So, the transitive closure, adding one edge at a time to an existing path whereas this, these things could be combining to longer paths, just passing through a new vertex. So, what we are going to do is use this transitive closure algorithm to compute all pair shortest paths. (Refer Slide Time: 9:13) And this is called the Floyd Warshall Algorithm. So, the Warshall algorithm is the transitive closure part. And Floyd contribution is to show how to adapt this algorithm to compute all pair shortest paths. So, together, it is called the Floyd Warshall algorithm in most books. So, as we did for this Warshall’s algorithm, let us compute this quantity. So, there we call it B for the transitive closure part of it. But here, let us call it SP for shortest paths. So, shortest path SP to the power k of i comma j is going to be the shortest path from i to j provided I stick to vertices between 0 and k minus 1. So, earlier it said, does there exist a path from i to J, which sets between 0 to k minus 1. Here I am computing a quantity, I am saying this is the length of the shortest path, which sticks within 0 and k minus 1 between i and j. So, as before, if the index is 0, then I am not allowed to pass through any intermediate vertex, because the set 0 to 0 minus 1 is empty. So, SP 0 of ij is just a wait.\"},\n",
              " {'id': 'a61386d2-49b8-447e-a53f-c26b5d682ad0',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'There is a direct edge, there is a weight, there is no direct edge, it is infinity. Now, if I want to now calculate what happens when I go from SP k to SP k plus 1, we have again, the same possibility. I do not need this new vertex, the index k plus 1 allows me this new vertex k. So, I do not need it, in which case, the shortest path using all of 0 to k is the same as the shortest path I already had from 0 to k minus 1. So, the SP k plus 1 is the same as SP k or I do need it, if I do need it, I need to go from i to j and then any i to k and then I need to come from k to j. So, then I combine SP k of i comma k with SP k of k comma j. So, I look at both of these and I decide the minimum of these two to be the answer. So, SP now if I look at the nth iteration of this, then again like intransitive closure, it tells me there is some path which passes through all the, which is allowed to pass through all the vertices 0 to n minus 1. Here, it says, this is the shortest path, which passes through any of the vertices 0 to n minus 1. So, this gives me the shortest path. So, this is the Floyd Warshall algorithm, which is Warshall’s algorithm adapted to the case of shortest paths. (Refer Slide Time: 11:40)  So, this is the same negative weight graph that we use for Bellman Ford, except now we are going to try and compute all pair shortest paths. So, if I look at the 0th matrix, these are all the direct edges. So, for instance, there is an edge 1 5 with week 2. So at 1 5, I have week 2. Similarly, there is an edge 6 1 with minus 4. So, 6 1 is minus 4. So, every direct edge is represented by its weight in this graph, and every nonexistent edge is represented by infinity saying that there is no edge at this point. So, now I want to calculate SP 1. So, SP 1 is all things which go from i to 0 to j, plus i to j. So, either of these two is allowed.'},\n",
              " {'id': 'da9e3308-7e4a-4be7-8032-5b7676e46ce3',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'Now, if you look at 0 in this particular case, it has a peculiar property, that there are no incoming paths to 0 because both the edges from 0 outgoing edges. So, even if you allow a path to go through 0, and no other vertex, remember, SP 1 is going to allow me to go through only 0, no other vertex, if I am only allowed to pass through 0, then I cannot really do anything because 0 has no paths coming into it. So, I cannot get from anywhere to 0, and then go from 0 to somewhere else. So, therefore, SP 1 turns out to be exactly the same SP 0, allowing myself to go through 0 and not allowing myself to go through 0. I have no impact, both of them essentially only allow me to use direct edges. But now when I go from SP 1 to SP 2, now I am allowed to pass through 1. So, I am allowed to do things like go from 6 to 1 and then 1 to 5. So, these are both direct edges. And I am allowed to use this, or I am allowed to go from, say, so I am allowed to say, for example, go from 0 to 1 and 1 to 5 again. So, these things become possible. So, things which pass through one now become possible. So, now if I look at the matrix, it says 0 to 5, which was earlier infinity has now become 12 Why is that because I can now go from 0 and 10 and plus 2 and reach 5 and 12. Similarly, 2 to 5 is become 3, because I can go from 2 and then come to 5. So, there is a, an edge from 2 and so passing through 1 now, I can take anything, which points into 1 and then anything which points away from 1 and I can get an update. So similarly, 6 to 5, I can go minus 4 plus 2. So, I can get a net weight of minus 2 from 6 to 5. So, SP 2 has some interesting new distances. (Refer Slide Time: 14:29) Now, that I have discovered distances in SP 2, I can now look at SP 3. So, SP 3 allows me to go through 0 1 and 2. So, if I go through 2 for instance, it turns out that I can now do things which I could not do before. So, I am look at 5. Earlier, I could not get from 5 to 5.'},\n",
              " {'id': 'be58b955-bfaa-4001-baec-f20d9c0cda93',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'But remember that we said that there are some cycles. So, now if I goes from 5, I can go if I am allowed to go to 2 and 1. I can go 2 to 1 and then come back to 5. So, now I have a cycle from 5 to 5 and there are no negative cycles we assumed. So, therefore, we find that 5 to 5 has length 1 because it is minus 2 plus 1 plus 2. So, now I have all these paths which get constructed. So, for example, now that I am allowed to go through 2, I can find a way to go from 5 to 3, minus 2 plus 1 is minus 1. So, 5 to 3 also is a new entry because 5 points to 2. So, in adding 2, you can check in this particular graph the only incoming edge 2 to is from 5. So, the only new things I will discover are in this row for 5 because these are the only new things that get enabled by going through 2. So, I keep going through like this and eventually, for instance, after I have done pass through 0 to 6. I have come with this matrix. And then if I go from 0 to 6 to 0 to 7, then I stabilize. And this turns out to be my final thing. And then the final step, I get these entries. And if you go back and look at the Bellman Ford example that we did last time on the same graph, you will find that Bellman Ford found exactly the same things as the shortest paths from 0 to all the other vertices which is what you would expect, because among the all pair shortest paths, in each row are the single source shortest path for that row. And notice that some things are not reachable. So, from 5 no matter what we do, we cannot reach 6. And it is not symmetric, because from 6 we can reach 5. But we cannot get back from 5 to 6. So, this is an interesting thing that, it kind of tells us things about the structure of this graph, which may not be very obvious when looking at it. Now, again, this is a very simple thing to implement, like Bellman Ford, because it is just a bunch of nested iterations. (Refer Slide Time: 16:48) So, what we have to do is we have to keep track of the shortest path matrix.'},\n",
              " {'id': '33c9807b-2417-458c-abb6-5aa1be119e84',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'So, what is the shortest path matrix it is the form SP k i comma j. So, this is n cross n. And this one starts at 0 and goes up to n. So, I need n plus 1 versions of this matrix to go through the different stages. So, the shortest path matrix is n cross n for the i comma j vertex pairs, and k n plus 1 for the number of iterations that I go. That is the index k that I am talking about. So, I start off by setting up this matrix. So, first I, as usual, I read the input weighted adjacency matrix, I find out this suitable candidate for infinity. And now I will set up the shortest path matrix to be 0 matrix with n rows n columns and n plus 1 dimensions, in terms of the stages that I am going to construct. So, the first step is to construct the zeroth stage. So, I want to say the zeroth stage. Zeroth stage is initialize everything to infinity. Then having initialize it to infinity, what I do is for every edge in my graph. So, for every ij, for which this adjacency matrix has one entry in the zeroth column, I set that shortest path to be the weight, which is the one entry in that column. So, remember that WMat ij 0 tells me whether there is an edge or not, WMat ij 1 tells me what is the weight of that edge. So, this is the initialization. So, this is setting what we had earlier called SP 0 of i comma j. And now we just do this very trivial iteration. So, from 1 to k, or 1 to n rather. We just go through every pair i comma j. And for every k, we just say what the equation said, which is either we keep the shortest path, remember there it was in terms of k plus 1, and k here is from k and k minus 1. So, it is just slight shift of notation. So, there we talked about SP k plus 1 in terms of SP k. Here, we are talking about SP k in terms of SP k minus 1. So, SP k minus 1 of ij or SP k minus 1 of i k minus 1 plus SP k minus 1 j at level k minus 1.'},\n",
              " {'id': '45cc41cf-e27f-4d8b-9887-6f43d7036036',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'So, this is just the either you have a new path whose weight is shorter going through the new vertex k minus 1, or you have the old path which did not need it at all. And then finally, you want to return the last kind of, so you have these n plus 1 levels at which you computed the shortest path. So, you want to return the last one. And that is the one which is at index n, which is this column number of columns. So, this is what we get. So, this is the implementation very straight forward like Bellman Ford, it is just a bunch of nested iterations. (Refer Slide Time: 20:08) So, when I initialize it updated, and so on, it is very clear that this particular thing is n cube. And once it is n cubed, it is quite easy to see that nothing much is going to happen if you use because we have to literally I mean, for this is not a question of edges. It is not like Bellman Ford, where or in Dijkstra where I am updating something based on the edges. I am literally saying for every pair, i comma j, compute a better value for i comma j. So, I have to do this n squared calculation every time regardless of whether there are direct edges or not. So, having an adjacency list does not help at all. So, I might as well just write it in terms of adjacency matrices, if you rewrite it in terms of adjacency lists, you will find that there is no improvement in the complexity. But one interesting thing is there, which is that notice that when I look at the k eth step, what I am looking at is the k minus 1 slip. So, I should think of this as having these kinds of. So, this is my SP 0, then this is my SP 1, this is an SP 2 and so on. So I am computing this shortest path matrix in stages 0 1 2 3k. So, but in order to know the entries for k. So, I initialize the entries for 0, then I get the entries for SP 1, looking at SP 0. So, that is, if you look at the way we worked it out, this is exactly what we did. So, we said, If I know SP 0, then I can get SP 1. If I know SP 1, then I can get SP 2.'},\n",
              " {'id': '2c426285-7aef-4c05-8e20-c3b3b3d59a9e',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'So, in order to get SP 2, I do not need to know SP 0 anymore, I can throw it away. SP 0 is done. Similarly, from SP 2, I can get 3 from 7, I can get 8 and so on. So, what we are really saying is that we only need the previous slice in this. So, this particular thing here, I only need the previous slice in order to compute the current slice. So, in principle, I have this n cubed sized matrix, which I am using to calculate shortest paths, but we are saying we only need two n squared slices of it. So, I can reduce the space from n cube to n squared. So, I maintain the current slices in the previous slices. So, I maintain the I have the previous slices, I compute the current slices, then I say now this is the current slices. So I make this the previous slices. So, I just keep updating it, I throw away the previous slices, and make this the previous slices. I compute the next slice from this. So, I need to keep only the current copy of the SP to the power k and the previous copy SP to the power k minus 1. So, those are time complexity cannot be really improved, the space complexity in this case can be improved. (Refer Slide Time: 22:52) So, to summarize, Warshall algorithm is a different way to compute transitive closure, and we can now adapt it. And this is the Floyd Warshall algorithm to compute shortest path. So, the shortest paths tell us in this algorithm, how to get from i to j by going through a prefix of the vertices from 0 to k minus 1. So, this again works with negative edge weights, assuming no negative cycles. And it has a very simple nested loop implementation. So, it is very trivial to write the code for this and it takes time order n cubed, and this order n cubed, unfortunately, cannot be helped because it is dependent. The n cube comes in the fact that for every iteration, I have to physically update every pair every ij whether or not there is an edges. So, it is not proportional to the number of edges in the graph.'},\n",
              " {'id': '0235ec32-7459-4d44-bb5e-ed87052192f8',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 5},\n",
              "  'source': 'All Pairs Shortest Paths (Floyd-Warshall algorithm).pdf',\n",
              "  'content': 'It is just proportional to the square of the vertices. So, it is always going to be order n cube. However, by doing this trick of reusing the slices, SP k minus 1 SP k, SP k plus 1, I throw away k minus 1 and use k. By keeping only two slices of this thing I can do this calculation using n squared space rather than n cubed space, which can be a saving.'},\n",
              " {'id': '0ba2edad-5b16-48a9-89f5-c87d4cac1c29',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Heaps (Refer Slide Time: 00:09) So, we are now going to look at heaps, which are a tree implementation of priority queues. (Refer Slide Time: 00:14) So, remember that in a priority queue, our goal was to maintain a collection of items. And each item has a priority. And there are two things that can happen to this collection. Either I can insert a new item with high priority or low priority, or I can process an item. So, I need to find the highest priority item and delete it from this collection. So, that is called delete max. And delete max need not be unique, there could be duplicates. So, we are not demanding that these are all distinct. So, if there are more than one high priority item, you pick any one. So, what we said is that if we do it in a naive sense, by maintaining this collection as a sequence, either as a sorted sequence or as an unsorted sequence, one of these two operations will be linear. In a sorted sequence, inserting is hard, in unsorted sequence, finding the maximum is hard. So, in either case, if I do N operations, I end up with a quadratic cost. So, we decided to move to 2-dimensions, and we came up with this square grid idea, square root of N by square root of N grid. And then we said that the cost comes down from order N for the worst case to order root N for both. So, earlier, I had one operation constant or one ordered N, now both become order root N, both insert and delete max become root N time. So, therefore, overall, the complexity comes down from N squared to N root N. So, our goal is to improve this further. And that is why we are going to look at trees. (Refer Slide Time: 01:38) So, what we are going to look at are binary trees. So, we have seen trees in the context of graphs. So, tree is basically, if you remember, and acyclic connected graph.'},\n",
              " {'id': 'cd0c437c-6dad-45ed-9bc2-ddc16d3b2a2c',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'But normally, when we look at spanning trees, and so all the trees have very different shapes, I could have trees, which look like this and so on. But a very useful way of thinking about trees is that you pick one of these nodes, so you pick this node, and you kind of hang it here. So, you make that the top, and then these two edges will be below it, and then say, this edge will be below that. And then from here, I will have three edges. So, this will be a different. I mean, it is literally like you take this and you hang it up. So, imagine like you have these edges are pieces of string and each of these nodes is like a bead or something, then it will just fall into place. So, this is how you orient it, you route it. So, we are looking at trees which have a particular structure, which are called binary trees. So, in a binary tree, when you hold it up like this, every node has at most two things below it. And these two things have a direction, there is a left and a right. So, this one, for instance, has only one thing below it, but that one thing is on its right. So, 37 is the right child of 83, whereas 83 is the left child of 72. And similarly, 14 is a left child of 44. So, this is what the child of a node is when I hang up this tree, the nodes that come immediately below it are the children. And in a binary tree, you have up to two children. So, you could have one child as we see here. So, this is a one child case. And of course, we have all these nodes at the bottom, which have no children. So, these which have no children are called leaves. And on the other direction, if I go up, every node is hanging from a unique node above it, there are no two, because it is a tree. Remember, the underlying thing, it is a tree. So, there is only one way to get to this node, I cannot come to it from another path. So, if I look at a node, it is hanging from the path which connects it back upwards.'},\n",
              " {'id': '1f9301d2-6b50-41db-b0f8-9540577c622c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, every node has a unique parent, the only node which does not have a parent is the root because the root is the start of the whole thing. So, other than the root, every node has a unique parent. So, there are two quantities of interest in this tree. The first quantity is of course, the total number of nodes in the tree. And this is what we usually call the size of the tree. So, the size of this tree is 1, 2, 3, 4, 5, 6, 7, 8, 9. So, the size of this tree is 9, because there are 9 nodes. And then the height is the number of levels. So, this is 1, 2, 3, 4, 5. So, the height of this tree is 5. You can think of the height in terms of number of levels, or you can think of height in terms of number of edges on this longest path. So, it is either 4 or 5, it is off by one depending on whether you have measured it in terms of height in terms of the node level or in terms of the number of edges, but it is conventional any way like in everything else, so number this is level 0. So, this is level 0, this level 1, this is level 2, level 3, and this is level 4, so that is how we will deal with this. (Refer Slide Time: 04:53) So, heap is a binary tree with some additional constraints. So, the first thing is that it does not have this kind of wonky structure, so I cannot have things which are missing in the middle. So, binary tree is filled level by level from left to right, and the value at each tree is at least as big as its children. So, for instance if I take a tree which has like 83 and then 76, and 44. So, this is a binary tree which is filled level by level, so the first level is full, the second level is full, and at the third level I have three nodes filled from the left, so I do not have this node so this node is missing, it is not allowed to leave out this node then it is not filled from left to right. Similarly, if I do this and then if I add this note, so it is now left to right and then add something here this is also not good.'},\n",
              " {'id': '94c7087c-2e22-422f-9fac-c5b72dfd2fd4',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, what the binary tree does is it has the heap does it has two such constraints, the first constraint is this structural constraint that it is level by level from left to right, the second one is the value constraint which says that at every node if I look at the value here it must be at least as big as the values below it. So, 83 cannot have a value bigger than 83 below it. Now, because of this it is easy to see that the root must have the largest value because this level must have values bigger than the next level. So, level 0 has level values which are bigger than the two nodes at level 1, level 1 in turn must have values bigger than the nodes at level 2, so therefore 0 must be bigger than 2 also 2 is bigger than 3 so 0 is bigger than 3, so by induction at any level J you can only go down in the value chain. So, the root is always the biggest value in this tree. (Refer Slide Time: 07:03) So now, if we look at trees which are not heaps as we said there are two possible reasons, they are not heaps the first is that they are structurally wrong. So, why is this not allowed because there should be a node here because we said that a binary tree should be filled from top to bottom left to right, so it is okay to have some gaps on the right-hand side of the tree but it is not okay to have a gap in the middle of the tree. So, this is an example of a binary tree which is not a heap. (Refer Slide Time: 07:32) This is also a binary tree there is not a heap for a structural reason that is because I did not finish this level, I left an incomplete node at level, this is level 0, level 1, level 2. So, at level 2, I did not finish it. So, without putting the last node at level 2, I have gone ahead and added nodes at level 3 this is not allowed. So, a binary tree to be a heap must have been filled top to bottom left to right.'},\n",
              " {'id': '0f5fc6b2-e218-4969-9f4d-2926bffe80e3',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, the only incomplete level, so this is an incomplete level right because there are more levels, more nodes that could have been at this level, the only incomplete level can be the last level, so that is one way to think about. The other properties the max heap property the one which says that every node must be bigger than a student. (Refer Slide Time: 08:14) So, here is a tree which has no problem structurally. So, this is full, this is full, this is full, and this is my partial level. So, level 0 is full, it has one node, level 2 is full, it has both the children and the first node, and level 3 is full, level 2 is full, because it has both the children have the level one node. But the problem is that this node has a value which is smaller than this node. So, 53 is smaller than 54. And this violates our condition that every node must have a value which is at least as big as its children. So, here it is the problem is not the structure but the value. So, if I draw a binary tree, so remember in a binary tree, every node can have 0, 1, or 2 children, but I must have the structural property that all the levels up to the bottom level are complete, only the bottom level can be incomplete. So, that is a structural property of a heap. So, the nodes are filled in a particular sequence, or the nodes are created, if you want to think in a particular sequence. And the values in the nodes must obey this local property. And this local property always guarantees that the value at the top is the largest value. So, you can think of it as the top of the heap. The biggest guy in this collection. So, this is a heap. Now the question is, what are we going to do with it. (Refer Slide Time: 09:29) So, we need to implement ideally our priority queue. So, the values in our collection are the values in these nodes. So, each node is a value in our collection. And we need to be able to support the two operations that priority queues require.'},\n",
              " {'id': '6b5eb107-52c7-4566-8abb-884b4a3b7ac3',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'One is to insert an arbitrary item and one is to delete the maximum item. And of course, we need to maintain this heap structure when we do it. So, there is a certain structural property that needs to be maintained. So, let us suppose we have this heap. So, first we check it is a heap. So, it is a heap because the top level is full, second level is full, next level is full. So, there are no gaps. So, structurally, there is no problem. And 83 is bigger than 74 and 72, 74 is bigger than 54 and 27, and 72 is bigger than 44 and 31. Notice that the heap property is local, I only have to look at these three nodes, I do not have to look at, I do not have to compare, for instance, this node with this node, I know that 83 is bigger than 27. Because 27 is smaller than its parent, and 74 will be smaller than 83. But I do not have to compare it. So, it is sufficient to look at these neighborhoods in order to get it right. So, this is my current heap. So, now I want to take a new value and insert it. So, the first thing that is going to happen when I insert a value into this tree is I have to create a new node because obviously that new node has to sit somewhere. So, where is it going to sit? Now, this is where the structural property the heap comes in, there is only one place that I can add, because the next node is fixed. Can I add it here? No, because it must go in the next level, so this level is full. So, it must create a new level. But in the new level, it must be the leftmost thing. So, the only place that I can add in this tree is here, I have to add a node here. So, this is the place the tree will grow structurally, I have no choice. The structural change in the tree is determined by the number of nodes. If I have, if I tell you this tree has in this case, 8 nodes, now it has 7 and I want to put an 8th node, there is only one place the 8th node can go. (Refer Slide Time: 11:33) So, what I do is I put that 8th node there and I put this value 77.'},\n",
              " {'id': 'f5948631-dcd6-46f5-9218-f796b79b7985',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'But now, the problem is that if I look, so the rest of the heap was a heap, but in the vicinity of the new note that I have added, there is no guarantee that the heap property is satisfied. Everywhere else, I have not touched anything, the rest of the tree is untouched. So, the rest of tree if it was a heap is still a heap. But here the question is, is this a heap? And it is not. So, I have to fix it. So, how do I fix it? Well, why is it not a heap because it can only be because the new node, I have added is somehow bigger than its parent when it should be smaller. So, since it is bigger than its parent, when it should be smaller, the obvious thing is to exchange it. So, what I do is I fix this property by exchanging it. (Refer Slide Time: 12:19) So, I move it to the top and I move 54 down. But now, I still have a problem. Because every time I move a new element into this position, the 77, which I added was never been, has never been reconciled with the rest of the tree. So, I reconciled with the 54. So, now I have to again, reconcile it with its parent. So, it is I see that it is still, unfortunately, not in the right place with respect to 74. So, I swap again. (Refer Slide Time: 12:50) And now, I look at its parent and I find, Okay, it is fine. So, 77 is smaller than 83. So, there is no problem. So, the question you might ask is, what about this side? Because 77 has now come next to 27. So, why is it not the case that I have to check 77 versus 27? Why do I only look up? Well, remember that before this happened, I had 74, 27, and 77. So, I knew that, I know that this is already bigger than this. So, 74 is already bigger than 27. And the reason I moved this here is because 77 is bigger than 74. So, I have 77, which is bigger than or equal to 74, which is I move it up. But this is already bigger than 27. So, therefore this is also already bigger than 27. So, when I move something up this heap, it will automatically be bigger than the other side.'},\n",
              " {'id': 'f796afe6-d9e3-40af-9988-2ceca1621d26',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, I do not have to worry about what happened on the other side, I only have to look at the path walking upwards because that is where I could have created a contradiction by putting a big value below. So, I restored this property along the path to the root. (Refer Slide Time: 13:54) So, let us do another one. So, supposing now I insert 44. So, 44 again, this was my prior to this, this was my heap structure. So, the only place that the next node can go is here, as a right child of the 74 node. So, that is where I put my 44. Now, I have to check again this heap property, so I look at its parent. And in this case, fortunately 44 happens to be smaller than 74, so I can stop. So, I just insert it and I stop. We will do one more and then we will analyze this. (Refer Slide Time: 14:24) So, let us put 57. So, if I put 57 then it must now go below the second node at the previous level, because now there is no more space here, the two children are taken up. But so, I just complete, so I am just going, as I keep adding I will be adding at this level from left to right. So, now I see that there is a problem again, so I will do this 57 and 27 I will swap. (Refer Slide Time: 14:46) Now, I will look at this. And now I am done. So, this is the end of my insert 57, so this is our insert. I take it to the new position. That is the right most position in the bottom most row or I start a new row in case the bottom most row was already full. And then I move it up the tree until everything along the path that I am looking at, is correctly placed with respect to its children. (Refer Slide Time: 15:13) So, how much time does it take? Well, I need to, every time I insert, of course, I have a fixed place to insert it, but I need to walk up the street. So, it is going to take in principle, I need to go all the way up. So, for instance, supposing I insert at this point, a 99, then 99 will get exchanged with 57, it will get exchanged with 77, it will also get exchanged 83.'},\n",
              " {'id': '666ea1c0-6e9e-4c29-98ea-b30fbd32d2ef',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, 83 will come down, so 99 will become the new root. So, in worst case, I will have to go all the way up. But remember that that is the height. That I have to keep walking up as many levels as there are. So, the time it is going to take me in terms of compare and exchange is going to be proportional to the height of the tree. (Refer Slide Time: 15:53) So, what is the height of this tree? So, remember that we said that this is level 0. So, at level 0, I have one node, which I will write as 2 to the 0. Now here, going from level 0 to level 1, I have two children. So, I have twice as many nodes at level 1, as I have at level 2. And I go from here to here, I have two, so each node propagates two more children at the next level. So, at the first level, I have one node. At the second level, I have 1 times 1, second level, third level, I have 2 times 2, because each of them gives 2, then I have 2 times 2 times 2, and so on. (Refer Slide Time: 16:32) So, in general, if I go down j steps, I have 2 to the power j, so I have 1 at the first level 2 the power 0, 2 at the next level 2 to the power 1, 4 at the next level 2 to the power 2, 8 at the next level 2 cubed, and so on. So, if I go down the number of slots, which are available at level j is 2 to the power j. So, if I have filled up k levels, if I have k levels like this, and I have filled it up, then I have 2 to the power 0 on the top, 2 to the power 1 to the next level, and so on down to 2 to the power k minus 1 because my levels are 0 to k minus 1, if I have k levels. So, I have 2 to the power 0 plus 2 to the power 1. So, you can think of this as the binary number with k minus 1 bits. In binary, if I write on k minus 1 once, then it is 1 with k minus 1 0 is minus 1. So, this is 2 to the power k minus 1. So, 2 to the power 0 plus 2 to the power 1 plus 2 squared plus 2 cubed plus sub 2, 2 to the power k minus 1 is the same as 2 to the power k minus 1.'},\n",
              " {'id': '710830c7-2b7c-4a2e-ad13-6a4f08a71f38',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, what we are saying is that if we fill up a tree up to k levels, then we have 2 to the power k nodes. So, conversely, if you go the other way, if I have N nodes, then the height is, so the size we are saying is 2 to the power height. So, this is what this is telling us. The number of nodes in my tree grows exponentially as 2 to the power of the height. So, therefore the height is going to be the log of the size, by the definition of log. (Refer Slide Time: 18:16) So, if we have N nodes, then we have at most log N plus 1 levels. The plus 1 is because of this k minus 1 business. So, then this tells us therefore that the complexity of insert is log N, because if I am inserting into a heap, which already has N nodes, the number of times I have to do this exchange moving up to find its correct position for the new value is at most log N, because that is the guaranteed upper bound on the height of the tree. So, we have already achieved one of the two things we wanted. So, remember that our goal was to reduce this priority queue operations of insert and delete max, we want to reduce them further from square root of N to log N. So, we have now shown that if I keep this clever kind of binary tree, which is filled up nicely with this local heap property, and with this structural guarantee, then I am guaranteed that the height is log N. So, insert is log N. So, what about delete max, that is the other option. (Refer Slide Time: 19:13) So, we noticed that because of the heap property, as we said, by induction, everything at the level above is bigger than everything of level below. So, by induction, the root has the largest value. So, the root has the largest value. And then when I want to delete max, there is only one thing to do, which is remember there could be more than 1, I could have had an 83 here also. So, there is nothing to stop that from happening. But I will take the value of the root and say this is your maximum. So, the maximum value is at the root.'},\n",
              " {'id': '64dad8d8-0b48-4d60-939c-3f117974f5ca',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, I removed that and return that as the, remember delete max gives you back a value. It gives you back a value, and it removes that value from the collection. So, they are two separate operations. So, giving back the value is very straightforward. It is like a sorted list. In a sorted list, we said that the maximum value is at one end so I just pull it out. But if the list is sorted, just pulling it out remains leaves it in a sorted list. But here if I pull out the root of this tree, now I have created a hole, and the structure is wrong because now I have N nodes, but I only have N minus 1 values because this is not gone. That is not allowed. (Refer Slide Time: 20:15) So, after we delete one value, the tree has to shrink here. So, this node has to go because this is how the tree grows and shrinks, it grows, it shrinks at the other end. So, I remove value at the top of the tree, but the node that disappears is the bottom of the tree. So, this is the problem. Now when I do that, I have another problem, which is that I have this lonely value 27, which was sitting inside a node, but that node is gone. It is house has been destroyed. So, 27 is homeless. But I have an empty home over here. So, the logical thing to do is to take the homeless value and put it into the home that exists. (Refer Slide Time: 20:53) So, I take this 27 and move it there. So, I removed the route as the delete max and returned it to you. And now my job is to clean up the heap. So, when cleaning up the heap, I realized that I have got the last element, the bottom most right most value in the heap without a place to stay. But I have created a place because I removed the root. So, I moved to the route. But now of course, the problem is that is not the wrong place, 27 is not bigger than in general, it could be. But in this case, you can see that it is smaller than its two children. So, I need to fix this problem. But unlike an insert when I was fixing it upwards, I am going to fix it downwards now.'},\n",
              " {'id': '37c4c2c4-e000-485e-9bf1-c2a1d7749680',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, I have to look at both its children. So, I look at both its children. And I have to now do what, so it is smaller than 77. It is smaller than 72. So, which one should I fix? So, I have two options. So, either I can move this here, or I can move this here. But notice that if I move 72 here, then this is a problem. If I move this smaller child up, even though it will fix the problem for 27, it will create a problem on the other side. Because 72 will be smaller than 77, which I do not want. So, therefore the obvious thing is I look at its two children. So, I look at both these children. And I move the bigger one up. So, this is how. So, when I am doing the insert, there is no choice, I look at my value and my parents value there is only one parent. So, if it is smaller than my, if it is bigger than my parent I exchange, if it is smaller, I stop. Here, I have two children to look at. And I could be wrong with respect to both of them. So, which one of them should I fix? Well, I fixed the one with the bigger one. And now as we argued, if I fix the bigger one, the bigger one is going to be okay with the smaller one. So, if I move 77 here, this is going to be fine. Because I know that 77 was bigger than 72. That is why I moved it up. So, therefore I am not going to have any problem on the other path. So, this is what I do. (Refer Slide Time: 22:55) So, I take the 77 and I move 27. So, this is what I have done. Now I have still a problem. So, again, it might be smaller than its two children. But I have to pick the bigger of the two, there is no point to fixing it in 57. Because then 57 will be too small with respect to 74. So, I fix it with respect to 74, and then I come down one more step. Now, I have another problem here to fix. So, I fix it with respect to 54. And now I am done. Now, the main thing to realize is that I followed one path.'},\n",
              " {'id': '804e5f5a-4b61-4f20-8997-aa11f576c312',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'And the reason I only follow one path is at every point when I have to flip, I choose the bigger of the two children, so I do not the other path remains undisturbed. So, if I went left, then this part is not touched. And I went left, this part is not touched. When I went left, this part is not touched. So, whatever I bypass, the direction I go in, I am doing something, everything else remains undisturbed and retains its heap property. So, I moved on only one path. So, an insert I was moving up one path. Here I am moving down one path. And by the same calculation as before, since I am moving down one path, I am working in time proportional to the height of the tree. And we argued that the height of the tree is logarithmic in the size. So, again, delete max will also be logarithmic in it. So, insert was algorithmic, delete is logarithmic. So, we are in good shape. Of course, the problem with this is that we must have a heap. So, how do we construct the heap? (Refer Slide Time: 24:23) So, before we do that, let us see how we will do all this manipulation. So, it would be a bit tedious if we had to construct a kind of a graph representation. Remember that we have adjacency matrices, adjacency lists or we can do various things to maintain this thing. But we do not want to maintain this tree in this very complicated way. Because that would be a lot of cost, just this thing of finding the parent and all that we do not want to incur that cost. So, it turns out that because of this heap structure, there is a very simple way to maintain it. So, I numbered these vertices in order, so this is the 0th vertex, this the first vertex, the second vertex. So, these values are in a particular sequence, if I follow the tree from top to bottom left to right. So, I will represent this, this particular tree as a sequence in this order. So, this order is not a value order it is a position order.'},\n",
              " {'id': '56d9f2ae-9cd9-4292-94c4-af0ddf77c43d',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, I will take this, if this is my heap with these 10 values h0 to h9, I will represent it as a list or an array with 10 positions with the values h0 to h9 in that order. So, the root will be the first one, the children are the root will be second and third position and so on. So, the same sequence in which I can read off the tree left to right top to bottom, I will keep it in my list. Now, I need to have this operation which will fix the heap. So, what do I need to fix the heap? I need to look at a value and find out which are the two children of that value, or I need to look at a value and find out which is the parent of that value. So, I need to be able to go up and down these edges. And say that if I am at index i, what are the children of index i, if I am at index j, what is the parent of index j. So, a little bit of thought will tell you that this is the formula that if you are at index i, then the children are 2i plus 1 and 2i plus 2. So, we can just verify by inspection if you want to convince yourself. So, if this is i, so i is equal to 2, so 2i plus 1 will be 5, and 2i plus 2 will be 6. So, indeed, 5 and 6 are the two children. So, this just follows you can show this by induction, but it is easy to compute for yourself, or even by inspection to guess this pattern. So, if I am at index i, then it is little bit like we talked about array access. To find the children, I just have to compute the offset, I am at a particular place. So, where are my children? My children are that much further away, there are 2i plus 1 and 2i plus 2 to my right, and it is at a fixed position. So, I then I can go and look it up. So, if I want to compare and fix that when I am doing the delete max, if I want to compare the value here, the value here, and the value here, then I know exactly which indices I must look at. And as I go down, I can keep doing that. So, the children of i are at 2i plus 1, 2i plus 2i.'},\n",
              " {'id': '5b85fd5f-64cb-4fad-85f4-2c99666e34e5',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'And if you invert this, then it turns out that the parents of i are at i minus 1 with integer division by 2. So, if I invert this, for example, if I take 9, then I take 9 minus 1, and I divide by 2, and I get 4. And 10 would be the next vertex. 10 minus 1 divided by 2 will be 4.5, which will again be 4. Because this is the integer division by the double slash, so 2i plus 1, 2i plus 2 takes you forwards and integer division, you subtract 1 and then integer divided by 2 takes you upwards. So, parent and child can be left child, right child and parent can be calculated as indices within this list. So, I have a very simple indexed list representation, I do not have to go into so many complicated graph representations or like our linked list using objects and pointers, and all that we do not have to any of that, we can just keep it as a simple sequence, and use the positions to record the children and the parents because of the structured way in which we grow and shrink this list. So, this is how we represent our heap. (Refer Slide Time: 28:20) But the problem that I was alluding to earlier is that we need a heap. So, initially, I am giving you this collection, or supposing I give you the collection as an arbitrary list. So, how will you take this arbitrary list and put it in the right order. So, it is like a bit like saying that I want to sort before I do a binary search. So, am I going to spend a lot of time building this heap and only then get the benefits of this log N time. So, I have to make sure that I can build a heap quickly. Fortunately, there is a very simple way to build a heap. The simple way to build a heap is say, Okay, I have an empty tree, I have no heap, and then I insert the first element in my list. So, I insert v0 into my empty heap, I will have a root consisting of v0. In this, I will insert v1, so I will put a v1 here, and then I will possibly exchange it and I will get a new heap.'},\n",
              " {'id': '34bddb3c-5e50-47ee-8cae-0d8f58de9cf7',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'Then in this, I will insert v2 here, so the values I am getting from my list are in some arbitrary value order. But each time I use the heap insert, so I have already built up a heap with i minus 1 values, the ith value will be inserted like, so much like insertion sort, keeps an inserted, a sorted list and keeps growing it, I will keep a heap and I will keep growing it. Now, we know that each insert in the eventual thing is log n. So, I am doing n inserts of n elements. So, it is going to be n log n in the worst case. Now, n log n is okay, because finally I am going to do n inserts and n deletes. So, it is okay that n log n is what I am going to pay anyway. So, it is not going to add to my cost. So, this very naive heapify as it is called, builds a heap out of a given sequence by repeatedly inserting into an empty heap. So, I start with an empty heap put the first element in, put the second element in, put the third element in and overtime I get in order N log N time I get the heap that I want. Now, it turns out that there is a cleverer way to do this. (Refer Slide Time: 30:16) So, let me think of this as a heap. So, I have, it is not a heap but let me think of it as a heap. So, this is what the values would look like if this was interpreted as a heap. So, at some point, some of these values the last few values will be at the end. So, they are the leaves. How many leaves are there? Well, the number of nodes at each level doubles. So, it turns out that if I have a complete tree or of level k we saw earlier that it will be 2 to 0, 2 to the 1, 2 to the 2 and so on and this will be 2 to the k minus 1. And remember the whole thing adds up to 2 to the k minus 1. So, this is 2 to the k, so this is half of this. So, half of this is 2 to the k minus 1. So, in a complete binary tree of where everything has been filled up to the kth level, half the nodes are leaves.'},\n",
              " {'id': '58c7b7cc-e18b-489c-8892-076da0960153',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'And the other way of thinking about it is if I take an index and I compute 2i plus 1, 2i plus 2 and it takes me beyond N. If 2i plus 1 goes outside the length of the list then it must be a leaf because it has no left child, it has no right. So, what value will take me out of the list the midpoint, at the midpoint if I double it, it will take me to the end and if I add 1, I will go outside. (Refer Slide Time: 31:50) So, if I calculate the midpoint of the list then the entire slice from the midpoint onwards is already a leaf node. Now, for a leaf node the heap property is trivial because for a leaf node there is nothing to do. I have to say that a leaf node is bigger than both its children but leaf nodes have no children. So, in some sense half your list that you start with is already a heap, if you think about it locally, the bottom most level of your heap. So, I construct this thing so this part is always, the bottom most level is always in itself a heap, it is only a problem with the higher levels. So, then what I can do is I can go to the level above it. So, I have 2 leaves here l1 and l2. And I have some node here and then I asked Oh, is this node correct? And if not, I fix this part. So, I will either exchange v with l1 this is exactly what we did when we were doing delete max, we were taking a vertex and comparing it to its two children and deciding which one of them I should exchange if any. (Refer Slide Time: 32:55) So, I can go up 1 level and fix the heap property downwards for the second last level. So, I have the last level which is okay and then I look at all the nodes which are 1 level up and I can look down at their children and exchange. So, I have to do one swap and then this heap will now be okay up to the last two levels. Because the last level is already okay and now, I have explicitly fixed the second last level. So, now I have to go to the third last level, so the level above this.'},\n",
              " {'id': '56af5e2d-a3bd-4bfb-9f8c-1641a7af1b67',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'But now, when I fix something here it might require me to fix something again. So, I might need to do two steps because I might need to exchange something at the third last to the second last and then that might require me to move something to the leaf level. (Refer Slide Time: 33:39) So, if I fix it for the third last level, I have one more work and then I do it. So, I walk upwards, so at the bottom most thing is fixed the next one requires one swap, the next one request two swaps and so on. And finally at the end, I will do it for the root which will require log N swaps exactly like in a delete max. So, this is a different way of doing it bottom up. And what have we gained? Well, every time we go up as we saw when we did the leaf level, we have to do 0 work, they are all already satisfying the heap property. At the next level we have to do one swap and the next level we have to do two swaps, next level we have to do three swaps, but just like when we go down, we are doubling when we go up, we are halving. So, though we have to do one extra step per node, the number of nodes that we have to fix in the next level above is half the number of nodes that we had to fix here. And remember that overall, half the nodes are at the leaf. So, if this whole thing has n nodes, then I can kind of in a complete sense at least, I can assume that N by 2 nodes are leaves. So, the first level that I have to fix has only N by 4 nodes. N by 2 nodes the last half this is the slice from the midpoint onwards. The half of the leaf, half of the list is already fixed. (Refer Slide Time: 35:00) So, if I count, the second last level has N by 4 nodes, and I need to do one swap to fix it in the worst case, I may not need to do any swap, but I need to compare. So, some order one work I have to do, constant, and I have to check its two children and swap if needed, so I have some constant, n by 4 work.'},\n",
              " {'id': 'd98e7efd-923a-49d3-acec-00280b5b250a',\n",
              "  'metadata': {'chunk_idx': 18, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'Now I go up, I have to do two steps, I have to do this thing twice, but I have to root for n by 8. If I go up one more level, I have to do it for three steps, but I have root for n by 16. So, therefore, you have this is not immediate, what is leading up to, but it is saying that you are halving the number of nodes, but at each step, you are increasing the amount of work by one. And finally, for one node, you will do log n work. So, the root node you will do log n work. So, if you add up this complicated expression, so this is n by 4, which is the first level, plus say 2n by 8 to the second level, plus 3n by 16, the next level would have been say 4n by 32, and so on. If you add this up until you reach log n. It turns out that this is only overall order n. So, as opposed to the naive heapify, the naive heapify said, think of your initial heap as empty and insert each element into it like insertion sort, except you are doing a heap insert. Here, you are saying that kind of fix the heap bottom up. So, just pretend that the list you are given has a heap, and now go backwards from the right to the left and fix the heap property by examining the children of every node. And in this process, you might have to move things, but the number of things that you have to move far reduces as you go to the left. So, this heapify turns out to be linear. So, you can do a heapify in linear term. (Refer Slide Time: 36:42) So, what we have seen therefore is that if you draw a heap like this, a max heap, then you can do insert in log N time, you can do delete max and log N time. So, of course, over N operations, this means you can do N log N time. And if you do that, second heapify that we saw, the one which does bottom up, starting from leaves and going up, you can build a heap also in order n time. So, order n times set up an and order log N time per operation is what heap gives you. So, this gives you a priority queue implementation, which is N log N time over N operations.'},\n",
              " {'id': 'a503a0bf-e11c-4e1d-8c11-26614bff2b0c',\n",
              "  'metadata': {'chunk_idx': 19, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': \"So, at the moment, it is not yet enough for us to fix our problem with Dijkstra algorithm and Prim's algorithm. But we have come really far, and we will see how to take this and implement those things. So, one of the things that we have mentioned is that in Dijkstra algorithm and Prim's algorithm, we are looking the opposite way, we are not trying to delete the maximum but the minimum, we are trying to pick the smallest node to add in terms of distance from the current tree or the current set of visited vertices. So, what we need is a min heap not a max heap. So, a min heap is just dual. So, now it says that this is smaller than or equal to, so every node is smaller than or equal to, rather than bigger than or equal to. So, the inversion in the value so every node is smaller than its children. So, the smallest value is at the top. And the corresponding operation that you need is to delete the minimum rather than the maximum. That is what we need in the Dijkstra algorithm. We need to delete the minimum rather than the maximum. So, why is it that we cannot use a heap directly? The problem is that Dijkstra algorithm and Prim’s Algorithm both have a property which we have not used, there is an operation on a heap which we have to perform, which we have not done, which is sometimes when we visit a vertex, we have to update the distances of all its neighbours. So, they will be sitting inside this heap. And those will not be inserts, but update. So, I might have to say take 47 and reduce it to 44. It has a new distance. So, Dijkstra’s algorithm had discovered that the shortest path to this unvisited vertex has now shrunk from 47 to 44. This one has shrunk from 61 to 32. And in this process, we might have to fix the heap in a different way.\"},\n",
              " {'id': '6b352d78-6364-4b60-85b3-49e76cf6f557',\n",
              "  'metadata': {'chunk_idx': 20, 'week': 6},\n",
              "  'source': 'Heaps.pdf',\n",
              "  'content': 'So, in addition to this, delete, minimum, and insert, we also had this update, take a position in the heap and update it, and we need to make sure that that can also be done efficiently before we can claim that heaps can solve our problem with Dijkstra’s algorithm and Prim’s Algorithm complexity.'},\n",
              " {'id': '8d493261-d5e3-47cd-8f1c-8c1507623bed',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Using Heaps in Algorithms So last time we saw that heaps can be used to implement a priority queue. (Refer Slide Time: 00:14) So, remember that in a priority queue, there are two basic operations, insert which adds an element and delete max. So, in a priority queue each element has a priority, so when you remove an element from the queue you must remove the one with the highest priority not the one that was inserted earliest. So, in a normal queue, it is first in first out, whereas here you look at all the elements that are there and you take out the one with the highest priority. Now you could dually also take out the one with the lowest priority in which case it could be delete min. So, we could implement this in an efficient manner using heaps, so heaps are balanced trees, balanced binary trees, in which insert and delete max or delete min can both be done in log n time if there are n elements currently in the heap. And then to start with you must make a heap so we saw that you could take a list of n elements and construct a heap from it in order n time. The main advantage of working with heaps is that because of the balanced structure remember that the heap is filled layer by layer left to right you can now index the heap starting with the index zero for the root 1 and 2 for its children and so on. And then by just doing a simple index calculation, if you remember 2i plus 1, 2i plus 2 will give you the children of i. And if you take i minus 1 and take divide by two integers, you will get the parent of a node. So, we do not actually have to maintain a graph like structure we can just work with a list or an array and just move around in the array using the simple index arithmetic to get from parents to children.'},\n",
              " {'id': '0b943c39-b3f2-4f64-8178-ee513ec21703',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So now that we have this heap in which we can maintain these values, so that we can extract the minimum or the maximum in logarithmic time, and also insert in logarithmic time, what more do we need to use it in an algorithm? And which algorithm are we actually going to use it in? (Refer Slide Time: 02:08) So, the reason that we started looking at heaps to begin with is, because of Dijkstra algorithm, if you remember in Dijkstra algorithm, what we do is we start with a single vertex, and then we keep updating the distances to all the remaining vertices based on the values we have already visited. So initially, we do not know anything about any distance, then we start with one vertex, it is at distance zero, then we update the values of it neighbors, then pick the nearest neighbor go to that update the values of its neighbors and so on. So, in this way, we sequentially visit all the vertices in the order of the distance from the starting node. So, we have to maintain these two dictionaries, one which tells us which nodes have already been visited. And the second one, which tells us how far it is to each unvisited node. So, when it is visited, the distance becomes frozen. After the distance after the note is visited, you do not find a shorter distance that was the proof of Dijkstra algorithm, but till it is visited, you will keep finding shorter paths because you will discover new paths to it. So, you initialize this distance dictionary to infinity, and then you initialize the start distance of the starting vertex. So, this is this distance of s to zero, and then you keep finding the smallest unvisited vertex. So, you call that next vertex, the smallest distance unvisited vertex, you set its visited status to prove and now this was the problem that you have to actually re compute the distance for all the neighbors of this new vertex. So, you have two problems here, you have to find the unvisited vertex, and you have to update all the distances.'},\n",
              " {'id': '6957e111-c764-45f3-97bc-861d41d0846c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, the first bottleneck is to find this unvisited vertex. And what we said was that there is no simple way to do this, because the vertices are not maintained in any particular structure. So, we have to just scan all the vertices and check. So, this is what we do here. So, we run a loop saying, take every vertex that is not visited and find the one with the minimum distance. So, we really have to do an order n scan in order to find the next vertex at every iteration. And there are n iterations because we are finding the vertex to finding the distance to n vertices. So that is what we got for our n squared worst case complexity for Dijkstra. So now our obvious suggestion is that we can use a min heap because what we want is we have a collection of distances and we want to take from this collection, the smallest value or a smallest value of more than one. So, remember that this min actually gives us a collection of values which could have the minimum distance and then we actually take this next vertex list and pick the kind of smallest one in that. So how do we do this? Well, we should be able to keep a min heap. So, if we keep a min heap, so we keep the distances in a min heap. Now why do we need a min heap because we need to delete the minimum, not the maximum. So, you take a min heap, remove the smallest vertex. And what we have just seen is that this deletes min should always work in logarithmic time. So therefore, if we do this n times, we will have an n log n cost for finding the next vertex over our n iterations. But there is still one bottleneck, which is this part. The bottleneck is not so much to do with the complexity of the loop here. But the fact that we have to do something on the heap which we have not described how to do, so we have to take the elements on the heap, which are all the unvisited vertices, and we have to change their values, because the values on the heap represent the distances.'},\n",
              " {'id': '3e9af8c8-3006-4608-b5c1-1c338e8bf19e',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'And when we visit a new vertex, the main property that we have to ensure is that now we recompute the distances from this new vertex and see if it is smaller than the distance we already knew. And if it is smaller than we reduce it, so we take the minimum of these two distances, the new distance and the old distance. But so far, in a heap, we have not seen any possibility of modifying a value. We know how to insert a value. We know how to delete the maximum or the minimum in this case is min a minimum. But how do you take an arbitrary value which, so if it was a minimum value that we wanted to change, it would be easy, you delete it, and then you insert the new value. But we are changing some value, which is neither the minimum nor is it a new value to be added, we have to take some arbitrary value in the middle of the heap and change it. So, this means that we cannot directly use a heap for the access algorithm without making some additional observations about how the heap works. (Refer Slide Time: 06:48)   So, what we are going to see now is how to update values in a heap. So, what we have seen earlier is how to insert a new value into a heap, add a node, delete the minimum value, remove the value at the root, remove one node at the bottom, and then readjust the heap. And now we want to modify values. So, supposing we take this value 54, and we want to change it to 35. So if we update it to 35, then what happens is that we could because now it is a min heap, so each value has to be smaller than its children. So, if I reduce this from 54, down to 35, it is definitely going to remain smaller than its children. 54 was already smaller than 68 and 59. I have taken 54 and made 35. So, there is no problem with its own children. Remember, the heap properties are all local; I only have to look at a node and its children. And then inductively, it shows that the smallest value must be at the root. So now, there is no problem here. But what about here?'},\n",
              " {'id': '35bb273c-8f20-4212-ac06-6f4bb5c40071',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So here earlier, 38 was smaller than 54, and 47. But because I have reduced 54 to 35, I have a violation of the heap condition with respect to the parent of the node that I am changing. So if I have a violation with respect to a parent, this is something that we have seen before this happens typically, when we insert a new node, when we add a new node here, it could very well be something like 11, and it might be smaller than 47. And then we have to do some work. And the work that we do is to swap it with its parents. So, we can do the same thing here, we find we reduce the value, we check with its parent. And then we exchange. So, similar to insert, we exchange. And remember, similar to insert, this is guaranteed to be smaller than this because 35… The reason I am swapping it is because it is smaller than what its parent was 38. But that parent was earlier smaller than 47 anyway, so by moving a smaller value up, I am not going to violate the heap property on the other child. Now, it so happens and stops here. But supposing it had been 25, then 25 would have been exchanged one more time. And I would have got 29 here in 25 there. So, this is very similar to insert, except we are starting from not the bottom as we did an insert with a new node but somewhere in the middle of the heap. So whenever in a min heap, we reduce the value, it might need to be bubbled up the heap, swapping with its parents along the path to the root. And of course, the other problem or the other kind of update we could do is to increase the value. So, supposing I now take this value, and I make it 71. Now, because in a min heap it must be smaller than its children, the problem cannot come with the parent of course, this is the root node but supposing I take this 31 for example, and made it made it safe for instance 81. So, this cannot create a problem with its parent because it was already bigger than its parent because the parent was smaller than it. So, 81, 31 was already bigger than 29.'},\n",
              " {'id': '80f324c7-bfcd-4b14-a957-1ecc124917e8',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, 81 will remain bigger than 29. But the problem could happen here, so 81 now becomes bigger than 61. So let us stick to the example that we have. So let us not fix this node, but fix 29. So, 29, we change to 71. So, like we did, now, this is similar to the delete min, this what this is what happens sometimes when we, we delete the minimum, and we put a big node A big value up there, and then we have to move it down, so we move it down to the smaller of its children. So, 29 has to be exchanged with either 35 or 31. But we want to move the smaller one up so that the heap property is fixed it, so we will exchange it from 71, we will exchange it on the righthand side. So, we have done an exchange here. And now 71 is still not in the right place, because it is still bigger than 61. So, I have to do it one more time. And then eventually, so 71 has come all the way down here and these two values have gone, so this is very similar to what happens when we delete the top element, the root element, we delete the minimum, we will take a large the value which is at the last leaf and move it there. But that could be an arbitrary value need not be the new minimum. So, then it has to bubble down. So, these two update operations are exactly like what you will do for the two other operations that we know, insert and delete min. So, in this case, reducing a value behaves like insert in terms of update you have to make except you start from the middle of the heap. And similarly, increasing value causes operations like delete min, except you start somewhere in the middle of the heap again. So, both of these updates are there for log n, because we know that insert as a whole is log n. So, this is a special version of insert in a sense. Similarly delete min is log n. So now if that is the problem that we had, how to do that. So, remember the bottleneck that we had in Dijkstra algorithm was that at this point, we had to update all these things.'},\n",
              " {'id': 'decb7979-c01d-4c27-8897-3ba20e4db589',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'And the question was, how do we do it in a heap, because we did not know how to do it in a heap. So, what we claim now is that you can do it in a heap in log n time. But there is still a small problem, which is when I say change 54 to 35, what I would really typically have is I will have in my graph, I will have a vertex, say I will say that this is V7, and I will say update the value of V7 from 54 to 35. So, I am talking about the values with respect to the nodes in my original graph. Now, these values have been moved around and they are living somewhere in this heap. So, the question that I have to now resolve is, how do I find that value 54 that I wanted to change in this heap. So, where, so, I have a node in my graph, and I have a distance to that node. And that distance is currently sitting in the heap in some position based on all the previous operations that have happened, now, I want to make this distance something else, I want to reduce it. How will I go into that heap and find where this distance is? because unfortunately, this heap has no other particular value, notice that in this min heap, for instance. I have values on the left, which are smaller than the values in the right but I could also have values on the right which are smaller than values on the left. So, there is no particular order, once I go after the minimum property within that within the neighborhood, there is no global property, which says where to find, so finding will become an exhaustive search. If I have to find that value, I will be in trouble. (Refer Slide Time: 13:34) So, in order to keep track of that, we have to actually maintain a kind of separate index. So, since we are talking in the context of graphs, like Dijkstra algorithm, I will assume that the original values that I am keeping track of are the vertices, and these vertices live in the heap at some positions. Now, there are n vertices, and there are n positions.'},\n",
              " {'id': 'd8a15bb9-a17a-4553-81e2-94e8c0a6beab',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, there is a one-to-one map, each vertex is in a unique position, and each position has a unique vertex. So, I want to keep track of this map, and you can think of it as a dictionary, which says, given a vertex, what is the position of the node given a node which vertex is here, so it is convenient for us that the vertices are numbered 0 to n minus 1 and the positions are also numbered 0 to n minus 1. So let me kind of represent that same heap in slightly more detail. So let us assume that the blue numbers, which are in the nodes represent the vertex number. And the black number is the distance so it is a min heap with respect to the black numbers, 29 is smaller than 38 and 31, 31 is smaller than 84 and 61, and so on, but these vertices are like this. So, I have this mapping here which says that vertex 0 is at position 7. So, if I want to update vertex 0, I will go 1 2 sorry, so remember that I will number a heap from the top 0, 1, 2, 3, 4, 5, 6, 7, 8, so these are my heap indices, which are just determined by this top to bottom left to right order. So, it says 0 is at 7, so I just have to go to heap position 7. And indeed, I find that the vertex there is 0, If I look at vertex, 6, for instance, it says its position is 6. And indeed, that position 6, I have 6. So, this is my vertex to heap, given a vertex, if I want to update vertex j, I look at this V to h dictionary, and I look up the value for key j, and it will tell me which node in the heap has that value. But then, I also need the reverse, because I need to kind of, when I exchange, I will go to a new position, and I need to know which vertex I am moving, so that I can update this thing. So, I have the reverse thing, which says, at position 3, which is here, at position 3. The vertex is 1. So, this is a heap to Vertex Map, it tells me if I am at position, k in the heap, which vertex i is stored in that position k.'},\n",
              " {'id': '366b7228-8b17-4823-a4cf-3582e55ec3e1',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, if you go through this, so if you saw, the reason I have put these blue vertex numbers on the heap is that in the normal set course of things, of course, you will only keep the distance. But here, I do not have the graph. This is a kind of imaginary example. So, I am just claiming that this was the graph, So the graph had these vertices, 0 to 8. And at some point, these were the distances, so I am just keeping the vertex names to just show you that this mapping is actually consistent. So now the problem has to be to update this. So, when I come to update, suppose supposing now Dijkstra algorithm says, change the distance for 1, which was 54, Change it to 35. So, then I know that I am going to be doing something with this entry rate. So first, I look at this vertex to heap and I find that the node 1 is at position 3. So, remember, this is position 123. So, 0123. So now I know what position I have to update. And because it is a heap, I know its parent is at position 3 minus 1 by 2, which is 1, So I know this position. So, I look at position 1, and I decide I have to swap because 35 is smaller than 38. But what is that position 1, this is where I need the other thing, I go to position 1, and I see that what is there is node 5. So, then I have to go here and look at node 5. So that is going to change. If position 1 is swapped with position 3, the where position, node 5 is? Is going to change from 1 to 3. And this is also now going to change because what is it 3 is going to become 5, and so on. So basically, after this exchange, I have to make this update in these 2, so I have to exchange the values. But exchanging the values in the heap also entails updating these entries. So, the entries which corresponded to vertex 1 and 5 get swapped from 31 to 13. And the entries corresponding to positions 1 and 3 in the heap, get swapped to vertex 1 and vertex 5 from 5 and 1.'},\n",
              " {'id': '0700f05a-2a2a-4b6c-a690-a034875e6e1f',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, every time we make a swap in my, in our heap operation, we have to update this, but this is just because they, they kind of reference each other, it is just a constant number of lookups, I go from V to H, I get the value, I go back from H to V. And then I update both the corresponding place, so I do these two swaps in this V to H and H to V array, and I am done. So, this then is how you do this Dijkstra algorithm with the heap. So first of all, you use a min heap, to keep track of the distances of the unvisited vertices. But because you have to do this update, which behaves like insert or delete min, now Dijkstra algorithm, you are always going to reduce, the value. So, it is always going to behave like insert. So, it is always going to become smaller than perhaps its parent and go up, you are never going to update a value in Dijkstra algorithm are going to become bigger. So, all of the Dijkstra algorithm updates are going to be of the type where you reduce the value and therefore you move up but still, whatever you are doing in the heap in terms of updating, you have to first find that element. And for that you need to keep track of these extra index dictionaries telling us where a vertex is in the heap and which point in the heap which vertex position in the heap points to. (Refer Slide Time: 19:40) So now that we have that under control, so now, we do have a heap-based implementation of Dijkstra algorithm with this extended version of a heap, so this extended version of a heap, First, it allows us to identify the next vertex visit in logarithmic time, because it is basically this min heap. So, I just have to do a delete min. Now updating the distance, because of the way we said the update works in the heap is log n times per neighbor. So, I have to take the vertex that has just been visited and update all its neighbors, because those are the only ones for which the distance has now changed as a result of this becoming visited.'},\n",
              " {'id': '0af28442-ce06-4473-aaab-5466b88e3b3d',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'But if I am keeping an adjacency list, as we saw before, then then I can efficiently enumerate its neighbors by just walking down the list. So, the work I will do for a given neighbor is proportional to the degree of that neighbor. So therefore, over all this, I can expect that if I update, if I visit n vertices, each time I have to choose the vertex to visit and that takes log n time. So, I take n log n time to identify the sequence in which I visit vertices. And when I visit a vertex, I have to update its neighbors, which takes log n time per neighbor. But if I add up like we have done before, for breadth first search and depth first search, if I use this degree property that I am using the adjacency list, so the total number of updates I am making is proportional to the total number of edges because I will update only across an edge in particular index are only once because from the visited site to the unvisited site, I will never go back because I will never update and unvisited neighbors. So, I will make as many updates as there are edges. So that is going to take me m log n times. So, m is the number of edges. So, n log n time for finding the vertex to visit m log n time to do the updates. So, this gives me an overall time of m plus n times log n. So, remember that m plus n is roughly the size of our graph, it is the number of edges plus the number of vertices. So, we saw earlier that when we claim that say BFS and DFS were linear time it was order m plus n. So, this is really something like the equivalent of n log n we have already seen that in when we use for instance, in Kruskals algorithm, if you use union find we got a similar complexity. So, this is more or less the best you can hope for a non-trivial algorithm, because we cannot do it in a single linear scan, but we can at least do it and then kind of n plus n times log n type of scan. (Refer Slide Time: 22:17) So, there is something else interesting that we can do with the heap.'},\n",
              " {'id': 'e975f7de-daac-4def-9bce-4f68abd97e2b',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, one part is to implement Dijkstra algorithm and get an efficient solution. But we can also sort a list using a heap. Now, you can realize that heap is somewhere in between an unsorted thing and a sorted thing because at least it is found the maximum or the minimum. So, you are not in a totally unsorted case. But you are not in a totally unsorted, you are not fully sorted case either, because the values outside the root are in arbitrary order. So, if I want to actually sort a list, what I can do is first build a heap. And as we know, building a heap takes linear time. So, I can start with an unsorted list and build a heap. And now I can repeatedly find the extreme elements. So let us assume it is a max heap, for instance, then I delete max, I have got the maximum. So, it is a bit like a clever way of doing selection sort. So, if you remember, this is what we did in the selection sorted, we scanned the entire stack of papers, which we were given by our teacher to grade, after grading, we took the maximum one and put it aside, then we went through the whole stack again, took the second maximum one and put it away. Now the thing is that instead of doing this in an arbitrary way, we first built a heap out of these papers, then delete max gives us one, but one way delete max, magically, the heap reorganize itself. So the maximum is again at the top, so I keep deleting the maximum n times. And after I deleted n times, I have extracted the elements in descending order, So therefore, I have actually found a sorted sequence corresponding to the original sequence. And how long does it take? Well, it takes me log n times for each delete maximum and n log n time I have sorted. So where do I keep the second stack? This is always a problem, so we saw that in selection sort, we would move that extreme element to the beginning and then sort from the second element onwards.'},\n",
              " {'id': 'a4694b34-8974-424e-ab2a-8ea2e4a430c2',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'In a heap, we can do the same thing except we can turn it around and move it to the end, so we can take the heap which has positions 0 to n minus 1. And remember when I have 0 to n minus 1, I think of it as 0 1, 2, then 3, 4, 5, 6, and so on. So, the logical thing we saw said when we delete an element is to delete the last element. So, the heap shrinks by one means that the positions go from 0 to n minus 1, they become 0 to n minus 2. So, this position, henceforth is outside the heap. So, this is a position that is available to me. So, the value that I just deleted can go here and it will not be touched again. So, we can store the maximum value at the end of the current heap at position n minus 1 because the heap is now lost that value. So, it only exists in the space 0 to n minus 2, now I delete from 0 to n minus 2, then the n minus 2 positions will become free. So whatever value delete max gives me, I can put it there. And I have all the elements again in a heap from 0 to n minus 3, so I can keep doing this. And in this process, I actually get an In place order n log n sort, I take a list in some sense, I heapify it in place, Go the heapify operation also just did some exchanging and put it in place, then I do a delete max and I take that value and put it to the end, I take the next value put at the end, and so on. So I have an order n log n in place sort, so remember, the only other order n log n sort that we have is merge sort. And it is not in place, because the merging operation requires us to create a new list. So in that sense, this is an improvement on merge sort. (Refer Slide Time: 25:54) So, to summarize, we have seen that you can take this heap, which has insert and delete min, delete max, depending on whether it is a min heap or a max heap. And we can extend it to have this update operation. And this update operation will also work in logarithmic time. But in order to use this update operation, we need to be able to decide which position to update.'},\n",
              " {'id': '5833d44a-0870-4311-8f9b-4845e37711c9',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 6},\n",
              "  'source': 'Using Heaps in Algorithms.pdf',\n",
              "  'content': 'So, if you need to point to the position and say update this position, we need a way of going back and forth between the values that we are keeping track of and the values in the heap. So we said that we will keep these two dictionaries, which gives us this one to one mapping between the values we are like the nodes, the distances of the vertices and the nodes in the heap. So, with this extended notion of a heap, the Dijkstra algorithm came down in complexity from n squared to m plus n log n. So, the other algorithm which is similar to Dijkstra algorithm is prims algorithm for the minimum cost spanning tree. So, we have already seen that Kruskals algorithm for minimum cost spanning tree with the union find becomes m plus n log n, Dijkstra algorithm with the heap becomes n plus n log n. prims algorithm is very similar to the Dijkstra algorithm. The only difference is that when you take the distance and update it, you are not updating the cumulative distance but the shortest edge to the tree you have built so far. So instead of taking the distance of the current vertex, and adding it to the edge cost, you are just taking the minimum of the edge cost. But otherwise, it is identical, you have to find the smallest cost edge or smallest cost unvisited vertex to add to your tree. So, you can do exactly the same thing that we did just now for Dijkstra algorithm, and implement prims algorithm using a min heap and get the same complexity of m plus n log n. So, everything is the same as for Dijkstra because essentially prims algorithm and Dijkstra algorithm have more or less the same structure, it is only the distance calculation, or the distance update, which is different. And finally, we saw that heaps can be used to sort a list in place in order n log n time. So we have an order n log n sort, which is merge sort, which requires extra space. Now we have an order n log n sort which actually sorts in place.'},\n",
              " {'id': '3755cb8f-467d-438f-9144-602802f7646c',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'Programming Data Structure and Algorithm using Python Professor. Madhavan Mukund Union-Find Data Structure When we studied minimum cost spanning trees, we saw two algorithms. The first one was Prims algorithm, which, like Dykstra’s algorithm, tried to extend a tree by finding the shortest edge connected to the current tree. And the other was Kruskal’s algorithm. So, in Kruskal’s algorithm, we built up a tree bottom up. We started with disconnected nodes, and then we connected them by adding the shortest edge possible. So, we begin by processing all the edges in ascending order, of course. So, we sort the edges, you pick up the smallest edge, and each time we pick up an edge, we see if the edge forms a cycle. So, we have a collection of components, which have already been joined together by the edges. And if the new edge does not create a cycle within a component, we add it. And in the process, what happens is that the endpoints of the edge u and v belonged earlier to different components. Because if they were in the same component, the component is connected, it would form a cycle. But having now connected these two components together, we must merge the components as we go along, so that we do not add another edge within this component. So, the main difficulty that we saw with implementing Kruskal’s algorithm efficiently was precisely this task of keeping track of where the components were at any given time. So, we know that in the beginning, it is all disjoint because it is a bottom up algorithm, everything is a singleton. But as we go along and edges are added, which edges are, which nodes are part of the same component, which ones are different components, this was the problem. So, abstractly, what we have is a collection of vertices, which are partitioned. So, this is a collection of disjoint sets, every vertex belongs to one of the partitions. And no vertex belongs to two partitions. So, the collection of partitions together gives us the full set.'},\n",
              " {'id': '3cd16e10-4a34-4aa1-95a5-c754b7e7bfbf',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'But each collection in that partition is in, is in, has an empty intersection with every other collection. And now given this partition, so, we have this kind of picture that we have a set. And it has now been divided like this, into these disjoint partitions. And now I say, I give you a vertex u and I asked which partition it belongs to? So, that is, this expression, this function called find. So, find tells us which partition a given vertex belongs to. And similarly, now when I take say u belongs to this, and say v belongs to this, then after this edge has been added. So, I have now found an edge which connects u and v. Now, these two become the same partition. So, I have to have a second operation, which says given two elements of the set of the collection with the set I am dealing with, take their partitions and make them into a single partition. So, the union operation and the find operation and what we actually saw in Kruskal’s algorithm was this, this union operation is what took us time. Because we had to find all the other vertices which are in the same collection as u, same partition as u and convert them to the same partition as v. So, what we need is a data structure, which can implement these operations. So, essentially we have a set, a set of elements which is partitioned into components. So, as we said, partition means that C1 union C2 union Ck is actually all of S. And each element, small s in S belongs to exactly one of the Cis or Cjs. So, this is what it means to be a partition. And now, we need to first create the initial partition. So, the initial partition is going to be the one where every element is on its own. So, you have a singleton partition containing just s for every small s, in your set. And then we have these two operations as we said, find and union. So, find tells us the name of the partition or the name of the component that s belongs to.'},\n",
              " {'id': '0737a7e5-7dc6-4a49-b1c7-6a179e62503c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'And union given two set elements, s and s prime tells us, gives us back a single component for all the elements in the same component as s and s prime together. (Refer Slide Time: 4:05) So, let us for simplicity assume that our elements are numbered 0 to n minus 1, this is what we typically do even for vertices when we deal with graphs. We deal it, we will typically use 0 to n minus 1 as the names of our vertex set. So, we will set up a mechanism to keep track of the components and the easiest way is to have either a list or an array or a dictionary, where now because these are 0 to n minus 1, I can interpret these as indices in the list or as keys in the dictionary or as positions in the array. So, if I look up the eighth element of component, it will tell me which component the element i belongs to. So, now what does make union find do? Make union find basically creates names for the components, which are the same as the vertices. So, I have, I have elements 0 to n minus 1 and I also have components. So, this is just for convenience, I do not want to create another set of names. So, the components are also called 0 to n minus 1. So, 0 goes to component 0, 1 goes to component 1, and so on. Because I need a name for the set. The component containing 7, the component containing 6, so I need a name for that set. And those names are going to be just the same 0 to n minus 1. So, component of the element, i will be the component i, that is what this is saying, component of the element i. So, this is the element, and this is the component number. This is how we set up our thing and make with this make union find. So, we have this, say, dictionary component. And now when I want to find out which component vertex i belongs to or a set element i belongs to, I just query this dictionary, I will look at probate with the index, or the key value i. And the, what is stored in the dictionary is the component number.'},\n",
              " {'id': 'ef75ff42-fff4-4253-bb7c-2b6fda798407',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So, initially, it is i and it will change as it goes along because of unions. So, let us see how union works. So union, as we saw is basically a sequential scan. So, you want to now take two elements, i and j, and combine their components. So, over time, they have reached some component name. So, if I look at component of i, it is some c old and c new because I am going to merge in this, I am assuming that I am going to put everything which is in component of i into the same component as j. So, I am not going to change the names of the component for the same component as j, I am going to change the name of the component for things which belong to the component of i. So, let us say component of i is old component c, and component of j is a new component c new. Now, I have to take every vertex, and this is the part that is inefficient. I have no idea which other nodes have the same component as i. So, I have to take every other vertex. And if that vertex has a component, which is the same as i, I will reset its component to be the same as j. So, this is once linear scan, which is proportional to the size of the set, which I have to do every time I do a union. It does not really matter whether the component of i was small or not. I have to look at every other vertex because I have no indirect information about which other vertices have been merged with i, so far. So, what is the component of i contain? So, with this, it is easy to see that this make union find takes order n. Because I have to go through and label the component number as i for every vertex i. So, this is just a simple linear scan across all the elements in my set. Find just looks up a dictionary. So, if we assume that this is a dictionary, or an array with a constant time lookup of the ith element, then in constant time, or O, one time, I will find the component for a given element i. But union as we can see, takes time proportional to the size of the set.'},\n",
              " {'id': '4d598ea9-d50c-4c71-b845-1c2e89151a36',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'Because I have to go through every element in the set and check if it has got to be merged or not. Whether or not it has to be merged, I have to check it. So, this is much like an adjacency matrix representation of a graph, in order to find the neighbors of a vertex, you have to scan all the columns whether or not this vertex has few neighbors, or has many neighbors, you have to scan all the columns to find the neighbor. So, in the same sense, you have no idea how many elements share a component with i, so you have to scan all of them. And wherever you find something which shares a component with i, you have to move it to j. So, this means that one union operation takes order n time. So, if I give m union operations, then it is going to take m times n time. So, this is going to be the bottleneck that we saw. Because in Kruskal’s algorithm, we have to add n edges or n minus 1 edges to make a spanning tree. So, we do order n merges, and naively speaking, if we do order n merges and each merge takes order n time, then I spent order n squared time executing all of Kruskal’s algorithm. And that is what we are trying to improve. (Refer Slide Time: 9:03) So, how do we improve this? Well, what is the bottleneck? So, the bottleneck is that we do not know, so if I am sitting here and if I have a component which contains i, I do not know what are the other vertices in this component. So, this whole component has some name c. So, I know that component of i, is C. The question is what other vertices have components c. So, which are the other vertices in the same component? So, now we keep track of an int, an dictionary going the other way. So, component tells us given a vertex, which component it belongs to. Members tells us given a component, which are the vertices which belong to it. So, members of c will be list, it will give you a list of vertices. In general initially, for example, I will have members of, of the ith component will just be the list i.'},\n",
              " {'id': 'dfe678ab-6006-4972-ab68-b9652997b1c1',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': \"Because, initially every component has only one vertex, but as it grows, it will be a longer list. And of course, now since this is a list, I can take its length. I can take the length of this members c, and I will know how many vertices belong to this component. But I do not want to actually spend time computing the length each time. So, I will separately store this as size of c. So, I am keeping track of two new dictionaries. One is members of c, members of c says give me a component and give me the list of vertices belong to this component. Size of c says, how many vertices are there, which is just the length of that list. But I do not want to compute that list explicitly each time, so I will just keep track of it because it is easy to update as we will see. Now given this, let us see how to do our implementation. So, first we have to implement this make union find initial step, where we create the singletons. So, we had this old step which we repeat, which is we for every vertex for every element i. I set the component of i to be the actual component i itself. So, remember that we were using i for both 0 to n minus 1 to name both the elements and the components. So, the component of i is just a component i. On the other hand, we have to now do the reverse map. So, we have to say what does component of i contain? Well members of i is the list containing i. And size of component i is 1. Because I have only singleton's at all point. So, this is the initialization, very straightforward. (Refer Slide Time: 11:34) What about find? Well, find does not update anything, find very looks up the component. So, it does not have to look at members or size because it just needs inform, information in the dictionary component. So, as before find of i just looks up component of i. Union is where we were having problems, so let us see if we can make union a little more efficient.\"},\n",
              " {'id': '25b918e8-a91a-4f50-8317-791412036074',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So, as before, we find out the names of the components of the two vertices are two elements we are dealing with i and j. So, we are going to as before, assume that we are moving everything in the component of i, to be in the component of j. So, we call the component of i as c old and the component of j as c new. And this difference now is that instead of going over everything, before we were doing this for loop over every element in our set, for every vertex in our set. So, we are going from 0 to n minus 1. But now I do not have to do it for every vertex, (())(12:30)I know which ones have to change. The ones that have to change are the ones that are in the same component as i. So, I just have to look at the list members of c old and for every k members of c old, I will first of all change its component. I will update its component from c old to c new. But now I have to keep track of these other things as I go along. So, I have to take this c new component and append this new element to its list. So, a k is now being added as a new element in members of c new. So, as I change the name of the component of k from c old to c new, I go to the members of C new and I add k to it. And when I add k to it, its size also changes. So, I also increment the size of c new by 1. So, these are two extra steps, which happened when I re label, when I move vertex from this component to that component I must add it to its list members and I must upgrade the size by 1. So, this is simple enough. (Refer Slide Time: 13:28) So, the question is what have we gained? Because what we have gained really is this step. Compared to our earlier implementation, this is still order n and this is still order one. So, the question is what have we gained in union? So, the point is that by going over members of c old we can get away with only working with this many elements rather than this many elements.'},\n",
              " {'id': '9b9eef1c-1ebc-48fe-9c14-f8044c03926c',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So, when I re label instead of looking at order n elements, I am looking at order of the size of the component. So, this is similar to our graph theoretic things where we say that when we look at the neighbors of a vertex, we are interested in the degree of the vertex, the actual edges rather than all the neighbors which are not (())(14:14). So, in the same spirit, we are only interested in those elements which belong to this component. We are not interested in scanning all the components, so we are explicitly keeping that in this members list. So, the question is so, this is fine, we are using members here. But why are we keeping track of the size? Because there is no apparent reason in this whole thing except that we have to update it correctly and all that. Nowhere are we using size in this union operation. We are just updating it. We are updating it for sure, but we are not using the fact that the size is available to us. So the way we will use size as follows. We will always choose whether to, so we remember we have a choice. We can either go from i to j or j to I, which component gets relabeled as which component. Do I make everything in come opponent of i point to j or make everything and component of j point to i. So, that is where size will play a role. So, size will say that we only always merge the smaller component into the bigger component. So, we have basically two components. So, if i belongs to c and j belongs to c prime, I have to decide whether to make everything in c the same as c primer, or everything in c prime. So, the final component that survives is going to be c or c prime? So, the rule is that the larger component will survive. So, if c is smaller than c prime, I will rename, re label everything reliable everything inside c to be c prime. And if c prime happens to be smaller than c, I will do the opposite. So, that is the crucial decision that size gives us. So, a priori does, does not seem to be so helpful.'},\n",
              " {'id': '4050184b-678f-4a1c-9824-4a8db7ddf821',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'Remember, our problem was that when we were doing this as range of n, blindly, we were spending order n time on every merge. But now we are spending something proportional to the size of the smaller set. But the size of the smaller set could still be almost half. It could be half plus a little bit, half minus a little bit. So, in the worst case, a single merge operation could still be order n. Because it could be taken by two steps. So, we have not really guaranteed that an individual merge operation is less expensive now, up to an order constant factor. Of course, it will be half as expensive, but half is not good enough. It is a constant factor, we want something which is a little more strong than half. So, it turns out that this we cannot deny, I mean, it could be that an individual merge will cost half n time. But it will not happen frequently. So, we have to do our accounting a little carefully. So, we are not going to change the implementation. This is our final implementation. But we are going to argue that this is good enough, when we have this rule, of always merging the smaller component to the larger component, we are actually going to get a sizable improvement. (Refer Slide Time: 16:59) So, why does this happen? Well, basically, if I take a vertex i and I track what happens to i. So, initially, its component is i. The first time it is forced to change its component, it changes its component from some c to c prime. Then from c prime to c double prime. But what I have guaranteed now is that when I take i and I look at the component it belongs to and Now supposing this is the other component, which has j and I now merge this. Then i moves from c to c prime, but c prime was at least as big as c. Because c prime was a bigger component. So, if c had some k elements, then c prime has more than k elements. So, totally now, the component of i has gone from k to something bigger than 2k. So, the size of the component of i doubles every time it is labeled.'},\n",
              " {'id': 'a936c1ce-ac41-4bf6-b6d0-f9e521ae5b50',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So basically, each time I re label the component of i, I move it to a set, which is twice as big as it was before, at least twice as big, could we could be even bigger. Now the other point to note is that when I do a merge, how many different elements change their component from the initial state. So, initially, remember, I have 0 to n minus 1, as components, 0 to n minus 1. Now at some point, I do a merge. So, maybe I take i and j. So, I take these, and then they will both go say to j. So, at this point, I have touched i and j. So, I have changed their component structure. In the case of one I have changed the component name and the other one, I have changed it to be in a component, which has new elements. Now the question is, after m such things, after m union operations, how many elements have had their component touched in this way, either it has moved, or something has come into it? Well, you can see that, if I take two elements, then I had touched two. The next merge could, next union could take two other elements and touch two. So, I could do 2 plus 2 plus 2, which is 2. The first merge, first union merges these two, next union merges these two. But I cannot do any more than that, I claim. Because if I do not take two new elements, then I must be taking some set which has already been touched and one more element. So, at most, I can touch 2m elements. So, this means that at most 2m elements have had their component updated since the beginning. But if at most 2m elements have had their components updated, the worst that can happen is that all of them are merged into a single component. I cannot have a larger component than that. So, the largest component of any vertex in my set is at most 2 times m. So, many of them will still have singletons, this is what we are saying. So, after m union find, m union operations, the largest set in my partition cannot be bigger than 2m. So, we have now these two facts.'},\n",
              " {'id': 'bd81e542-95a1-4782-9e0c-f0a31950f773',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'The fact is that each time I change the partition of i, it moves to a set, which is twice as big. But it cannot go beyond a certain size because across m operations, it can only grow up to 2m. So, combining this, we see that the size of a component will grow from 1 to 2 to 4 to 8, but it has to stop before 2m. Because it cannot get beyond 2m, because 2m is the largest set I could possibly have, after m union operations. So, that means is doubling can happen at most log m times. That means if I pick a vertex, and I examine how many times it changes its component, it can be at most log m times. So, a single vertex cannot change its component more than log m times. So, this is what gives us now a better bound for union find. (Refer Slide Time: 20:42) Because it says that basically over m updates. First of all, we know that at most, 2m elements are relabeled. And each of them is relabeled at most log m times. So, the remember our relabeling operation is now optimal, I never look at a vertex to re label unless I need to. Because I am looking only at members of c. So, every time I did decide whether to re label a vertex or not, I am going to re label it, I never look at a vertex and, and do not re label it. So, if I only need to know how many reliable operations are there, because I never look at vertices, or try to do anything with vertices in union when I am not relabeling them. But I know that a, there are at most 2m of them, because that is the limit of how many vertices ever get relabeled. And each one of them gets done at most log m times. So, if I add it up, it is like, again, it is a similar analysis. But in a more complicated setting. When we said that, when you do breadth first search or depth first search using an adjacency list, then as we process all the vertices, we do the sum of the degrees. So, an individual vertex might add a lot of neighbors to the queue in breadth first search but others will add less.'},\n",
              " {'id': 'a234f497-f012-4d11-9f42-d9861ece99f1',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'And totally the number of neighbors I have to look at is going to be the sum of the degrees and therefore it gets bounded by the total number of edges. Similarly, here, we are saying that the total number of operations of relabeling across all the M union operations is going to be m times log m. Because 2m, order of m times log m, let us say. Because 2m vertices are going to get elements are going to have the labels change, and no element is going to be touched more than log m times. So, therefore, over m union operations, I will not do more than m times log m work, in terms of changing component numbers. So, this is where we have saved. So, instead of going to m times n, we have come to m times log n. So, this means that if I have, since I am assuming that there are m of them, I can take m times log m. And I can say what is the cost of one of them? I divide it by m. So, on an average, in a sense, each of them is only taking logarithmic time. So, it is not that any one of them is actually taking logarithmic time for sure. Because I know that sometimes I have to merge a large component. But then many of the times I will be merging very small components. And if I add it up, so this is again, this notion that we saw with, as I said, with BFS and DFS, when we were dealing with adjacency lists, that we count the cost across the entire operation and say the total sum of the operation can only be so much. Even though individual, individual operations can be more. So, this is, this notion of amortized, what you gain in one place, you will lose in another place. But overall, it will all work out. So, the amortized complexity of one union operation is log m. Even though individual union operations could be more expensive, totally m log m is the cost of m such operations. (Refer Slide Time: 23:39) So, how does this affect Kruskal’s algorithm? So, this is where we started looking at these operations, because this was the context in which we needed it.'},\n",
              " {'id': '5424414f-fd8d-4eef-964f-6a2eae70672d',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So, remember that, in Kruskal’s algorithm, we start by sorting the edges. So, we take the edges, there are m of them, and we sort them in ascending order. After this, we have to start this business of creating the tree. So, we have to keep track of these components. So, we first do a make union find, which will take order n time for our n vertices. So, this is going to be ordered n time. Now, when I check whether a vertex needs to be added or not, I need to look up the component name of the two endpoints. But this is order 1 time because I just have to look up my component (())(24:27). And now this is the step which was bothering us. So, earlier this was taking order n time when we did it naively. But now we know in this amortize sense is going to take log m time. So, how many union operations are we going to actually perform? Well, because they are constructing a spanning tree on n vertices and a spanning tree on n vertices remember will have n minus 1 edges exactly. We are going to do exactly n minus 1 union operation. So, we can think of it as the order n union operations. So, we saw earlier that if we do order m, if you do m operations is m log n. So, here it is n operation, so it is n log n. So, the total cost of all these union operations across each addition that we actually perform, remember that if find u turns out to be find v we just throw away the edge, so we do not do anything. So, only when find u is not equal to find v, when they are in two different components, we have to do a merge. But now what we are saying is that this merge, will actually happen overall, the overall cost of these merges, they will be n of them will be n log n. So, this is the amortized cost over all of the union operations. So, now, if we go back to the first step, this is another expensive step. This takes m log m time using an optimal sorting algorithm. So, we have, we have seen that, things like merge sort, for instance, are m log m.'},\n",
              " {'id': '36b4eb2a-f5b4-4064-8a8a-aa0b9bf1d7db',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'And a naive algorithm like insertion sort of selection sort could be n squared. But you cannot beat m log m, we said without giving a proof saying that that is a lower bound. So, we need to spend m log m time to sort. So, we need m log m time to sort and n log n time to do the unions. Now, this is kind of bit awkward to have log m here and log n there. But notice that m is at most n squared. So, if I take logs on both sides, I will say log m is less than equal to 2 of log n. So, this is how logs work. So, log of m is actually big O of log of n, it is some constant times log of n. So, I can say that m log m is the same as m log n. So, now I have the same thing on both sides. Now, I am in good shape, I have n log n and I have m log n. (Refer Slide Time: 26:37) So, I can now combine them and claim that the overall time of Kruskal’s algorithm is m plus n times log n. So, remember that this really is the size of of G. So, we are really saying that in terms of the size of G, it is N log N. I mean, if you think of capital N as the size of G, it is within capital N log N. And this is probably the best that one could hope to do given that you have to do sorting and all that. So, this is how this union find algorithm actually helps us bring Kruskal’s algorithm down from n squared to something like n log n. (Refer Slide Time: 27:10) So, what we have seen is that if we use this two way mapping, so component is the obvious thing. It tells us given an element which component it belongs to. But if we keep the reverse mapping, given a component, which are the elements which belong to it, then we can relabeled these components efficiently by also keeping track of the size, always merge a smaller component into a bigger component.'},\n",
              " {'id': 'c1f665bf-6ea4-4302-820d-9b7794c54d26',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'And if we just follow this rule of merging smaller components into bigger components and keep track of the components using this member array, then or member dictionary, then we can guarantee that across m operations, the amortized complexity of each union operation is log m. So, m log m is the total cost of all the unions, find remains constant time because it is just looking up a dictionary value. And make union find remains linear time because I have to actually reset the component to a singleton for everybody. So, this is a very direct and easy way of doing union find, there is a slightly more complicated way where instead of keeping this members of k as a list of vertices, you can keep it as a tree. And if we keep it as a tree, then it turns out that union operation becomes very simple. So, basically, I will have members of i. So, this will be some tree. And I will have members of say j and this will be some other tree. And the rule is that if you belong to the same tree or the same component, so the union operation will just either connect this tree that way, or connect this tree this way, so it will just take the root of one tree and put it as a child of the root of the other tree. So, the union operation is very simple, I just stick one tree under the other tree. But now find means, I am sitting here and I must go up to the root, the root vertex will tell me which component I am in. So, we have to be careful that these components, these trees do not become very tall. And there is a clever way of keeping them not only from growing very tall, but actually flattening them. There is something called path compression that you can do. And if you do this path compression, eventually your tree will look like this, where everything so if I have a component and I have all the other vertices which belong to the component will be only one step below. So, it will only take me one lookup in order to tell you what component is.'},\n",
              " {'id': '8992ca29-cfdf-424a-b59a-aa6dfdaab749',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 6},\n",
              "  'source': 'Union-Find Data Structure.pdf',\n",
              "  'content': 'So, it will have an amortized complexity of almost constant time. So, both union and find become so union is constant and find is almost constant. So, this is really the ultimate implementation that you could have for this where you can keep track of this and then each operation is a unit cost. But we will not go into that now. But you can look it up. It is a very interesting optimization of this union find data structure.'},\n",
              " {'id': 'e500c71d-2f07-442e-b358-d5029e5c9b17',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Search Trees So, we have seen that heaps are binary trees which help us maintain a priority queue. (Refer Slide Time: 00:17) So, let us look at another problem where we can use trees. The problem where we have dynamic data which we want to keep sorted. So, remember that sorting is a very important step for us because once we sought some information, it is very easy to search it we can use binary search, but the problem typically is that the data that we are trying to search changes dynamically. So, the example that we gave right at the beginning to motivate all this was this question about SIM cards and Adhar cards. But neither of these are static people are buying SIM cards every day and people are getting new people are born and acquiring Adhar cards every day. So, the list of Adhar cards keeps changing. And the list of SIM cards keep changing. And of course, some SIM cards die also because they get decommissioned for various reasons and people die. So, their Adhar cards are no longer important to keep in the database. So, we have periodically inserted items and deleted items from this information that we are trying to keep sorted. And the problem is if we try to naively keep it sorted by just inserting and deleting in that sorted sequence, then each of these inserts and deletes will take linear time because we saw that you have to depending on which representation you use, either you spend order and time searching for the value to delete or you spend order and time compressing or expanding the space. So, whichever way insert and delete in a sorted list cannot be done in less than linear time. So, this is a problem then if you have dynamic data which needs to be kept sorted for you to search efficiently you need a better data structure and what we will see is that the best way to do it is to move to two dimensions.'},\n",
              " {'id': '6e88dd9c-2df5-4d97-97f9-23c2cf88f68c',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, just like we argued that if you keep a priority queue as a sorted or an unsorted list you will end up with this quadratic cost. So, same way if you try to do this dynamically sorted data with a linear representation you cannot avoid this order and cost of maintaining the sorted property. So, we will move to a two dimensional tree a binary tree that we saw with the heap but with a slightly different structure. (Refer Slide Time: 02:18)  So, what we are going to use is now called a binary search tree. So, like in a heap, every node will have a value and it is a binary tree so a node can have at most two children and you could have situations like this where there is no left child. So, you could have only a right child so whether it is a left child or a right child is important. Now in a heap the property that we had was a local property with respect to the values of the parent and its two children. So, either in a min heap you must be smaller than your children or in a max heap you must be bigger than your children. But there is no for instance property in a heap about the relative values of a left and a right child. So, if I take a heap and I exchange the two children of the root, it is still a heap because the properties only talk about above and below it does not talk about left and right a binary search tree talks about left and right also. So, in a binary search tree everything which is to the left of a node must be smaller in value. So, remember the values are all stored in the node. So, all the values which are stored to the left subtree of a node must be smaller. So, everything that is to the left of five must be smaller than 5. So, in this case is 1 2 and 4 are all the values to the left. So, all of these must be smaller. Now this is a property which occurs uniformly. So, it is not only a property of the root node. So, similarly, if I look at this node, then all the values that lie to the left of it must be smaller.'},\n",
              " {'id': 'd78be11a-5ad5-4000-a0ca-e11af61fa8eb',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'Now notice that there may be a node like this where there is nothing to the left. So, the only two nodes in this tree actually which have things to the left are 5 and 2. So, 5 has everything below it and 2 again has everything below. So, this is the left side so logically the right side will be all the values that are bigger. So, the right side will have so with respect to 5 everything here is bigger. (Refer Slide Time: 04:09) And now with respect to 8, this is bigger but also with respect to 2, 4 is bigger. So, at every node we have the property that all the values on the left are smaller all the values in the right are bigger. And this is what is called a binary search tree. Now another important property to make this whole thing work is that we never keep two copies of a value. So, in a binary search tree, either a value is there or it is not there we do not keep multiple values. So, in a sorted list in general, we allow duplicates to happen, but we are always keeping now the sequence of the, these values that we want to maintain in this so called sorted order, we are keeping them without duplicates. So, that is one of the constraints. So, this is a binary search tree. (Refer Slide Time: 04:54) So, now we need to describe this binary search tree in programming notation. And unfortunately, it is not as straightforward as we saw with the heap where we had a very clear structure, at the heap, we had a root and then its children. So, we had this level by level structure. And we have said that we could always guarantee that you do not have. So, this kind of a hole is not allowed in a heap. I must put a left child before a right child. And therefore, you can guarantee that the nodes can be enumerated, from top to bottom left to right in a fixed order. And then given the index, I can find its position. But here, there is no ambiguity because it is 0 1 2 3 4 5. Now in a heap, this is 5. But here, I do not know whether 5 is a left child or the right child.'},\n",
              " {'id': '9333c53d-a4a5-4e55-a5f9-391030c22a34',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, I must, unfortunately, keep an explicit representation of this tree. So, this is similar to what we did when we did this flexible list the linked listed, we had a node, and then we said this node had a value, and it would have something which points to the next node. So, we had then a collection of these nodes, each pointing to the next node, and you string them together, and you get a list. So, similarly, we will keep these tree nodes as a collection of nodes, each pointing to its children. So, this is the kind of structure that we will have. So, we will have this structure for the root node, and each node will have now three parts. So, in a linked list, we had two parts, so we had a value, and then we had a pointer to the next node, and then we had a value, then a pointer to the next node, and so on. Because there is only one direction, it is a linear structure. Now, this is a non-linear structure. So, it is a kind of two dimensional structure. So, we have not one value, but two values, the left child and the right child. So, we have a value, a left child and a right child. So, the left child obviously points to the left the node with containing the next value, so 5 the left child is 2, so the left child of 5 points to the node whose value is 2. Likewise, right child points to the node whose value is 8. Now 8 has no left child. So, we for now in just in this representation, we will have to find a way to write it in Python, but in this representation, we are, I am just using a dash to indicate that there is empty value. There is nothing there. So, there is no left child. And it points to a node 9 which has neither left hand or right child. So, remember, this is a leaf node, a leaf node is one which is the bottom of the tree, it has no children. So, it has nothing on its left nothing on its right. So, similarly now this 2 has 1 and 4. So, it points to 1 on his left, 4 on its right, both of which are leaf nodes.'},\n",
              " {'id': '8c5f584b-269e-4003-8b20-3710e16a96dc',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, this is the pictorial representation of how we would like to describe this thing in Python. Now before we actually get to the Python representation, I am going to make a slight complication of this, what will look like a complication. But what I am going to do is I am going to say that, I do not want to keep these kinds of empty pointers in the middle of a tree, I do not want this kind of thing in the middle of a tree. So, I am going to actually create a extra layer of empty nodes at the bottom of the tree. So, I am going to add this frontier, with empty nodes, where every field is this blank value. So, it will have no value, no left no, right. And all these so called leaves will point to such empty nodes. So, this will actually point to one empty node, this will actually point to an empty node. So, everywhere, I will append one extra empty node, wherever currently, I do not have a pointer. And in particular, I if I have an empty node right at the top. So, this is just like our empty list, if you remember, our empty list had no value and no next value was none. Next was none. So, in the same way, here, if the value is none, and the left is none, and the right is none, then it is empty. (Refer Slide Time: 08:33) So, this is how it will look if we draw this as a picture. So, wherever earlier we had a leaf, I now create this empty node with all three values empty, and make it point to that. So, I create this kind of, so these are all these frontier nodes. So, at the bottom of my tree, now, I have this layer of empty nodes, kind of hanging off the, the real leaves in some sense. So, why are we doing this? Well, it is just that it is kind of easier to figure out whether we are at a leaf or not by using this so it is easier to write. It is so the most natural way. So, we saw already when we did lists that this is a kind of inductive structure. So, I have this node and I have the rest of the list.'},\n",
              " {'id': '9109b05d-753c-4807-85c9-56a81d93f6ec',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, many things that we did in a list in insert, delete one most naturally written inductively. Either I process it here, or I push the processing to the rest of the list. Now in a tree, it is even more important to do that because I have to do it on the left or the right now the binary search tree will tell me whether to go left or right as we will see now, but recursion is the best way to do it. And in order for the recursion to stop properly at the leaves, it is useful to have this this is not the only way to do this, but this is one convenient way to do it. (Refer Slide Time: 09:44) So, now that is the picture. So, as we saw in in the linked list thing, we have to create these nodes by using a class. So, we have a class tree. And when we create a node in the constructor we have three values. Now we have the value, self dot value, self dot left, self dot right. So, when I initially create a node in the tree, it is going to be like a leaf node. So, it may have a value, but it will have no left no right. So, this is the way that it works now. So, if so we are going to use first of all this Python value none, which represents no value for this arbitrary empty value, hyphen, which we had been writing in the picture. So, if there is a leaf node, then it should point we said to two empty trees. So, this is now what a leaf node looks like. It is not a single node. So, if I have an initial value, which is passed to this tree constructor, if there is a value, then I must set the left and the right to be an empty tree, which I get by calling the tree constructor with no value. And what do I get when I call a tree consecrator with no value, I get self dot value none self dot left none self dot right none. So, that is this case. So, if I create an empty node, then everything is none. If I create a leaf node, then I put a value and I recursively, create two empty nodes below this and point to them. So, that is the constructor. So, this is reflected now in these two functions.'},\n",
              " {'id': '54f4d4e3-056d-48e1-8d5f-3c8d92f6d936',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, what does is empty say? Well is empty says that my value is none. So, if the value is none that I do not even have to look at the rest. The only situation where the node value is none is when I have an empty node. But if I have a leaf node, then it must be the case that the value is not none. And the left and the right are empty. Now technically speaking, is empty will not be the same as none, because is empty will require a node. So, I actually do not need to do this, I can just say that if the left is empty, and the right is empty, then this is a leaf node, because then the value must be something. But just to be explicit, let us say that the value is not none, left is empty, right is empty. So, this is our basic building blocks. So, this gives us this building block that we were seeing earlier, which had a value, a left and a right. Now we are going to use this building block to write these functions, which will allow us to navigate the tree and do interesting things with it. (Refer Slide Time: 12:23) So, the first thing that we will do before we do anything interesting with the tree is to just examine it. So, we want to see what are all the values in the tree. So, we need a representation of the tree. So, we need to list out the values which are present in the tree. Now, this tree is supposed to represent somehow dynamically sorted data. So, it would be nice if we could get the output of this representation in this sorted thing rather than for example, in a heap enumerating it left to right from top to bottom because that would not be in some very meaningful order. So, what does it mean to enumerate in sorted order? Well, I should enumerate before the root remember that I have the root and then I have these two trees hanging off it. So, everything here is less than V, everything here is bigger than V. So, I should enumerate this first, this second and this third.'},\n",
              " {'id': 'c66e9780-ca53-408c-b8c3-baa4e9a001f7',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, I should list out so I want to create a list of values, which represents all the values in my tree, so I should first list out this thing then insert this value and then list out this thing. So that is what this code is saying here it is saying that if the tree is empty, then return the empty list otherwise recursively apply this is called in order traversal because as you can imagine, in order means sorted order. So, in order means do the left first then the root then the right. So, take self in order. So, that will produce a list from all my left children all the trees all the values below the left child, then pick up the value here a singleton and then recursively pick up the value on the right and put it all together to be a list. So, in particular, now, we can take this list and remember in this class definition if you define this underscore underscore str. This function will be called whenever you call the print function. So, the print function will just return this list it will just return the string version of this list created by applying in order to the current object. So, this is the kind of byproduct of having this thing. So, let us see how it works. So, supposing this is my tree. So, it says if I want to print this I must first print this. So, if I want to print this, or list this I must first list this and this as if I have to first list this I have to first list its left child, its left child is empty. So, there this left.in order, so I will get an empty list from here and I will get an empty list from here. So, I will initially get empty plus 1 plus empty. So, this will produce the list 1 here. Similarly, if I come here now, this will produce a list 4. So, if I come here it will be 1 plus 2, plus 4. That is what this is saying you take the list from the left, add the list containing the current value and take the list coming from the right. So, therefore I get 1 2 4. Similarly, here, you will see that this will give me a 9.'},\n",
              " {'id': '047ce170-ab58-4ba4-a3be-5e154548e2fc',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': \"So, this will give me empty plus 8 plus 9. So, this will give me 8 comma 9. And finally, this is going to give me 1 2 4 plus 5, plus 8 comma 9. So, this is going to give me 1 2 4 5 8 9. So, that is how this recursive thing will proceed. But abstractly, it is easy to imagine, you, you should always think about recursive functions in terms of some kind of delegation, you should not think deeply about how it is being done. But just imagine by magic that somebody has produced the correct answer for the left and somebody else's produce the correct answer from the right, then how do you put it together. And because it is well founded that in the base case, you will get the correct value. automatically. If you combine the values that come from the left and right correctly, you will pass the correct value up. So, that is how you should think about it. So, you should not think about the mechanics of how I described it. Now, you should just assume that, the values on my left that are coming, so you should just be at the top level. You say, assuming the thing works correctly, on the left, the left is going to produce 1 2 4. Assuming it works correctly, on the right, the right is going to produce 8, 9. So, what should I do to produce the correct output now, and that is to put 5 in between these two. So, that is this step. So, that is how in order traversal works. Now, there are other contexts in which you might want a different traversal. So, you could enumerate this in, in different ways. You can, there is this node, left tree and the right tree. So, there are three, three things to be listed out. So, the in order traversal, is do two first, then one, then three. First do the left, then do this then to the right. So, this is in order. Now you could sometimes do the left, then the right and then at the end, put out this thing. So, this is called post order. So, post order refers to the fact that the current node is listed last.\"},\n",
              " {'id': '8e94f33a-6d6f-486f-bade-937664bd7d6e',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'And symmetrically, I could do the current node first, and then do the left then do the right. And these are called pre orders. So, these are three. So, in terms of the code, it just means, you know, shuffling this return statement, the plus gets reordered, I do self dot value, I put it at the beginning or the middle or the end, I will get pre order, in order, post order. And they have different connotations. You will not because of this left, right thing, you will usually not put 3 before 2, you could have 6, three more orderings, in which 3 comes before 2. But those are usually not interesting. (Refer Slide Time: 18:21) So, now that we know how to list out the values in a tree, what is our basic goal, our basic goal, having sorted the sequence was to find a value. So, how do I find a value in a search tree? Well, I started the route, I check if that is the value that I am looking for. But if it is not, the value tells me what to do? If the value is smaller than the route, I must look on the left. If the value is bigger than the route, I must look on the right. So, this is exactly what we do with binary search. So, replace the midpoint in binary search by the root of the search tree, the root of the search tree functions, so at every point, the root of the tree, you are looking at functions like the midpoint of the binary search that you were doing earlier, and it tells you whether to go left or to go right. So again, if we try this here, and we say search for say, 9, then I will come here and I will look for 9, and it will say no, no, I must go right then I look for 9 here it will say no, no, I must go right and I find it. As before, supposing I say, I want to look for 3. So, then I will say look for 3, then it will say smaller look for 3. It will say it is bigger. I will come to 4 and it is smaller. So, I will look below 4 and I will find nothing. And I will say no.'},\n",
              " {'id': '7ba1d535-c7b0-4fd5-970a-80b797872d93',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, one of the properties that we said about binary search is that in logarithmic time, it will also find out that the value is not there. So, here also, by just navigating down this one path of the tree, I am able to tell you the 3 is not there. I do not even look at the right hand side in this case. So, in that sense is very similar to binary search. Except of course we will have to guarantee that the complexity works out. Which, you will come to later on. So, the code is very simple, it says, if the tree is empty, that is I have fallen off the tree in some sense it is the same in binary search as my interval is shrunk to the empty list. If the tree is empty, then I have not found it. So, this is false. So, now this is why it is useful to have that frontier because if I go from the leaf, I can still take a step, If I did not have this frontier I will have to check whether my left child exists whether my right child existence and so on. If since I have the frontier, I can wait till I fall into the frontier and check it I am there by saying, am I in an empty node? So, that is why the frontier is useful. So, I can check that if I have reached the frontier, if the current node I am searching for is actually empty, then I am done. Otherwise, if the current value is the correct value, I return true. And if it is not, then I search left or right, depending on how the value stands with respect to self dot value. So, this is very straightforward. (Refer Slide Time: 20:59) So, where is the largest node in a tree, in such a search tree? Well, the largest node will be on the right, and the smallest node will be on the left in general. And how far right how far left? Well, I must keep going left. As long as I can keep going left, I am going to a smaller thing. The moment I cannot go left anymore, I have reached the smallest thing that left most node could have children, but those will be bigger. And we will see an example in a minute in the same (())(21:26).'},\n",
              " {'id': 'cff88ea4-e5f7-4be3-b350-ee9479524d7a',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, let us look at this tree for example. So, if I go left, and then left, I cannot go left again. Because there is no left here. I can still go right. But anything to the right of this node in this direction is going to be bigger, so I must not go there. So, the leftmost node is actually 1. And 1 is indeed by inspection. You can check the smallest node. Same thing, I start here I go, right I go right. And I stopped when I cannot go right anymore. So, the largest node is 9 because after that, I can still go to left, but that left does not give me anything. So, the minimum value just says if the left is empty, then I am at the minimum, otherwise recursively find so this should be minimum. Otherwise recursively, find the minimum value on my left. And similarly, the maximum value says if the right is empty, then I found it. Otherwise recursively go to the right and continue searching for the maximum value below it. So, the minimum and the maximum value we can find in this easy way. (Refer Slide Time: 22:28)  So, now let us try to insert a value. So, how do you insert a value? Well, essentially, the position where you have to insert is determined by what values are already there. So, the best way to find the position to insert a value is to actually try to locate the value v. Because if I follow the current values, it will take me left, right, left, right. It is the same actually in binary search. In binary search if you wanted to know where to insert a value that is not there, you would find it and you will find the interval of two elements between which it should go. So, this exactly the same thing that happens so you try to find it, and then insert where the find fields. So, for example, supposing I have this tree, and I want to insert 21. So, to insert 21, I first try to see whether I can find 21. So, I look. So, I start with 21, and it says no 21 must be to the left, 21 must be to the left, 21 must be to the right. And now I find that at 28.'},\n",
              " {'id': 'c0df62cf-f421-4670-be93-8676859f55db',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'I run out of things to look at. So, I basically locate the place where 21 has to be inserted. And now with respect to this node, what can I do, I have to basically put it somewhere below this. And since 21 is smaller than 28, the only way to insert it is to add a new node to the left and call it 21. So, whenever I insert of course, I have to create a node. So, in the heap, if you remember, insert was fixed by the structure of the heap, the heap had only one fixed structure. So, I knew exactly where to put the node, then I had to fix the heap property. Here, I have to put the insert where it is consistent with all the other values. So, I have to first navigate the tree to find out where it should be and put it there. So, for instance, similarly if I do 65, so now 65 will say it must come to the right of 52. And now when I come to 74, it says it must be to the left of 74, but there is no node there. So, therefore it goes here. So, I create this note here. And remember that we do not have duplicates. So, if I try to insert a value that is already there, like 91, then I will first search for it, and I actually find it and since I found it, nothing happens, the tree does not change. So, the insert has no effect, it does not throw an error, but it does not do anything. So, that is what this code is saying. So, here at the base case, if I have nothing in my tree if my trees empty, then I will create a singleton leaf by setting the value to the new node new value that I want, and replacing the none and none by two empty trees. So, this goes from this node which looked like this, to a node which looks like this. So, remember that this is what is happening. So, whenever I promote a node to a leaf, I have to create its children which are empty. If I actually find the value, I do nothing I return. And now the rest is very simple. If the value that I want to insert is smaller than my current position, I try to insert it on my left.'},\n",
              " {'id': '57251433-17a4-4d5f-a80d-a35efb3441a3',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'If it is bigger, I try to insert on the right, so this is very straightforward. So, this is the reason why it is so important to make these things recursive and come to this base case. So, basically, what this is saying is that when I come here, and I want to insert something, I am at an empty node. And I will replace it by this, so I will put a V here and I will create two more children. (Refer Slide Time: 25:50) What about deleting a value? So, deleting a value is obviously more problematic, because it is not going to just attach itself, it is going to create a hole somewhere in my tree. So, if V is, so the first of all, you have to decide what it means to delete a value. So, it means that if the value V is there delete it, if there is no V, then forget it. So, first, we have to find it. If we do not find it, it is fine. So, this is the first thing saying that if and the process of finding. So, implicitly, all our things behave by navigating down the tree. So, in the process of finding V, if I do not find it, if I reach the empty tree, that is I reach a leaf node, and I try to go off the leaf into the frontier, then I just returned in this case quietly doing nothing. Now, the easy cases are the recursive cases, if this is not the node to delete, if this is not the node to delete, then I must delete to my left, so I just recursively call delete on my left. Similarly, if it is bigger, I call it on the right. So, the interesting case that have to handle here, so if you remember the in the insert, if I found the value, nothing had to be done here, if I find the value, something has to be done, if I do not find, then it is easy. It is just recursion, I go left or I go right. If I find it, now I have to do something. So, the easy case is that this is actually a leaf node. So, if it is actually a leaf node I have to do the reverse of what I did before, so earlier, I created a leaf node, now I have this leaf node, which I want to delete.'},\n",
              " {'id': '38ab5896-23b3-4f4e-9c36-6e9ae87b0742',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, what I have to do is I got the replace this by an empty node. So, I have to delete this, delete this and replace it by this, that is what this make empty does. So, if I have the current value is V, and it is a leaf node, then I just make the current node into an empty node, and it will be legitimately an empty child of the parent. If it has only one, so if this is my node, and I have no left. So, I have something like this, I have nothing on my left, then what I can do is I can just push this whole thing up there, so that we will see. So, I will copy my right sub-tree here, I will just promote the entire right sub tree. Symmetrically, if the right is empty, I will copy the entire left sub-tree. So, these are again, easy cases. The difficult cases that have something on both sides. So, I have a value v, and I have something hanging off here. And I have something hanging off here. And of course, this is coming from somewhere from the top, so it is hanging in the middle of the tree somewhere. So, this is like our delete max problem that we had with the heap. So, I delete this value. So, it goes out. What should I put there? So, this is one solution is to take the biggest value here and put it here. Now remember that, if I have say V1 to Vk here, and I have some V prime 1 to V prime l here and I had this V, then I know that V1 to Vk are all smaller than V. And this is bigger than all, I mean smaller than all V1 prime to VL prime. So, everything on the left was smaller than V, and everything on the right is bigger than V. So, logically, I should take the biggest thing here and move it here. So, it will be still bigger than everything on the left. And it will still remain smaller than everything on the right. So, I must find the maximum value here. And move it there and then delete that from that tree. So, this is what it is saying.'},\n",
              " {'id': '2130728d-ade7-425a-812c-68e89ad5f2df',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'It is saying replace self dot value by the maximum value on my left and then delete on my left the maximum value because I know it is there because I just copied it. So, it must be there. So, go and delete it and there is only one copy of it. So, delete that copy. Now it looks like you are postponing the problem by creating one more delete, this delete generating one more delete. So, how is it going to work? Well, notice that the maximum value has this property. So, it goes right right right and then finally either it has no children or it has only left child. So, the deleting the maximum value will come into this case, we will come to the simple case where I just have to promote it. So, I will not again have to go and look for one more maximum value below that and continue this process. So, deleting the maximum which is happening here is actually going to converge in, in the easy case where once I find that maximum value, I just have to take its left child in particular, because it will have no right children, take its left child and put it up. So, that is what this code does. So, we will come back and look at those two pieces of code that we have missed out. But let us do examples just to understand what is happening. So, the first easy case is I delete a leaf. So, I look for 65. So, I walked down here and I find 65. But 65 is a leaf. So, I am in this case, is leaf so I just make it the empty thing, and I effectively delete it from the tree. So, now 74 is pointing to an empty node. (Refer Slide Time: 31:02) Now I want to delete 74, for example. So, I come down to 74. And now I am in the case where it has no left child. So, I will just promote this tree, meaning I will just copy 91. And its entire thing up one level. So, what this will mean is I will just move that whole thing up to the next level. So, this is the easy case. So, there will be a symmetric thing if you do it when there is no right child. But let us not worry about that.'},\n",
              " {'id': 'bd0a2305-4e33-4687-945f-d2f05dddafc2',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'Let us go directly to the complicated case. So, let us suppose we want to delete 37, which is sitting in the middle of the tree. So, I come down to 37 and I find that it does not satisfy the leaf case it does not satisfy the left is empty does not satisfy the right is empty. So, I have to do this. So, first, I have to find the maximum value in the left tree. So, I have to start here and go right as far as I can. And I stop at 28, because that is the right most value I can get. So, 28 is the value, which is the maximum in my left sub-tree. Now I have to take that value and copy it here. So, I have to take this 28 and copy it. So, I do that next. (Refer Slide Time: 32:14) And then I have to go into this left sub-tree and delete this 28. So, that is what is happening here. So, I come down to 28. And now I find that I am in this situation where the right is empty. So, right is empty means I just promote this 21. And anything that is below it, that whole structure, I promoted one level up. So, I get this. So, this is how the delete works. There are three cases the leaf case, one of the children is empty case. And the non-trivial case where I have to find the maximum now this is I found the maximum, we could have dually found the minimum on the other side. So, remember that we want to take everything on the left of V, everything on the right of V and I want to replace this by something which fits between these two. So, I took the maximum from here and did it. But I could also take the minimum from here and do it. So, it is a matter of taste, but you have to do one or the other, either you take the maximum value in the left sub-tree, copy it here and delete the maximum there, take the minimum value in your right sub-tree both trees are there otherwise, you will not be in this case. Take the minimum value in your right sub-tree, copy it here and delete the minimum there. So, then you have to dually write a thing for deleting the minimum.'},\n",
              " {'id': '8ac4dca7-004f-431e-87a8-e5a854cdcabc',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': '(Refer Slide Time: 33:22) So, this is the full thing. So, the left hand side is what we already had. So, now here are these three functions, which we talked about. The first one is to make a leaf into an empty node. So, what it does is it does the reverse of creating a leaf, it takes the value makes it none. And it deletes the pointers to the empty leaf and makes the none, so it just makes all fields none. So, this is how you take a node which was not, which was a leaf node and make it empty, you remove the value and remove the left and right children. What about promotion? Well, promotion is basically saying that if I had a situation like this, supposing I have, I want to promote the left child. So, I have a left child. But I have nothing here, this is empty, then what I do is I just take this left child, and I just copy all these values here. So, that means that I effectively whatever this is pointing to is now being pointed to from here. So, I bypass that left child in some sense, I basically, this was earlier pointing to the left child, but I take this the left value and copy it into the current value, I take the left’s left and copy it into my left, and I take the left’s right. So, I have to be a bit careful here actually. So, I should do this in the reverse order. I should do self dot right first. Because notice that if I update self dot left, then this value does not make sense anymore. So, these two things actually should be exchanged, I should first copy the right thing. Because currently I am pointing like this so I must first copy this and only then copy this because if I copy this, then I am no longer have access to the right. So, these two statements should be exchanged. And if I do the symmetric thing on the right hand side, then the order is okay. Because I first copy the value from the right, then I set the left pointer to the left pointer of the right child.'},\n",
              " {'id': 'c1683ff8-5638-4405-8efc-39a3af8d2327',\n",
              "  'metadata': {'chunk_idx': 18, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'And then I set the right pointer to the right pointer of the right child, and then everything is fine. So, this is how we delete a value. (Refer Slide Time: 35:26) So, effectively, what we do in all these cases find of course, we walk down a path and try to find it and we hit the leaf we say it is not there, insert behaves like find, we try to find the place, if we find it, we do nothing. If we do not find it, then where we should have found it, we create a node. So, again, we are walking down one path and delete also, we will walk down one path to find the place to delete. So, I will typically come down to V. And now in the worst case, I will have to find the maximum below the left sub-tree. So, I will continue by taking a left edge to the left sub-tree and then taking the most path. So, in all cases, I am walking down only one path, I am never walking down multiple paths. So, all these operations are going to be in the worst case as expensive as the height of the tree. Now the danger with this is that there is no structure in the tree unlike in a heap which guarantees that this height is good. So, I could have a stupid tree which looks like 1 and then the right is 2 and the right is 3 and so on. If I just take a sorted sequence and put the smallest or the largest value as the root, then I will just get one long chain of things in one direction which represents a tree. So, technically this will be a tree but it is effectively a line, so this will have height order n. So, this is obviously not what we want, the idea when we move to two dimensions is to distribute the value so the height is small. So, we would really like to have this log n height which we argued is the case for a heap. So, in the heap, we make sure that everything has two children, except possibly the lowest level, or the level just above that.'},\n",
              " {'id': 'd54cf258-e8a8-4d86-a3b1-32ba42912c75',\n",
              "  'metadata': {'chunk_idx': 19, 'week': 6},\n",
              "  'source': 'Search Trees.pdf',\n",
              "  'content': 'So, we never have these unbalanced kind of things, but in this tree that we are constructing here, if I start with the sorted list, and I start with an empty tree and then I insert 1, then I insert 2, then I insert 3 and so on I am going to get a tree which looks like this. So, we have to make sure that we restore the balance. So, we have a balanced tree when we insert or delete from balanced tree the balance could get damaged. So, we need a quick way to restore the tree back to a balanced tree so that as we go along through a sequence of inserts and deletes, we maintain the balance and therefore we maintain this logarithmic property. So, this is what we will do next, how to maintain balance in a search tree.'},\n",
              " {'id': 'dbf363c8-7777-4b38-8967-59859b8fe9ed',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Priority Queues So, union find is one data structure which arose out of a need to optimize one of our graph algorithms, namely the minimum cost spanning tree. The other algorithm which we need to optimize in the graph thing is Prim’s algorithm and also Dijkstra algorithm. And there we had the problem that at every step in the iteration, when we were trying to extend the shortest path, or grow the smallest tree, we had to scan the vertices which are not been added and find the smallest one to add. So, this operation turned out to be expensive, we had to go through a collection of items and find the smallest one. And there was no easy way to do this, because those items were being reduced in somewhat unpredictable way. So, we will not be able to keep them in some meaningful sorted list. So, we had to keep scanning all of them to find the minimum. And this was again giving us an order n worst case per loop and giving us N squared overall. So, now what we will look at is a data structure to handle that problem. So, abstractly, although it does not seem to be immediately relevant, this is what is called a priority queue. (Refer Slide Time: 01:16) So, let us look at an independent motivation for priority queues. So, priorities happen all over the place. So, for instance, in computer in an operating system, this is something which is routinely done, which is that there are a number of tasks which are running. But if there is something very high priority, which comes then whatever, is running has to be suspended, something else has to be taken in space. So, this is done by something called a job scheduler. So, a job scheduler keeps a list of tasks which are running, and then it has to decide at each unit when it is allowed to switch tasks, which one to schedule next. So, a job scheduler maintains his list of pending jobs. Jobs are yet to be done, and each of them has a priority.'},\n",
              " {'id': '456227fe-36d9-4571-9059-13adbebd1737',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, the rule that you would typically like to guarantee because the priority is supposed to indicate some preference to do it first. So, when the processor is free, the scheduler should pick out a job, which has highest priority and schedule it for some time. So, the question is. How can you do this efficiently? Because new jobs keep coming. This is not a static, see, if it was a static thing, then you can sort it, you can sort it by priority once and for all, and then move from highest priority to lowest priority. But the problem is that this is a dynamic thing. So, as people are using the computer, they will start new tasks. And these new tasks might have higher priority than old tasks. So, new jobs may join the list at any time. So, the scheduler has to maintain this growing and shrinking list, every time it takes something out of the list, it removes a job of high priority and executes it something else may come which will disturb the priority order of the remaining things. It is a good analogy of this in a kind of human context, is when you go to say a very crowded temple. So, there are people who have bought a ticket and they are standing in line. But then every now and then a few VIPs will come who will come and disturb the queue. So, these are the high priority people and whoever is managing this process has to accommodate these people without of course upsetting the other people too much. So, this is this priority. So, it is not really a queue, you are standing in line, but your place in line could be taken by somebody who works come in with some influence some VIP privileges. So, with priority, they can overtake you. So, that is why a queue normally is you will not stand in a line, when your term comes, you get served, in a priority queue you can get jumped. Somebody can come and bypass you because they have higher priority. And the question is, if this is the desired outcome, then how do you manage it.'},\n",
              " {'id': '68b1489d-f598-4c40-aaf3-bfc381e18596',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, abstractly in a priority queue, we have to maintain a collection of items. It is not a queue in the normal sense. So, we cannot guarantee that, it is a list or a sequence, we have to maintain it in whatever way we want. But the two operations that we primarily have to support one is to of course, extract the highest priority thing. So, we will call it delete max. So delete max says find the maximum priority item in this set, and pull it out. That is the one to process next. That is the next devotee who gets to go and do darshan in the temple. This is the next job that gets scheduled on a processor. So, now it is removed from this collection. So that is the simplest thing that we can do. But this is dynamic. This is not a once and for all thing. Otherwise, you could just sort it and be done with it. So, there is also an insert operation. So, periodically new people will come for darshan and they will have their own priority. Some of them would have some local official vouching for them. Some of them will be coming from some, some international dignitary some of them will just be aam janta. So, then each person will have his own priority. So, each job will have its own priority. So, you have an insert and you have delete max, and you have to balance the two so that you do everything efficiently. That is the whole problem that we are facing with Dijkstra algorithm, that as the distances change, how do we keep track of the shortest distance. So, there is not maximum, but minimum, but it is doable as you can see it removing the smallest priority, or removing the largest priority or just the same problem, you are just reversing the comparison. (Refer Slide Time: 05:12) So, this is what we want to implement. We want to implement, delete max and insert and what we were doing in Dijkstra algorithm, was naively maintaining just this array or list of vertices with their current distances. So, this was an unsorted list.'},\n",
              " {'id': '2db56b6c-272a-445c-98a9-fba835017153',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, in an unsorted list, when we discovered a new vertex, to which we have found a path, we can just update its thing. So, it constant time we can insert a vertex into this list. But when we want to delete max, we have to scan all the unvisited vertices to find the smallest one. So, remember, max and min are dual. So, in this similar way if I want to find the maximum priority in a list, I have to go through, the max algorithm, you have set the first one to be max and you keep updating. But there is if it is not sorted, you have no guarantee where the max is, so you have to look through the whole list. So, max is order n. But insert, you can just put it anywhere, you can put it at end of the list, and you are done. So, insert is order one. So, of course, if this is a static list, as we said, you could be clever and sorted once and for all. So, if you maintain a sorted list, then delete max is easy, because the delete max is at the end. So, remember, if you are using a Python list. We saw that Python lists you can delete at the end better than deleting the beginning because they are arrays with an end pointer. So let us say we keep it in ascending order then the last element is my max, so I just pop it constant time. But if I insert it, now we go back to our insertion sort, you have to find the place to insert. So, you might have to shift a lot of elements. So, you have to scan and find and shift. So, whether you are using a random access or a list, you have to spend order n time. So, you trade off one for the other. If it is unsorted, adding is easy, but finding maximum is hard. If it is sorted, finding maximum is easy. But adding takes time. So, this muralist tells us that with kind of one-dimensional implementation, in some sense as a list or an array, we cannot avoid this. Because these are the two extremes anything, there is no other thing that you can do with a one-dimensional list in some sense. So, either you can keep it sorted or unsorted.'},\n",
              " {'id': '058ae92d-49f5-471f-8153-0a6eb7ed7531',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'And one or the other will become expensive. So, if I am thinking in terms of n elements. So, in the temple in the morning, the temple is empty n people come and n people leave, by the end of the day. So, I have to process n inserts and n delete maxes. In the worst case, I am going to end up doing n squared time, because either the insert is going to be order n time, or the delete is going to be order n time. So, we want to do better than this. So, since we have exhausted the possibilities of doing this in one dimension, using a list or an array, the natural option is to move to two dimensions. (Refer Slide Time: 07:53) So, if we move to two dimensions, then we have to remember this is why I said at the beginning that this is not necessarily a sequence, I am not claiming this sequence is a collection, it is up to you to maintain this collection in whatever way it is best for you to find the maximum and deleted. So, I am going to keep it as a matrix in some sense, as a two-dimensional grid. So, just for convenience, I am going to assume, we will come back to this at the end, I am going to assume that I know totally how many things are going to come I have some estimate that I am going to do N of these operations over a period of time. So, given that N so in this case, let us say N is 25, what I do is I keep a square, which is root N by root N. So, in case N is 25, this is five by five. So, I have a square matrix where I can keep track of these. So, remember these are all have to be identified somewhere. So, every person who is in the queue or every job that is waiting to be processed has some identity and has some priority. So, I need to be able to pick out that individual element which has the highest priority. So I need to store it somewhere. So, these are the values. So, in this case, I am collapsing the two so the value is the priority. So, 17 has a lower priority than 43, for example. So, I am just using the values themselves as the priorities in this thing.'},\n",
              " {'id': 'f16fb4c3-af70-4567-853a-26e1d491a589',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'And the priorities need not be distinct. So, I think I am not very sure if there are duplicates here, but there is no reason why priority should be distinct. So, there could be more than one highest priority thing. And in that case, delete max just says choose one. So, there is no guarantee that the priorities are distinct. (Refer Slide Time: 09:43) So, there is another pattern to this, which is that every row that I am storing increases in priority from left to right. So, there is it looks sort of random, but it is not completely random. So, in the sorted array implementation, the entire set was kept sorted. Now I am keeping these sort of sorted segments of size root N. So, the first row is sorted, second row sorted. And you can see that they are of different lengths. So, there is no particular order to the lengths of these arrays. So, I am not, filling up this thing necessarily, and keeping it filled from top to bottom. So, but in every row, it is filled from left to right. So, I will not put this 20 over here and leave a gap in the middle, but the rows are of different lengths. So, each row is sorted, and I have square root of N rows, and each row has square root of N element. So, this is my proposed two-dimensional way of storing the list or the sequence of values or the collection of values which are yet to be processed. So, now given this, I have to tell you how to do insert and delete max. (Refer Slide Time: 10:53) So, it turns out that because these rows can be of different capacities, in terms of how many elements are there, it is better for me to keep track of that separately. So, I will keep track of a separate column, which tells me how many elements are currently stored in each row of my matrix. So, remember, in this case is five by five, so the maximum that can be stored in a matrix, row is five, but I could have fewer. So, here there are five and five. But here there are only three elements here there are four elements here that are two elements.'},\n",
              " {'id': '25771f7d-2a3f-45e8-8981-7db9fe0ff6f6',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, that is what this red column is saying. So, I keep track of the size of each row. Now I want to insert. So, where would I insert? Well, I will insert it in the first place that I can insert it. So, I just walked down this, this matrix, and I tried to find the first row in which there is space. So, I insert, so I come down here, and I will find that the third row has space. So, whatever I want to insert, next we will go into the third row. And insert means in that row, I have to maintain this ascending order. So, depending on the value that I insert, its position in this row will depend on what it is. (Refer Slide Time: 12:01) So, let us do a concrete example just to illustrate so supposing we want to insert the value 15. So, what we will do is we will first look at the first row. So, how do we know that it cannot fit, we do not have to scan the row, we just have to look at the size, the size tells us that this row is full. So, I do not have to look at this row element by element I already know it is full. So, I move to the next row. Again, the size tells us that this row is full. So, I move to the next row. But I am just scanning the rows from top to bottom. (Refer Slide Time: 12:24) So, I now find that this row has space. So, since this row has space, I will if I start inserting from the left, I am guaranteed that I will be able to accommodate 15. So, I insert from the left and I accommodated it. So, now 15 is inside this grid in a place which is appropriate given its value. It is in a row in ascending order in one of the rows. And the other thing now I have to do is I have to update the size because this row used to have three elements. That is what this three is saying. But it no longer has three elements. So, when I insert into that row, I also have to update the size to be four.'},\n",
              " {'id': 'a56392f1-6e50-4f3d-b045-75d5d52957ac',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, this is my insert operation, walk down looking at the size, try to find which row the first row from the top, which has space then apply our usual insert the same insert that we use for insertion sort, insert it into the correct position and update the size. So, how much time does it take? Well, remember that this is root N. And this is root N. So, I am doing a scan from top to bottom. Remember, I have assumed that I always have space. So, I have created a large N of square that I have space for all the N things which could ever be there simultaneously. So, there is always going to be at least one row which has space, it might be the last row in which case I will scan all the rows. So, I will have to scan this entire column in order to find the first row which has space. So, that is going to take me root N time. Because I go from top to bottom in a list (())(13:52) to 10. And then having found it as we remember from insertions or to insert something in a row of size k, I will spend order k time because I may have to walk the entire list to find the place to put it in. So, this is also going to be order root N. So, I have ordered root N to find the row, order root N to insert a totally is going to take me order root N time. So, this is my insert. Now, what about delete max? (Refer Slide Time: 14:15) So, delete max. It does not take an argument it just says find a maximum limit and return it. But what do we know? Now, we have not used so far, the fact that these rows are sorted in ascending order. So, remember that when we were looking at a single list, we said the sorted list is good. Because the maximum is always at one end of the list. Well, here is not at one end of the list, but it is at one end of one of the rows. So, we know for sure that for each row the maximum is on the right most elements if I take the maximum of those is going to give me the overall maximum. So, the maximum in each row is a last element.'},\n",
              " {'id': 'bdddcd2a-db59-4a22-be30-1c9b5c405571',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'And what is the last element because remember the position of the last element is different in different rows, well this tells me. This tells me that the last element is the fifth element. This tells me the last and the fourth element and so on. So, I can go and find the last element by looking at the size of the element size of that row. So, I can quickly find out the last position in each of the rows, which has a value. And now I have to take among these so is the maximum of these maximums, the overall maximum will have to be the maximum in its row, it will be at the rightmost endpoint. And it will be the maximum among all the rightmost endpoints. So, I will have to walk down this list of these five numbers. So, these are the five numbers that now I have to walk down. So, I have to walk down this list of five numbers and find the maximum. So, I will do that. And I find that the maximum amongst these is 67 currently it is not second row. So, I have to delete it. So, I will ever take this and pull it out. So, I pull it out and then when I pull it out, I have to update this sides, the size used to be five and now I have to make it 4. So, find the maximum by looking at the rightmost elements and all the rows and having found it remove it and update the size. (Refer Slide Time: 16:06) So, how long does this take? Well again it takes order root N time because this first step of finding this maximum I need to construct this list of size root N and find a maximum minute which is a root N operation. Because I have to do one scan and then in this case, having found it the delete is a constant operation I just have to remove it from the array I have to and removing it from the array also is technically I have to return the value I do not have to remove it, I just have to logically remove it by saying that the row is one level shorter. So, do not look at the fifth element now look only up to the fourth element.'},\n",
              " {'id': '4869d3af-4b96-43f5-9062-dfe566a7ec10',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 6},\n",
              "  'source': 'Priority Queues.pdf',\n",
              "  'content': 'So, deletion of the element from the array is order one but totally delete max root N. (Refer Slide Time: 16:48) So, what we have now is that if we move from this one-dimensional list of size N to a twodimensional array of root N by root N, then we trade off the two, in that original thing one was order one either insert was constant time and delete max was linear time or insert was linear time and delete max was constant time and that was giving me in the worst-case N into N. Now instead I have made them both come close to each other. So, I have pushed 1 2 square root of N. But I brought down the other to square root of N. So, both insert and delete max are now square root of N. So, now if I do this N times, I take an empty collection and I insert into it N times and I remove from it N times then I will do N root N work for insertion, N root N work for the deletion and so, the total time is going to be N root N. So, I have come down from N squared to N square root of N. So, this is a saving from N to the power 2. I have come to N to the power 3 by 2. So, this is certainly saving. Now turns out that we can do much better than this, much better in the sense that in our asymptotic complexity sense we can come down from the square root of N to logarithmic time. So, we need a slightly more sophisticated two-dimensional structure which is a binary tree and a specific type of binary tree which we will see which is called a heap. And this heap will have so here I have this height is square root of N this binary tree height will be logarithmic. So, that will be the way this N log N comes from and all our operations will turn out to be proportional to the height of the tree. And as a result, insert will become logarithmic delete max will become logarithmic. So, over N such operations instead of doing N times root of N, I will be doing N times log of N. So, this is what we will see next.'},\n",
              " {'id': 'debf4255-e90b-4656-a6d5-37049a45fb24',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Greedy Algorithms-Minimizing Lateness (Refer Slide Time: 00:09) So, we explored one greedy algorithm in the context of scheduling. Let us look at another one in a similar type of problem. (Refer Slide Time: 00:16) So, remember that a greedy algorithm is one that makes locally optimal choices. So, we are trying to achieve some global optimum. We are trying to achieve some minimum or some maximum overall. But we proceed step by step by taking local choices. And at each local choice, we look at some local measure or some local cost or some local distance or some local time and choose something which is optimal right now and we never go back and examine this choice again. So, the real difficulty is to justify that the strategy that we use to make these local choices is actually optimal. So, in our first scheduling algorithm, which was interval scheduling, what we wanted to ensure was the maximum number of requests for this resource were satisfied. And we came up with several heuristics, which we could show were wrong by giving counter examples. And finally, we came up with a greedy strategy, which says that we should take the earliest finishing time as the next job to process. Now, when we did this, what we said was that the optimal solution need not coincide with the solution that our greedy strategy gave, because there could be different subsets of jobs which satisfy this criterion of being the total number of jobs scheduled being the maximum. So, since we cannot directly argue that the optimum solution is equal to the solution of the greedy algorithm, we had an incremental approach by which we said, okay, if we order the jobs in the sequence in which they appear in the schedule, then we can show step by step that at step K, the greedy algorithm has always stays ahead in time. So, it is always doing better than any optimum solution that you could get.'},\n",
              " {'id': '574e1586-ac9c-4d87-8ce1-1f89e15f6bbc',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': \"So, this was one strategy that we use to show that a greedy algorithm performs optimally. Now, today we will look at a different type of problem, where actually the two solutions are comparable, that is you can take the solution of the greedy algorithm, and you can argue that any optimum solution must also have a similar structure. And then we will show that we can prove optimality by taking the optimal algorithm and transforming the structure from that into the greedy algorithm so that the two have the same structure. And this transformation will not violate optimality. So, if you start with an optimal solution and you keep making transformations which preserve optimality and you eventually reached the solution of the greedy algorithm, and the greedy algorithm itself has produced an optimum solution. (Refer Slide Time: 02:49) So, as I said, this is another scheduling problem. So, in the earlier case, we were talking about a lecture room where faculty wanted to record lectures and they said they gave preferences in terms of times when they wanted to come. And we wanted to allocate the lecture rooms so that two faculty, who came to use the lecture room and had the same, had an overlapping timing could not be allocated at the same time. Here we have a different type of resource constraint. So, let's suppose we have a resource like a 3D printer. So, different people want to use this 3D printer. So, there is no fixed time that they use it that, but each person has a different type of requirement. So, they want to print some object. And this 3D printer takes different amount of time. So, each user i requires the printer for time Ti. And though they do not have a fixed time interval when they want to use it, they do have a deadline, so they want to complete and have their object ready by some deadline D of i. So, each user has a time required for that person's job T of i the user has a deadline D of i.\"},\n",
              " {'id': 'a41b87d4-56ef-4908-961d-441882062c98',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': \"Now, in the earlier scheduling problem, we said we would only allocate a subset of the users those who do not contradict each other's timings. Here on the other hand, we are going to make sure that everybody is actually allowed to use the printer. But what we want to minimize is by how much they miss their deadlines. So, each user starts at some time and because the time that the printer takes is fixed, so you have this T of i. So, the time that the user will finish is guaranteed. If they started S of i, then they will finish at F of i which is the start time plus the time taken by the task that the i-th user has for this printer. Now, our goal of course is that the i-th user should finish before the deadline. So, we would like F of i to be less than or equal to D of i. But it could be that F of i exceeds D of i in which case this user i is late. The user i has missed their deadline. So, there is a lateness for L of i, which is the finish time minus the deadline. If that finish time is after the deadline. If it is before the deadline, then this thing would be 0. We would not take it as late as being negative. So, you would only use this when the lateness is positive, when the finish time is after the deadline. So, what we want to compute is, how bad is this across all the users. So, we are not trying to add up the lateness of all the users, but rather say which user has been delayed the most. So, we want the maximum value of Li over all the users who are trying to use this resource. And our goal is to minimize this maximum. So, we are not again computing the sum of all the lateness. We are saying, take each user and see how late that user is. And we are trying to minimize this maximum. So, this is our goal. So, we want to schedule this resource allocation for this printer in such a way that we minimize the maximum lateness. (Refer Slide Time: 05:57)  So, like the earlier scheduling thing, we could come up with many heuristics which seem reasonable at first sight.\"},\n",
              " {'id': '327498f6-ff21-43c5-99c3-7c1981aedc2a',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, first thing would be to try and finish off short jobs earlier so that people do not get delayed. So, you can schedule requests in the ascending order of the expected time that it takes for that job. Remember, we know we have a prediction T of i saying that the ith job will take exactly T of i time. So, we sought the jobs in terms of this and do the shortest jobs first so that people are not unnecessarily waiting. Unfortunately, we can construct a simple counter example with this does not work. So, here it is. So, we just have two jobs. We have one single job which requires only one time unit, and we have a job which requires 10 time units. So, we will always assume that we start at time equal to 0. So, we are measuring this in some unit hours, minutes, days, it does not matter. So, at time equal to 0, if we allocate the first job, so a time equal to 0. Then at time equal to 1 it will finish. So, this will be job 1. And then at time equal to 11, the second job will finish, because it is going to take time, take 10 time units. As a result of which the second job has got delayed by 1 time unit to make the deadline because the deadline was 10. So, this is not a good allocation. But obviously you can see that if you reverse this allocation, because the first job though it takes only 1 time unit has a very relaxed deadline. So, instead, what you should do is you should allocate job 2 first and finish it on time so that there is no delay. And then in the next time unit, you can finish off the first job and it is time delays is not at all in question, because it can afford to wait till time 100. So, therefore picking the smallest time job first is not a good strategy. Here, you would like to pick the larger time job. So, this is not a good algorithm. On the other hand, you can ask, so here we have a kind of tight deadline. So, this job takes 10 units of time and it must finished by time 10.'},\n",
              " {'id': 'e5800043-2f7b-48bd-94b1-8277f63132bd',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, that means that there is not much flexibility to move this job back and forth, whereas this job has a lot of flexibility. It requires only 1 time unit, but it can finish anytime up to 100. So, you can move it up and down a lot. So, you can look at this difference how far away the deadline is compared to how long the job is going to take. And that gives you a measure of how flexible the scheduling of the job is, how much you can move it up and down. So, what we might argue is that those which are inflexible, like this one, so this one, I do not have really much, I do not have any flexibility. If I want to finish by the deadline I must start it right away. So, we can start with those requests which have the least flexibility, so the least slack time. The slack time is the amount of time that I have to play with between the deadline and the time that the job will actually take to finish. So, supposing, I give priority to jobs with smaller slack time, then would this work. So, again as you might expect, we can come up with a counter example which is sort of similar to the earlier one, except we move this deadline down to 2. So, now let us see what happens. So, if we take the smaller slack time then we do this first. So, then we have, this thing is delayed by, is finishes in time 10. So, this is job number 2 finishes at time 10. And then job number 1 finishes at time 11. But with respect to the deadline of this job, the deadline was 2 and it finished at 11. So, the cost is actually 9. For job 1, the cost turns out to be 9 in this case. On the other hand, if you did it the other way that is you pick the job which had some flexibility first, then you would finish the first job in time, at time 1. So, it would not incur any penalty. And then you take up the second job and it will finish at time 11. But now the deadline has only been violated by 1. So, the lateness of job 2 is only 1.'},\n",
              " {'id': 'c7969148-450d-409f-ab0f-78f3811428ac',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, by taking it in the order, where you take the one with more slack time earlier, you actually incur a smaller penalty in terms of lateness because the deadlines were later. So, neither of these strategies actually works. And in both of these cases, we have come up with specific counter examples. Now, the reason to go through these kinds of arguments is to show that it is very tempting to come up with greedy strategies with sound plausible. But when you come up with a greedy strategy, as we have seen, what you need to do is justify that it works. And that is the whole purpose of this exercise. So, let us look at a third strategy. So, the third strategy says, okay, instead of asking which jobs can finish first or which ones are most flexible, which jobs take the least time or which jobs are most flexible, let us ask which jobs are the most urgent, so which jobs have the earliest deadlines, which are the ones that we must do first. And it turns out that this actually works. So, we actually schedule the jobs in the order of their deadlines. So, when we do it in order of deadlines, we are ignoring how much time it takes for that job. We are also ignoring how much slack we have between them. We are just looking at when the job should be finished. Now, if you think about the way that, if you have a lot of commitments, this is one way to process your list of to do things, which are the things which I have to do by tomorrow, which are the things I have to do by day after tomorrow, and so on. So, it is sort of natural. But why is this more or less natural and the other is not clear and why should it work? Well, it does work. And our goal is to prove that it works. (Refer Slide Time: 11:34) So, what we are going to do is a very simple. We are going to take all the requests that have come. These are print jobs on this 3D printer. So, we call them jobs or requests. So, we take all these jobs and we will now sort them by their deadline.'},\n",
              " {'id': '050b1f32-e17d-46f4-aed5-07798125b279',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, remember D of i is the deadline, T of i is a time it takes. So, we sort them by the deadline. So, that the numbering is such that the job is 1 to n. Job 1 has the earliest deadline, job 2 has the next deadline. Now, you might be multiple jobs with the same deadline. We do not rule that out. So, all we know is that D1 is less than or equal to D2 is less than or equal to D3, and so on. It is possible that they all have the same deadline. It is possible they all have distinct deadlines, but they are arranged in increasing order of deadlines. Now, once we do this, the algorithm is very simple. The schedule is just, because we are going to schedule everybody. Remember, it is not like the earlier interval scheduling case. We have to pick out something and then we have to remove all the conflicting things. Remember, that if we allocated person i to take the room, then anybody who’s slot conflicted with i had to be removed from the set of possibilities. Here, there is no such thing. Everything is possible. It is just a question of delay. So, we just, once we sort this in terms of deadlines, we are just going to allocate it in that same order. So, once we have got this sort, we have got the allocation. It is just 1 to n. So, in this allocation job 1 that is the one with the earliest deadline starts at time 0 and it goes on to time T1. T1 is the amount of time job 1 takes. So, S2 the second job starts at the time that the first job finishes which is T1. So, S2 starts at F1 and then F2, the time when the second job finishes is F1 when it starts, sorry S2 when it starts plus the time it takes S2 plus T2. Similarly, if I have finished Fk then the next one will start at Sk will be equal to Fk and then it will be Sk plus Tk. So, in general, the last job Sn will start when the n minus job is, n minus 1th job is finished. And it will finish at time Fn which is Sn plus Tn. So, this is a simple algorithm. But what we have to justify is that this way of doing it works.'},\n",
              " {'id': '8fc6e695-4356-41f8-aeb1-2f043df71659',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, one thing to observe is that when we do it this way, we are not having any gaps. There is no idle time. There is no time when the printer is not being used, because as soon as the first job finishes, we start the second job, as soon as the second job finishes, we start the third job and so on. So, a printer is being used continuously from 0 to F of n. And the claim is that this is in general a good strategy. That is there will always be an optimal schedule in which there is no idle time. Now, why will there be an optimum schedule in which there is idle time. Well, you could have very large delays. So, supposing I have two jobs, each of which take only time 1 and each of them has a deadline of 100, then I have many different ways in which I can schedule them. I can wait and put them after time 99 for the one at 98 and one and 99. And I will still finish within the deadline. Remember that if we finish within the deadline, we do not count the lateness. So, we are not asking anything about jobs was finished ahead of the deadline. So, there could be optimal schedules with idle time. But we are saying that there will always be an optimum schedule without idle time. And that is very easy, because what happens if you have idle time is that you have your job here and at some point, this is your job allocation and there is a period in between when nobody is using the printer. Well, if I just take this entire segment and move it to the left, then obviously everybody will finish earlier than they had finished before. So, therefore, if they were late, they will be less late. If they were not late, they will not be late now as well. So, it can only reduce lateness. So, so compressing a schedule with gaps into a schedule without gasp, removing all the gaps, reducing the gap by removing everything left can only reduce the lateness.'},\n",
              " {'id': '21c47944-66b5-43b5-b614-9047161a49cf',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, if I have an optimum schedule, as I said, if I have this stupid example where I have a deadline of 100, and I have two jobs which take time 1 each, and I decided to allocate them here, at 98 and 99, then I can take this whole thing and shift it left and allocate them at 1, 2 and finish them off. So, this is what we mean by saying that there will always be an optimum schedule with no idle time. So, it is sufficient to look in that space. So, what we want to argue is that our greedy solution is optimum. And like last time, what we do is compare it to an arbitrary optimal schedule. So, there are many optimal schedules we could compare it to, but we will only compare it to optimal schedules, which like our solution has no idle time. (Refer Slide Time: 16:03) So, now, let us look at correctness. So, let us before we said, look at an arbitrary optimal schedule. So, let us look at an arbitrary optimal schedule O. So, we have a schedule which our greedy algorithm produces which is sorting and just saying 1 to n. So, remember, the numbering has been fixed by our sorting. So, what we are going to show now is that now notice that any schedule has to schedule all the jobs. It is not like the earlier case where the optimum schedule could have given a subset of instructors the lecture room and my schedule could have given another subject, we just had different subsets of the same size. It is not like that. The optimum schedule here is going to have exactly the same indices 1 to n, but in a different order. So, what we are going to argue is that we can take the sequence in which O allocates this printer and transform it into something which looks like 1 to n without changing the optimality of O. So, notice that in our schedule, if a number 2 appears before number 3, it means that the deadline of 2 is before the deadline of 3. So, that is the property that we add for this numbering. So, in our schedule there are no inversions. And inversion happens when i appears before j.'},\n",
              " {'id': 'c9945fe9-bf9c-418b-8a6c-88871d81a0f2',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'But the deadline of j is actually earlier than the deadline of i. Remember, now the numbering is already in some sense reflecting the sequence of the deadline. So, obviously by construction, we cannot have an inversion in our greedy schedule. But this optimum schedule could have, any time you have something which is different from the greedy schedule, it will mean that there are two indices which are in the wrong order. And if they are in the wrong order, then necessarily it must mean that the later one has something which is not necessarily, but in general it will mean that the later one has a deadline which is earlier than the previous one. And therefore, it should have been done in our greedy schedule in the opposite order. So, now first look at a solution in which there is no idle time, which we have already argued must be the case. And there are no inversions, but they are different. Now, how can two schedules be different, but have no inversions? Well, the difference can happen because I have equal deadlines. So, when I said before that we do not assume that the deadlines are distinct, I could have that Di is less than or equal to Dj. And actually, in fact, or Di plus 1, let us say, just say two adjacent things. In fact, this is equal to. So, our greedy algorithm chooses one order. It chooses i and then i plus 1, but maybe the optimum schedule chooses i plus 1, then i. So, then these two would be different schedules. But there is no inversion, because inversion says that the later one must be strictly earlier in deadline than the previous one. So, it is an inversion only if the one that appears later has an earlier deadline, not an equal deadline. So, therefore, if I have no inversions, and no idle time, the only difference can be in the order of requests with the same deadline. So, what we have to argue is that this does not matter that they all produce the same maximum lateness.'},\n",
              " {'id': 'bca10356-78f8-46b1-9d92-1f77dd808683',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, any reordering of jobs which are next to each other with the same deadline is not going to create a problem. So, let us look at the situation. So, supposing we have at this point, so we are here maybe, and now we have the next three jobs in our list had the same deadline which is this. And our schedule, the greedy schedule, produces this order, the green one followed by the yellow one, followed by the orange one, and the actual deadline is somewhere here in the middle of this yellow patch. So, this is the actual deadline. And let us assume now the optimum schedule it cannot invert, but it can reorder this, because it will not be an inversion in the sense that we have defined it, because they are greater, they are all equal. So, let us assume the optimum schedule actually does yellow first, then orange, then green. So, the relative lengths of these segments is the same, because it is the same job, but it is just that the sequence has been shuffled. But notice that in the sequence shuffling, the total time it takes the time for the green job plus the time for the yellow job plus a time for the orange job in whatever order I take. So, if I look at the beginning and if I look at the end, the end of this group happens at the same time as it does in the earlier case. So, if I look at the lateness of the maximum lateness then it is determined by the difference between the end of the green job now and the deadline which is the same deadline for all of these. So, earlier the orange job was the witness for the maximum lateness. Now, the green job is the witness for the maximum lateness. So, which job is being delayed is changing, but the amount by which it is being delayed is not changing. So, any two schedules with no inversions and no idle time have the same maximum lateness. So, shuffling around things with equal idle time does not matter, with equal delays does not matter.'},\n",
              " {'id': 'ffe79ca2-855c-4727-874a-9e283e289b7c',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': '(Refer Slide Time: 20:55)  So, now we want to argue that there is an optimum schedule with no inversions and no idle time. So, our schedule, the greedy schedule is such a schedule. It has no inversions and no idle time. We want to argue that there is such an optimum schedule. So, we start with, as before, we start with an arbitrary optimum schedule, but we can always assume that this optimal schedule has no idle time, because we as we said, we can always compress an optimal schedule without increasing the lateness, because everything just shifts to an earlier time. So, now our problem is that there may be inversions, because we want no inversions and no idle time. So, no idle time is okay, but no inversions is not something that we can assume in general about an arbitrary optimal schedule. So, if there is an inversion, then I claim that there will be an inversion between two adjacent jobs, because if you start with, so my optimal schedule orders them as i1, i2 up to in. So, this is some permutation of 1 to n. Now, if you start from here and if I look at D of i1, then it will be smaller than D of i2 up to some point. And then at some point, I will, because it is, there is an inversion, there is something later which is earlier. So, this deadline which keeps increasing must go down at some point. So, if I look at these to, the place where it goes down, the first point where the deadline decreases instead of increasing. In our greedy schedule, the deadline as we go from left to right keeps increasing. In a schedule with inversions, it must decrease at some point. Otherwise, there is no inversion. So, if I look at that point where this decrease happens, I am guaranteed that there will be two adjacent things in which the second job has an earlier deadline than the first job. So, the inversion is actually between two adjacent elements. So, if I exchange these two, so i and j are these two.'},\n",
              " {'id': 'b971c36e-968a-47f3-96d2-049860d9a806',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, the reason they are called i and j is because the numbering i and j is with respect to my greedy schedule. So, they are consecutive elements in the optimal schedule, but with respect to their job numbering, there are some i and some j and they come after each other and j is after i. So, I can say, put j before i. Now, because j, because the assumption of the inversion is that Dj is less than Di, so if I exchange these two, then this one inversion goes away, because earlier now this was inversion. Now, I swapped them. So, it is easy to see that the inversion decreases. But what is the property that holds for the lateness, because that is what we want. So, we can swap these two jobs. But does it affect the optimality of O? So, the claim is this does not. So, it is not obvious. So, we have to prove this that exchanging i and j is the obvious thing to do to remove the inversion. But why does this not O any worse than it was before? Why does it keep the maximum lateness not worse than what it was before? So, this is the picture we have. So, we come up to some point and then we have these two jobs i and j, such that the deadline of j is before the deadline of i. So, let us assume that the things have passed. If the deadlines are to the right, there is no problem. So, let us assume that the deadline has passed. So, the deadline of j is like this. So, we have this delay between the deadline of j and the time that j finishes. So, this is the lateness as observed by j. Now, I am going to exchange these two. In the process, the lateness of j is going to reduce clearly, but the lateness of i is going to increase, because it is going to go to the right, so what is going to happen. So, when I exchanged these two, now I have a problem with the lateness of i. The lateness of j is no worse than before, because it is finishing earlier. But what about the lateness of i. Well, the lateness of it certainly worse than before.'},\n",
              " {'id': 'a0fc5c88-d2e5-4aa2-9136-fb16059aff91',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, I have to compare the earlier lateness of j and the current lateness of i. But here I have this property. I know that the deadline for j was strictly earlier than the deadline for i. So, if I look at the difference from this point to the deadline of i, it is going to be less than from here to j. So, whenever I do this swapping of adjacent things which have an inversion, I am guaranteed that the lateness due to these two does not change. Now, it could be that there is some other job which has a higher lateness, but I am not affecting that job is finishing time anyway. Everything to the left of this and everything to the right of this, all finish at the same time as they did before after the swap. This is only affecting the finishing time of i and j. So, within i and j the lateness only reduces. It could be there is some other job which has a maximum lateness which is higher than this that is unaffected. But if j was the culprit, if j was the maximum, in fact, we have reduced it. But certainly it is not increasing. So, this is the property now that we have shown. We said that we have an optimum schedule. We will find if there is an inversion. We will find two adjacent elements which have this inversion among them and we can swap them and in the process the lateness does not change. So, using C we can remove each adjacent inversion without increasing lateness. Now, how many examples of this could there be? Well, every time there is a lateness, it is with respect to some pair i comma j. So, there are only as many possible inversions as there are pairs i comma j and the number of pairs i comma j is n into n minus 1 by 2. It is a very simple observation. The number of ways you can pick two elements from n is n into n. So, they cannot be more than n into n minus 1 by 2 inversions to start with.'},\n",
              " {'id': '1cb31226-f5f7-47e3-b315-6df6def90346',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, if I just keep repeatedly removing these inversions without affecting optimality, I am gradually moving from O towards a scheduled which has no gaps, no idle time, and no inversions. And once I finally do that, so ultimately this must be the schedule that we have in our greedy algorithm. Our greedy algorithm has this property that it has, it is defined this order. So, it has no inversions by construction and no gaps. And I have taken an arbitrary optimal schedule and done this exchanging. So, this is called an exchange argument. By exchanging components of that solution, I have gradually transformed it into the solution that I have. So, this is proof that any optimum schedule can be converted into a schedule which looks like our greedy schedule. And since this conversion does not affect optimality, our schedule that we produce because of this greedy algorithm must be optimum. (Refer Slide Time: 27:36) So, to summarize, we have a simple situation here. We have one resource. And we have users trying to access this resource. And they do not have fixed time intervals when they want to use it, unlike the lecture on scheduling. But they do have a requirement in terms of Ti of how much time they need and they have a deadline by which they would like to finish it. And our goal is to minimize the maximum lateness. That is among all the users who do not meet the deadline, I want to find out what is the maximum slippage of this deadline that I have. I am not trying to add up these delays, but rather just find out by how much does the worst affected user get delayed. So, our simple greedy algorithm just sorts the requests by this deadline and then processes them in that order. And what we said is that this is guaranteed to be correct by our exchange argument. And this takes n log n time to sort. And then once we have sorted it, of course, the schedule is just the order in which the sorted array comes. So, that just takes linear time.'},\n",
              " {'id': 'c85508f7-fcdc-40e2-9482-4f642db05985',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Minimizing Lateness.pdf',\n",
              "  'content': 'So, this exchange argument is critical, which says that we can take any optimal solution. And then first we made some assumptions that this optimum solution looks a little bit like our thing, because it has no idle time. And once it has no idle time, we can keep exchanging these adjacent inverted elements until we eventually remove all inversions, and therefore, we must have reached our greedy solution without sacrificing optimality.'},\n",
              " {'id': '478032b7-3e8c-4f1a-a115-a951ec3d745e',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'Programming Data Structure and Algorithms Using Python Professor Madhavan Mukund Greedy Algorithms: Interval Scheduling After spending some time on data structures, we have looked at the union find data structure for disjoint set, we have seen heaps, how to maintain priority queues in heaps. We have seen balanced binary search trees. Now let us get back to algorithms per se, and try to understand some general strategies that are there for designing algorithms. So, one of the tempting strategies for designing an algorithm is what is called a greedy strategy. So, let us look at greedy algorithms, some examples of them and see what it is that we need to show in order to justify a greedy algorithm. (Refer Slide Time: 00:45) So, first of all, what is a greedy algorithm per se? So, usually, we are computing some kind of a global quantity. So, we want to find out for instance, a minimum cost spanning tree. So, among all the trees that you can draw on a graph, you want to find the one that has a minimum cost. In order to build this tree, we have to add edges at step by step. So, we need to decide whether to add an edge or not to add an edge to construct this tree. So, we need to make a choice as to whether to keep an edge in the tree or not use it in the tree. So, that is a local choice. So, you need to make a sequence of choices such that the overall outcome that you get is optimum with respect to whatever it is you are looking for. So, what a greedy strategy does is it makes this choice, this next choice based on some local property, it says, this is the smallest thing I can see right now, or this is the closest thing I can see right now. So, let me add it. And it will never go back and revisit this choice. It is never going to go back and say, the smallest thing that I chose then did not work, maybe I should go back and check the second smallest thing, maybe that will give me a better one.'},\n",
              " {'id': '3f7db8de-414c-4388-a145-89caabb3fee3',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, a greedy algorithm will only go forwards, it will all pick up something which looks good now, put it into its solution, and never go back and change that solution. So, if you do not do that, I mean, if you actually go back and change then you follow some strategy to find a solution you say this is not good enough, then you will go back and you will find another strategy then you go back and find another strategy, this is called backtracking. So, you keep going back and searching through various possibilities. And somewhere somehow you stumble upon the correct algorithm. So, you have to search through a number of possibilities to find the optimum. But a greedy strategy, has this very local way of saying okay at this point, go this way at this point go this way. So, it just follows one path through your state space and finds the optimum solution. So, this drastically reduces how many solutions you have to evaluate for optimality, and therefore it makes the algorithms very efficient. But the real problem is that when you have such a greedy strategy, you have to justify the fact that you are never going to go back and relook at something which you have chosen before. How do you know when you pick up something which seems to be good now that this is going to be the best choice you have ever made. So, just imagine that you are trying to buy something, suppose you are trying to buy a phone, and today the price seems to be low. Now, the strategy that you might want to do is say that, if this is the lowest price I have seen so far, I should use it, but what if tomorrow there is a discount offered and then you will regret it. So, greedy strategy is kind of in that same situation that it sees something which is good. And it is, let us take it but there is no guarantee in general that something better will not come later on or put it another way you buy this cheap phone and then tomorrow breaks down.'},\n",
              " {'id': '2bbd3880-2da4-4606-b326-a4cbbd48e1af',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, if you are paid a little bit more and got a slightly better phone, then the phone would have lasted allot longer. So, in the long run you would have paid less. So, this is the difference between in real life a greedy strategy and a strategy which looks ahead a little bit to see the consequences of what will happen. So, greedy strategy does not look ahead. So, we have seen several examples already of greedy strategies. So, the first example was our shortest path algorithm due to Dijkstra. So, we started at a source vertex and we tried to find the shortest path to every other vertex. And what we did was we maintained this collection of visited vertices and we kept incrementally keeping track of the shortest distance to every unvisited vertex from this set. And we said that whenever we find now the shortest unvisited vertex, we will add that to the set and freeze its distance once and for all we say that now, there is no better way of getting to that vertex. So, the global property that we want is that we want the shortest distance to every vertex from the start vertex that we began with and the greedy strategy is to locally pick the next unvisited vertex whose distance is currently the smallest. And we showed that because we do not have negative edges, you cannot find a better path later on. So, we proved it actually. So, for the Dijkstra algorithm we actually proved that this works. Another algorithm which is greedy was Prim’s minimum cost spanning tree thing. It was very similar to Dijkstra in spirit, you grow this tree, which is like the visited set in Dijkstra, and you add a new note to the tree which is connected by the shortest edge among all the other unvisited nodes. So, you add a new node to the tree and then we use that separator lemma to show that the edge that we are using must be in the spanning tree because it is something which connects this set to that set. It is a, it is a separator edge which connects two partitions of your vertex.'},\n",
              " {'id': 'c68eefb5-6f8f-4f63-9951-f179f4b19284',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'But again, we needed a proof so, we have a strategy. So, but why does this give us a minimum cost spanning tree overall, because the separator lemma guarantees that whenever we pick up in this greedy strategy, it is actually belongs to the spanning tree. And the same is true in Kruskal’s algorithm, that is also a greedy strategy, we have a bunch of edges which we have not used so far, we pick the smallest one which does not conflict, what we are doing, it does not create a cycle in the tree. Again, the reason it works is because of the separator lemma, but otherwise, there is no guarantee that picking up the smallest edge now does not create a problem later on. So, in all these greedy algorithms, we explicitly proved that the algorithm strategy was correct that what it produced was something which achieved the global optimum value, the shortest path of the minimum cost spanning tree that we were looking for. (Refer Slide Time: 06:17) So, what we are going to do now is look at other examples and argue why it is not trivial to do this, again, why you need to actually show that the greedy strategy that you have picked is a good one, then we will look at some approaches to show that. So, let us look at this problem. So, suppose like I am sitting now, there is a special classroom in which one can record these lectures. So, a number of people want to do this. So, different teachers need to use this classroom but suppose there is only one. So, each teacher has a particular time when they are ready and willing to record it. So, they have certain preferences or slots. So, every instructor has a preference saying I want to record from two o clock to five o clock on Thursday afternoon, and other person will say I want to record from eleven o clock to one o clock on a Friday morning. Now these two are fine, but somebody else wants to record from nine o clock to twelve o clock on a Friday morning then nine to twelve and eleven to one are not compatible.'},\n",
              " {'id': 'c52c5203-cc02-44ad-902b-23898434fb09',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': \"So, you might have slots chosen by the instructors which overlap, so you may not be able to honor all the requests for booking. So, now, if we have next week's bookings available to us, then the question is how many instructors can be allotted their chosen slots so that we maximize the number of happy instructors so you want to choose the number of subsets of the bookings such that these do not overlap and if I collect together all the bookings which are actually honored, this set is as large as possible. So, this is the property that is the global optimum, I want to choose a subset of compatible bookings of largest sites. Compatible means that no two bookings in the set overlap in time so it does not require two people to be using the studio at the same time. So, since we are discussing greedy strategies, what we will do is we will try to build up this set of allotted bookings one by one. So, eventually we want a collection of booking saying these are all the bookings which are granted for next week. But you must start somewhere so, you must pick up a booking according to some local strategy you will say I like this booking because it is a small slot or I like this booking because it overlaps with very few slots. So, there might be many reasons why you think a particular booking is a good one to pick first. So, you pick the next booking to allot based on a local strategy. Now because you have picked this booking just like in Kruskal’s algorithm because you picked an edge certain other edges now are going to be not useful anymore because they will create cycles. So, here what will happen is you pick a booking every booking which overlaps with this is now ruled out because you can no longer allot those slots. So, you will eliminate some bookings from your option and then you will keep going like this, now you will pick the next booking which survives this pruning process and say now I have got a second booking which I can add to my set.\"},\n",
              " {'id': 'f20dc368-f0f3-4d78-a2df-54fc86a44c56',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': \"Now you will remove everything which is incompatible with that. And finally, you will get a sequence of bookings and when you have run out of bookings that you can add then all bookings are in conflict with some booking you have taken you will say this is the final answer. The question is why is the final answer that I have produced now the correct one. So, let us see what we could choose as this local strategy, the next booking to allot and see why it is not such an obvious thing to choose one which is correct. (Refer Slide Time: 09:41) So, let us start with a very trivial strategy. So, I pick up the first booking in terms of time so the person who has asked for a Monday morning eight o'clock slot I just give them the booking because there is the first come first serve. So, they are the ones who asked for it, earliest not that they made the booking earliest but they required the studio earliest in the week, so I give it to them. So, would this be a sound strategy? Well, one way to show that it is not sound is to produce a situation where this will not give you the correct answer. So, here is a situation. So, here time is going from left to right. So, let us just assume that we are keeping a calendar and we are just keeping time from left to right, Monday to Sunday, or whatever it is. So, now the green lines are the intervals that the booking occupies. So, this is called interval scheduling, each request spans an interval. And if two intervals overlap, they are incompatible. So, now the earliest booking, unfortunately, happens to be a very long one. It takes the entire week. And my goal is not to maximize the usage of the studio in terms of hours. So, that is also important, it is important to know what it is you are trying to optimize. So, I am not trying to maximize the amount of time the studio is being used, I am trying to optimize the happiness of my teachers. And I want to keep as many instructors happy as possible.\"},\n",
              " {'id': '60143450-d883-4e57-b917-d1ef16de4ce1',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, I want the maximum number of people to get the slot that they asked for, even if there are gaps in between where the studio is not being used. So, in this case, it is clear that it is best to give these five non overlapping slots of choice, rather than give this one long slot a choice, but this one long slot is the earliest slot. So, strategy one fails, because you could have a situation like this. So, this is the type of analysis that you need to do you have to come up. So, it looks like a plausible strategy, maybe not a very clever one. But you have to say why this strategy is not going to work? Or if it does work, say why it does work. So here is another strategy. So, maybe starting earliest is not a good idea. But surely it is a sound strategy to pick small slots. So, if I take a small slot and allot it, then it will leave a lot of time free for other slots. So, can I take the smallest slot. So, if somebody wants to record only for 20 minutes, so can I say you can go ahead and take 20 minutes, because if you use it only for 20 minutes, then you are less likely to interfere with the other people slots. So, it seems plausible when you think of it that way. But this also does not work, because you could have an example like this. So, you could have two long slots. And you could have one small slot, which unfortunately overlaps with both of these. So, if I now allotted to the shortest slot, I will rule out two slots. So, only one teacher will get a chance to use the recording studio. Whereas if I had not used the strategy, but given it to the two longest slots, I would have got two people using it. So, remember, the criterion is the number of slots allotted and not necessarily the time duration for which the studio is in use. So, this is also not a good strategy. So, let us look at a third strategy. So, when I take a slot, there are various other instructors whose slots intersect with my slot.'},\n",
              " {'id': '2c7b6645-6806-4235-8eb5-e7b7d7f28028',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, my slot is incompatible with a number of instructors, because they overlap. So, what if I pick a slot, which is overlapping with the smallest such set, so I pick a slot, which is least incompatible in some sense, then again, this seems plausible. If I pick something which is least incompatible, it will reduce the pool by the minimum amount, remember what happens, I pick a slot and then once I pick that slot, every slot which is overlapping with it has to be thrown out. So, if I pick one which throws out the least number of other slots, then maybe I am in good shape. So, this requires a slightly more complicated example to describe, but here is one. So, in this if you look at any of this lot, so this is incompatible with all four of these. So, is this so this is also incompatible, all four. So, all the top row things have four things which are incompatible. And these four things have themselves so the three others in the same time slot plus the two above, so they have five things we are incompatible. (Refer Slide Time: 13:55) So, here I have four incompatible things. So, actually, here I have five, because this is also incompatible there. And here I have five, the three which are in the same stack and the two above. Whereas this particular slot is only incompatible with two the two in the middle on the top row. So, by this strategy, I would start with this. But if I start with this, then unfortunately what happens is that I have ruled out these two. Therefore, the only thing I can do is to take one of this left hand side and one of these right hand sides. So, I can only complete this with three recordings. I can take the middle one, and then I can take one from the left hand side either top row or the one from the stack, because they are all incompatible so I end up with a choice of three. Whereas if I actually look at the top row clearly I can achieve four but this four does not include this slot which had the minimum conflict.'},\n",
              " {'id': '607d3abf-9518-4a0d-87ce-87a1f60fd2d2',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, choosing the booking that overlaps the minimum number of other bookings also does not work. So, what you can see from this is that it is easy to think of greedy strategies, which seem plausible and even give an argument as to why this greedy strategy seems like a correct thing to do. But it is a different thing to actually justify that it works. And in some cases, it is easy to say it does not work by showing you some situation where it does not apply. In some cases, you have to come up with a little bit more thought, to come up with a situation where it does not apply. So, by no means is it obvious that a greedy strategy will work? And in most situations, unfortunately, greedy strategies do not do a good job. So, here is the fourth strategy. So, we started with the strategy which says, let us choose one who is starting time is earliest. So, here is a dual one. It does not sound very promising, because it looks very similar. But let us start with the booking whose finished time is the earliest. So, can we come up with a counter example? So, supposing you porter around like we have done for some time. It is difficult to come up with a counter example maybe. So, either you have to come up with a counter example, counter example. Or if you are convinced that this is actually the correct strategy, then you must prove to me that it is correct. So, it turns out that this is actually a correct strategy. So, this particular strategy of not choosing the starting time that is earliest, but using the finishing time that is earliest, is actually a good strategy for doing this. But how do you prove that it is correct, that is what we are going to argue. (Refer Slide Time: 16:38) So, first, let us describe the strategy more formally. So, I have a collection of bookings. Remember, each booking has a starting time and a finishing time. So, I have a collection of bookings.'},\n",
              " {'id': 'e05d00ec-c276-4241-a1c2-76c96fcb870c',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'And from these bookings, I want to construct a subset of bookings which are mutually compatible, they do not overlap with each other such that this subset is as large as possible. So, I start with that, and I have a new set, which I am going to pull out from the set B of accepted bookings. So, initially, I have no accepted bookings, everything is in this booking set, that is bookings which have yet to be processed, and A has the ones which have been accepted. So, initially A is empty. So, at any given point in the greedy strategy says among all the available bookings now, the available means by assumption are not conflicting with any booking I have already chosen. So, among all the available bookings, if there is at least one so it is not empty, pick the one with the earliest finishing time. So, you pick a small b in that set the earliest finishing time and add it. And that is the greediness, I just pick it up. I do not have any second thoughts about it, I just pick. Of course, if there are multiple ones the same finishing time, we will choose one, but you just pick one with the smallest finishing time and put it in. And now you have to prune everything which is in conflict with the small b has to be removed, because we cannot honor those anymore. So, you have to remove from capital B every booking which overlaps with small b. So, this is the algorithm it is very straightforward. So, let us see how it works. So, supposing I have these bookings 1 to 9. So, you can see that again, time is going from left to right. So, this is the direction of time. So, the earliest booking in all these that ends is 1. So, 1 is the booking that whose time slot ends the earliest. So, 1, 2, and 6 all start roughly together, but 1 certainly ends earlier than others. So, I remove, I add 1 to my set A and in the process, these two get removed because they overlap with 1, none of the other bookings overlaps with 1 directly.'},\n",
              " {'id': '3bebac89-9214-458b-b5b8-13c5b0691c3c',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'But these two bookings I cannot honor because they would require the studio at the same time as booking 1. So, now these dashed lines are the ones which are ruled out. So, among the remaining things, it is clear that 3 is the next one which should be chosen because it is the smallest finishing time among the unprocessed. So, the black solid lines are those which are still in capital B. The green one, so this is A, the green ones are in A and the dashed lines are the ones which have been eliminated from B because they conflict with something in A. So, I choose 3. And in the process of choosing 3, 4 gets ruled out because 4 overlaps with 3, but 5 7 8 and 9 are okay. Now, I look at this, and then the next thing I choose is 7, because 7 finishes before 5. So, I choose 7, and in the process 5 gets ruled out. (Refer Slide Time: 19:31) And finally, if you look carefully, 8 finishes before 9, so I choose 8. So, this is how the algorithm works. It looks among all the remaining ones, which one has the earliest finishing time, picks it up and then rules out everything else. So, this is the greedy algorithm. (Refer Slide Time: 19:48)  And what we want to argue now first is how to show that this actually produces the largest subset of compatible jobs. So, we saw that the algorithm takes a set of bookings B and it produces to the solution A by following this greedy strategy, but why is this optimal? So, you have to do something to prove this. So, when we looked at Dijkstra’s algorithm, Prim’s algorithm, we looked at the problem we were studying and we said, in the context of this problem, there is a lemma or there is something which shows that a later thing cannot change anything. So, like that we have to come up with a strategy. So, here, we are not going to come up with any fixed because we do not really have any handle on this problem. So, let us just say, let us look at some optimal solution.'},\n",
              " {'id': '9ad3ece5-dd9f-4599-894a-89a6af896235',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': \"So, let us say that there is a different solution with somebody else's produced through whatever algorithm which I do not know. And the claim is that that is optimal. So, there is a capital O, which is a optimal set of bookings. And I have a set of bookings A. Now, the first thing to notice that I am not claiming that A and O are the same. Very obviously, I could have, for instance, a situation where I have bookings like this. So, supposing these are my bookings, then it could be that my algorithm because it is the early finishing algorithm picks these two, and the optimal algorithm picks these two. But you can see that there is no way to pick more than two. So, the optimality is in the number, but it is not in the choice of which slots are honored. So, remember, we are not looking at other constraints, like, whether the thing is used for a longer time or not. For instance, I could have a situation where I have two really long slots. And two really short slots. Now, my algorithm, the algorithm A that we described, will pick this one, because it finishes first, throughout this one, then it will pick this one and throw out this one. So, it will pick two, but it will pick two with use of studio for a very short time. And perhaps O instead picks this one and this one. So, in a sense O has done better because it is using the studio for more, but it does not matter because we are only counting the size. So, what we really want to argue is that the size of A as a set and the size of O as a set are the same. So, we just saying to the same number of people may not be the same people with the same number of people are being satisfied. So, to do this, we will first to prove that this is true that the size of A and the size of O are the same, we will just reorganize A and O to be a little bit more structured than they are. So, remember that each job has a starting time and a finishing time. So, obviously the starting time of a job was made before its finishing time.\"},\n",
              " {'id': '13af6f82-82ec-42a0-9ded-f079b6d7b6f0',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'Otherwise, it does not make sense. I cannot have a slide slot that starts at three o clock in the afternoon and finishes at two o clock in the afternoon. So, every starting slot, starting time must be before the corresponding finishing time of that job. But since these are compatible in A, we know that the all these things are disjoint. So, I can order them in time. So, I can pick a sequence in which to enumerate them such that the finishing time of the first job is before the starting time of the second job. And this finishing time of the second job is before the starting time of the third job. So, I am just ordering them in time. So, the first job finishes, then the second job finishes, then the third job finishes, and I know that they do not conflict because otherwise it would not be in my solution. And I can do the same thing for this so-called optimal set which somebody has produced, they are also an order in the same sequence I can say that j1 is before j2 in time j2 is before j3. And the claim is that I have produced k but the optimal solution has produced mn if it is optimal not and I am not optimal with my algorithm, then m must be bigger than K. So, my goal is to show that k and m are the same. So, we want to show that k is equal to m that will prove that the two sets are of the same size. (Refer Slide Time: 23:58)  So, the way we will do this is to use this sequence to argue that at every point in the sequence, our in time, the solution produced by A is ahead of the solution produced by O. Solution produced by O in time. So, it is ahead in time. So, what I mean by that is that if I look at the corresponding job if I look at i1 and j1 and if I look at i2 and j2, for each job in the sequence, if I look at that l th job, then the l th job in my set in the A set finishes earlier than the l th job in O set equal to earlier.'},\n",
              " {'id': 'ba77892d-46c8-4c4e-be5d-a51d892bbbbf',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, f of il should be less than equal to remember f is the finish time, the finish time of the l th job in the accepted set is less than or equal to the finish time of the l th job in the so-called optimal set. So, why is this true? Well, we will prove it by induction by l equal to 1 so on. So, for the first job remember that what our algorithm does is it starts with the pool of all jobs and it picks the job which ends the earliest globally. So, by definition, i1 the algorithm are the set capital A contain consists of the first job which ends globally before every other job. So, the set O cannot have a job which starts and finishes earlier than that. So, therefore, i1 cannot finish later than j1. So, initially it is true. Now, let us see what happens inductively. So, we are doing it interactively. So, we are assuming that we have come up to some point up to some l minus 1 and so far, we have proved it every step the job on top finishes before the job at the bottom and now we have to argue that it also holds for the current job number l. So, why is this? So, we want to show that f of il is less than equal to f of jl. So, remember that at the previous step, we already knew this, f of i of l minus 1 by induction. So, this is induction f of i of l minus 1 was already smaller than f of j of l minus 1 and this is because B sorted out. So, since B sorted, I know that the next job that is in B starts after the previous job ends. So, the starting point of so this should be sorry, this is this is J sub l. So, the starting point of the jth, lth job in the j sequence is after the finishing point of the l minus point 1. So, the first inequalities because my induction shows me that the A sequences ahead of the B sequence at l minus 1 and the second inequality says that the B sequence is sorted. So, the lth job must start only after the l minus 1 job is finished. From this it follows that f of il minus 1 is less than s of fj. Because I have these two inequalities.'},\n",
              " {'id': '96d1f478-8b4d-46c1-9e48-34944d8a5dbf',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, therefore, if this was actually going to be smaller. So, the job j is compatible with my thing, because f of i have l minus 1. So, remember, this is the s of, j of l. The l minus 1 job, which I have put in set A finishes before this job that the optimal one picks. So, if that job was available to me, it is available to me. And if it finished before, il I would have picked it. Because my strategy says pick the one with finishes earliest. So, if f of jl is actually strictly smaller than f of il, then the greedy strategy would have to jl. So, if you are claiming that he did not pick jl, then it must be that il is smaller than or equal to. So, that is the proof. So, therefore, if I have already got this greedy strategy, maintaining its lead, in some sense, finishing before the optimal strategy up to l minus 1, it still stays ahead one more step. So, so far, so good. (Refer Slide Time: 28:15)  So, now then what? So, now, we still have to justify that k is equal to m. What we have shown is that at every stage, we are no later in time when we have finished that many jobs in A then we are in O. So, why does this guarantee that I have no more jobs in O, than in A. So, supposing it is not the case. So, supposing I have more jobs strictly more jobs in M, than in A. So, when I finish at ik, and I am a jk, at this point, I know that the this is smaller than this. This is what we have proved. Now if this next job is there. And I am not going to add it, it means that at this point, I have run out of jobs to add, so everything else is incompatible with the set I have picked, so no more jobs remain, that is why I stopped at ik. But the job that is remaining is not incompatible, because for the same reason, we know that ik is smaller than jk in the finish time but jk is smaller than jk plus 1 in the start time. So, the next request which comes after that in O is not finished. So, this means that B is not finished at that time.'},\n",
              " {'id': '23dace31-d9ad-471e-bf65-0d834400cb38',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, therefore, our assumption that B is empty and we stopped is wrong. So, we could not have stopped at K because B became empty. So, we must be able to add k plus 1. So, if m is strictly bigger than k, then there is something wrong with our assumption and therefore m cannot be strictly bigger than K. So, this is this proof by contradiction. You assume something and you show that it leads to some absurdity. And because that absurdity is not possible, the original thing must have been wrong. So, it cannot be that therefore, this is not a valid assumption. So, m cannot be bigger than k, can be at most k. Since you claim that O is optimal, and I claim that I have produced k, then m must be equal to k. (Refer Slide Time: 30:12) So, implementing this algorithm like many greedy algorithms implementing it is not very hard, so, what I need to do first of all is to be able to look up the next job by finishing times I need to sort. So, n log n time I need to take the set of bookings which are given in capital B and sort them by finishing time. And now, for convenience, I will sort them and renumber them. So, the first job now job one in my sequence will be the one that finishes first job to second and so on. So, the starting times may be jumbled up, but the finishing times are in sequence. So, I will now keep the start and finish time for easy lookups. So, I will say capital S of i is a dictionary which tells me the starting time of request i. F of i is the finish time of request i. So, now I pick up the first booking. And what I will do is I know it is finishing time, so I can now go ahead. So, all the jobs to my right finish later than this, but I need to find one which starts later than this. So, since they are ordered by finishing time, I am going to be processing them in earliest finishing time. But I need the earliest job, which finishes after this, but which starts after this.'},\n",
              " {'id': '6353fc37-4587-420f-ab66-ae0c0e4b3e35',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, in general, after adding looking j, I need to go to the right and find the smallest r such that, of course it will finish later than this because I am doing it in order to finish time. But it must also be compatible. So, it is starting time must be bigger than my finishing time. So, I go to the right, so I will skip over everything between j and r. Then after looking at r I will skip over it. So, I am not going to go back because everything before that is already ruled out because it is already overlapping with something. So, I go from r and I look for the next r prime which is to the right of r prime which starts after r ends, and so on. So, this is a single scan, I need to only go through the jobs in one pass. I start with the first one, it will cancel out a bunch of jobs. And I will find the first job which is not canceled, which can be started after this job ends. And because it is an order of finish time, this will be the earliest finish time among all compatible jobs. And then I keep going, so this is the implementation. So, it is going to be an order n log n sort plus a linear scan. So, it is a very efficient algorithm. (Refer Slide Time: 32:27) So, what we have seen is that this notion of a greedy algorithm is one which makes a lot of choices locally, it looks at the current situation and says this looks best right now, in order to achieve something which is globally going to be optimal. So, we pick the shortest edge now, or the shortest distance now in Dijkstra. In this case, we pick the shortest finish time, among those things which are remaining right now, such that globally, we are going to achieve the maximum number of jobs which are satisfied, or we are going to globally achieve the minimum cost spanning tree or whatever. So, in order to do this correctly, we have to make sure that this not going back and revisiting a choice is actually correct.'},\n",
              " {'id': '839736e3-5966-4cae-819d-67affc43485a',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Interval Scheduling.pdf',\n",
              "  'content': 'So, since the algorithm never goes back and revisit, the choice is very efficient, because it only follows one trajectory. So, it is going to be a very simple algorithm, like the one we saw here, which is actually n log n is to sort and then a scan. But we need to show that the strategy is optimal that we know we do not go back. And because we do not go back, we are never going to reconsider so that we have always made the correct choice. So, and in many straightforward problems like this interval scheduling thing. The problem is that there are many things which look naturally greedy. So, greedy strategies are not difficult to think of, and to informally justify. So, we have to somehow make sure that these greedy strategies that we come up with are not wrong. So, we already saw that the first three greedy strategies we came up with for interval scheduling, we could come up with counter examples. So, most greedy strategies are wrong. So, when you pick up a greedy strategy, if you are following a greedy strategy to solve a problem, you need to provide a proof that what you are doing is actually optimal. Otherwise, it is suspect. So, every greedy strategy needs a proof. So, just keep that in mind. So, we saw in this particular case, one way to prove this, which is to show that step by step, if I just examine the order in which things are taken step by step, the greedy solution stays ahead of the optimal solution. So, there are many different ways to prove greedy things. And there is no uniform strategy, unfortunately, but we can just look at a number of examples to come up with different ways of proving it. So, this is one particular solution which says that I can take an optimal solution and I can kind of incrementally show that my solution is never worse than that. Later, we will see other examples where we can take an optimal solution and rearrange it to be the same as the actual solution that we get. So, that will be a different way of proving it.'},\n",
              " {'id': '5bec03f9-e690-4c2e-9565-66c60443f351',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Greedy Algorithms-Huffman Coding Our first two examples of greedy algorithms were in the realm of scheduling. Now, first of all, you should not assume that all scheduling algorithms have greedy solutions. There are other types of scheduling algorithms which do not have greedy solutions. So, it very much depends on what criterion you are trying to optimize. So, the kinds of criteria we used admitted greedy algorithms but others may not. But now, we will change focus and look at a different domain. And this domain is that of communication. So, we are going to look at efficient communication. (Refer Slide Time: 0:40) So, when we communicate, when we send a WhatsApp or a message or an email, we are communicating in some language. So, it could be English, or Hindi, or Tamil or any other language. And each language has its own alphabet, its own set of symbols that need to be send. So, when I read an English message, I read certain letters. If I read a Hindi message, I will be reading Devanagari as different character set. But as we all know this whole communication happens digitally. And in digital communication, the only thing that gets transmitted from the sender to the receiver is a sequence of 0’s and 1’s. So, we have these symbols which are meaningful to us ABCD ka kha ga gha and all that. But they are actually being transmitted 0’s and 1’s. So, therefore, there must be some encoding, there must be some way of taking the letters or symbols that we need in our natural language and writing it as a sequence of 0’s and 1’s So, that at the other end, we can transform it backwards from the 0 1 sequence to the letters and read it. So, for instance, let us take a very simple case supposing we just take the lowercase letters, A to Z in English in the Roman alphabet which is what we use for English. So, as we all know, there are 26 letters in this alphabet.'},\n",
              " {'id': '534fa206-c554-49b3-938e-c47f8224aa50',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'Now, if we want to take 26 different characters and encode them using binary strings, we cannot make do with 4 bits or 4 binary digits, because with 4 binary digits, we can only create 2 to the 4 or 16 combinations. So, 2 to the 4 is too small. So, the next power of 2 if I go to 5 bits is 2 to the 5 is 32. So, with 32 combinations I can use 26 of them for my letters and I have 6 spare. But so, I need if I use this encoding I need to use 5 bits for each letter and each letter will be denoted obviously by a different sequence of 5 bits. So, the question about efficient communication is, is there some way to do better than this obvious approach. So, the obvious approach says take the size of your symbol set or alphabet see which power of 2 is the smallest power which exceeds that and use as many bits. So, now, I will get a message which I will break up into sequences of 5 bits each and each five bits will be a valid letter and I will be able to recover it. So, the intuition is that not all letters are used as frequently as each other. So, some letters happen more frequently. So, if you play a game like scrabble for instance you will know, that some letters in the scrabble board have lower points. And they also, occur in the set with more frequency. So, there are letters like E which for instance there are more E’s because you needed to make more words. Whereas letters like Q and Z which are rare have higher points but you also, get only one of them this side. So, there is a natural frequency with each letter depending on the language. So, English has a different frequency, German has a different frequency. So, even languages which use the same underlying set of letters may not have the same frequency because of the vocabulary. And of course, in devanagari if you are using Hindi or Marathi, you might see a different frequency of the letters that are there. So, the idea for optimization is take those letters which occur very frequently and try to use shorter codes.'},\n",
              " {'id': 'd03cb842-1c15-483a-a5ae-6a0091181e36',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And those letters which occurred more rarely use longer codes because you will use them only once in a while. So, it is okay to spend a few more bits transmitting them. So, this takes us from what we call a fixed length coding. So, this is a fixed length coding. So, every letter was encoded using 5 bits. Now, we go to a variable length coding. And one of the earliest and most wellknown variable length encoding is the one that was used originally in telegrams. It is called Morse code. So, in Morse code everything is encoded as dots and dashes. So, this was a physical device. So, you if you pressed it lightly you get a dot if you press it for a longer time, you get a dash. So, it is like, analogous to sending 0’s and 1’s you have 2 symbols dot and dash. So, if you look at the Morse code you can look it up on Wikipedia or somewhere you will see that the encoding of E is a dot. The encoding of t is a dash. So, in our translation here e is a 0 and t is a 1 because for us 0 is a dot and 1 is a dash and a for instance is a dot followed by a dash. So, a would be 0 1. So, Now, let us look at one difficulty that we have with this variable length coding. So, supposing at the other end I receive the sequence 0101. Now, my next job is to decode what the sequence are so, I have to transform it back into these letters. So, if I look at 0101 I can see that a is 01. So, I can decode it as a followed by a. But notice that this is not the only thing because I could also break it up as a followed by e followed by t for example. So, I can get aet or I can say that this these 2 are both e’s and these are t’s or I can say that this is a and this is an e. And so, there are all kinds of interpretations of this because it is not very clear where the boundaries of the letters are when we are doing a fixed length encoding, we know that every 5 bits constitutes a letter. So, I can just break it up into blocks of the same size and decode each block.'},\n",
              " {'id': '8a35149d-d5a2-4d17-9df3-e64e7050c72f',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'Now, the blocks are of different lengths I need to know where the block boundaries are. So, what does Morse code do? Well, Morse code actually uses a subtle thing. So, it there is a kind of a gap. So, Morse operator who is sending Morse code will actually press a dot for an E and then wait press a dash for a t and then wait and then quickly press a dot and a dash and then wait. So, there is a pause. So, implicitly actually there is a third letter which is giving you these boundaries. So, it is not a 2-letter encoding but a 3-letter encoding. Because there is a pause, which you can interpret as an extra letter telling you where the boundaries between the encodings of 0’s and 1’s are. So, we are working with a 2-letter encoding. So, that will not work for us. (Refer Slide Time: 6:46) So, what we really want to make sure is that we have this unambiguous way of taking an encoded sequence of 0’s and 1’s and recovering what we started with. So, for this what we want to make sure is that when we are processing a sequence at any point, we are not unsure. So, here this is the problem. If I read this 0, at this point it could be an e. Or it could be that I have to read the 1 and make it an a. So, the problem is that when I have finished reading the 0, I am not sure whether the letter is over is it an e or the letter is not over. So, we do not want this kind of a situation where the code for 1 letter is included as the beginning of the code for another letter. So, it is called a prefix. So, what we want really is a code an encoding where for any x if I take the encoding of x, it is not the prefix of encoding of any other y. So, when I see x as my encoded letter, I am guaranteed that this is correct I can stop here and then extract those letters and not proceed. So, in our case as we said the encoding of e in Morse code is 0 and the encoding of a is 01. So, if I look at a prefix of this, I get the encoding of e. So, this is not a prefix code.'},\n",
              " {'id': '773836a8-6c2a-4fba-9ac9-9c7e8355d902',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'A prefix code is a code in which I never have this kind of overlap, I do not have any letter which is a prefix of another letter after the encoding. (Refer Slide Time: 8:13) So, here is an example of a prefix code. So, if I look at this, for instance there is so, 0 could be 01, or 000, or 001. So, that means that there is no letter which I can encode with just 0. Because if I read just 0 and if I can stop then I do not know whether I should wait or not. So, you can see that 11 is not a prefix of anything. 01 is not a prefix of anything. If I have 00, then I cannot stop because I must go to 001 or 000. So, this is an example of a prefix code. And when you have a prefix code. Now, if I give you a sequence of letters like this it is quite easy. Because I can start with this, I can say case 0 does not correspond to anything. 00 does not correspond to anything. 001 does correspond to something so, this must be a boundary. So, I can do this and say that the first letter must be a C. Now, again, I can start from here and do the same thing. And I will find that at 000. That is the first time I hit a boundary where I have a valid encoding so, that must be an e. And then again, I have a 001 which is a C and then I have a 11 which is an a and a 01 which is a b. So, if I have a prefix code then every time, I see the encoding of a complete letter I can break it off and say okay now I have seen this letter and I can read off the thing unambiguously there is no 2 ways of decoding this code. So, that is the kind of code that we want. (Refer Slide Time: 9:32) Now, our goal, if you remember was to find encodings which reduced the total number of bits that we sent we said that we have a frequent if we have this fixed length encoding, we are going to send 5 bits per letter. But if we have more frequent and less frequent letters it may be a good idea to vary the length so, that the more frequent letters have shorter encodings and the less frequent letters have longer encoding.'},\n",
              " {'id': '82807a62-bcd0-443c-b6c1-4491aa4c4946',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, that was the whole point of going to prefix codes. So, what we are really interested in this building some kind of an optimal prefix code, a prefix code, which will use as few lead bits as possible when transmitting a message. So, the first step is to measure the frequency. So, we need to know each symbol that we are going to encode how frequent it is. So, we have to measure the frequency of every letter in our alphabet. So, how do you measure the frequency? Well, the standard way to measure the frequency is to just take a large set of documents. So, you could go to a library of course nowadays, it is easier because if the things are digitized you do not have to count by hand you can use your program to count it. But you will count all the letters, you will see how many letters there are across all these books. Then you will see of this how many are a’s how many are b’s how many are c’s. So, all the letters which are of interest to you, you will count them and establish their frequency as a fraction the number of times you see an a divided by the total number of letters a to z, now (())(10:53) and so on. So, this is a fraction so, this we will call f of x. So, if I have an alphabet with n letters in it x1 to xn. And then each of these letters will have a frequency. And because it is a fraction of the total number of letters that appears across all the documents that I have counted these fractions will add up to 1. So, if you remember in probability this is how it works you have a probability assigned to each event and the total sum of the probabilities is 1 and each probability is between 0 and 1. So, you can think of f of x1 as the probability that a given letter is x1 because this is the frequency with which x1 turns up. So, if I am just generating letters at random then x1 will turn up with probability f of x1. Now, this probability is of course estimated from the documents that I read if I read a different set of documents might change.'},\n",
              " {'id': '7bdd76a0-4fbf-4f8d-89d4-4160246ec396',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And as we already said it might vary from language to language. So, if I read a set of English documents the frequency of the letters may be different from a set of French documents or a set of documents in some other language which uses the same alphabet. So, if we look at our message. So, now there is an unfortunate thing here. So, let us do this n is not the same as that and I will fix it later on. So, let us now, use small n for the total length of the message. So, we are sending a message. So, we are sending a message in English or Hindi or Tamil and that message has so many characters in the source language, in the language we are trying to send. Now, each letter in our source language appears these many times the its frequency times n. So, if for example, letter x1 has a frequency of 0.1 it means that one tenth of every message will have x1. So, if I have 100 letters in my message then 10 of them will be x1. So, that is what we are saying we take the frequency it is a fraction multiplied by the length of the message you will get the total number of times that particular letter occurs. (Refer Slide Time: 12:48) So, we now are trying to construct this encoding. So, we are wondering about how many bits I eventually use to send my message M. So, each letter in our encoding has a representation in 01 which we write capital E of x. So, x like a or b, or c is denoted by 01001 and so, on that is e of x. So, each letter is blown up by the length of its encoding, So, every time I see an a we will replace it by the string representing a in binary. So, each symbol single symbol a will be blown up to 3 symbols or 4 symbols or whatever it is that I have. So, if I look at the total length of my message now it is going to be the number of letters times the frequency of the letter. So, this together tells us the number of times x occurs times the length of.'},\n",
              " {'id': 'bf3a7749-01e7-4e62-875a-6818bc7a97af',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': \"So, if I look at a n times the frequency of a will tell me how many a's there are multiply by the number of bits I used to a. Then b times in frequency of b show me how many b's there are then the times the length of the b and then n times the frequency of c and so on and that is added up. So, this is my total message length. And if I take the total message length this gives me the overall number of bits. So, I use so many bits overall. And I have by assumption small n letters encoded so, if I divide by n I will get the average rate. So, the average is just this part. The summation of F x E x the other ways that is why you can think of it as a probability. If you remember probability this is the expectation. The expectation is the probability of each event times the event itself. So, this is the probability times the length of that thing. So, this is what we really want to think about how many bits on an average are we using per letter in our encoding. (Refer Slide Time: 14:44) So, let us look at a concrete example to understand this. So, let us take this earlier encoding that we had, so we had come up with a kind of just to show what a code look like we had come up with this encoding. So, this is the same encoding. But now we also have to associate some frequencies. So, I am just putting some random frequencies add up to 1. So, you can just check that this adds up to 1. So, if I take this, then I need to assign to each of these I mean so, the length is clear. So, the length is 2 to 3 2 3. So, if I want to compute now how many bits, I will take for a message what we said is that we will have to basically go back and just so it does not matter how long the message is because I can divide by n. So, I can get to this I just want to take the average by taking the number of the frequency of each letter times its length. So, if I take this, I have 0.32 times 2 from this from this part, then I have 0.25 times 2, then I have 0.2 times 3 0.18 times 2, and 0.05 times 3.\"},\n",
              " {'id': '908f3917-1e44-44e0-aa1c-094b02f1c264',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And if I add this up this comes to 2.25. So, what this is saying is that if I use this particular encoding for this an assuming. So, it is always assuming the frequencies if I assume this is the frequency and this is the encoding then I am going to be using on an average 2.25 bits per letter. (Refer Slide Time: 16:08) And remember that if I wanted a fixed length encoding, I have to use as many bits as I need to cross 5. So, I cannot use just 2 bits because if I have only 2 bits, I can only encode 4 letters so, I am forced to use 3 bits. So, in a fixed length encoding I would be using 3 bits per letter. Here, I am using 2.25 bits per letter. So, by going to this variable and length encoding on the length of the message I am reducing it by one fourth. 0.75 by 3 so, 25 percent saving in terms of the cost of transmission. But the problem is that this choice of encoding is not fixed and there may be a better one. So, here is a better one. So, I have just permuted this row here a little bit. So, in some sense, the a and e are the same as before but these three values have now been shuffled. So, in particular I have moved the long sequence from c to d. And there is a reason to do that because you can see that d has a lower frequency than c. So, it makes sense from our intuition that you should use a longer sequence to represent lower frequency letters. And indeed, if you calculate the same quantity this quantity if you calculate it now for the new thing. (Refer Slide Time: 17:21) It turns out to reduce from 2.25 to 2.23. Now, 2.23 is per letter. So, you can imagine that if we send a message which has got 1000s of letters there is a substantial saving even though the saving per letter is only come down by 0.02. So, this is our goal then our goal is that we are given this table we are given this row and this row we are given the letters and from some experimental evidence we are given the frequencies. And our goal is to find this best encoding.'},\n",
              " {'id': '55c7f07e-9cf6-4788-8d80-2795ff36df25',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And the best encoding is defined in terms of this average bits per letter which we will call abl. So, given an alphabet and with this associated frequency information compute an encoding which minimizes abl this is our goal. (Refer Slide Time: 18:08) So, to do this we will first look at our codes as binary trees. So, here is the encoding that we had before and here is a binary tree. Now, how does this binary tree represented? Well, first of all, as you can see the letters are sitting at the leaves of the tree. So, that is one part of it. So, the letters are at the leaves of the tree. (Refer Slide Time: 18:32) And now, how do you read off the code when you read off the code by actually looking at the path from the root. So, if I named these as 0 for left and 1 for right then 1 1 takes me to a. So, the code for a is 11, 001 001 it takes me to d. So, the code for D is 001. So, in that sense the paths of this binary tree represent the sequence of letters which encode that letter at the leaf and there is nothing in the middle. (Refer Slide Time: 19:08) And why is there nothing in the middle because if I had something here if supposing I use this to encode f then that would mean that the encoding 0 represents f but then if I extend the encoding to 01 it represents c. So, this is not a prefix code anymore. So, by ensuring that all the encoded values are sitting at the leaves no encoding leads to another encoding. I mean I do not have an encoding sitting below another encoding which is the same as saying that no encoding is a prefix of the other encoding. So, there are no internal nodes in this tree, which encode letters. So, we will reason about our codes using this representation because it is easier to prove things about this representation. (Refer Slide Time: 19:45) So, the first observation is that this tree will always be full. Now, full is something we have not seen before we have seen complete. So, complete binary tree is one where every level is filled.'},\n",
              " {'id': 'bca2bdea-703e-4094-a970-7dc6a25788f5',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And then we saw this heap kind of thing where we fill level by level. Now, full is something which is not complete and it is not like a heap is not necessarily filled level by level, but it has a property that I never have one child. So, I cannot have just one thing sticking off here this is not full. So, why is it that any optimal tree must be full? Well, imagine that I had only one child. So, supposing I have a situation where at some point I have only one child and then below that I have something. So, this node has only one but then if it has only one child if you remember we did this also when we did a delete for a binary search tree I could as well move this whole thing up and nothing will change. So, if I have only one child, I can promote that child and replace the node above by the child node and I will get a shorter tree and nothing else will change because there was no other child. So, no path is getting confused by this. So, I will end up with a valid encoding which has shorter paths to some of the letters So, shorter encodings for some of the letters so I might as well use it. So, every optimal prefix code must be full. Because otherwise I could make it more optimal by reducing the path lengths. Now, second thing is that if I take two leaves we take a leaf like this and if I take a leaf like this, then c is some sense is higher than e. So, if depending on which you are thinking about it, usually we talk about the depth. So, we start from the root and come down. So, the depth of c is smaller than the depth of e. And what we would like to claim is that things which are at a smaller depth, which are higher in the tree, they have shorter encodings and if they have shorter encodings, they must have higher frequencies. So, what we claim is that if your tree is optimal and x is sitting higher than y then it must have a frequency which is bigger than y you cannot have a smaller frequency than y.'},\n",
              " {'id': 'd62e6a5d-627d-4203-9307-739a47dee962',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'And the reason is simple because if in fact it was the case that f of c is smaller than f of e. So, c has a shorter encoding, e has a longer encoding but f is actually c is actually less frequent than e then I could just swap these 2 labels. And if I swap these 2 labels in that formula you will see that the multiplication now, is multiplied by a different frequency which brings it down. So, if I exchange the labels that average bit length will have to improve. So, this is the second claim. (Refer Slide Time: 22:25) Now, the third claim is that if something is at maximal depth is a leaf at maximum depth then it must also have a leaf on its side. So, for example this is a leaf whose partner whose sibling is not a leaf but c is not at maximum depth say the maximum depth is here. And here this has another leaf here. Now, this is obvious because if this thing does not have a partner which is a leaf, then that must have 2 children because remember this is a full tree. So, if it has a partner that is not a leaf it must have children but then those children must eventually lead to leaves which are further down. So, those leaves which are further down will be deeper than the leaf that we started with. And we were only talking about leaves at maximum depth. So, if I am at maximum depth and if I am leaf then my partner is also a leaf that is a very simple state. (Refer Slide Time: 23:26) So, we have these 3 claims. So, every optimum prefix tree must be a full tree every node has 0 or 2 children if I have a leaf which is higher than another leaf then the frequency of this leaf cannot be smaller than this one it must be greater than equal to and the third thing says at a leaf at the bottom most level must be paired with another leaf. So, this gives us enough to construct a tree optimum tree recursively. So, how does it work? So, the first thing is that we should be claim three I think so, from claim 3 we know that at the bottom most level of the tree we have pairs of leaves.'},\n",
              " {'id': '15c2492d-6842-4804-b221-c39cea5f43c4',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'Now, those leaves at the bottom most level of the tree must have the lowest frequencies among all the leaves they cannot have higher frequency than anything which is higher. Remember that this is always be three I will fix it later. So, now these must be the lowest frequency. So, basically, I can target that I can say that push the lowest frequency letters to the bottom of the tree. So, what I will do is I will pick the 2 letters x and y which have the smallest f x and f y the smallest frequencies in my alphabet. And I will assign them in principle to the bottom. But the way I will do it is I mean I am going to actually build up this thing recursively. So, in principle, these have to go to the bottom. So, let us assume that they are assigned to the bottom. So, now I have to process the rest of the tree and how do I process the rest of the tree? Well, I pretend that these 2 have been processed. So, I pretend that these 2 have been processed by combining them into a single node which represents both x and y. So, I will call it a compound letter x y. So, x y is a new letter which stands for a combination of x and y. So, basically, we are saying that at the bottom we are assuming now that we have something like this. So, we are going to pretend that instead of this we have something like this. And what is going to be the frequency of this x y? Well, x, y has to combine how many times x occurs and how many times y occurs. So, it is going to have a frequency of f x plus f y. So, what I have done is I have taken my original alphabet. And I have removed these 2 nodes corresponding to two letters x and y with smallest frequency and replace them by a letter a kind of fictitious compound letter x, y whose frequency is the sum of the 2. So, I have transformed my alphabet into something with 1 letter less. So, a prime has a minus x comma y plus the new letter x y.'},\n",
              " {'id': 'cdd66544-3472-454c-b817-58a000378bb0',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': '(Refer Slide Time: 26:04) Now, will have a smaller alphabet I will just assume that I can solve the smaller alphabet. So, we will see what is the base case, the base case is when your alphabet has only two letters. When you have any two letters then you have no choice. So, you have two letters. So, you will just put, say x1 and x2. So, this is my smallest encoding that I can do take two letters and call one, 0 and call one the other one 1 in terms of the representation in binary. So, I will just keep going down. So, I will start with n letters go down to n minus 1 letters n minus 2 letters. When I get 2 letters I have a trivial tree and now, I have to go back so, this is the point. So, the recursion has to be unraveled. So, in this recursive solution, which I have just constructed so, I had added this letter x y and the rest of the tree is sitting here. Now, I do not know what the rest of the tree looks like but certainly there will be because I am solving it for this alphabet there has to be a leaf labeled x y. So, I will take that leaf labeled x y and I will expand it by I will reverse the process what did you do earlier I took this and I imagined that I had done this. So, I will now do this in reverse I will take that leaf and replace it by an internal node and 2 children labeled x and y. So, this is a recursive algorithm and this is called Huffman coding after the person who discovered or invented it called David Huffman. (Refer Slide Time: 27:37) So, let us see how this algorithm works to get an idea about what it is doing. So, supposing we have that original example that we had constructed but now, I am only recording the frequencies I am not assuming the encoding because I have to discover the encoding. So, these are my letters and this is the frequencies This is the same table that we had earlier. So, you can go back and check that. So, this is the same frequency table I add here 0.32 2.25 0.2 0.18 0.05.'},\n",
              " {'id': '3c7631d9-48d0-44cb-ad0a-d41c18dc311f',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, now I have this frequency table now, what I will do is I will look for the 2 smallest letter. So, it is the 2 smallest letters here are d and e. So, I will combine d and e as a single letter de and I will reconstruct this table where I have one letter less, I remove d and e separately and I produce a new compound letter called d which is a frequency 0.18 plus 0.05 which is 0.23. Now, I do the same thing now, I look in this and I see which are the 2 smallest I find that this and this are the 2 smallest among the frequencies that I have now got. (Refer Slide Time: 28:44) So, I will combine the letter c with this compound letter de and I will create a new compound letter which I will call cde and its frequency is going to be the sum of these 2 it is going to be 0.2 plus 0.23. So, is going to be 0.43. Now, if I look at this, I find that these 2 are the 2 smallest letters. (Refer Slide Time: 29:02) So, I will now, combine a and b as the letter ab. Now, I have reached an alphabet a new alphabet which has come down to 2 letters. There is a letter called ab and this letter called cde but in this compound sense if I do not care what the letters are called there are only 2 letters. So, this is my base case. So, in the base case I can build a tree and what is the tree look like it just looks like a root node with 2 children. So, 0 encodes in I mean I can choose either order it does not matter 0 encodes a b 1 encode cd I could do the other way around also. Now, I have to unravel this recursion so, I have to take the last thing I did the last thing I did was to combine a and b into ab. So, I will now take that leaf node remember what it said, d prime you take the leaf node label x y and you split it as 2 node leaves called x and y. (Refer Slide Time: 29:50) So, I will basically split this compound leaf. So, that ab which was here is now replaced by 2 new leaves below it called a and b. Now, if I look at this step the next thing, I did was cde.'},\n",
              " {'id': '22f65a7b-4800-47f0-845c-8ff5dc7ac9d4',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, I take this letter and split it. So, I split this letter. (Refer Slide Time: 30:06) And again, I can do it left or right it does not matter I could have also done de and c it does not really matter how I do it. Finally, the optimum code will be up to this 01 relabeling. So, there is no clear reason why one is better than the other. But I just create 2 children based on the what I had combined. And then in the final step, I split this de because that was the first thing I do. So, I split this de and I get this code. Now, earlier if you recall we had reversed encoding of this. So, the earlier encoding of this was actually optimum. But we had done the encoding in reverse. So, we actually split it from right to left rather than left. But this is exactly the same encoding except that every 0 is a 1 and every 1 is a 0. This is what we were working with before. So, that is actually that 2.23 which we found was actually an optimum encoding. And this is how the algorithm would find it. So, now, the question is why is this optimum? So, this is how the algorithm works. But like any greedy thing which is, so here, why is it greedy? Because at every point, we are just taking the two letters, which have the current lowest frequency, and we are combining them without asking whether anything better is possible. (Refer Slide Time: 31:20) So, the optimality of this is by induction on the size of the alphabet. So, as we said, if you have only 2 letters in your alphabet there is nothing much you can do you can just split them into 2 leaves and call them one of them 0 one of them 1 So, that is optimal. Now, let us assume that for any alphabet of size k minus 1 are algorithm produces an optimal tree. And Now I am working with a tree of size alphabet of size k. So, the algorithm says combine the two letters of lowest frequency construct a tree for the smaller alphabet, which by induction is going to be optimal, and expand the leaf to get t.'},\n",
              " {'id': '004f5533-1823-4ef0-9f5c-4ec695c7c6b8',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, the question is what happens when I expand this leaf. So, the claim is that the bit length of this expanded t is exactly the bit length of the old the optimal t prime which I got in the recursive thing plus the combined frequency of this new letter which I have combined and then destroyed f of x y remember f of x y is f x plus f y. So, this is a this is saying that after I split this tree of course, the path length to those new letters is one more than it was before. So, the average bit length is going to increase. And how much is going to increase is going to increase precisely as f x plus f y which is the same as f x y. So, notice that nothing else changes the tree otherwise was untouched. So, the only difference is that I take out so, I have this x y. So, I will remove it and instead I will put an x and a y below it. So, this is the only change that happens. So, I am going to subtract from the bit length, the contribution of the letter x y that I had before, and that x y, remember, depth is a proxy for the path length which is a proxy for the encoding length. So, if something is a depth 1 it is 1 bit with the depth 3 to 3 bits and so on. So, if I take depth of the letter encoding x y times the frequency of x y that is the bit length of that hypothetical letter x y which was contributing to the average bit length of t prime. Now, I am going to remove that node so, that is no longer there. And instead, I am going to have two new contributions which is the depth of y times the encoding of y. And I mean the depth of y times the frequency of y and the depth of x times the frequency of x. But these are all connected the frequency of x, y is just the frequency of x plus the frequency of y. That is how we defined it. And the depth is now exactly increasing by 1. So, I am removing. Let me call it d d of x y times f of x y and I am adding instead d of x f of x plus d of y f of y. So, this is d of x sorry this is f of x plus f of y. And this is 1 plus d of x y.'},\n",
              " {'id': '8aac4b36-5d87-4870-a432-a94067a7ad0b',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'This is 1 plus d of x y. So, I am going to cancel d of x y f of x d of x y f of x I am simply going to cancel d of x y f of y and d of y d of x y f of y. So, what I am left with is 1 copy of f x and 1 copy of f y and that is precisely f x plus f y. So, this is what is being added back and f x plus f y is just f of x y. So, if you work it out for yourself you can see that by subtracting depth of x y times f of x y and adding back depth of x times f of x plus depth y times f of y the net changes f of x y. So, the net increases f of x plus f of y which is f of x y. (Refer Slide Time: 35:03) So, this is what we have now, we have to somehow argue so, all we can show by induction is that this tree was optimal. So, why is this tree that I have got by doing this reverse transformation optimal. So, let us assume that we have some optimal tree S for the current alphabet. And let us assume that that tree is better than the tree that I just constructed. So, its average bit length is strictly less than average bit length of t. Now, in that tree the leaves could have any arbitrary label but I know that the lowest leaf labels will have the lowest frequencies. So, they could be paired in different ways. So, if I have 4 labels I could have a, b, c, d, like this. But I could also write a, c, b, d and this would have the same bit length. If I just exchange the labels at equal depth there will be no change need no change in the bit length. So, it could be that S has some arbitrary way in which it has done it. But I can always shuffle it around keeping things at the same level. And I know that my x and y, the ones I am interested in must be at the lowest level. So, I can bring them together at the lowest level because they occur in pairs. So, I will take S and I will first rearrange the leafs at the lowest level so, that x and y come together having got them together in that tree S I will merge that exactly as I did here.'},\n",
              " {'id': '2753f4f9-ca1a-491d-b845-f4883ecc4be1',\n",
              "  'metadata': {'chunk_idx': 18, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'But I am not making it a recursive call I am physically taking S the tree S and I am looking at the leaf x y and sorry the leafs x and y and I am merging these into a single new leaf called x y. So, I am going to contract these. So, I get a tree S prime. So, I have taken an optimum alleged optimum tree S for a I have found a leaf in it which is called x and a leaf called y which are siblings and I have merged them to form a new node called x y. So, now this is a tree S prime which is a encoding of a smaller alphabet this smaller alphabet A prime. So, S prime is over A prime but A prime we already have an optimal thing T prime is already known to be optimal by induction for A prime. So, S prime cannot be bigger than T, S prime cannot be smaller than T prime. So, it must be that the average bit length of T prime the known optimum is at least as good as that of S prime. So, that means that if I look at the difference between S and S prime is going to be by the same calculation the same as difference T and T prime namely f of x y. This claim holds for S and S prime because I have done exactly the same operation. And therefore, it means that if this is smaller than this then this must also be smaller than that. So, that the T that I constructed cannot be worse than the S that I constructed. So, this assumption that I have is strictly better S is wrong. So, that is the proof of optimality by induction. (Refer Slide Time: 38:07) So, how would you implement this? Well, what we have to do is we have to keep looking for these two letters of minimum frequency and replace them with a combined letter of composite letters or combined frequency. And if you do this naively then you will have to scan the frequencies find the two minimum, minimum and second minimum and combine them, add them and put them back. So, this will take a linear time to scan and we do this we have to keep shrinking by one.'},\n",
              " {'id': 'b6138c85-2a54-4923-b694-4d1b2144c2a2',\n",
              "  'metadata': {'chunk_idx': 19, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, we have the time I mean we do linear scan and everything overall is going to be a quadratic calculation. Now, as you might guess whenever we need to find repeatedly find the minimum and then replace it back with something else and keep maintain the minimum the correct thing to do is a heap. So, if we keep the frequencies of the letters in a heap then finding the two minimum is doing delete meant twice which is a logarithmic operation and then you insert back an element with the sum of the frequencies that is also logarithmic. So, every update is logarithmic. So, overall, you can move from k squared to k log k. And of course, then you have to also keep track of the labeling and all that. But that is very easy to do once you have got this choice fixed properly. (Refer Slide Time: 39:18) So, why is this a greedy algorithm, is a greedy algorithm because we are making this choice locally, every point when we have an alphabet with frequencies, we are finding the two minimum ones combining them. And we are never going to go back and combine letters in any other way. So, those two will be siblings forever. So, that is something which we do not go back and do. So, it is a very local choice, is a locally optimum choice which we never revisit we never go back. So, it follows our classical paradigm that we have shown seen for greedy that is we have a number of choices and make one based on some local heuristic. And we stick with that. So, this problem actually has an interesting history. So, I will just conclude with that. So, this idea about encoding thing optimally is actually the subject of a very interesting area called information theory which was invented by Claude Shannon around end of the second world war it was intimately tied to the rise of radio and telecommunications. Because at that time people was seriously being becoming interested in transmitting messages over long distances and trying to minimize the cost of sending these messages.'},\n",
              " {'id': '56c9da5b-938c-4aae-8b5d-641d5a5d758c',\n",
              "  'metadata': {'chunk_idx': 20, 'week': 7},\n",
              "  'source': 'Greedy Algorithms-Huffman Coding.pdf',\n",
              "  'content': 'So, information theory does this kind of analysis of frequencies and so on. And it says if you want to encode this kind of an alphabet with these frequencies you must have an average bit length of so much. So, it gives you a lower bouncing you cannot do better than this. But information theory on its own is not constructive theory it does not tell you how to achieve that thing. It might say that you can do it in you can cannot in better than 2.23 bits per letter or whatever bit it will not show you how to do it. So, constructing the optimum node and knowing that the optimum code has a certain size are two different things. So, in fact Shannon worked on this problem along with another mathematician called Robert Fano. And they came up with recursive solution which was different from this which uses the kind of partitioning of the alphabet. And they came up with something called Shannon-Fano code so that these Shannon-Fano codes were not optimal I mean with respect to the what could be achieve they do not always produce the smallest average bit length. So, the person who invented Huffman coding, David Huffman was actually a graduate student at MIT studying this topic under Fano. So, he was a student in Robert Fano’s class and Robert Fano are assigned this as a term paper. He said either you can write a paper showing how to come up with the optimum code or you can write the final exam. So, the story goes that Huffman tried and tried he was about to give up. So, he had almost given up on this term paper and he was all set to write study for the exam. And then suddenly he came up with this idea which we are now known as Huffman algorithm. So, it was actually a kind of a graduate student who came up with this idea after being told by somebody who tried to prove it and could not prove it. So, it is an interesting story and hopefully inspirational for all of us.'},\n",
              " {'id': '1fdedc9f-a7ac-442a-a7ef-200c19224cbe',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Madhavan Mukund Balanced Search Trees (Refer Slide Time: 00:13) So, we saw that if we have dynamic data, data in which values are being inserted and deleted and we want to maintain a sorted property so that we can search efficiently in it, then we need to move from a linear structure to a two-dimensional structure and we came up with this idea of search trees. (Refer Slide Time: 00:30) So, in a search tree, if you remember, at every node you have values and all the values on the left are to the smaller than v and all the values on the right are bigger than v. So, this tells us how to search for a value. We look at the root node and we can decide whether to go left or right. And remember that in a search tree there are no duplicates. So, with this we had this property that if you actually started navigating the tree you would always walk down a single path whether you want to find a value or insert a value or delete a value. So, therefore all operations on a search tree are actually in the worst case proportional to the height of the tree. But the problem that we saw was that you could have an unbalanced tree, for example I could have a sequence where I start with an extreme value on the top and I could have just a linear sequence going down one side of the tree. So, this would have height which is proportional to the number of nodes or to the size. So, what we would like to do is come up with a way to have balanced trees which have logarithmic height. So, if we have logarithmic height then we are guaranteed that these operations all take only logarithmic time, and so over a sequence of n such operations you will take n log n time, which is similar to what you would do if you do n binary searches on a sorted array. So, even though the data is changing from time to time including the cost of inserting and deleting items and the cost of searching for them using this find operation, everything takes log n time.'},\n",
              " {'id': '56f21b9a-5b09-4091-8e56-4924bda6d267',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, over n such operations if you could balance the tree, you would do n log n work. (Refer Slide Time: 02:05) So, our question today is how to balance the tree? So, we know that the tree might be balanced to start with but as we start inserting and deleting items then the balance will change. So, what we need to do is incrementally adjust the balance so that at any point the tree does not become too unbalanced because it will take a lot of work to reconstruct the balance tree which becomes too unbalanced. So, we want to incrementally keep the balance so inductively as we go along we always have a balanced tree. So, to get started on this we not, first need to agree on what we mean by balance right. So, balance roughly means what we think of. So, if you take a balance in the conventional sense, when you go to say the vegetable market the person has a balance. So, the balance means that what is on the two sides, the weights that the person puts and the vegetables on the other side balance out, they are about the same. So, in terms of a tree what you are saying is that if I look at the tree and I look at its left sub tree and the right sub tree then these two trees must be roughly the same in some quantity, some quantitative measure. So, we have two quantitative measures that we can define on trees as we have seen. So, the first one is size. So, size is just a total number of nodes. So, this is just a total number of nodes. And height is this length of the longest path from the root to a leaf. (Refer Slide Time: 03:30) So, supposing we first try to define something on size. Now of course, a perfectly balanced tree, now balancing is something which has, has to happen everywhere. We do not want the tree to be unbalanced anywhere. So, at every node we want the same number of children on the left and on the right. Now, what happens now is if I have one node it is fine. If I put something here I am forced to put something here because I need them to be the same.'},\n",
              " {'id': '96a49360-9582-4f0f-869a-a04101d0986e',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'Now, if I put something here because this, this part must be balanced I must put something there. And similarly, now because the must be balanced, I must put something there. So, either I have a tree with only one node or I have this tree with two levels complete or I have a tree with three levels complete and so on. So, these are called complete binary trees. So, you can think, remember in a heap we might have had a situation where we had only this much. So, we did not have these two nodes. So, heap builds up a tree in a kind of complete fashion by doing it level by level but it is allowed to have some incomplete nodes on the last level. In a complete binary tree every level is full. So, either you have one node or you have one plus two nodes or one plus two plus four nodes. So, every complete binary tree will have 2 to the power k minus 1 nodes for some power of k, to 2 to the power 0 minus 1 2. But now if this is the only situation in which you can meet this, it is obviously not useful because why, how do we guarantee that over this period of sequences, and sequence of deletes and inserts that the number of nodes we have has this nice property. So, we cannot demand that the size must be equal at every node because that is too restrictive. It only applies when the size of the tree is a particular value. (Refer Slide Time: 05:04) So, we could relax it a little bit by saying okay, we do not want it to be exactly equal, we just wanted to be within one, for example. We want roughly as many nodes on the left as the right, and this will give us some better way of doing it. But it turns out that maintaining size on the left and the right is very difficult to do in an incremental fashion. So, we will, not although this is something which could work we will do something which is different from this using height. (Refer Slide Time: 05:32) So, remember what height is. So, height is the number of nodes on the longest path.'},\n",
              " {'id': 'fef4c2aa-ba2b-431d-add0-a9467a2ed4c8',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, you start at the root and you walk down and you pick the longest path to, among all the leaves that you have. You see how many nodes you visit. So, if your tree is empty then there is no root, so the height is 0. If the tree has only a root then the height is 1 because the longest path, the root is also a leaf, the longest path consists of just that one node. In general, if I have a tree then if I want to find out its height so I will, inductively I will have a height here and I will have a height here. So, this is going to be 1 plus the maximum of h 1 and h 2 because the longest path from here is either the path that follows the longest path on the right or it is the path that follows the longest path on the left, and in either case I am taking that longest path in my left or right sub tree and adding this one node to it so it is 1 plus the maximum of the heights of the left and right sub tree. So, this is how you formally define the height. So, what do we mean by a height balanced tree? (Refer Slide Time: 06:38) So, let us assume that there is this function in our python notation called height which we can apply to a node and it will return the value of the height. So, what we really want is the height of the left sub tree, self-dot left dot height, and the height of the right sub tree, self-dot right dot height should not be more than 1 off. So, they need not be equal but they cannot be more than 1 different from each other. So, they must be within 1. It does not matter whether self is, left is more or right is more. Either of them could be more but the difference should not be more than 1. So, it turns out that we can actually maintain height balanced trees. And height balanced trees are also called AVL trees, and this is named after the two people who first discovered it so there is a Russian mathematician computer scientist called Adelson Velskii, and there is another one from the west called Landis.'},\n",
              " {'id': '1f2c375f-616d-4f23-a141-c43238367ff3',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, both of them in some sense parallelly discovered this class of trees and how to manipulate them to maintain this height balance. So, these height balanced trees are also called AVL trees. There are other types of balanced trees which you can use but we will not look at them. We will only look at this particular one to illustrate that it is possible. (Refer Slide Time: 07:54) So, of course now we have chosen something plausible. We said we have size and height, and we kind of, without giving a proof we said that maintaining size balance is difficult. So, we decided to opt for height balance. But the whole purpose of this was to get this logarithmic height. So, how do we guarantee that if we have a height balanced tree and we have n nodes the height is proportional to log n. So, we first have to establish this mathematical fact otherwise there is no point in going for a height balanced tree because it does not serve our purpose of making everything logarithmic. So, what we want to try and see is how bad can it be. So, bad in this sense means that I would like height to be small with respect to size, which is the same as saying that I would like size to be big with respect to height. I want to size to grow as a function of height very fast. So, what is the smallest size I can restrict a tree to if I give you a given height? So, this is our, so what is the minimum height balanced tree of a given height? So, if it is 0, we know it is the empty tree. If it is 1 then we just have this trivial tree consisting of a root node because that is the only tree of height 1. If it is 2, it is easy to see it is either this one or it is the one symmetrically, which has a right child. I want height 2 so I have to have a child of the root but I want to have the smallest number of nodes with such a property so I do not put the other child. I could also put both children but then I would get a larger tree.'},\n",
              " {'id': 'e91487e7-ab68-4ac3-bfa2-c87f540291fd',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, the goal is to find the minimum size tree that is height balanced for a given height. So, by inspection is easy to see that for two I can either put a left child or a right child and I will get a tree with just two nodes which has height 2. What about three nodes? Well for three nodes first of all I need to construct a path of length 3, that is the minimum that I need. Now, once I construct a path of length 3 on one side then remember that the two sides cannot differ by more than 1 so I am forced to construct a path of length 2 on the other side. So, I am forced to put something on the other side. But that can be only one node because I do not need to put, as, I do not need to put more than necessary. And because of the property that this part is already height balance I do not need to put a right child at the bottom and I do not need to put, of course, anything there. So, for a height of 3 I can get away with the tree which has only four nodes. So, basically if I have 0, no height, I get 0 nodes, if I have 1 height I get one node, if I have 2 height I get two nodes, if I have 3 height I get four nodes. So, it does not seem as though we are getting very far in terms of being, our goal is to say that if I fix h then size must be large. And it does not seem to be growing very fast. It is growing very slowly, as a matter of fact. So, how would we actually proceed to the next level? If I want to find, now for 3, we did it by, kind of inspection. By, by 4 it is not very clear. So, if I draw a tree of height 4, I first have to construct a path of length 4, and then I have to start balancing it all over the place. I need to make sure that this part is balanced so I need to put something here, then I need to make that this part is balanced so I need to put something there and so on. So, how do you systematically construct the next level tree, which is small with respect to the height 1 more, height increasing by 1 more?'},\n",
              " {'id': 'bc24f9d0-417a-412b-8e5e-9f9cbc1254a3',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': '(Refer Slide Time: 11:20) So, it turns out that this, in fact is, I mean up to symmetry and all that, of course, all these things remember, are symmetric. I could have drawn this tree also by putting the thing on the right-hand side for example. So, there are many possible ways of drawing these trees. So, up to symmetry I claim that this is the smallest tree you can draw of height 4 which is height balanced. Now, if you look at it closely you will see that on its left it has this tree. And on its right it has this tree. And this is not really a coincidence because if I want to increase the height by 1 then the left sub tree must have height minus 1. So, I need, so if I want the overall height to be 4, I need something on one side which has height 3. And I want the smallest possible tree of size of height 3. So, I already know how to do that. I take the smallest possible tree of height 3. I know it is height balance and I put it on the left-hand side. Now, in order to keep this height balanced, because I put 3, there I must put something which is 2 on the other side, at least, because otherwise the difference will be more than 1. So, I take the smallest possibility of height 2 and put it there. So, this is the general strategy. If I want to build a tree of height h which has the smallest number of nodes possible, I will first find the smallest tree of height h minus 1 and stick it on the left sub tree because that h minus 1 plus this new, the new root gives me a height of h. Similarly, I will take h minus 2 and stick it on the hand side. Again, I want the smallest tree of that size. So, if I if I keep incrementally finding the smallest tree of every size then when I get to h I know the smallest trees of the smaller sizes. So, I can stick the correct ones together to get myself a height balanced tree which is smallest possible of size h.'},\n",
              " {'id': '402f99b1-0491-4e6e-b3ff-7dc4ca061120',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': '(Refer Slide Time: 13:05) So, so this is our, I mean we argue that this is the smallest tree that we can construct which is height balanced and has a given height. So, our goal is to show that these trees are growing fast. So, notice that up to here it looked like it was growing slowly because I had, remember, one node here, two nodes here, four nodes here, but here suddenly I have got 1, 2, 3, 4, 5, 7 nodes here. So, there is some hope that things are actually growing faster than, that the next level would be even faster because remember, the next level will have this on the left and this on the right. So, it will have seven nodes on the left and it will have four nodes on the right. So, overall this will have 7 plus 4 plus 1, it will have 12 nodes. So, there is some apparent increase in the growth. So, can we quantify this? (Refer Slide Time: 13:57) So, let us try to define this function. So, the capital S of h is the size of the smallest tree we can draw which is height balanced and has h. This is what we want to calculate. How big is or how small is S of h compared to h. So, we have seen that if the height is 0 then it is 0. The size of the smallest tree with height 0 is 0, and for 1 it is 1, that we drew. Now, our strategy tells us that the way we construct the smallest tree of size, of height h is to first take the smallest tree of the lesser height, h minus 1, take another tree of h minus 2 put on the other side. So, this, this node is balanced. And then I have to account for the new root that I have put in, so I have to add a 1. So, that is what we said here. If I go here, this is 2 plus 1 plus 1. So, I get 4. Here I get 4 plus 2 plus 1, so I get 7. And the next one as we said will be 7 plus 4 plus 1 will give me 12. So, this gives me this recurrence which says that S of h is 1 plus S of h minus 1 plus S of h minus 2. So, how do we figure out how fast this grows? Why is this going to become a large number? Well, I am not going to show you why it grows fast.'},\n",
              " {'id': 'c6b69f3c-5069-4ea0-872c-f70e955c6c68',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'I am just going to show you why it should grow fast compared to something which you may have seen or you can go and look up very easily, which is the Fibonacci sequence. So, if you remember the Fibonacci numbers, you start with 0 and 1, and then you keep adding the previous 2. So, the next number is 1 plus 0 is 1, the next number is 1 plus 1 is 2, then 3, then 5, then 8, then 11, and so on, and these numbers grow pretty fast. So, the recurrence for this is that the 0th Fibonacci number is 0, the 1th Fibonacci number is 1, and the nth Fibonacci number is the sum of the previous two. So, if you look at this, the big difference between our recurrence and the Fibonacci recurrence is that we are adding 1. So, we are not just taking the previous two values but we are adding 1 to it. So, what this says is that S of n is definitely bigger than or equal to F of n. Initially it is the same but after I get to 2 it is going to become bigger because the second Fibonacci number, if you notice, is 0 plus 1, whereas here I get 2. So, the size of this tree is growing at least as fast as the Fibonacci numbers with respect to the height. And now you have to go and find out for yourself that the Fibonacci numbers grow exponentially. So, the nth Fibonacci number is actually exponential with respect to n. So, given that the nth Fibonacci number is exponential with respect to n, the size of the h height tree must also be exponential with respect to h. (Refer Slide Time: 17:02) So, this rests on the argument that Fibonacci gives you an exponential growth and the size of this smallest tree of height h is sitting above the Fibonacci sequence because I am actually adding 1 with respect to what I would be computing if it was a Fibonacci sequence. So, with this argument we are now, we are now showing that the size grows fast. And if the size is exponential in h, if I turn it around then it means that h is logarithmic in the size.'},\n",
              " {'id': '50e33b14-e2d3-40df-8ecf-79dbfb51dfbe',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, if n is the size then n will be exponential in h, so h will be logarithmic in n, that is what we want. So, this justifies mathematically why height balanced trees are of interest to us. If we can maintain height balance then we can actually get logarithmic height trees. And if we can get logarithmic high trees as we saw all our operations on search trees will be proportional to log n. (Refer Slide Time: 17:56) So, the difficulty is that this balance gets disturbed when we change the tree. So, how much does it get disturbed and what is the measure of imbalance? So, from a natural perspective supposing somebody holds up a balance, as we said, with weights, and it is not balanced, then it will tilt to one side. So, let us try to quantify this slope. So, let us say that the slope at a node is the difference in the height between the left and the right. The slope could be now positive or negative because it is not the absolute value. It is saying take the left height subtract the right height. If the left height is more you will get a positive value, if the right height is more you will get a negative value. (Refer Slide Time: 18:45) So, our definition of height balance says that these two quantities cannot differ by more than 1. That means the , the slope can be 0 if they are both equal or it can be plus 1 or minus 1 depending on which one is more. So, there are only three legal values allowed for the slope of a node in any balanced tree. Now, what happens when we insert or delete a value with respect to a balanced tree? So, if we insert a value, so remember that I could have a tree which looks like this. And supposing this has slope plus 1. So, the left hand side is already more than the right hand side by 1. And remember what insert does. It walks down until it finds a leaf where it cannot locate the thing and then it sticks a new value there.'},\n",
              " {'id': 'a7b2b8c7-3889-4d19-98cb-3cd62c6ec3ce',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, it will now increase the height perhaps of the longest path on the left by 1, and so the slope will go from plus 1 to plus 2. And the same thing could happen on the right hand side. Instead of this if it had been minus 1, it could have been that I come here and add something and it becomes minus 2. So, insert can clearly make something go from plus 1 to plus 2, or minus 1 to minus 2. And delete can do the same thing in reverse. So, supposing this is already plus 1, and then I delete a node here and this, this three actually shrinks. Then the tree which had a plus 1 balance becomes plus 2 because the shortest side is shrunk. So, in general if we try to arrest this imbalance as soon as it happens then I claim that the only slopes that are of interest to us, which are outside the balance range are plus 2 minus 2. It cannot be worse than that because I have only deleted or inserted one node at this point so the height has not changed. So, notice that that is all that happens if you go back and look at the search three things. It either increases the length of a path by 1 or it decreases by deleting the maximum, it deletes one, one path by 1. So, how do we fix this balance? (Refer Slide Time: 20:40) So, the general way to fix this balance is by moving things from one side to the other. There are too many things on the left, you move it to the right. There are too many things on the right, you move it to the left but you have to do it in a sensible way to respect the search tree property. Remember, the search tree property says that the root must be in between the values on the left and the values on the right. So, let us first look at the case where there is too much on my right. So, the right has a height of h plus 2, so this is where we have a slope of minus 2 technically because the left minus the right. The slope is minus 2. So, I have too many nodes on the, I have too long a path on the, on the right sub tree. So, I want to bring things from here to here.'},\n",
              " {'id': '0fe35373-3ae2-4c81-a65e-2c0ee998d013',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': '(Refer Slide Time: 21:18) So, in order to do that what I will do is I will first expand this tree. So, notice that this has height of at least 2 because h plus 2, h can be at, at least, cannot be smaller than 0. So, there are at least two things, so there is at least one node, and then one more node. There is at least two nodes on that path there. So, I can fill up, pick out the root of this and then split it into two parts. So, if I expand this, what I mean by expand is, I focus on the root of this, the left, the right child of the original node, which I will denote with this diamond symbol to, so the diamond symbol is a different value from this dot symbol. I do not know what they are but I know that the diamond is a bigger value than dot. So, what do we know? We know that everything here is smaller than the dot, everything here is smaller than the diamond bigger than the dot, everything in R is bigger than the diamond. So, I have three different groups of values. So, everything in the leftmost tree is smaller than everything, everything in the L tree is between the dot and the diamond value, and everything in the R tree is bigger than both the dot and the diamond. Now, I want to transfer something from here to there, but I want to do it in a kind of local way. I do not want to go down and move things around. So, I want to do some surgery at the top of the tree. So, the surgery at the top of the tree will mean that I am actually going to take this and kind of make it like this. So, let me just draw it properly. (Refer Slide Time: 22:37)  So, this is my rotation. So, I have taken this and I promoted this to the root. Instead of being the right child it now becomes the root. So, what was the root now becomes a left child. So, that is my rotation. But now I have to make sure that all these other values are in the correct place and I want to do a minimum amount of work. So, remember that this was to the left of everything so it remains to the left of the black dot.'},\n",
              " {'id': 'd1376dea-0e2b-469e-849e-87231234a019',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'This was bigger than everything so it remains to the of the diamond. And this was bigger than the black dot, and smaller than the diamond so now it is smaller than the diamond bigger than black dot. So, by just keeping track of the original property of what these values had with respect to the nodes above, you can figure out where to do this. And as we will see that this rotation actually is very easy to implement because I just have to change some left-right pointers at the top of the tree. I do not have to go deep into the tree to, so I only need to do a constant number of changes. So, this is a left rotation. I take things from the right, and I kind of, I kind of rotate the tree. But in the process some big chunk from the right actually comes to the left. It is not just at the top, but some stuff from the right also comes to the left. And of course, what happens here, if you now look at the slopes carefully, let us try to analyze what has happened. So, first of all we said that there was a difference of 2. So, let us assume that the left was h, then the right must be 2 more, exactly 2 more because that is the only case you are interested in, so h plus 2. If this is h plus 2 then something below it, below this must be h plus 1. So, this says that either the left tree is h plus 1, or the right tree is h plus 1 or both. And because it is balanced they must be at least h. So, it says either I could have h plus 1, h plus 1, both are h plus 1 therefore the diamond is at h plus 2. Or I could have h and h plus 1 so that the diamond is still at h plus 2 but it is balanced so there is a difference of 1 or it could be the other way. It cannot both be h, but just for conciseness I have written it like this but this, this plus this is not possible. I cannot have, I cannot have a situation where I have this and this. So, that is not allowed. So, now when I move these things, of course I am not destroying any nodes. So, the nodes are still the same.'},\n",
              " {'id': 'b5405e94-c7d5-4fc2-91ff-16e89055da9f',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, this tree is unchanged, so this remains of height h. And these trees are also unchanged. So, what is the worst that could happen? The worst that could happen is that this actually was h, and this was h plus 1. Remember, either possibility was there. So, if this is the case then this is at h plus 2. And so this thing has actually gone to something which has a bad slope. So, this is, from here it is coming as h, so the difference between the right and the left is actually plus 2. So, it has gone from minus 2 to plus 2. On the other hand, the good case is that this was h and this was h plus 1. If this is the case then both of these are h. So, this is a tight h plus 1, but this is also a tight h plus 1, so I actually have a 0. So, I could have a 0, and if now both are h plus 1, then I will have plus, I will have h plus 2 here, and h plus 1. So, I could have any of these. And remember I have gone from minus 2 to plus 2. So, on its own a rotation need not fix the imbalance. We will see that you can combine rotations to fix imbalance. So, this is a left rotation. I take things from my right sub tree and move it to the left because the right has too much. I could do the same thing the other way. (Refer Slide Time: 26:20) So, there is symmetric right rotation which takes things from the left and moves it to the right. So, now my left is too, too tall, and I want to move things to the right. So, again I expand the left exactly as I had done before for the right. So, this is a completely symmetric operation. Now I am going to take things from here to there. And again I observe that this is extreme left, this is extreme right, and this is in the middle. (Refer Slide Time: 26:45) So, after I move it by the same argument what will happen is that this remains to the left of everything, and this remains to the right of everything, and this will come now between these two nodes. So, it comes to the right of the diamond and to the left of the black dot.'},\n",
              " {'id': '8598f34b-baeb-4397-bf9f-ef485ef0ec24',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'And by the same argument I started with a plus 2 slope, and in the extreme case I might end up with a minus 2 slope because this could be h and this could be h plus 1. So, this could be at height h plus 2. Or I could get a zero slope or I could get a minus 1 slope. So, we have these two rotations. They are symmetric operations, they are exactly the same except they go in opposite directions. (Refer Slide Time: 27:22) So, let us see how to implement it. So, this is now the left rotation, the first one that we saw. So, we want to take this and go there. So, we are going to rotate at this point. So, this is my self. I am applying the rotation at this node marked v. So, what I am going to do is I am going to create a new node here, and I am going to copy v there, and I am going to promote, so I am going to basically do some surgery on this tree. So, I am going to create a new left child. And what am I going to put there? I am going to put this value v. So, how did I get the value? I first said what was the value that I started with, so that value, this is a little bit like inserting at the beginning of a linked list. I am going to copy the value that I have one position down. And then I have to now take these three trees and put them in the place. So, I have a left tree, this is tl, I have a right left tree, that is if I go right and left, and I have a right right tree. So, I have these three trees. And of course I have a value there also which I have to preserve. So, finally what I end up doing is I cannot change my current node. This is the basic principle. I cannot change the node that self is pointing to. So, I have to copy whatever vr has into self. So, I am going to have self-dot value has to be the value of vr. And I have to set its left and right correctly. Its right is going to be whatever it already was, which is the right tree that the node was pointing to. The left is going to be what it already was, which is this one, this is the new node.'},\n",
              " {'id': '78dbeba4-4555-4f35-a76e-9da2992ccef0',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'And the new node now has on its right this tree trl which was already. So, so I am just doing, now notice that this is a top-level constant number of operations. I am I am doing some five assignment statements, then I am creating a node and I am doing some four more assignment statements. So, it does not, it does not matter how big the tree was underneath. I am only doing some surgery at this top level. I am only interested in exploring the top level of this tree and making some local, local choices. So, this is a constant overhead. Making a rotation at the root of a tree takes a constant amount of time independent of how big the trees actually is. (Refer Slide Time: 29:38) And of course now you can obviously imagine that the right rotation is symmetric. So, I copy the current value as v. Now, I take the value from my left and keep it as vl, and then I create a new node to the right, so this is this new node, and then I populate it with my current root value. And then I put the right trees, I hang off the trees correctly. So, I take this tree and I hang it off the root, I take this tree and I hang it to the left of this, and I take this tree and I hang it to the right of this. So, this is the rotation. So, rotation is very easy to implement given our existing python description of the tree as a class. So, that does not take any effort at all. So, what we have seen is that you can take an, a tree with balanced plus 2 or minus 2 and we can do some rotation and transfer the balance to the other side. It may or may not fix it. So, now we need to make sure that we fix it. So, we now want to use this to actually, so rotation so far is not itself a balancing operation. It is a basic block for balancing. So, let us see how to rebalance. (Refer Slide Time: 30:35) So, remember that the only two situations that we will ever rebalance are when the node is plus 2 or minus 2 because we are rebalancing at every point.'},\n",
              " {'id': 'f8f1543f-a447-42fd-acdb-b506d33a7375',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'We are never allowing the balance to become worse than plus 2 or minus 2. So, it was previously plus 1 to minus 1 in that range, and we did one insert or delete and something must have changed so it might have become plus 2, might have become minus 2. So, let us look at the plus 2 case. So, in the plus 2 case the left hand side is bigger than the hand side. So, since it is plus 2, we must have something here overall. If this is h this left-hand side must be h plus 2. So, as before we have expanded this h plus 2 with this diamond as the root node, and now I am going to look at the slope at this point. The slope at this point is plus 2, that I know. What is the slope at the diamond? So, the slope with the diamond now, can it be something other than minus 1, 0, 1? Well we are going to argue that it cannot happen because we are always going to ensure that, I mean we will see that this rebalancing happens in such a way that when I am touching a node and trying to rebalance it, we can assume that its sub trees have been recursively rebalanced because remember that all our updates happen recursively. So, what we will see that every time we make an update, we rebalance. So, at every point below we have already rebalanced before coming here. So, that is what we have to keep track of. So, we can assume that this tree, capital L, and the street capital R are already balanced. So, now we have to look at the slope, as I said, of this diamond node. So, what are the possibilities? It is going to be either 0 or plus 1 or minus 1. So, I am going to consider first the case where it is not minus 1. So, either 0 or plus 1. If it is 0 or plus 1, remember that this whole thing has to have h plus 2, so one of these two things has to have h plus 1. And it cannot be only the right-hand side because then the slope will become minus 1. So, the left-hand side must be h plus 1, it cannot be h. The right-hand side could be h or h plus 1. If it is h I have 1, if it is h plus 1, I have 0.'},\n",
              " {'id': 'a5fb8dc0-57d2-4d94-8f22-9f9fdabfc8e2',\n",
              "  'metadata': {'chunk_idx': 17, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, this is a concrete picture that we have under the assumption that the root node, that black dot has a slope plus 2, and the left child, the diamond node has either 0 or 1 as its slope. So, now I want to rectify this using those rotations that we have just seen. (Refer Slide Time: 33:10) So, it turns out that here a single rotation is enough. And what does that single rotation do? Well I just rotate at the top. So, I will basically, if I erase all this, I am applying essentially a right rotation at this node. And that is exactly the rotation that we saw earlier. We will take the diamond node and we will hang that, this node off it, and then this right tree will remain there, this R tree will come here, and this L tree will be here because that is the the relative values of L, R and that original tree on the right are like this. So, this is what we get. So, we get this tree here. So, now, what we can see now is that the height here is positively h plus 1. The height here could be h plus 1 if I only look for this, but it could also be h plus 2 because this is h plus 1 plus 1. But in the worst case it is h plus 2. It cannot be more than h plus 2. So, the slope of this node now, this, this node which came from the left, this value that came from the left, it is either 0 or it is minus 1 because it is 1 less on the left than on the right. But it is within range. So, all the nodes, you can see, are balanced because everything below this is definitely balanced. And this node is balanced because it is either plus 1 or 0. Either the right, this three named R is either h plus 1 minus h plus 1 or it is h minus h is 0. So, if I have the, going back if I have the diamond node having a slope of 0 or 1, a single rotation at the top fixes the problem for me. Unfortunately, if it is a minus 1 it is a little bit more tricky. (Refer Slide Time: 34:45) So, supposing the slope here is minus 1, then I know for sure that this must be the situation.'},\n",
              " {'id': '94f29a03-ca88-4b1b-8758-15969e848c82',\n",
              "  'metadata': {'chunk_idx': 18, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'This has to be h plus 1 and this has to be h because I need to get a height of h plus 2 at this point. The only way I can get a height of h plus 2 at that point is to have 1 h plus 1 child, either left or right, and I know which one it is because the slope is minus 1. So, since this is h plus 1, I will expand this tree. I am going to expand this tree now. (Refer Slide Time: 35:15) So, I am going to redraw this tree and say that under the, this thing there is a star that is a third node which I have kind of pulled out. So, I am explicitly extracting the root of that tree. And it has these two sub trees which I am going to call X and Y. So, this together, this whole thing before was what I had called R, the capital R. So, this whole thing together was earlier this capital R. So, I have taken this capital R and decomposed it into the root and there is two sub trees. So, this whole thing before was h plus 1. So, so this should be h or h minus 1. So, at least one of them has to be h but the other one could be h minus 1, but not both because if this whole thing is h plus 1 then one of these two trees must be of height h. So, now I take this thing and I expand it and I have rotated it at this point. (Refer Slide Time: 36:11) So, I take this sorry and I rotate it here. So, originally my balance, the problem is here. But I am not going to the root at all. I am doing it at a lower level. So, I am first rotating the left sub tree of the root. So, I rotate it this way. So, again, remember that this must be h minus 1. It does not matter as we will see it will work out okay. So, now we have this situation. So, in this situation I still am not promised something because I could have an h plus 1, and so I could have an unbalanced situation because of h, sorry, so this could be at h plus 1 and this could be at h plus 2 and I know that this is at h. So, the situation here could still be bad in the sense that it could still be a situation where I have a plus 2 at this point.'},\n",
              " {'id': '915b848b-632f-495d-94e9-53404f665236',\n",
              "  'metadata': {'chunk_idx': 19, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'I have not changed the imbalance at that point by doing this rotation inside. But now I will do that previous rotation. Actually now it turns out that I am in this case with respect to the star node. (Refer Slide Time: 37:25) So, if I do one more right rotation, then you can see that either everything is h or I have h and h minus 1. So, basically these two nodes are balanced because they have either 0 or plus 1 minus 1 slope, and these two nodes are at the same height so this node is balanced. So, in case I have one situation as single rotation suffices, and other situation two rotations suffice. So, the rotation on its own, as I said, does not fix the balance. But by combining the rotations cleverly you can always fix this thing in a constant number of operations. Remember, each rotation took a few reassignments of the nodes near the top of the trees. (Refer Slide Time: 38:07) So, this was for the plus 2 case, and we saw that everything is symmetric. So, here we did a right rotation and left rotation, there you do a left rotation, right rotation. So, it is the same thing but you just do it symmetrically. So, if you can do plus 2, you can do minus 2 by just applying the rotations in the opposite directions. (Refer Slide Time: 38:26) So, how does this all fit together? Well we, we saw the code for rotate, and what we described now is a procedure of how to apply this rotate by looking at the slope. So, if you combine it, I have not shown the code, if you combine it, you can combine these rotations judiciously so that when you have slope plus 2 you can make it back to a normal slope, and when you have slope minus 2 you can make, by doing the rebalancing appropriately. So, we can assume that we can take those rotations and stick them together in the correct sequence to always take a node and rebalance it whenever the slope is outside the permitted range. So, our goal therefore, is to keep it always within the permitted range.'},\n",
              " {'id': '269b0b66-8a63-40bd-8bfa-b3f3a760b7b6',\n",
              "  'metadata': {'chunk_idx': 20, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, what we have to do is go back and check the code that we have that modifies the tree. So, what is the code that modifies a tree? Insert and Delete. So, wherever we modify the tree that sub tree must be rebalanced. So, if you remember Insert, it said okay, if it is empty then I create the value here which is not a problem because that is a balanced tree because it has only one node. And of course, if I find the value I am not making any change. If I go to the left and I insert it, then potentially I have to rebalance it. So, after I insert on the left-hand side, the left-hand side might change and therefore I will re, rebalance it. Now notice that this node, so I am going here and I am saying rebalance this thing, but the, the Insert could have come to me from above. So, it will now, at the next level, ask to rebalance this node. So, that is what I meant by saying that this rebalancing happens bottom up. So, when I change something to my left I ask my left to change itself, to rebalance itself. Now, when I return the value back up, the parent who has delegated this Insert to me is going to ask me to rebalance myself. So, I will get rebalanced after my child is rebalanced and therefore this rebalancing of assumption that the rebalancing is working bottom up is validated. And the same thing holds, of course, when you do the right-hand side. If you insert on the you do a right rebalance. (Refer Slide Time: 40:31) And similarly, when you do a Delete, there is the empty case which we do not have to worry about, but if you delete on the left you do, should be left, if you do delete on the left you rebalance on the left, if you delete on the right, you rebalance on the right. Now, notice here when you delete the maximum on your left sub tree that will in turn create its own rebalancing. So, you do not have to separately rebalance it. It is only when you kind of do it yourself that you have to do it.'},\n",
              " {'id': '4858268c-e1b0-4dff-b4eb-db80d5a893f5',\n",
              "  'metadata': {'chunk_idx': 21, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': '(Refer Slide Time: 41:00) So, all this requires one extra step which we have kind of not looked at very carefully, which is we need to determine the slope in two senses. First of all we need to check when we rebalance whether we are in the range of plus 2 minus 2, otherwise nothing is to be done. And even when it is plus 2 or minus 2 remember, we have to look at the child and decide whether its slope is plus, 0 plus 1 or minus 1. So, we have to repeatedly find the slope of a node, and the slope of a node we said is determined by computing the heights of the two children and taking the difference. So, this is the Python code which expresses what we said at the beginning that the height of an empty tree is 0, and the height of non empty tree is 1 plus the height, maximum height of its children. Notice that this also takes care of the leaf because the leaf will be 1 plus the two children which are empty trees. So, it will be 1 plus the maximum of 0 and 0. So, the height of a leaf will turn out to be 1 as we would expect. So, the problem with this particular thing is that in order to compute height we have to go down the whole tree because I have to look at the left sub tree and the right sub tree and in turn when I look at the root of the left sub tree it will look at its left sub tree and right sub tree. So, in fact every height computation will traverse all the nodes. It is not like Find or Insert which says either go left or go right. All our search tree operations so far were clever in the sense that they never went down both paths. Either it said go down the left path or it said go down the right path, but never both. But this is now requiring me to go both ways so it is going to be actually an order n operation. And this is going to kill me because even though the final product is something which is balanced, if I am doing order n work each time in order to find the slope and rebalance it then my rebalancing is already a linear operation.'},\n",
              " {'id': 'a2a5a289-5c98-4d9b-b0d8-dc936e96c62b',\n",
              "  'metadata': {'chunk_idx': 22, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, I am going to do n into n, n squared operations at the minimum across my entire work. So, we cannot afford to do this. So, we must do something more clever. (Refer Slide Time: 43:00) So, the solution is to say that I do not need to compute the height because I can incrementally update the height whenever I change things around. So, what I do is I actually keep, in addition to value, self-dot value, self-dot left, self-dot right, I keep an extra field called self-dot height. And self-dot height basically can be updated by taking the maximum of the left and the right. So, so do not, it is, this is not definition of height. The definition of height is just to return self-dot height. But whenever I do an update this is what I need to do. (Refer Slide Time: 43:40) So, if you look at the update for instance, so whenever I rebalance, and the heights have possibly changed I will update my height and incrementally and inductively I assume that my children have adjusted their heights. So, their height field is correct. So, I can just look up the height that my left child reports, and the height that my right child reports, and add 1 to the maximum of these. I do not have to recursively go down because inductively every node has maintained its height in this local variable. So, by keeping a local variable height I am able to keep track of the height as we go along without recomputing it wastefully going up and down the tree. So, this is the extra trick that you need in order to make the rebalancing work, is to keep the height explicitly recorded as part of the node along with the value of the left child and the right child. So, there is an extra, local integer that you have to keep. (Refer Slide Time: 44:30) So, what we have seen is that these binary search trees which we introduced as a twodimensional data structure to keep track of sorted values which are changing with Inserts and Deletes can be kept balanced by doing these local rotations.'},\n",
              " {'id': '22f8facc-4992-4c73-8802-769758aa2d0e',\n",
              "  'metadata': {'chunk_idx': 23, 'week': 7},\n",
              "  'source': 'Balanced Search Trees.pdf',\n",
              "  'content': 'So, by combining these rotations judiciously and by keeping track of the slope we can actually keep a height balanced tree, and we prove that height balanced trees are good enough. A height balanced tree is weaker than a size balanced tree but it still has the property that the size overall is exponential in the height, or the height is logarithmic in the size. So, this guarantees that whenever I do Find, Search, or Insert or Delete which walks down a single path in the tree that cost is never more than logarithmic in the number of values that I am maintaining. So, overall, I get this, over a sequence of n such operations, no matter which of these operations it is, it is not going to cross n log n. So, that is the beauty of balanced search trees. These are not the only, so height balanced trees, I should again remind you, are not the only way to keep balanced search trees. There are many other types of ways of keeping balanced search trees which you can read about if you look at algorithm’s books. But this is one of the simplest ones to explain and you can see very readily why it maintains the balance and why this balance gives you this logarithmic height.'},\n",
              " {'id': '92ce391e-7a18-4ccc-9c5c-8e5ef16eaddc',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Divide and Conquer-Counting inversions (Refer Slide Time: 00:09) So, we will now look at more examples of an algorithm design strategy called divide and conquer. (Refer Slide Time: 00:15) So, in divide and conquer, what you do is you take your problem and you break it up into parts, typically into two parts, which are disjoint, which do not refer to each other. Then you solve these parts separately and then you combine the solutions efficiently. So, a typical example of this is merge sort. So, if you remember, in merge sort we break the list into the first half and the second half. So, imagine in the TA situation that you have two TAs, so you can give each of the TAs one half of the papers to sort. They do not have to refer to each other. So, these problems are disjoint. And then when the TAs bring the merge, the sorted papers back, you have to merge them. So, that is the step of combining the disjoint sub-problems. So, you create these disjoint solutions and then you have to combine them and you have to do that combination efficiently to make sure that you have got the benefit of doing this. So, quicksort was also a divide and conquer algorithm. The division process was a little more complicated. We did not just take the midpoint of the array or list and split it. Rather we split the list based on a pivot. So, we created a lower partition and an upper partition. So, it took a little bit of effort to split them. And then to combine them the lower partition sits below the pivot and the upper partition sits above the pivot. So, you combine them by just placing these two partitions with respect to each other correctly around the pivot. So, these are two examples of divide and conquer and now let us look at another problem. (Refer Slide Time: 01:37) So, the first example we are going to look at is motivated by something called a recommender system.'},\n",
              " {'id': '830bb4e2-6644-4d44-88eb-1d57e8358d55',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, when you log into an online website, typically an ecommerce site, you get some recommendations. Now, it could be ecommerce, it could also be an entertainment site like Netflix. So, you are told that here are some shows that you might want to watch. So, of course, some of these are based on your own past history. But a lot of the recommendations are based on the histories of people which the system believes are similar to you. So, the recommender system tries to compare your profile with the profile of other customers. So, how do we do this? Well, we have to somehow identify other customers who have similar likes and similar dislikes. And based on this, the recommendation believes that if somebody else likes similar things to you and they have liked something subsequently which you have not seen, then it should be recommended to you. So, our task is going to be related to this question of how to compare profiles. So, how do you compare one customer against another customer? How do you find out whether this customer is similar to that? Well, the assumption is that these customers over time have given some ratings or rankings. So, it essentially boils down to checking your likes and dislikes, which is the same as comparing the rankings that you give with the rankings that somebody else gives. So, let us look at comparing rankings. So, supposing there are five movies that you and a friend have both seen, let us just call them A, B, C, D, E. So, your ranking is the D is the best, B is the second best, then C, then A, then E. And on the other hand, your friend thinks B is the best, then A, then C, then D, then E. So, there are many ways to compare such rankings, but one of the simplest ways is to just look at each pair of movies and see whether you and your friend agree on how they should be ranked relative to each other. So, for instance, if you look at B and C, then both you and your friend agree that B is better than C.'},\n",
              " {'id': '819d29d3-5ada-4bcc-b7da-ff6391938396',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, you agree on the relative merits of B and C. On the other hand, if you look at D and B, so here you believe the D is the best movie overall, and hence it is better than B. But as your friend believes B is the best movie overall, and in fact, ranks D quite low. So, we are not really concerned about whether D is very low or not compared to B. We are just concerned about the fact that you prefer D to B and your friend prefers B to D. So, this is one point of difference between you and your friend in terms of preferences. So, in this way, you can do it for every pair of movies and see how many movies you agree on, how many pairs of movies you agree on, and how many pairs of movies you disagree on. (Refer Slide Time: 04:27) So, essentially what we are looking for is some measure of this disagreement. So, we are looking for these inversions in preference. So, where you prefer, say D to B and your friend prefers B to D. So, this means this is a pair over which your preference with respect to your friend is inverted. So, clearly, if every pair is the same, for every pair of movies if you and your friend agree on the relative ranking, you can think about it, there is no way that your overall ranking can be any different. So, if all pairs are equal, that is there are no inversions, then you have identical taste. On the other hand, if you disagree completely, this means on any pair of movies where you think one is better than the other, your friend thinks the opposite. So, if every pair is inverted, then you are maximally dissimilar. You cannot be more different from that from somebody else in terms of tastes. So, the minimum number of inversions that you can have between two people in this is 0. They agree on everything. The maximum number is that they disagree on every pair. So, every pair among n elements is n into n minus 1 by 2. So, the number of inversions ranges from 0 to n into n minus 1 by 2.'},\n",
              " {'id': 'e8546eb1-470f-4ed8-8366-b244bf2e6631',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, 0 means minimal that is they are exactly identical, n into n minus 1 by 2 means they are maximally dissimilar. So, this gives us some measure. So, now, remember, our goal in this recommender system was to say that one customer is similar to another customer. So, what this number is giving you the number of inversions is giving you is the opposite of similarity. It is giving you dissimilarity. So, obviously, if you, something is more dissimilar, it is less similar. So, you can just invert this and say that people who have a load inversion number are very similar, those who have a high inversion number are not very simple. So, it is a bit of a nuisance to work with this A, B, C, D, E and to look at two different sequences and compare them for inversions, but we do not really need to think of them as two different sequences, because we can fix one as a reference. So, for instance, we can take your sequence and renumber or rename all the objects to be 1 to n in the order that you prefer. So, the other person now has a different ordering. If their ordering is also 1, 2, 3, 4 up to n, then they have an identical ordering there are row inversions. But in general, with respect to your preference, this is my movie one, this is my movie two and so on, there will be a different order that the other person gives, which will be effectively a permutation of these numbers 1 to n. So, in this situation is now easy to describe what an inversion is. So, as far as you are concerned the numbers should be in ascending sequence. So, the higher numbers have lower preference for you. But if they are inverted, then it means that a higher number has come before a lower number. So, you have an I which appears before J in the sequence, but actually I is bigger than J. So, this is how an inversion appears.'},\n",
              " {'id': 'cf4bf653-92db-430b-b899-2c726a3c99b4',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': '(Refer Slide Time: 07:24)    So, as we said the number of inversions ranges from 0 to n into n minus 1 by 2, so they could be order n square inversions and our goal is to determine this dissimilarities. So, we want to actually count the number of inversions in order to give a number to this to quantity to this similarity between two people. So, let us say that your ranking as before is D, B, C, A, E, so as we had said, we will renumber these things as 1, 2, 3, 4, 5 in that sequence. So, 1, 2, 3, 4, 5 represents now the new naming or numbering in the order of preference that you have. And now if you look at your friend’s preference which is B, A, C, D, E, then correspondingly the numbering becomes 2, 4, 3, 1, 5. So, now, we want to see how many things are inverted. So, effectively it means looking for each number how many things which are smaller than that number come before it or alternatively for each number, sorry, bigger than that number come before it. So, for example, if we look at 1 so I want to see how many numbers which are bigger than 1, everything bigger than 1 should come to its right according to our thing, but anything that is to its left is an inversion. So, 2 is an inversion, 4 is an version, 3 is an inversion, because these are all, 1 is the smallest number. What about 2? Well, if I look at 2, then to its left there is nothing. So, there is no number which is inverted with respect to 2 in this sense. What about 3? Well, in terms of 3, 2 is before 3, but 2 is smaller so it is fine, but 4 is before 3 and this is an inversion, 4 should come after 3. What about 4? Well, for 4 the only number before it is 2 which is smaller than 2, so they are in the correct order. And finally for 5 which is the biggest number everything is before 5, so there are no inversions. So, in this if you remember we got 3 inversions for 1, we got one inversion for 3 and nothing else. So, the total number of inversions in this is these pairs.'},\n",
              " {'id': 'be17993e-ac31-44d8-9677-e9ae489910cc',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, 1 and 2 are inverted because they are in the wrong order, 1 and 3 are inverted because they are in the wrong order, 1 and 4 are inverted because they are in the wrong order, and 3 and 4 are also inverted, because they are in the wrong order, every other pair is in the correct sequence with respect to your 1, 2, 3, 4, 5 ascending order. So, we would say that in this case, there are four inversions. So, there is another way to picturize this if you like. So, you can actually write down these two sequences your sequence and your friend’s sequence as two rows of numbers. So, 1, 2, 3, 4, 5 is your sequence, 2, 4, 3, 1, 5 is your friend’s sequence and you connect the number on top to the corresponding number on the bottom. So, you connect every pair j comma j. So, 1 is connected to 1, 2 is connected to 2 and so on. Now notice what this means. It means that 1, 2 and 2, 1, if I look at these two lines, it means that they are in the opposite order below because they should be in the same order. If they are in the same order, then the lines will be something like this. They will not cross. Because if 2 comes after 1 on the top, 2 will also come after 2 on the bottom, then the two lines connecting 1 and 2 to the bottom 1 and 2 will not cross. They will kind of go down without meeting. Whereas if they do cross like this case, that means that 2 has somehow jumped before 1. So, every time you see a crossing, you will see an inversion. So, just as we saw there, we will see that we have these four inversions. This is the 1, 2 inversion the first one, this is the 1, 4, 4, 1 inversion is this one, then we have the 1, 3 inversion is this one, sorry, the 1, 4 inversion is this one, and the 1, 3 inversion is this one, and the one on top is the 3, 4 inversions. So, we have four inversions represented by four dots. So, it is not very important which one you choose.'},\n",
              " {'id': 'ed60a618-ff4f-4d34-bf8b-7273341b2443',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'But the point is that if I do this, the way we did it now, and we are checking for every pair i comma j, whether j has been pushed in front of i, then we are effectively taking order n square terms, and we are computing for every pair whether it is an inversion or not. So, we are explicitly counting the inversions by observing them and so this is going to take order n square, because there could be order n square inversions and we are checking every pair by Brute force. So, the question is whether we can do this more cleverly. (Refer Slide Time: 11:48)    So, since we are talking about divide and conquer, obviously, we are going to do this by trying to split the sequence into two parts. So, we have, your friend has given you, so remember that our permutation is always 1, 2 up to n. So, we have a fixed ordering. And with respect to this fixed ordering our friend has a different ordering which is given by this permutation i1, i2, up to in. Now, we want to count the number of inversions between our ordering and this new permutation. So, what we do is we divide it into two lists of size half. So, we go up to n by 2. So, i1 up to i n by 2 is the left list and i n by 2 plus 1 up to i n is the right list. So, we break it up into two parts. This is very similar to what we would have done in merge sort. We take the first half and the second half. Now, within this by dividing and conquering, we assume that we can recursively find the solutions. So, we can count how many inversions are there within i1 to i n minus 2, n by 2, and then we know how many inversions there are in the second half between i n by 2 plus 1 two i n. But this is not the only source of inversions, because there could be inversions which involve some number i here and some number j here. So, there could be inversions across this pair which will not be captured in this recursive solution. So, we need to add those back. So, we need to add the inversions across the boundary between L and R.'},\n",
              " {'id': 'fe26316c-71c3-47eb-ac3d-7eabb04ae56e',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, the inversion here is something where, remember, that L is a smaller list and R is a bigger list. So, it is something where the smaller list has a number which is bigger than the bigger list. So, i is on the left, j is on the right, but i is bigger than j. So, the number of inversions across this boundary is the number of such pairs, pairs where the left hand side is an L, the right hand side is an R and the left hand side is bigger than the right hand side. So, therefore, our task to complete this divide and conquer solution is to do this efficiently. So, this is our combination phase. So, remember that divide and conquer consists of two parts, divide the problem into sub-problems that are disjoint, solve them recursively, that is phase one. So, here the division is easy. You just take the first half and the second half. Phase two is you take those solutions and put them back together to form a solution as a whole and this you have to do efficiently. So, we do not want to again fall into this trap of comparing everything on the left with everything on the right and checking which is an inversion, because everything on the left is n by 2, everything on the right is n by 2. So, again, we will end up checking n square pairs. So, we do not want to do that. We want to do something more clever than that. So, how do we count the inversions across the boundary? So, one way is to assume that there is something a little bit more structured in L and J, L and R. So, L and R right now as given to me are the two lists of preference, of elements as ordered by my friend split in the middle. But what I can do is I can structure them by arranging each of them in ascending order, for example. So, what I am going to do actually is to do more work than I need to do. All I need to do is count inversions.'},\n",
              " {'id': '527ab5ba-7151-4261-a76f-f335cda382e0',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'But in the process of counting inversions, if I actually sort the list, it will help me to do this inversion counting cleverly, because I will just use a version of merge sort where I account. So, I am going to adopt merge sort. And I am going to have a stronger merge sort which not only sorts by using the merge sort procedure, but also while sorting counts. So, I am going to write a version of merge sort which sorts and counts. So, I will call it sort and count. And in order to sort and count, I need to count somewhere and that count is happening at this point. But what is this point, this is a point where I am taking the left and the right and I am merging them. So, what I am going to do is adapt that merge procedure of merge sort to actually count inversions while merging. So, by counting inversions while merging, while I am sorting, I am also simultaneously keeping track of how many inversions there are, and recursively, I will add up all these inversions and get a total count as a part of the byproduct to the process of sorting. (Refer Slide Time: 16:03) So, let us look at how merge and count would work. So, merge and count, we will take these values say, let me say that this is 1, 3, 7 and now this is 2, 6, 8. So, supposing this is my left and this is my right. So, remember what happens now, I want to check how many things on the left are bigger than things on the right. So, I want i in L, j in R such that i is bigger than j. This is an inversion. So, this is, for example, true of the numbers, 3 and 7. So, 3 is bigger than 2 and 7 is bigger than both 2 and 6. So, between 3 and 7, I actually generate three inversions, because 3 is bigger than one element on the right. So, 3, 2 or 2, 3 is a pair which is inverted, and 2, 7 and 6, 7 are both pairs, which are inverted. So, how do I discover this? Well, if there were no inversions, then I would enumerate everything in L before I enumerate R.'},\n",
              " {'id': 'c2d1d201-f75b-4caa-885e-42bda84af554',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'When I am merging, remember, I am picking out the smaller of the two elements at the top and pulling it out. If everything on the left is smaller than everything on the right, then I will first exhaust L and only then move to R. So, if at any point if I pick out something from R, before I pick out something from L that means I have discovered an inversion. There is something still left in L which has been overtaken by something from R. So, we will count the inversions while merging which is that, whenever as I said before, so let us draw that example again. So, we have 1, 3, 7, and 2, 6, 8. So, now when I am merging, I first pull out a 1, and I say this is fine. Now, when I pull out a 2, it has overtaken some elements in L. How many elements in L? Well, everything, which is an L at this point should have been enumerated before 2 if there were no inversions. So, I can directly keep track of the position I am in L, and subtract it from the length and say that, okay, now I have got two inversions because of this 2. Now I get 3, but if I take from the left, there is no problem. Every time I take from the left, I have no inversions, because I am supposed to do left before right in the non-inverted case. Now, I next number to pull out a 6. So, when I pull out the 6, again, I look at the left list, and I see how many elements should have been coming for this or how many elements 6 has overtaken. So, it says plus 1. Now, then after this I pull out 7. And then finally, when I pull out 8, I do the same check. But now the left list is empty, so I get nothing. So, in principle, every time I pull out something from the right list, I have to look at the other list on the left and see how many elements are left and all those elements have been overtaken. So, that must be added to my count. So, this is what it says. So, whenever we add some number im from the right side to the output, then im is smaller than everything which is currently in the left list.'},\n",
              " {'id': 'e46d6146-35d7-4d30-840d-b9dac95a1c13',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, it is inverted with respect to all of them. So, I must add the current size of L to the inversion count. So, this is the old merge procedure which has now been enhanced to keep track of this count. So, the new thing here is that we have this count variable which is initialized to 0. So, we are assuming now that A and B are sorted lists. So, when we are merging, we are going to count the inversions in this way. So, every time I pick from the right, I must add the length of the left. So, remember, the length of the left is m, which is its total length, minus i, because this index is passing through the list A, and the index j is passing through the list B. So, when I finished L and I am only picking from the second list, in principle, I will be adding. It is like that 8, the last number 8 which I pulled out, it happens after I finished everything in A. But at that point there are no inversions. So, I just append one more element from B and I increment the counters, the pointers inside B and C as before. Whenever I pick from A there is nothing to be done, because no element from A contributes in inversion. Likewise, if the first element is smaller in A than in B, there is no problem with this. But here when I pick something from B, then I must not only move the pointers j and k, the respective pointers inside B and C by 1, but I must also update this count. So, I must take the count and update it by m minus i, because m minus i is the current length of L. It is what is remaining in L after I have processed i elements. So, n is the number of elements I had to begin with. I am at position is i, so 0 to i minus 1 has been processed. So, i elements are gone. So, the remaining n minus i elements, m minus i elements are inverted with respect to the current p element. So, that is the only change I need to make, keep this count and increment it every time I pull it out. And of course, now this merge and count must return this value.'},\n",
              " {'id': '4e4d6129-5977-45ed-9da2-c1d3c6f291a5',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, in addition to the merge list, it also returns a count of how many inversions it observed. So, once we have done this, it is a very simple matter to employ this in our merge sort to make it a merge and sort. So, we have a sort and count. So, sort and count is exactly merge sort, except that I recursively assume that sort and count returns the sorted list and the number of inversions in that sorted list. So, this is the divide part. So, I recursively sort the left and I see how many inversions are there within the left. I recursively sort the right and see how many inversions are there within the right. And now I do this merge of L and R. And this returns the overall sorted list, which is what merge sort is supposed to return. But it also returns these crossing inversions, how many inversions were observed while merging. So, now the total number of inversions that merge sort has discovered is those inversions completely on the left plus those inversions completely on the right plus these inversions that the merge procedure discovered. So, this is the total number of inversions that merge sort returns along with the sorted list. So, this is how we use this counting of merge along with sorting to keep track of the total number of inversions in our whole list. (Refer Slide Time: 22:35) So, obviously, since this is very similar to merge sort, and there is really no extra work that is happening except for keeping track of this count variable and adding 1 to it at every point, the recurrence is going to be very similar. If I have a trivial list, then I return in constant time. If I have a non-trivial list, then I have to solve each half of it, so 2 times n by 2, and I spend order n work in the splitting and merging operations. I do not do any more than order n work. The merge is still the same, because remember that when I have to increment the count, I just keep track of that length which is remaining and just add a number.'},\n",
              " {'id': '0de116ef-dcbe-470f-88ca-753200908136',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'I do not have to scan anything one more time. So, obviously, if we solve this, we will get T of n as order n log n, which is exactly the same as merge sort. So, the thing to note is that we are not claiming that the number of inversions is reduced. So, we earlier said that the number of inversions could be as low as 0 if they are identical lists, or it could be as high as n into n minus 1 by 2 if every pair of preferences is inverted. So, there could be order n square inversions. Now, when we did a Brute force algorithm, we were counting them one by one. We were looking at every pair i, j and deciding whether it is inverted or not. So, if there were n square inversions, we would actually examine n square elements. Even if there are not n square inversions, we have to examine every pair of elements to determine whether they inverted or not. So, we are kind of enumerating the inversions one by one. Whereas here, even if the number of inversions is large, we are kind of, we are doing it in chunks. So, if you go back to that merge and count, see at this point, the count increases by a chunk. So, if there are lots of elements that this current value B of J is inverted with, it will add all of them without going through each of them and saying, I am inverted with this, I am inverted with that. So, we are not explicitly examining every inverted pair. So, we are not going to exam, for example, one of the things we do not get from this version of the algorithm is what we did earlier. So, when we first solve this problem, we got this explicit list of all the inversions that was a byproduct of the Brute forcing is an explicit listing of all the inversions. We do not get an explicit listing. What we are getting is the number of such pairs. And if you are asking for this recommender system, that is what we want. We want to know how similar two people are. We are not particularly concerned about which movies they disagree on. We want to know how many movies they disagree on.'},\n",
              " {'id': '07d91a5d-d4e5-4de2-9e0e-00613b34d4a9',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Counting inversions.pdf',\n",
              "  'content': 'So, we just want the count. So, by doing this divide and conquer, we are solving the problem in a slightly weaker way in some sense than the Brute force algorithm, but this is what we want. We want the count. We do not want the list. The list could be large. If you want to enumerate the list, you have to do order n square work. But if you just want the size of the list, you can do it in order n log n time.'},\n",
              " {'id': '523f00e8-a4ab-4304-8fdc-fbddb4e62ee1',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Divide and Conquer-Closest Pair of Points (Refer Slide Time: 00:09) So, we saw one divide and conquer algorithm that was new, which was to count inversions, which was a version of merge sort in which we also kept track of how many times we pulled elements from the right hand side. Now, let us look at a completely different problem, but one that we promised we would solve at the beginning of the course. (Refer Slide Time: 00:26) So, if you remember one of the motivating examples for looking for efficient algorithms was this video game. So, we said that there are several objects on the screen. And perhaps one of the basic steps that the video game has to perform every time there is a change in the situation is to find the closest pair of objects on the screen. So, we said that this can be done by Brute force by looking at every pair of objects on the screen, computing the distance and then computing the minimum amongst these, but this would take order n square time and we promise that we would find an algorithm which does something better than this. So, what we are going to report now is this clever algorithm which is going to take n log n time. And this clever algorithm is going to use divide and conquer. So, let us set up this problem more formally than in this video game context. (Refer Slide Time: 01:12) So, what we have are a collection of points and we want to find the nearest or closest pair of points. So, what is a point? A point is just a point in two dimensions. So, it has an x coordinate and y coordinate. So, every point p has an x part and a y part. This is just the normal point in 2D. So, if I give you two points x1 and y1, x1, y2 and x2, y2. So, you know that, by Pythagoras formula, you know this distance. So, it is the square root of the length of the hypotenuse. So, I mean, it is the length of the hypotenuse, is the square root of the squares of the other side.'},\n",
              " {'id': '2e1bb0e0-acad-41c6-9fef-5493a7df725b',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, this is what is called the Euclidean distance. So, we are just going to use this distance to determine which pair of points is closest to each other. So, we are going to make one small assumption. So, we are going to assume that there are no two points which have the same x coordinate. In the sequence of n points, there is x1 to xn and y1 to yn. So, we have n x coordinates, n y coordinates. We are going to assume that no two points lie on the same vertical line, no two points lie on the same horizontal line. Now, this may look like a serious restriction. But imagine that you have such a situation that you have two points which lie on the same vertical line, you just rotate the points by a small degree. You just rotate them by a little bit. Now, they are no longer in the same vertical. So, it is just a little bit of change of orientation. So, you just perturb the points slightly by just rotating them. Their distances will not change. You can check that for yourself. If you rotate points, the points relative to each other do not change. Their distance with respect to the x or y axis may change, but the distance relative to each other does not change. If I just rotate something, they all stay with the same relative distance. So, I can assume that and solve it. Now, if you are not happy with that, then you can also take the algorithm that we are going to look at and you can actually perturb it. The algorithm can be made more sophisticated to take care of this, but it is convenient to assume that these points actually have distinct x and y coordinates. So, as before, with every such problem, we said that in the video game case, you can always do it by Brute force. You can take every pair of points pi, pj and you can compute the distance and then take the minimum of these. But this, the problem is will take time n square. If there are n points, I have to compute distances for n into n minus 1 by 2 pairs of points. And I want to do something better than that.'},\n",
              " {'id': '5a89f74d-27a9-4cd5-95a9-bb61978e3ce7',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': '(Refer Slide Time: 03:33)   So, let us see why this should at all be possible. I mean, why is it possible to even imagine that there is something better than n square. So, let us look at the same problem but in one dimension. Now, in one dimension also the brute force algorithm. So, what is one dimension mean? One dimension means that I do not have a y coordinate. I only have x coordinate. So, everything is laying along a horizontal line. And I have these points like this. And the distance between two points is just the difference between the two x values. And because there could be one bigger one smaller, so remember, in the other case, in the two dimensional case, we took the square, so we made it positive and then took the square root, the positive square root, so the distance is always positive. Now here, if this was say minus 3 and this was minus 2, then you have to be a little careful about how you measure the distance. Distance should be plus 1 and not minus 1. So, we will typically take the absolute value of the difference between the two points as a distance. So, once again, the naive thing would be to just take this absolute value for every pair of points. So, again, you have an n square algorithm. If you do it even for one dimension, the brute force algorithm does not, because it is one dimension, it does not change. It is still an n square algorithm. But we have a much cleverer algorithm for one dimension, which is very simple. We just sought the points. So, now if you sort the points, so these are, if I am now looking at points in sorted order, then where are the neighbors of this point? Well, it could be this point or it could be this point. It is certainly not the point which is two steps away, because no point which is two steps away can be closer than some point which is one step away in the sorted sequence. So, the nearest points to any point p in the sorted sequence are its neighbors before and after.'},\n",
              " {'id': '5dc7b207-062e-43de-a454-c7c5a864be9f',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, then all I need to do is take the sorted sequence and compare the gaps between the points. Nothing else matters. But the number of gaps is n minus 1. I have n points. So, I have a gap between the first and second, second and third, and so on. So, I can do one scan and an order n time I can scan all the points to find the minimum separation between adjacent points. So, therefore, the problem has been reduced from n square to n log n which is the time it takes to sort the points. So, this is our motivation for looking for a better algorithm n square in two dimensions. If I can do it in n log n in one dimension, can I do something cleverer than n square in two dimensions. So, since we are looking at divide and conquer, it is logical that we are going to somehow use divide and conquer to do this. So, what does it mean to divide these points? Well, remember that our usual naive way of doing divide and conquer is to divide the subset into two parts, the first part and the second part. We take a list, we take the first half and the second half. So, here geometrically, what it means is that we have a bunch of points. So, we will somehow divide it. It does not matter whether we do it horizontally. But just geometrically we will say, take everything to the left of the red line, take everything to the right of the red like such that it is roughly half on both sides. Then divide, we will say recursively find the solution within each half. So, I now know the shortest distance to the left of the red line, I know the closest pair of points to the right of the red line. But the problem is that I could have some points which are very close to the red line, which are in different halves. So, they might be very far away from the corresponding nearest points on their side. But they are very close to each other, but they are in the different sets, one is on the left set, one is on the right set.'},\n",
              " {'id': '8305316a-0e64-43ba-a244-5d5e490f5663',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, just like earlier, we had to count inversions from between the left and the right set. Here we also have to compare distances across this boundary. Distances within one boundary, within one of the two groups gets done recursively, but distances across the boundary is our merged step. So, we have to do the merging. So, how do we do that efficiently? So, that will be the crux of the problem. (Refer Slide Time: 07:38)  So, let us see how to do that. So, what we are going to do is we are going to add some structure to the points that were given. So, we are given these n points. So, this is the set of end points like the ones on the right, which are kind of randomly scattered. So, these are something like 20 points are there. But now I am going to do two sorts. So, I am going to list out these points in this direction. So, this will be 1, this will be 2, this will be 3, this will be 4, 5, and so on. And I am also going to list out the points in this direction. So, this will be 1, this will be 2, this will be 3, this will be 4, and so on. So, this green list will be the Px list. So, this is a listing of the points in order of x coordinate, and this is the listing of the points in the order y coordinate. So, I am going to first do this for the initial list of points. I am going to first sort the initial list of points by x and y. And then I am going to always apply my recursive algorithm assuming that the list of points that I am given has this property that it has a x sorted component and a y sorted component. I mean, notice that Px and Py, I am not dividing. I am just presenting to you P in two different orders in the x order and the y order. So, Px has all the points in P, Py has all the points in P. It is just that Px is sorted by x and Py sorted by y. And as we go along, we do not want to sort again and again. So, we have to make sure that when we do this divide, we can extract the sorted things for the two halves equally, I mean, easily.'},\n",
              " {'id': 'e40541ea-b143-4ee6-82fd-1337c17f3ac0',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, this is the divide step. I take a line such that I have roughly half the points on the left. How do I know that? Well, I know that Px has the x coordinates in sorted order. So, I can walk up halfway through Px. If I walk up halfway through Px, then at the middle of Px, the median point of the list Px tells me where to draw this line. Everything beyond that is in the right, everything before that in the left. So, this splits it into two parts. Let me call them Q and R. So, this was n points. These were both n by 2. So, now what we have to do is we have to take this Px, Py and divide each of these Q into Qx, Qy and Rx, Ry, because to continue this recursion I need to solve problem with a given set of points having an x sorted version and a y sorted version. And I do not want to sort again. So, this n log n I have done once. I do not want to do it again. So, how do I get these sets Qx, Qy. So, Qx is Q sorted by x, Qy is Q sorted by y, and so on for R. So, the x sorting is easy, because I just go from left to right and I know where this red line was drawn. So, everything whose x coordinate is before the red line will be the first half of Px. And then everything which is after the red line will be the second half of Px. So, splitting Px into Qx, and Rx is just a matter of splitting like in merge sort, splitting Px into two halves, the first half is Qx, second half is Rx. Now, the problem is that it is not so easy to split y, because I cannot split y this way. Because if I split the y this way, then some of the points above R on the left, some of the points above R on the right, similarly, some of the points below R on the left, some other points below R on the right. So, I cannot just do the same thing for y. So, for y, I cannot just say that the first half of Py is Qy and the second half is, it is not that way. So, I need to split y more cleverly. So, what I need to do is go through y from here to there and decide for every point should it be left or right.'},\n",
              " {'id': 'fce5d906-b80b-4724-806f-6feaf0d68a45',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, how do I know whether it is left or right? It depends on the x coordinate of that point. But what do I know? I know that the red line divides the two. So, for instance, when I am doing the split, I can either pick up the largest x value in Q or the smallest x value in R and use that as my boundary. So, let for example, xR be the smallest. So, this is the one. So, this is the smallest that is the left most point in the right set. So, now as I go up, what I do is I check this and I say where it is with respect to this. It says, it is bigger than that. So, then I will say, okay, then this will go to Ry. Similarly, this will go to Ry. Now, I come to this point, and I say where is it with respect to that. It will say, its x value is smaller than this xR, then this must go to Qy, this must go to Qy. So, in one scan going up by looking, so I am looking the y coordinates are sorted. So, I am doing it in y order. So, the output is coming in y order. This is important. I do not have sort again. I am getting the values. It is like an unmerge. So, in merge, we take two sorted things and produce a sorted thing. Now, I am taking a sorted sequence and dividing it into two groups. But the two groups I am creating are individually in sorted order, because I am building them up in the same sequence. So, Qy and Ry are being constructed by scanning Py in sorted order, so they are themselves in sorted order. So, if you accept this, then I just do this. For every P in Py that is in sequence if the x coordinate of p is less than R, xR, I put it in Qy, if it is at least xR or more, I put it in Ry. And this happens in one scan. And in one scan, notice the point is at Qy and these things are sorted. I do not have to sort them again. Now, these are clearly sorted, because they are the first and second halves of a sorted list. But these are being built up in sorted order. So, they are also sorted.'},\n",
              " {'id': '9581d985-8821-4b73-bf76-23b9d23d665e',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, therefore, in one left to right scan, I have split Px into Qx and Rx and in one bottom to top scan I have cleverly split Py into Qy and Ry. So, I now have on this side, Qx and Qy and on this side I have Rx and Ry. So, now I can tell these guys to go and do their job recursively, because they are the same structure as the original problem. And the key thing is that the whole, this whole splitting operation, drawing the line and splitting it into two parts and making sure that each part is decomposed into the x and y part, everything was done in linear time. This whole thing was done in two or three scans. So, order n time is all I did. This is crucial because we know that if you spend more than order n time and breaking up things, then your recurrence is not going to work properly. So, in order n time, I have done the dividing. (Refer Slide Time: 14:02) So, now I want to do the actual recursive algorithm. So, what I want to do is compute the closest pair for the original set which is given to me with this decomposition Px, Py sorted by x and sorted by y. It is not a decomposition, but two presentations. So, Px is all of P. Py is all of P. But one is sorted by x, one is sorted by y. So, then I will recursively I will split it so that I here have Qx, Qy and Rx, Ry. This is what we just showed we can do it order n time. And we will recursively compute the closest pair of both of these sets. So, now I know, for instance, maybe that this is the closest pair here and this is the closest pair here. So, the problem is how to combine these recursive solutions, because in the process of doing this, there may be some closest pair across this boundary. (Refer Slide Time: 14:56) So, let us look at just combining the solution and that is really the clever and very tricky part of this divide and conquer. So, remember that we have found recursively the smallest distance on the right and the smallest distance on the left.'},\n",
              " {'id': '1e7c3aca-fbea-493e-a6c9-3dc6be4c3cb5',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, the dQ is the smallest distance in the Q set, dR is the smallest distance in the R set. So, let me use delta, this Greek letter delta to denote the smaller of these two. So, it is the minimum of these two values. So, my goal is to find out whether there is something which crosses this boundary, which is distance less than delta. I know that within each of these two halves, I have nothing which is closer than delta. Delta is the smallest I see between the left and the right combined. So, what can we do with this delta? So, what we can do is we can focus our attention on this band which is within distance delta of the separator. So, this distance here is delta. So, I draw a line which is delta to the left of the separator, and I draw a line which is delta to the right of the separator. So, this delta band, so it is delta plus delta, so it is two delta wide. The claim is that if I want to look for these boundary crossing points, if I want to look for a point here and a point here, then I should look only within this band. Why, because if I have a point which is outside this band and it must cross this band to go to some point other than there, it must first cross the band. So, band is already delta wide. Now, this is outside the band. So, it is more than delta. So, the distance to the other side cannot be less than delta. So, anything which is more than delta I am not interested in, because I already have a delta solution here. So, no pair outside this band can be closer than delta. So, therefore, it is sufficient to focus on those points which lie inside the band. But there is no reason, in principle, why everything cannot lie inside the band. All of these points could lie inside the band. I mean, I could have a different pathological picture in which I have these bands like this and everything is here. This could be the picture of Q and R. So, in principle, there is nothing to prevent this.'},\n",
              " {'id': '4851d50b-0f6c-46c9-b5f1-77620f13cd4c',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, why is it that comparing the band across this band is going to be more efficient or somewhat efficient as compared to just comparing everything in Q with everything in R, because everything in Q and R could actually lie inside this band. So, what we will see is that this is not possible and this is really remarkable. It says that in this band, there can only be very few points. (Refer Slide Time: 17:38)  So, now, I am focusing on the band. So, this is that band. I am only looking at points inside the band. So, the left hand side is Q as before, the right hand side is R as before. And now this is my band. So, I have delta here, and I also have a corresponding delta on the other side. So, what I have done is I have taken this band and I have further subdivided it into these boxes of side half delta. So, each box, this is delta, this is delta, delta, delta. So, I have taken each thing, and I have divided it into these smaller squares of half delta. So, this is just to visualize. So, I have just, so I have basically taken the band in some sense and divided it into four strips. So, it was two strips of delta. I have now made it four strips of half delta and cut it across in half delta squares. So, as we said, some of the points from our old set lie inside this band. So, they are lying here, for example. But many of the points are lying outside. And those are those points I do not care about, because they cannot contribute to my best solution. Now, the first and the most crucial observation is that, actually, these boxes cannot hold more than one point each. So, we were earlier asking whether they can be lots of points inside this band, in fact, can all the points be there. But the first observation is that you cannot have, so these points have to be somewhat separated. Now, why is that? Because if you look at two points within a box, what is the furthest they can be. The furthest they can be is to be at opposite corners of the box.'},\n",
              " {'id': '276baacb-a8da-4148-9ed2-d765e6537795',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, if they are at opposite corners of their box, then the distance that separates them is the diagonal of the box. But remember that each box is entirely on one side, either it is on the left or on the right. So, there are two points which are within one box, they are either both in Q or they are both in R. So, their distance has to be at least delta. But this diagonal is actually smaller than delta. If we just calculate the, use Pythagoras theorem, this is delta divided by square root of 2. So, square root of 2 is more than 1, is 1.4. So, this is 1 upon 1.4 times delta. So, this is something which is smaller than delta. So, if there were two points inside a square, then they would be less than delta. But we cannot have on either side of the separator any two points which are closer than delta, because delta is the closest possible distance for any two points on the same side of the separator. So, that means that these boxes all contain at most one point each. So, now, we are looking at points across these boundaries. So, supposing I have a point here, and now I want to find a point which is very close to this point but on the other side. So, remember that this is delta. So, perhaps this is just inside this side and maybe this is just outside that side. So, this is about the closest I can get while being far away in terms of the boxes. So, the point is that if I start in this row, technically I cannot go more than three rows above. I cannot even reach the third row, but this is just to be safe. Now, let us look at our points in the y direction. So, we have points which are sorted in this direction. So, where do they lie. So, I could have a point just above this first point. So, let me write a 1 here. I could have a point which is very close to this one point, but it might be here. But then I cannot have anything else on that side. So, another point which is still in the same row must be somewhere else.'},\n",
              " {'id': 'cb530758-fb31-4932-998a-c485991ccb4e',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, there may be three other points which are bigger than 1 in the same row, but they must all lie in different boxes. So, now if I have a fifth point which is bigger than 1, a fifth point, the four points bigger than 1, then it must lie in the row above. So, I could have 5, 6, 7, 8. Now, if I have one more point, it must lie one more row above. So, basically, the worst case is that this point was actually here and then I have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 and 15. So, I have to check between 1 and 16. So, there is one point which is 15 points away in the y direction, but it must lie within this 4.4, 4 by 4 neighborhood of boxes. So, if I look at the box in which the point I am examining sits, then any point which, so I am, remember, I am always doing this in ascending order. So, if this point is close to a point which is smaller than this in the y direction, that point would have found this as a neighbor. So, I am starting with the smallest y and going upwards. So, therefore, when I am looking upwards, I am saying that if I want to look for points which are within delta, they cannot be more than outside this 4 by 4 box compared to where my point is. And in the 4 by 4 box there are 16 boxes, they can be at most 16 points. So, therefore, I need to compare this one point against at more 15 other points. It is a fixed number, it does not depend how many points there are to start with. I cannot have more than 15 candidates for any given point to compare. So, this is the reason why now this comparison becomes efficient, because I have to compare each point with a constant number of points, not a number of points which is proportional to the total number. So, essentially, what I need to do is first determine which points are inside this band in sorted y order. So, I do the same trick as we did when we split the thing.'},\n",
              " {'id': '864b41bb-04e2-4a94-ad49-642c82527b1e',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, remember, when we split the thing, we looked at Py, and we decided whether to put something in Qy or in Sy, in Ry depending on the x coordinate. Now, what I can do is I can go through Qy. I can do a kind of clever merge, basically. I go through Qy and Ry. I take the smaller y coordinate. I check if the x coordinate lies in the band. What is the band? The band is the separator plus minus delta. So, the x coordinate rise in the separator plus minus delta x, then I will put it into Sy. If it does not, I throw it away. Then I will pick up the next smallest y coordinate between Q and R. So, I am merging these two and then I filter. So, I want x to lie between this. So, from Qy and Ry I can generate the set Sy, which is all the points in sorted order which lie inside this band of interest. And now once I have got the points inside this band of interest, I take the first point and I compare it to the next 15 points, because those are the only ones which can lie within delta of this. If none of them are there, then I take the second point and compare it to the next 15 points. I will take the third point compare it. So, for every point in Sy I am comparing it to at most 15 more points. So, I am doing 15 times n work. So, basically it is order n work. So, all these things now are again it is a linear scan. It is some order n work to scan this. Now, this is a kind of very, I would say, conservative thing. You can actually do a tighter calculation than this. But this loose calculation is very clearly correct that you do not ever find a point which is more than 15 points away in the y direction regardless of the coordinate. We are not talking about, we are saying in the sequence if I am looking at x1, then I only have to look up to x16. If I am looking x3, I only have to look up to x18, and so on. So, this is what we do. (Refer Slide Time: 25:07) So, it is a little tedious to write this whole thing in Python and fit it on a slide.'},\n",
              " {'id': 'baf2790a-027e-4dc0-82e0-0c085d828c91',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, let me just write it in pseudocode to explain what we did. So, you are computing the closest pair of a set P, but we assume that set P is given to us in two sorted lists. There are two sorted lists, each of which has all the points in P, but one is sorted by x coordinate, one is sorted by y coordinate. So, first of all, if it is a very small list, if it is up to 3, I will just do it brute force. I will just compute every pair in that. So, there are three points. I will compare all the three pair-wise distances 1 to 2, 2 to 3, 1 to 3, and report the closest pair and the distance of that. So, I will just do it by brute force. If it is four or more then it makes sense to divide, because then I will have halves which are at least two. So, I will construct as we said before Qx, Qy, Rx, Ry by finding the midpoint. So, I would, remember that this is sorted. So, I do not have to sort again. So, I will walk up Px halfway and call that Qx. The remaining part I will put an Rx, I will remember the extreme coordinate of Px and Rx and using that Qx and Rx extreme coordinate I will walk up Py and split it as Qy and Ry by deciding which side each point goes to. Now, I will recursively find the solution for these two halves. So, it will report back to me the nearest pair of points. So, q1 and q2 are the points, r1 and r2 are the points and dQ is the distance between these two nearest points in Q and dR is the distance between nearest points there. So, now, once I have this, remember that implicitly this delta, we needed the delta only for the proof, we do not need it for the code. Delta is the minimum of this dQ and dR. So, using that delta I will construct this Sy. I will look for all the points whose x coordinate lies between the separator minus delta to separator plus delta and put it into Sy in ascending order in y coordinate. And now I will do this linear scan, scan every point with the next 15 points in order to find out the smallest separation within this band.'},\n",
              " {'id': 'f09851d3-8e57-4f5b-9ae7-5af128762b34',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, now I have three candidates. I have the left hand side candidate, which gave me dQ, the right hand side candidate, which gives me dR and this band provides a third possibility saying this is the smallest distance within this band for points which could be on either side. So, I take depending on which of them is minimum, dQ, dR and dS these three distance, whichever is minimum, I use that as my answer. And I say, okay, this is the pair of points overall which has a minimum distance and here is the distance. So, this is my algorithm. So, if you want to analyze this, there is an initial step which is outside this function, which is the sorting. So, initially, I have to do some n log n work to sort this. But it will turn out the whole thing as n log n, so it does not matter. So, now I have a recursive algorithm and all this stuff took On time. So, in order n time I was able to split Px, Py into these four lists Qx, Rx and Qy, Ry. Similarly, in the combined step, I was able to combine this by constructing Sy in linear time and doing a linear scan on S. So, all the splitting and combining all the work of generating the recursive step and using the solution from the recursive step is order n. And, of course, now I have to do two times the work for the half. So, the recurrence is just solve the problem for half the size of points twice, ones for the left, ones for the right, and do this order n work to split and combine. So, this is exactly the recurrence for merge sort, because we have a trivial case, where we will just do some constant amount of work in return. So, therefore, this whole thing is n log n. So, this part is n log n. And then we also have an extra, initial step of sorting the P to start with, because that does not come to a sorted. So, that is an extra n log n, but so we do n log n twice, is not a problem. So, we do it in n log n time.'},\n",
              " {'id': '81be29d8-3e44-4689-941e-185efd869e1e',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Closest Pair of Points.pdf',\n",
              "  'content': 'So, therefore, as promised that video game problem, where we wanted to keep track of the nearest pair of objects on the screen dynamically, can actually be solved in n log n time using this clever divide and conquer technique.'},\n",
              " {'id': '3bca05ed-23b6-41e3-99d2-5282690dace0',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': \"Programming Data Structures and Algorithms using Python Professor Madhavan Mukund Divide and Conquer: Integer Multiplication (Refer Slide Time: 00:10) So, for our next example of divide and conquer, let us look at a very simple problem that we learned in primary school, how to multiply two integers. So, the way we multiply two integers is to form these partial products. So, when we have two multi-digit integers, here, we have two-digit integers. We first take three, so we are multiplying x by y. So, we take the unit's digit of y, and create this product of the unit's digit of multiplied my all of all of x. Then we shift by 1 to account for the 10 factor and then we take the 10’s digit, and again, we multiply, so we form one more partial product. And then, we add up these partial products. So, we add up these partial products, and this gives us the actual sum. So, we have to add these. So, depending on number of digits, we will have as many partial products, as there are digits in the second operator in a second number in y, and we will have to add up that many partial products. Now, this is not specific to decimal multiplication that we have seen here in base 10, the same thing would work, if we transform the same problem or any problem to binary. So, for example, this is 12 in binary, so this is 8421. So, the top number is 12 in binary, and the bottom number is 13 in binary. So, if we multiply 12 by 13, in binary, then one times this thing is 1100, 0 times this thing is all zeros. And again, one-times that is 11001100. And now we have to add up these partial sums. So, the usual way, now you can see that we have a stack of partial sums to add, partial products to add. So, we do not really want to do a kind of long addition this way. So, the usual way is that we maintain this cumulative sum. So, we will take these two and we will create an intermediate sum which says 01100. Of course, the leading 0 we do not have to keep.\"},\n",
              " {'id': 'a0a4f212-bc75-46d5-985e-8171c0ca549e',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'Then to this, we will add 1100 at this point and we will get 001, so I think, so we will add, yeah, so this will come to the left, so 1100. And so, we will get 1111. So, in this way, we will just keep adding these sums as we go along. So, we will get this cumulative sum. So how long does it take? Well, each partial product requires me to, let us assume that both the numbers have the same length, so they are n bits. So, each partial product requires me to process n bits to come up with this number. So, that is an order n operation. And then, I have to add two n-bit numbers, so that is another order n operation. So, I do this n times, one for each partial sum. And so, overall, I get an n squared algorithm. So, this is our naive multiplication algorithm the way we learnt it in school, and this is a, this is an n squared algorithm. Now, the question is, is it possible to do any better than this? Because after all, in general, we cannot do away with any of these intermediate values. So, it seems logical that every row, so here, of course, we get some zeros, but that also helps us to shift. If you are doing it in binary, it looks like this row is not required, but what if it was there, we would have to add it up. So, in the worst case, if everything on the bottom was 1, every row would be required. So, it does look like all these n-squared operations are needed. We will get n partial products, and we have to add them up two at a time, so we have to end up doing n squared work or so it seems. (Refer Slide Time: 03:35) b  So, for divide and conquer, we need to divide something. So, usually, we have been taking a list or a sequence of values and dividing it into parts. This is what we did for things like merge sort, and we have also done that for other things. So, even for counting inversions, we counted the first half and the second half, and then we combined across them, and so on.'},\n",
              " {'id': '247de0d2-3510-46d6-8a2f-8770dcf17854',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': \"So, even for counting pairs of points, the closest pair of points, we divided the points that those to the left of the separator, the right is a separator and then we combine them. So, we need some way of splitting up. And what is the logical thing to split up here n is the number of bits. So, the logical way to split is to take half the bits and do them separately and half the bits and do them separately. So, this is what we are doing here. So, we are saying that if we take an n bit number, it has bits the lowest bit the usually the lowest order bit, that is the unit's bit is usually b0, the highest bit, which in this case in powers of 2 represents 2 to the n minus 1, so this is bn minus 1. So, 0 to n minus 1 are the n bits, so we are going to split it halfway. So, we go from 0 to n minus 2 by 1, n by 2 minus 1 and then from n by 2 to n minus 1. So, we will call the upper part of, so let, we remember we are multiplying x times y so we want to find the product x times y, so we will split x this way. So, we will take the lower half of x and call it x naught and we will take the upper half of x and call it x1. So, naught is the last half in terms of the bits or the digits, and x1 is the first half. And similarly, we have y naught and y1. So, now, what is the number actually? The number actually is, this plus this amount shifted by the appropriate power of 2. So, I can think of x as being x1 times 2 to the n by 2, so 2 to the n by 2 is just 1 with that many zeros, n by 2 zeros. So, if I now multiply x1 by that, it gets shifted to the left, and then I am just adding these two and getting an n bit number. So, I have 2 to the n by 2 times x1 plus x naught, so this is x, and this whole thing is y. So, x times y can be equivalently written in terms of these four smaller numbers, with an appropriate multiplication to make the scaling correct. But that multiplication is not a real multiplication, since I do not have to do any serious multiplication, this is just bit shifting.\"},\n",
              " {'id': '500d7e9a-c7ac-4877-8dc3-a0e4ab7635f2',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': \"So, if I was working in decimal, this is like multiplying by 10, and I multiply by 10, I do not have to do any real multiplications, just I have to stick a 0 at the end, so I just multiplying by an appropriate power of 2, which shifts it to the left by that many zeros. So, now, if we expand this out then we will get x1 times y1 times 2n by 2 to 2n by 2, so I get x1 times, y1 times 2 to the n. On the right, I get x naught times y naught, so x naught times y naught. And then I have these two terms. So, I have x1 times y1 times 2 to n by 2, and I have x naught times, sorry, I have x1 times y naught times 2 to the n by 2, and I have x naught times y1, times 2 to the n by 2. So, I have these two cross terms, which are shifted by n by 2. So, I have the leading term, x1, y1 one which shifted by n bits and the two cross terms were shifted by n by 2 bits and I have the last term, the unit's term in some sense, which is just x naught times y naught. So, I multiply this way, I multiply this way. And I multiply these two ways, so this gets shifted by 2 to the 0, this gets shifted by 2 to the n, and this gets shifted by 2 to the n by 2. So, in other words, I do four smaller multiplication. So, I do four multiplications, of these split numbers, which have half the number of bits. So, in order to multiply two n bit numbers, I am replacing it by the problem of multiplying four n by 2-bit numbers. So, four times I am doing an n by 2-bit multiplication, and I am achieving my n bit multiplication through that. So, this is a naive way of approaching this divide and conquer thing. So, this corresponds to the combined step. So, the division step would be to actually extract these two sections by bit shifting. So, remember, if I just shift right, I will n by 2 times I will get the upper half and the remaining part will go off. And if I shift left, I will get rid of the other thing, so that is how we get this. So, it is like multiplication and division and remainder.\"},\n",
              " {'id': 'c31ae1c6-e78f-4f3e-b6bc-3fa9d534fe03',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So, our recurrence now says that if I have to multiply two 1-bit numbers is just a constant amount of work. So, t of 1 is 1. And if I have to multiply two n bit numbers, then I have to instead multiply four n by 2-bit numbers, which is what I have done here, and I have to spend order n time. Now, one reason I have to spend order n time is I have to do this addition. So, remember, earlier when we did partial products, also, we said that each addition of two n bit numbers will take order n times. So, here we are adding two n by 2-bit numbers, but that is still order n. So, combining the partial product requires me to add these order n bit numbers, so that is why the combination term actually requires order n work. So, if we try to solve this recurrence, by the usual way that we have been solving so far, what we will do is, we will take this T n by 2, and we will expand it. So, we will say that T n by 2 by the same recurrence is 4 times T n by 4 plus n by 2. And then if I group these terms, I get 4 squared, and I get 4 times n by 2 is 2, so I get 2n plus 1, so 2 plus 1n. If I do it one more time, then pattern becomes a little clearer. I get 4 cubed here, I get 4 squared, and again, I am expanding this, so I get 4 squared times 4 cube times T n by 2 cube, and then this 4 squared and 2 squared will leave me with a power of will leave me with 4, which I can write as 2-squared. So, I have 2 to the 0, 2 to the 1, 2 to the 2 after three steps. So, in general after k steps, I will have k minus 1 here and I will have k here and I will have k here. So, as we know we want to go until we hit 2 to the k is equal to n which is basically k’s log of n. So, at log of n I have 4 to the power log n, I have T n by 2 to the power log n, so this is just T of 1. And then here I have this, telescoping sum 2 to the power 0 plus 2 to the power 1 plus 2-squared up to 2 to the power log n minus 1.'},\n",
              " {'id': 'c3d1c0fd-18bd-4e96-a668-9e23dffa4679',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'And this whole thing is actually going to be approximately n, because it is going to be approximately 2 to the power log n minus 1. So, this is going to tell us that this thing, we will look at this again a little later more carefully. But essentially, what we find is that by doing this naive divide and conquer, we have not really progressed because we have got, again, nsquared, which we had got even by our high school, or primary school method of doing it, digit by digit and adding up the rows. (Refer Slide Time: 10:42) So, by rewriting this, in this form, we got this recurrence T n is 4, T n by 2 plus n, and this was order n-squared. So, it appears that divide and conquer really has made no progress in terms of improving the complexity of this multiplication problem. But it turns out that we just have to be a little more careful about how we compute the terms that we need in our final answer. So, this is an algorithm due to Karatsuba. So, what Karatsuba said is, let us compute something which is not directly required in my answer above. So, let us look at this sum, this product, x1 minus x naught. So, remember that x is written as the upper half is x, and y is written as y1, y naught. So, I am going to take the lower part of x and subtract it from the upper part of x, x1 minus x naught and multiply it by the lower part of y, y naught subtracted from the higher part. So, this looks like a strange quantity to multiply. But the reason we are doing this is because it yields some interesting terms in its expansion. So, again, we get x1 times y1 then we get plus x naught times y naught, because both two negatives cancel, and I have two negative terms, minus x1 times sorry, x1 times this thing, the two cross terms and minus x naught times. So, this is the multiplication of this thing. So, the point to note here is that the terms that we really need are hiding inside this. So, now, we have achieved this by multiplying two n by 2-bit numbers.'},\n",
              " {'id': '11824e1b-7db8-4af8-bed5-bf2847079921',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'Because remember that x naught and x1 were half the length. So, if I started off with n bits, these are all half the length, so this is a n by 2-bit multiplication, just like as we were doing in the earlier divide and conquer. Now, I want to compute these two explicitly, as we were doing before, so x1, y1 and x naught, y naught. So, I have now done three multiplications, and now, I need to extract this bit. But this bit is easy to extract now, because I can just take the sum of these two and subtract this whole thing. So, if I take the sum of these two and subtract this whole thing, then what happens is in this in the subtraction, these two terms cancel, and so I am left with plus x1, y naught, plus x naught, y1, which is what I need. So, in other words, this term I can get by just subtracting, cleverly adding and subtracting cleverly the result of the three multiplications I have done. So, the crucial thing is that instead of doing four multiplications, which I did, in the naive case, I have brought it down to three multiplications. So, in order to multiply two n-bit numbers, I now only have to do three multiplications, or n by 2-bit numbers and then do some clever rearrangement to make it work. So, let us just look at the algorithm before we do the analysis. So, we are saying that, let us just keep track of the number of bits, so we are multiplying x and n, x and y, which are two nbit numbers. So, the parameters that our algorithm takes are the two numbers and the number of bits. So, if the number of bits is 1, then we just return the product. So, this is just a trivial case, this is the base case. Otherwise, we are going to split it, so we compute the half the number of bits. And then the way we split it, this is actually we say, as it is bit shifting, but you should think about x1 as x divided by 2 to the m. So, I start with x and if I divide by 2 to the m, all this part gets cancelled off.'},\n",
              " {'id': '8156156d-be63-4ad1-bb2b-5f6ccaec83f8',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So, remember, if you shift by divide by 10, what happens 22 by 10 goes to 2.2. And then if you drop the fraction, it is 2 or 27 by 10 for example. If I take 273, 2734 divided by 10, I get 273 by 4.4. And if I drop this and divide by 10, I get 27.3. And I drop this. So, I am just shifting right and dropping the decimal point. But it is actually dividing by that appropriate power of 10. If I want to take the top, remove two digits from my number, the rightmost two digits I divide by 10 squared. So, if I want to remove m bits from my number, I divide by 2 to the power m. And the remainder, if after the division is what I get as the lower bits. So, x naught is x mod 2 to the power m, so that is the remainder. So, this is division and remainder. But really because I am doing it with respect to this power of 2, it is just a question of shifting bit, so this does not really cost anything. This is a simple operation. I am not really doing an arithmetic division is really bit shifting. And then finally, I need to create this intermediate quantity, which I want to multiply, so I will just set a to be the left-hand side of that and b to be the right-hand side of that. So, I need to multiply x1 and y1, I need to multiply x naught and y naught and I need to multiply this subtraction x1 minus x0 by y1 minus y0. So, now I have set up all the things that I need, so now I do my 4 my 3 recursive multiplications. I multiply x1 byy1, so I will get this term. I multiply x naught by y naught, so I will get this term. And then I multiply a by b, so I will get this term. So, these are the three terms that I need, and I will call them P, Q and R. And how do I multiply them this is divide and conquer. So, I pass the arguments to the same algorithm is recursive, but with half the number of bits. So, I am using m bits instead of m bits, but remember n bits, remember m is n by 2.'},\n",
              " {'id': '3cddac99-bbc2-48c6-9d31-ca73e49a7572',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'And now when I get the answer back, I know that the highest order term is p, the lowest order term is q, and the intermediate term which has to be multiplied by 2 to the power n by 2 is given by this clever expression of Karatsuba. It is this p plus this q minus thisr. So, I can compute that intermediate thing, which should be x1 y naught plus x naught y1 by just taking the sum of the two higher and lower order products and subtracting the, this complicated product, seemingly complicated product. So, this is how Karatsuba’s algorithm works. So, what we have effectively achieved is that we have taken this 4 and made it 3. So, the question is, what does that buy us in terms of complexity. (Refer Slide Time: 16:52)   So, let us try to analyze Karatsuba’s algorithm. So, what Karatsuba’s algorithm says is, unlike our naive thing, which had 4 T n by 2 plus n, we have 3 T n by 2 plus n. So, the base case as always is when I have 1 bit is 1, but when I have n bits, it is 3 times T of n by 2 plus n. So, we do our usual expansion trick. So, I will expand this thing, and what I get, is 3 times T n by 4 plus n by 2. And if I rewrite this, I get from this 3 times this 3 I get 3 squared, I can rewrite 4 as 2squared, and now I have a 3 times n by 2, so I have a 3 by 2 and I have the original 1, so 3 by 2 plus 1 times n. So, if I expand it once more, the pattern becomes a little clearer. So, if I now replace this quantity, using n by 4, then I will get n by 8, which I will write is n by 2, 3times t of n by 8, and n by 2-squared, because that is what I passed. And, now, if I look at this, I have this 3-squared, divided by 2-squared. So, I have this 3 into 3 squared, 3 cube coming here, so this is 3 and this is 3. This goes with the number of times I have expanded. And here I have this again, telescoping sum, but now the telescoping sum earlier it was 1, 2, 4, 8, and so on, now it is 1, 3 by 2, 3 by 2-squared.'},\n",
              " {'id': '026c2b2e-f1df-41a5-9d62-5d4df00bf491',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So earlier, it was 1, 2, 2 squared, so that telescoping sum has come to 3 by 2, because I am only creating three copies instead of four copies. So, if I keep doing this, after our usual thing, when we bring down this thing to 1, I would have done this log n times, so then this 3 would have become log of n. So, I have just written 2 here, but you do not have it is always 2. So, I have 3 to the power log n, and this has again expanded 3 by 2 to the power log n minus 1. So, we have to know analyze this messy expression to find out what it means. So, first thing we can do is, we can kind of eliminate this thing because this is just 1. And then we are just looking at this expression at the bottom. Now, this is a geometric series. So, form a plus ar plus ar squared plus so it is where each term is multiplied by a constant factor r with respect to this. So, in our case r is 3 by 2. And in the geometric series formula is usually if I have n terms in my formula, it is r to the n. One of the formulas is r to the n minus 1 divided by r minus 1 times a. So, this is a formula so I am just applying that formula. So, 3 by 2 is my ratio, so that is my r. So, 3 by 2 minus 1 is in the denominator, and 3 by 2, to the number of terms in my thing is in a numerator. So, now, let us look at this term. This is a form 3 to the log n. So, in general, if I have a to the log n, I claim it the same as n to the log n. So, the reason for this is, if I take a to the log n, and say that this is some values z. If I take now log on both sides, I get log of a to the log n is equal to log of z. But remember that when I take logs like this, I can bring that down. So, this is the same as saying that log, I can bring this exponent down. So, I can say this is the same as log of n times log of a is equal to log of z. And now, I can interchange the roles.'},\n",
              " {'id': '2d2054ed-768f-4cb3-bfb4-2cdb42aa4a35',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So, I can, instead of thinking of this as having been the exponent, I can think of this as having been, so I can think of this as log a times log n is equal to log z I just interchange the order. And this is the same as saying that log of n to the power log a is equal to log z. Which is just same as saying that the original z I was trying to compute is also n to the power log n. So, if I have a to the power log n, I can interchange and say it is n to the power log a. In general, if I wait to the power log of anything, b, this will be equal to b to the power log of a, and it does not matter what the base is. This could be, in our case, base 2, but it could be any base. So, this is true, in general, a to the log b to the base c is equal to b to the log a to the base c, because of this way that the logs interact with each other. So, therefore, this 3 to the log n is just n to the log 3. What about this thing? So, this thing requires a little bit of work. But basically, the important thing is this and this stuff. This is the only term that matters, everything else is going to cancel out. So, n times 3 by 2 to the log n that is what we want to worry about. But now, if I use the expression above, it is I can interchange the n and the 3 by 2 and say this is the same as n times n to the power log 3 by 2. So, I have just swapped these two using the thing about it. I brought the n down and put the 3 by 2 in the exponent. But log, 3 by 2, log of x by y is log x minus log y. This is how logs work. So, the log of division is the subtraction of the logs. So, I get that this is n times and this numerator becomes log 3 minus log 2. But what is n, n is just n to the power one, and what is log 2, log 2 is just 1. So, I went to the power 1, and this thing is just 1. So, I get n to power 1 times n to the power log 3 minus 1. So now, I have a multiplication of two things I just add it up. So, I get n to the power 1, plus log 3 minus 1, so again, I get n to the power log 3.'},\n",
              " {'id': '2244149f-c65b-4a9c-b0c1-334acf22dade',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So, both, this term and this term are actually giving me n to the power log 3, so this is going to be my final answer. My final answer, is that, this T of n is actually n to the power log 3. Earlier, remember, that we were looking at n squared. So, what is the difference between n to the power log 3 and n squared, where log 3 is actually something like 1.59. It is of course a irrational number, but if you round it off to two decimal digits, it is 1.59, which is significantly less than 2. So, I have gone down from n squared to n to the power 1.59 by doing this clever Karatsuba, divide and conquer. So, if we do a naive divide and conquer, we got no benefit. But then if we look at that naive divide and conquer and we will look at the structure of the terms that we need to reconstruct the answer, it turns out that we can replace four multiplications by three multiplications and do some more clever combining to get the answer. (Refer Slide Time: 23:56) So, this algorithm has an interesting story behind it. So, let us just look at that. So, Andrei Kolmogorov. You may have heard the name is one of the towering figures of 20th century mathematics. He has contributed to numerous disciplines. Most areas have some influence of Kolmogorov. So, Kolmogorov in the 1950s conjectured that multiplication could not be done in less than quadratic time. So, this was a public conjecture of his. And in fact, he ran a seminar in Moscow University in 1960, where among other things, he stated this conjecture. He said, this is a conjecture, which I believe should be provable, but I do not have a proof of it, so he stated it as one of the problems that could be addressed among the various open problems that he discussed in the seminar. So, there was a young student in the audience our friend Karatsuba, Anatoly Karatsuba, he was a 23 year-old student. So, he was struck by this claim that you cannot do multiplication in less than n squared time.'},\n",
              " {'id': '5f64c3f0-2493-4cf4-8f73-a1aff67fc64e',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'And he thought about it, and two weeks after Kolmogorov mentioned it in this seminar. Remember that Karatsuba had not thought about this problem before until he was introduced to it by Kolmogorov. Two weeks later, he had come up with this clever idea and he presented it to Kolmogorov, and Kolmogorov, of course, immediately realized was correct. And then in the next seminar, he announced it, and then that was the seminar. The seminar was just the announcement that this conjecture had been settled in the opposite direction to what Kolmogorov thought we could be. So, small aside on this, is that Karatsuba actually used a slightly different formulation from what we use. So, instead of our formulation of this subtractions. So, remember, we use x1 minus x naught and x, y1 minus y naught, he used x1 plus x naught and y1 plus y, and this just changes the thing. So, in our case, what we did was we took the two terms x1, y1 plus x naught y naught, and we subtracted that r, so that we got the right thing. Now, instead you take r and subtract that and the same thing happens. These two things will cancel. And because these are already positively occurring, these sub-side products are already positively occurring, they will remain. So, you just reverse the order of the subtraction of the three terms. Instead of subtracting the complex term from x1, y1 plus x naught y naught you do it the other way. So, what is the difference? Well, the difference really is that this sum is the sum of two n-bit numbers. And now, when you sum to n-bit numbers, as you realize, the highest bit could generate a carry. And if you have a carry in the highest bid, then that n bit number plus n bit number generates an n plus 1 bit number is a sum. So, technically, your recurrence does not involve something which is half the size, but something slightly more than half the size it could involve. So, instead of T n by 2 you are saying T n by 2 plus 1, and that becomes a little messy to calculate.'},\n",
              " {'id': '6450ea4f-8e6e-46da-bce4-22297934a0fa',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Integer Multiplication.pdf',\n",
              "  'content': 'So, the analysis becomes rather messy if you use Karatsuba’s thing it still works, but you have to be a little more sophisticated about the way you do the counting. So, the person who actually came up with the simplification that we have used here was done Donald Knuth, who is again a towering figure of computing, if not mathematics from the 20th century and continues to be a towering figure. So, Donald Knuth realized that you can actually subtract instead of add. Now, when you subtract you do not have a problem. If I subtract one n bit number from another n bit number, it only becomes smaller, it cannot become bigger. And then you can use, as we saw the rearrangement in the other way to still recover the missing terms that you need for the final song. So, this is the history really of Karatsuba’s algorithm and the presentation that we have used in this lecture, which is actually a reformulation due to Donald Knuth to make the analysis go a little more smoothly. Now we have done this whole thing in binary just to be simple about talking about bits. But you can actually do this in any base. So, if you do this in base 10, it is the same. As long as you know how to multiply two digits in one unit of time you can take a base 10 number with n digits and divided it into two groups so n by 2 digits and everything is precisely the same. You can do it in base 16, you can do it in base 4. So, it really does not matter what base you are working it. You are just saying, take the digit sequence in that base and divide it by 2 and everything works. So, that is Karatsuba’s algorithm for integer multiplication, using divide and conquer.'},\n",
              " {'id': 'bb135466-ec97-423d-bdec-fdbf047bf060',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor. Madhavan Mukund Divide and Conquer: Recursion Trees We have seen three concrete examples of divide and conquer algorithms. We saw the problem of counting inversions, then we saw the problem of finding the closest pair of points. And then we looked at integer multiplication, which Karatsuba’s algorithm. (Refer Slide Time: 00:25) So, let us look at a more general property of how to solve or analyze these divide and conquer problems. So, remember that a divide and conquer solution requires us to break up the problem into disjoint sub parts. So, we break it up into smaller subproblems, which do not interact with each other, solve them recursively, and then we combine the sub parts. So, what we then have is that the complexity of the original problem which had an input size n, will now be a recurrence because it will depend on the complexity of smaller subproblems, which we get from dividing up the problem, and then the cost of splitting it and combining it which will be an extra cost. So, right at the beginning, without calling it divide and conquer, we saw this at work when we looked at searching and sorting, namely binary search and merge sort. So, for binary search, what we said was that each successive iteration halves the interval to search. So, to search an interval of size n, we look at the midpoint, and then we search either the left half of the right half, so we get T of n requires me to search half the interval, and I need to examine that midpoint. So, assuming that it is an array and I can get the midpoint by just some index calculation, in constant time, I can do that comparison and decide which half to search. And if I expand this out, then I get log n. So, this was binary search. And for merge sort, remember, that we had to recursively sort the two halves, and then we merge them using a linear time merge operation.'},\n",
              " {'id': 'd97a86b0-f9f6-4295-a540-f7e7f9ab18fb',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, it was two times T n by 2 plus n, and again, by just some expansion, we get order n log n. So, both of these were relatively easy to solve by this naive method of just repeatedly substituting and finding a pattern and kind of replacing the whole expression by something which encompass the whole calculation. Now, when we looked at integer multiplication, this problem became a little messy because these terms became a little more complicated. So, for the Naive divide and conquer, we had four times T n by 2, so notice that we have here 2 and 2, so that in some sense helps us manage the thing a little better, but here we have 4 and 2 and 3 and 2 for Karatsuba’s algorithm. And then we ended up with completely different analyses. So, we ended up saying that the naive algorithm will take us back to n squared. And Karatsuba’s algorithm actually gives us is very strange quantity, which is n to the power log 3, n to the 1.59. So, the question now we are going to look at is, whether there is some uniform way to get from this recurrence to this asymptotic value. Is there a way to kind of easily calculate what the answer should be without having to actually go brute force through that long expansion and making sure we combine the terms cleverly and recognize the pattern. (Refer Slide Time: 03:13)  So, for this, we will use a representation of our divide and conquer solution using something called a recursion tree. So, a recursion tree is just a rooted tree, it is a tree in the sense that we have been looking at binary trees and so on, but this will be not binary, it will have many children for each node, so it will be a rooted tree. And the property that we have is when we are doing divide and conquer, we are creating subproblems. So, the problem at the top creates, say two sub problems, and each of those creates two more sub problems until we reach at the bottom at the leaf level a sub problem which is trivial of size 1.'},\n",
              " {'id': 'f6c2e9f7-9b27-4278-8cc8-a4bf1e98115d',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, each node in this tree will represent one sub problem. So, each recursive sub problem it appears in the tree, and it is a child of the problem that created it. Now, we are going to kind of add-up the cost of the algorithm by associating the cost with each of the subproblems. And what is the cost we are going to associate? We are going to associate the cost that we spend in that sub problem without counting the recursive call, because the recursive call will be added up in the notes below it. So, we are not going to count the cost of making the recursive call, but the cost of doing the splitting and combining, so that is really the cost we record with each note. So, how much time does it take us to split the problem and how much time does it take us to combine the answers assuming that the recursive problem has been solved. So, this is the value of each node, it is the cost excluding the recursive calls. So, concretely, if we have an input of size n, we will have to spend some function of n on non-recursive work. Usually, we have seen that this is normally order n like we have to for example, in quicksort, we have to partition it into lower and upper that is also divide and conquer or in in any or we have to merge in merge sort that is the combination is order n. In rare cases, like in binary search, this work is order 1, we just have to find the midpoint. So, that is one of the case. So, f n is some function of n. Now, having done that, we split it up into some number of recursive calls. So, in Karatsuba’s algorithm, we made three recursive calls. In the Naive multiplication algorithm made four recursive calls. In merge sort and binary search, we make two recursive calls. So, this number of recursive calls is on some smaller part of the input, which is some fraction of the input. So, this is some n by c.'},\n",
              " {'id': 'b117a329-cf47-4164-bf47-74a270f0a837',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'Now usually, we have been seeing that these two things are correlated, because in binary search, for example, we made in merge sort we made two recursive calls, so r is equal to 2, and c was equal to 2 because we did it from size n by 2, so this is merge sort. But in binary search, r is equal to 1, and c is equal to 2. We make one recursive call on half the length. And in Karatsuba’s thing for example, r is equal to 3, and c is equal to 2. We make three recursive calls to the multiplication, each of them on a number of half the length. So, these two parameters r and c, are independent, in some sense. The number of times, number of subproblems we have to solve, and the size of each sub problem. Because the subproblems are not really directly it is not like we are overlapping. So, in sorting or something, it makes it look strange, how can we take half the array and sorted three times because there are only two halves. But we solve that in multiplication, we are actually multiplying three different quantities of half the size, they are not directly contributing to the answer, the answer is some clever combination of these. So, that is why we can have more subproblems than there are the size of the parts. So, if we do this, obviously, the recurrence that we get is that to solve a problem of size n, we have to break it up into size of problems of size n by c. And how many of them? Well, r of them because that is our assumption. And then, we are spending this is the non-recursive part, we are spending f of n time in doing the splitting and combining. So, if assuming that this gets solved, then the cost that this node has to incur on its own is f of n. So, if we now try to draw this recursion tree that we set for where each node represents a sub problem at the root of the tree, we have the original problem. And the original problem to break it up, takes f of n work, so we will label it with f of n as the cost of this thing.'},\n",
              " {'id': '8c8f5031-e819-4926-b681-94c4b4aabcc3',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'And then this will have r children, so the root will have r children, because I am splitting it up, and each of them will represent the cost of doing the problem on input size n by c. And finally, at the bottom, we will have the leave. So, if we go down d steps, then the size of the sub problem has been divided by C, d times, so I have come down to a n by C to the d, so the cost of that level is going to be f of n by C to the d. And just to make things work nicely, we are going to assume that this goes down to 1. So, eventually, C to the d will become n, so C is actually some power of n, so n is actually some power of C. (Refer Slide Time: 08:25) So here is a pictorial representation of this tree. So, this glorious tree has our root with labeled f n. Then at level 1, we have r nodes and each of them has the cost associated with 1 by C of the original input. Now, if I go to level 2, I have each of these r nodes splits into r nodes again, so I have r square node. So, each r node splits into this set of r square, r nodes, so that r into our r square nodes and all and each of them has a complexity of n by C, again, divided by C. So, n by C square. And, finally, when I reach the base of the tree, when I come down to 1, so let capital L be the number of levels in this tree. At the bottom, I have r to the power L, because each time I have been increasing the number by a factor of r, so our r square r cubed, so r to the power L nodes, and each of them will have this cost. So, this is the picture that you should have in mind about this recursion tree. And what we are going to say is that the cost, this T of n, that we are trying to calculate is actually just you just have to add up all these things. So, how do, we will see what it means. So, we just add up all of these things. So, this is the contribution of each subproblem to the overall cost. (Refer Slide Time: 09:44) So, the leaves correspond to the base case. So, this is T of 1, so T of 1 is some constant.'},\n",
              " {'id': 'fd4a4cec-e136-45c8-bc5b-6b462770318c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, it could be 5, it could be 1, it could be 10. But it does not really matter because we are doing asymptotic complexity. So, we can just assume that that constant will get absorbed in our big O notation. So, we will always assume that the base case T of 1 is 1. Now, we could actually have a base case, which is not 1, we might stop the thing at some smaller thing. For example, supposing we are trying to sort something, we might actually at some brute force ways sort three elements. So, we might actually stop short of 1, but in general, we will go down to 1. So, the base case is n equal to 1 and the cost of n equal to 1 is typically 1. So that is an safe assumption. So, remember, that the ith level has r to the i nodes, each with value f, and by C to the i, that is the, this is the size of the input at that stage. And f of that is the amount of non-recursive work we do per node and we have our to the i nodes, because that is how many times a tree has split. So, overall, since we assumed that the original n was a power of C, and we are dividing it into n by C, n by C, n by C after log to the base C of n splits, we have come down to size 1, so we have reached the leaf where we have this case of the base case. So, the capital L, which is the number of levels in our tree is just log n to the base C. So, now, if we want to add up, as we said, we wanted to add up all these nodes, so we have to add, if we added up level by level, then we start from the zeroth level, which is the root down to the leaf level, which is the last level. At each level, this is what we said, we have r to the i nodes, and each of them has value f of n by C to the i, so we just multiply that so r to the i times f of n by C to the i and add this up for all i from 0 to L. So, this is the total cost T of n expressed in this level by level framework. So, in particular, the number of leaves is r to the L.'},\n",
              " {'id': 'cbffd869-c5df-406b-b54d-4c7e622cdcf6',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, r to the L, so remember that L is log C to the base n, so r to the L times f of 1 because that is where the n has become 1. So, f of 1 we remember is always 1, and r to the L is just r to the log C of n because that is what we have for capital L, that is the height of this tree, which has got C branching. But remember earlier observation that a to the log C is the same as C to the log a for any base. So, if I have r to the log n is the same as n to the log r to the base C. (Refer Slide Time: 13:37) So, I this is what we have we have log C log n levels, the last level has n to the log r and the total cost is this level by level cost. So, what we are going to do is, we are going to think of this as a series. So, this is a cost of levels 0 plus the cost of level 1 plus the cost level 2, plus the cost of level L. So, question is, what is happening to this cost, as we go from left to right? So, there are three cases that normally arise in a divide and conquer scenario. The first case is that every term is a constant factor smaller than the previous term. Now, this is what happens in binary search, we start with the full thing, the next level only examines half, the next level only examines half of that. In such a case, what happens is, that the root dominates the cost. So, the root dominates the cost means the whole thing is basically f of n. Now, if the terms are equal, that is, the sum of the costs at level, i plus 1 is equal to the cost of the parent of that thing, then what happens is that the cost across all the levels is the same. So, the cost, the total cost across level 0 is the same as a total cost across level 1 is the same as total costs across level L. So, the total cost is always going to add up to f of n. So, we just take f of n times a number of levels. Now, the number of levels we said was log C of n, but if you are doing asymptotic complexity, log C log 2 are all the same. So, I can just get f of n times log n.'},\n",
              " {'id': '335f0788-32f4-414b-a96d-1573d3f76819',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, if I have all terms in the sequence have are of roughly equal size, then I get the cost of one node times the number of levels, which is log n. And, finally, if the thing is actually exploding, and this is what is happening and certainly it is happening in the naive multiplication that we did. Because we said, I go from n to 4 times n by 2. So, if I look at the number of problems and solving I am solving 4 into n by 2 so I am really solving double the number of problems in terms of bits. Even in Karatsuba I am doing 3 times n by 2 so that is more than n, so the number of subproblems is actually exploding as I go down and it is kind of growing exponentially. And in such cases, it turns out that it is the leaf some which dominate. So, when it is shrinking, it is the root sum which dominates when it is increasing it is the leaf sum that dominates. And the leaf, remember is this month. So, in such cases, I get n to the log r and this is what we got for Karatsuba. So, let us at some examples to understand. (Refer Slide Time: 15:13) So, let us look at merge sort. So, in merge sort, I have n, this is the recursion tree, I have an n by 2. So, n by 2 plus n is n by 2s n, n by 4 plus n by 4 is n by 2. So, this is a situation where the series is equal, in the sense that, the total contribution at each level is adding up to n. So therefore, I get n times log n, just by applying that analysis that we did previously. (Refer Slide Time: 15:36) Now, we saw quick sort in the worst case gives us n squared, but what if we had a kind of clever quick sort? So, remember, in quick sort, I have n elements, so if I had sorted it, then it would be in ascending order. So, what if I pick a pivot, which is always somewhere inside the center segment. So, let us say that it is between n by 3, and 2n by 3. So, the pivot is always here.'},\n",
              " {'id': 'd39293ec-5fea-422b-8361-e9e735e5a2b5',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'How we get that as a separate story, but let us assume that we had a magical way of picking a pivot, whose value was always between the one-third value and the two-thirds value. It is never in the smallest one-third of the list it is never in the highest one-third of the list. So, then in the worst case, it will split this as 1 by 3 and 2 by 3. So, the worst-case recurrence, if I have this clever pivot is T of n is T of n by 3 plus 2 times T of n by 3. Because I have to now solve the other thing twice. So, sorry, this will be 2n by 3 I think. So, this is now something different from what we saw before because in our earlier thing we said you have to do n by c, n by c, n by c, n by c and we have to unequal split. So, here is the recurrence. So, I have one node which has n by 3, and the other node which is 2n by 3. So, what will happen is that the right hand side will go deeper, and the left hand side will run out of elements faster, because this if I go down this path, then it is dividing by 3 every point. And if I go down, this path it is multiplying by two-thirds. So, the right hand most path in this tree is going to be longer than the left hand most, so I will have holes In the tree. So, it actually does not matter. In that recursion tree analysis you could have holes, what he really interested in is the depth, the longest path. So, the longest path is going to be now what happens in this two-thirds thing. So, it is going to be I am dividing by two-thirds, multiple times, and I will get end up saying that I have log to the base 3 by 2 of n, which is just log of n. Now, again, if you look at each level, n by 3 plus 2n by 3 is equal to n, this whole thing is going to be equal to n and so on. So, actually, if you add it up, this is an equal series, so we get n log n.'},\n",
              " {'id': '20f99dc7-a12d-47c9-adbf-a691f3589915',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, what it says is that if you are able to actually ensure that quick sort gives you partitions, which are at a fixed percentage, it could be one-third, one-forth, whatever you will actually end up with an n log n algorithm. Problem with n squared is because you have this algorithm which gives you 1 and n minus 1. So, if you have a pivot, which is very bad, then you get n squared. (Refer Slide Time: 18:11) What about Karatsuba? We will, the first naive multiplication says I take four subproblems of size n by 2 and I keep expanding them. Now, this is again an exploding situation now, because I am going from something which adds up to n to something which adds up to 2n. So, in this case, I have to look at the base. And so, the basis n to the log, remember, I have to do n to the log r. So, r is 4 so I have to the log 4, but log of 4 is 2, so I get n squared. So, in other words, that n squared that we derived by manually calculating you actually get by just plugging in 4 in this n to the log r. And for Karatsuba we manually calculated, but we got n to the log 3, we get n to log 3 here also because now r is 3, so this is n to the log r with r as 3. So, these are just examples to show that now if you have this recursion tree idea, and you ow, you can kind of look at your recurrence and decide whether it is increasing or decreasing or stable, then you can directly read off the answer to the recurrence, without having to manually expand and calculate it, as we did before. So, there are other versions of this calculation, which are sometimes called the master theorem for recursion, but it is useful to have this at the back of your mind. You can always validate it by working it out for yourself. But if it is very clear to you how it is going to work by looking at the recurrence, you can just write off the recurrence the asymptotic complexity without having to do the calculation.'},\n",
              " {'id': '2be6c49d-de49-48e8-a103-eb8c77f6f543',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Recursion Trees.pdf',\n",
              "  'content': 'So, to conclude, I would just like to thank Jeff Erickson for publishing this wonderful book in which these pictures are there which I could borrow. Thank you.'},\n",
              " {'id': 'fa423ace-55c7-4d52-88ac-da21feb0953c',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Implementation of Quick select and Fast select algorithms.pdf',\n",
              "  'content': 'Programming, Data Structure and Algorithms using Python Professor Madhavan Mukund Implementation of Quick Select and Fast Select Algorithms So let us do a couple of experiments to see how quick select improves when we use this median of medians idea. (Refer Slide Time: 00:17) So we begin with this traditional quick select as defined using the partitioning that we use for quick sort. So this is just the usual thing. We saw this earlier. We said that you see if the index you are looking for is absurd, then you just return none. Otherwise, this part is the usual quick sort portioning, which takes the left hand element in your segment as the pivot. And then finds the lower and upper partitions and then moves the pivot to the middle. Now, finally you do the recursive call. So this recursive call basically says that look at the length of the left. If the length of the left is bigger than the k you are looking for, recursively look in the left. If it is smaller, then check if it is exactly the length plus 1, in case the answer is the pivot. And if it is more than length plus 1, then look at the right but adjust the, the index to be k minus m plus 1. So this is our quick select. (Refer Slide Time: 01:13) So here, let us look at an example. So, so let us run that so we have it. So now, what I am going to do is that I am going to construct a random list of 200 elements. So I am taking a value between, random values between 1 and 1,000; 0 and 999 technically. And I am taking 200 of them and I am making a list. So this is the list A that I have. And now, what we will do is, we will try to find the kth largest element for every position between 1 and 1,000. But just to show you the, the, that the none is working, I am going to make it run from 0 to 1,001. So I am going to take length of A is 1,000. So I am going to make is length of A plus 2. So the upper limit is going to be 1,001, sorry, 201, and the lower limit is going to be 0. So if I run this then this is the quick select.'},\n",
              " {'id': '6c7f83fb-0042-4fda-9370-520f070bd9d3',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Implementation of Quick select and Fast select algorithms.pdf',\n",
              "  'content': 'So you, first of all you can observe, that this is going to come, so actually, this is actually, I have done it in reverse. So this is in ascending order. So this is actually taking the kth smallest and not the kth largest. But it does not really matter. So none is, because this 0 is smallest element is none. The smallest element is 1, the next is 5, and so on. So this, this should list it out in ascending order because I am finding the smallest, second smallest, third smallest and so on. So this is one way of sorting, for instance. So we saw, this is how selecting sort works. It picks the minimum or the maximum, moves it out and so on. And then, when, once we reach the last possible value, then I will get actually none. So this says that if the k is absurd, then it gives me a none, otherwise it works. (Refer Slide Time: 02:52)  Now, here is that median of medians algorithm. So what it says is that if the length of the list I want is less than 5, then I just sort the list and I give the middle value. So I take the length of l, divide by 2 and give you that position. Otherwise, I will construct this block median. So I will take blocks of 5, I will sort each block and upend the median of that block to this list M, and I will eventually recursively call this. (Refer Slide Time: 03:21) So here again, we can do the same thing. So supposing we, we create this random array of 200 integers. And now, what we can do is, we can compute, we want to compute the median of medians but I want to also convince you that it is somewhere between 3 10s and 7 10s and where it is with respect to the true median. So what I am going to do is, I am going to take a sorted copy of A and store it in B. And then I am going to show you the 3 by 10th position in B, the 7 by 10th position in B, and the median of B. So the median of B is the actual median of A because B is a sorted version of A.'},\n",
              " {'id': 'f85bfb1c-a974-477b-9314-5f080f65d7d4',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Implementation of Quick select and Fast select algorithms.pdf',\n",
              "  'content': '(Refer Slide Time: 04:00) So if you run this, for instance, it says that the actual median that we find is 568 by MOM. The real median in 541. It is not too bad. And the extreme values that are at 3 10s and 7 10s are 322 and 736. So it is indeed in that range. And you can do it again. For instance, you can take a different random set and do the same thing again. (Refer Slide Time: 04:22) And now for instance, it says that it is between 297 and 782 but the median we have found is 535, which is little bigger than the actual median. So this our median of block medians, which is not giving us the actual median but it is giving us something which is better than 3 by 10, and not, not worse than 7 by 10. So somewhere in that middle 4 by 10 of our array. (Refer Slide Time: 04:43) Now, we do our usual thing just to make sure that we do not have problems with recursion limits and we have this timer class because I just want to show you the difference between quick select and fast select. So here is quick select, the old quick select. So what I am going to do is, I am going to take an ascending sequence. So remember, that, like, quick sort, the worst case for a pivot in our traditional quick select is one where the extreme value is the smallest one. I am going to take an ascending sequence, and I am going to ask you for the, the 1,000th, the 10,000th smallest element. So the smallest element is the last one. So what is going to happen is that the first time I am going to get a lower empty and the upper is going to have 9,999, then it is going to have empty and 9,998, and so on, so I am going to actually hit the n squared. So if I run this, so I am taking i for i in range 10,000. So I am constructing a sequence 0 to 9,999, and then I am asking for the last element in this list using quick select.'},\n",
              " {'id': '35cffe71-bf11-483f-a6b9-99dfd1d90f1c',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Implementation of Quick select and Fast select algorithms.pdf',\n",
              "  'content': '(Refer Slide Time: 05:44) So if I run this, it will actually 9,999 but it will take almost 10 seconds because it is doing an n squared work on an array of size 10,000, so roughly 10,000 into 10,000, which is going to be roughly 10 seconds, as we know it. So it has taken 7 seconds, maybe. (Refer Slide Time: 06:00) Now, unlike quick sort, this is not symmetric. So if I actually ask it for the smallest element. If I change this to the smallest element, and I run it, then it will be very fast because now what is happening is that it is partitioning into lower and upper and saying, oh, the position is actually the pivot and it is giving me the pivot immediately. So instead of taking almost 10 seconds, it is taking me a fraction of a second. So it is important that quick select, the worst case also requires you to choose the index according to what you are doing. So if you take an ascending sequence and you are looking for the k smallest element, then you must find the largest element. If you are descending and you are trying to find the k largest element, then you must ask the index of the smallest element, and so on. So this takes about 7.3 seconds. (Refer Slide Time: 06:48)  So now, let us see what happens in this fast select. So in the fast select the difference is that we first use this median of medians to find the pivot. So we take the slice that we are looking for, l, capital L from l to r and we feed it to median of medians, get the pivot, then we find the position of the pivot because this just tells us pivot value. So, because the median of medians does not tell us where it is, so we find the pivot position and we just swap it with the leftmost value so that the rest is the same. So you can check actually that this code is identical to the quick select except on the recursive calls, we have to use fast select rather than quick select because we do not want to slip into that algorithm. So fast select will call itself as a post quick select.'},\n",
              " {'id': 'dd6d8cc8-7732-434b-af0d-a02749a51128',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Implementation of Quick select and Fast select algorithms.pdf',\n",
              "  'content': 'The only difference is this pivot selection procedure using median of medians. (Refer Slide Time: 07:35) So now, we do this. And now we do the same calculation that we did before, except we use fast select. So in quick select, what we said was, we start this timer, we take 10,000 numbers in ascending order and we find the 10,000th number using select. So we will do exactly the same thing. So we will do A equal to, A is i for i in range 1 to 10,000. But now we will take fast select instead of quick select. And now, if you do this, you will see that what was taking 10 seconds is not taking 0.01 seconds. So it is really coming out in linear time because you know that 10,000 is like, so 10 to the power 7, so 100th of a second is what you would expect, but this is what it is taking, 0.01. So this is really showing the power of moving from that naive pivot finding algorithm to the median of medians pivot finding algorithms.'},\n",
              " {'id': 'fa98e4ae-1e91-435b-b3d3-eb520ba0af45',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'Programming, Data Structure and Algorithms using Python Professor Madhavan Mukund Divide and Conquer: Quick Select So as a final example of divide and conquer, we will look at a problem called quick select. (Refer Slide Time: 00:18) So selection is the problem of finding a specific position in a list. So in particular we want to find the kth largest value in a sequence of length n. So of course one way to do this is to sort the list in descending order, and then the largest value is the first one, the second largest value is the second one and so on, so by just looking at position k in the sorted list in descending order we will get the kth largest value. So since we know we can sort in n log n, this would mean that the kth largest value, the selection problem of finding the kth largest value also takes n log n. But if we look at some special cases it perhaps suggests that we could do better. For instance, if we want k equal to 1 then we are just looking for the largest value in the list and that is just the maximum. And we know that we can compute the maximum in a list in a single scan so it only takes order n time, it does not take n log n time. Similarly, if k is equal to n, that is, we want the smallest value in the list, then we can just compute the minimum in a single scan of order n. So the question is can we do this for all k? So if we fix a k, then of course in the first pass we can find the largest, and then among the remaining in the second pass we can find the second largest. So we can make k passes and in order k times n time, we can always find the kth largest. Now, the problem is that as k becomes large, in particular, if k is like n minus 1 or n by 2 for instance, then this would become order n squared. So in particular the median is the value in the center of the list so if we sort the list then the median will be the value at position n by 2.'},\n",
              " {'id': 'c793f826-7975-4caf-9224-519a911d0ddb',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So if we just use this naive thing here then the time it will take to find the median is n by 2 passes on a list of length n which will be order n squared. Now remember that one thing we had said before is that if we can actually find the median quickly in order n time, then what we can do is we can take this list and then find the median as the pivot and then create this lower list and this upper list which are roughly half because the median will divide it into two parts which are half. And if the two are half and we recursively sort them then we will get a recurrence like merge sort. So quick sort will also become order n log n. So that would be a side benefit of being able to do the selection problem in order n time. (Refer Slide Time: 02:45) So first, let us look at a divide and conquer solution which is similar to quick sort. So remember that in quick sort we used a pivot to partition the sequence that is given to us into two sequences. The lower part, everything that is smaller than or equal to the pivot, and the upper part, everything which is bigger than the pivot. So now we have this lower part. So remember the way that quick sort partitions is to put the pivot in the middle and put the lower part before and the upper part after, and we know that everything in the lower part is smaller than or equal to p and everything in the upper part say is bigger than p. So now we are looking for the kth largest element. So the point is that if this particular segment, if this segment is less than equal to k then the kth largest element actually sits in the lower part. So we can look at the length of the lower part and determine which of these three segments we were going to find the position that we want. So if k is, so let m be the length, so m is the length of this lower segment. So this is m. So if k is less than or equal to m then the kth largest element of the whole sequence actually lives in the lower part. So we can search there.'},\n",
              " {'id': 'd31e1524-086a-487f-be4b-0cc72eef0fb5',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'If k is equal to m plus 1 then this must be the value that we want, so we can return the pivot. And if k is greater than m plus 1 then we know the answer lies in this part. So this now gives us a recursive strategy. So recursive strategy is that if k is smaller than or equal to m we just apply the selection procedure to just the lower part of our sequence and look for the kth element there. If it is m plus 1 then we just return the pivot, we do not have to do any more work, we have found the element that we want. And finally if it is bigger than m plus 1 we must select the upper one but now we must change the value we are looking for because we have this lower set. So this is m, and we have this one element which is the p. So we have to subtract, so now we are looking for the kth element overall, but we are looking, from here onwards we want a value which is k minus m plus 1. So we must change our, the position that we are looking for from k to k minus m plus 1 because we are now looking beyond the pivot. So this becomes now, a recursive way to divide and conquer and compute the selected value that we want. So this is something which, since it is derived from quick sort, let us call it quick select. So very similar to quick sort, we can write this quick select. So quick sort had a list and it had a lower and upper bound l and r, but we also now add a fourth parameter k, which is the position that we are looking for. So we are trying to find the kth largest parameter in the slice of this list from l up to r minus 1. So the first thing is that if this k is kind of observed then we will just quit. So if k is smaller than 1, so our, our values are from first largest. That is one, 1th largest to nth largest. So if it is less than 1, if I ask you for the zeroth largest thing then you will just return none. Similarly, if the k is larger than the number of elements in my list, so r minus l is going to be the number of elements in my list.'},\n",
              " {'id': '1be581c9-7d11-42b5-ac0a-6796a18a6405',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So if either of these two cases are there, then I will just return none as a kind of answer saying that this query is not valid. Otherwise we do exactly the same partitioning as we did for quick sort. So we set the leftmost value in our slice to be the pivot, and then we maintain these two pointers if you remember, we maintain these two pointers and then we slowly scan from left to and gradually partition this into lower and upper. So that is what happens here in this loop. And then once we have got that then we want to exchange the pivot with the last position and lower and that is what happens here. So this is exactly the same partitioning strategy as quick sort, nothing new here. Now, unlike quick sort where we are going to examine both halves and sort them recursively, here we are going to do this check. So the first thing is we compute the length of the lower segment. So the length of the lower segment is, the position lower is actually the first index outside that. So it is like saying that if, if it is n the, then it is n minus 1. So the lower length is the value lower, which is the position marking the end of that segment minus 1. So now if the value, the position k that we are looking for is less than or equal to the lower length then we will recursively quick select on the lower part. So I will go from the left limit l to the upper limit lower and look for the kth element. So that is this case. Otherwise, in the second case if, if the k I am looking for is exactly equal to the length of the lower half plus 1, then I will directly return the pivot, which is just the value at the position index lower. And finally if it is not either of these two then I will run quick select on the right half from lower plus 1 to r, but I have to change the index from k to k minus m plus 1, where m is now for me, this lower length.'},\n",
              " {'id': '59f60ea7-bebb-4b3b-a4ff-371727ba780c',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So what we have done is we basically taken quick sort, and we have taken the partition algorithm exactly as in quick sort, and then after partitioning, instead of sorting the two halves we are suitably choosing which part of the partition to select. So either we select in the lower part or in the upper part or we just directly return the pivot. (Refer Slide Time: 08:28) So if you want to analyze this, then the recurrence is very similar to quick sort. The only difference is we are not going to look at both halves. So we are going to look at let, let m be the length of the lower part. So we are either going to look at the lower part or we are going to look at the upper part, and in the worst case we have to go with a larger one. So instead of saying T of m plus T of n minus m plus 1, which is what we would do if we had quick sort because we would have to solve both problems, here we are going to do the maximum, one of the two, either the left half or the right half, and in the worst case it will be the worst one of the two so it will be the maximum. Plus of course, there is this order n work which is the partitioning part. So I have to go across this entire sequence in order to partition. So there is order n work to partition even though after partitioning I do not have to do any work to recombine unlike in merge sort. So of course, we know what happens which is that when our pivot is an extreme value then either I will have an empty lower part or an empty upper part. So m will be either 0 or n minus 1, and I can engineer it so that it happens with every recursive call. So every recursive call this maximum might well be where m is 0, for example, it will be T of n minus 1. So in the worst case this quick select will also have a recurrence which says T of n is T of n minus 1 plus n, just like the worst case for quick sort even though we are not doing both halves. We are doing only one half.'},\n",
              " {'id': '3c701cec-9681-4c8a-80d1-1a4c89f594e1',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'In quick sort, the other half disappears because even the worst case, sorting the empty half is trivial whereas here we are only looking at one half and that one half is a big half. So quick select is again n squared. So this divide and conquer has only got us our n squared thing, and we know how to do n squared by brute force. We can do n squared by just doing one scan for the first element, another scan for the second largest element, another scan for the third largest element, so it appears that this quick select, even though it is a clever way to do this is not really any better than the brute force way of doing n nested loops. But last time when we had looked at this recurrence tree analysis, what we had said was that if you can somehow find a pivot which is not necessarily the median but it is some fixed fraction away from the thing. So for instance, if it is between one thirds and two thirds. So if you arrange the values, if your pivot is guaranteed to be in the middle third. So supposing my values are arranged in ascending order, I am not promising you the pivot is in the middle but I am saying that the pivot is going to be somewhere in this segment, between one third and two thirds. And it can be one third, two thirds, it can be any fraction. It could be one fifth and two fifth. This is also fine. For, four fifths, sorry. So for any fixed if I can knock off a fixed fraction by choosing a good pivot, then the claim is that quick sort will actually become n log n. So last time we saw for this one third, two thirds. So supposing the pivot lies between the maximum value by 3 and the maximum value times 2 by 3. So if this is the case then the recurrence we get is that we are guaranteed that the smaller of the two partitions is at least one third in size because the pivot is no more than one third away, I mean at least one third away from the boundaries. So we have the, the smaller of the two partitions is n by 3.'},\n",
              " {'id': '27e42292-d0f5-491d-af30-ce8b49709604',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So the worst cases one is n by 3, the other is 2 n by 3, and we and if we solve this then we get n log n. So the question is whether we can somehow manufacture this for our case. So can we find a good pivot of this form, a pivot which is somewhat guaranteed to give us a fixed fraction of elements that will be pushed into one of the two partitions, rather than this extreme case of 0, which we have when we take just the position at the zeroth, the value at the first position or the last position in the list, or any fixed position. (Refer Slide Time: 12:15) So here is a very clever idea which achieves this. So this is called the median of medians. So what we will do is we will divide l into blocks of five. So we will take our list and then we will write the first five elements, and then the next five elements. So we will just block it off into blocks of five. So we will take the first block of five elements, the second block of five elements and so on. Now for each block of five elements, we can find the median by, if necessary by brute force. We can sort it and find the middle one or we can just rearrange it by any standard sorting algorithm and find the value in the middle. So for a block of five elements, it is a constant amount of work because the block size is fixed to be five. It is a constant amount of work to find the median of that block. So we can find the median of the first block, we can find the median on the second block and so on. So we can find the median of each block. Now of course, it could be that the final block has not got five, if it is not a multiple of five. But as usual we will assume that it is always going to be some kind of power of five for the analysis to be simple. So in this way we have now taken this and we have picked out one element from the first five, the median, one element from the second five. So each block we have got the median. So now we collect these medians. So we collect the medians.'},\n",
              " {'id': '1eef7054-c3dc-46bc-99b1-129d1f1fc795',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So we have, just to remember, so we have some x 1 to x 5, and then we have some y 1 to y 5, and then we have some z 1 to z 5, and so on. So from here I will get some value A 1, that is, the median of this thing. Then from here I will get a value B 1, which is the median of this thing. From here, I will get a value C 1, which is the median. So I will collect one value, namely, the median from every block, and I will create a new list, and this new list has the size one fifth of the original list because I am only picking one in five from every block. And now what do I do? Well, I will recursively do the same thing for this list m. So I will again divide this list of median block medians into groups of five and again find the median of each of those blocks and pull it out. So that is why this is called the median of medians. So I find blocks, I take the median of each block, I collect it into a list, I make blocks, I collect into list, I keep going until when well until the blocks that I have collected amount to 5 or less and then I just take the median at the end. So that is my base case. So here is the algorithm in, in python. So I take a list and I want to compute this median of block medians. So if the list is smaller than 5, then I will just do some brute force. I will just sort it and return the middle element. So I just take the length of the list and divide by 2 and that should be roughly the middle element. It might be plus minus 1, it does not really matter. Otherwise, if it is bigger than 5, I will do this grouping into fives. So I will start, so capital M is going to be my collection of median. So initially I have no medians to collect. Now, what I will do is I will look at indices 0 to 4, then 5 to 9, and so on. So I will say let i be 0, then let i be 5, then let i be 10, so I will take the range 0 to length of l in steps of 5. And now I will take the slice from i to i plus 5. So this is five elements, i, i plus 1, i plus 2, i plus 3, i plus 4.'},\n",
              " {'id': '48bffd0c-6a15-4554-89ab-a651fd05a3d8',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So I take those five elements and store it into a new list. I sort that list exactly as I did in base case, and I take the median of that list and append it to this collection of medians that I am going. So I am just taking the first 5, finding the median putting it in. Now one thing to note is that if I know that there are 5 elements then the median is always x 2 because x 0, x 1, x 2, x 3, x 4, so after I sort it, I know that the second position, the position index 2 is the median. But the problem is that I might have at the end, an odd block which is not 5. So if I have an odd block which is not 5, supposing it has only two elements, then if I ask for position 2 in a 2 element list it will be outside and I will get an index error. So it is always safer to just say that take for each block that I construct, which is typically 5 but the last block may be smaller than 5, sort it and take the middle position by taking the length of that block divided by 2. So what we have done is basically we are sequentially going block by block. We take the first 5 elements, put it in x, compute this, sort it, take the middle element, and put it out into M, take the next block and so on. And now, having constructed this M we just recursively call our function on this list. So this will again divide by 5 and do it and keep doing it until the shrinking by 5 brings us to a list which is smaller than 5, and then the base case will exit. So this is our median of median algorithms. So the goal of doing this was to guarantee somehow that the value that we pick out of this, so you can check that this is not going to be the actual median. But we do not want the actual median. What we want is a pivot value which is guaranteed to give us some fixed fraction of elements in each of the partitions. That is all we need in order for our quick select or quick sort recursion to give us an n log n or order n thing.'},\n",
              " {'id': '1b6d02e0-9039-4da5-9bb2-bd0f75f10397',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': '(Refer Slide Time: 17:40) So, so let us see why the median of medians that we are constructing is a good value. So if we visualize the blocks, so this is my block 1. So these are my first five elements. So each column in this is, so this is my entire, from left to right, is my entire list, and I have been grouping it in groups of five. Now of course, each of these five is not sorted but let us pretend that it is sorted. So let us visualize it so that each of these blocks of five is internally sorted like we did here. So we are looking at this value this x after it is sorted. So we keep this. So we are not actually doing this in the original list. We are just doing it block by block but let us see imagine that we actually collected all the sorted blocks. Then where would the median of each block lie? It will lie in the middle of that thing. So these yellow things are the medians of each of the blocks. And now the yellow things are going to be finally passed back to the median algorithm. So we are going to get this blue thing which is roughly the middle of that sequence as our final median of medians. So the question is what can we say about how many elements are below or above that blue one? So this is our question. (Refer Slide Time: 19:00) So if you think about it, everything which is to the left of this, all these are our old medians, these are all smaller than the blue ones. And within each of those blocks everything which is above is also smaller. So all these, these positions which are marked in this brown color are smaller than our median of medians. And symmetrically if you drew it here, everything in this section would be larger than our median of medians. So you can see why. Because everything in this row, the same row is a median, is a block median which is smaller. This is the median of the block median. So this row is all the block medians and these are all smaller than the blue element.'},\n",
              " {'id': '654e6045-2f6b-4f96-9b10-109bd7ac2dd6',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'And within their corresponding blocks there are two elements smaller than each of them because they are the medians of their respective blocks. So how many brown elements are there? How many brown elements are there? Well this is roughly half. So if I had length of l, so this is roughly half, on the left of the blue elements, and within that I am taking 3 by 5. Out of every 5, I am taking 3. So I am taking 3 by 5. So this, the number of things in the brown thing should be 3 by 5 times the length of l by 2, so that is 3 by 10 times the length of l. So this is going to be the smaller part of the thing. So at least 3 by 10 of the elements are smaller than this median and at least 3 by 10 are larger than the median. So that is going to say that the median lies between these values. So if you take the maximum value and I take the things is going to be between 3 10, so I am going to knock off at least 3 10th, this is what it means. (Refer Slide Time: 20:50) So now this gives us a strategy to get this block of medians. So let me change that quick select and call it now fast select. So fast select is like quick select, except that instead of picking the first position as the pivot, it calls this median of medians. So you want to find the, the quick select the kth element in, in capital L from the slice l to r, instead of taking this, the position l as your pivot you call this median of medians algorithm on this slice. Now of course, this will give you a pivot somewhere in this slice. So you have to find that position. So we will find the first i in this range, which has the value of the pivot. And then we will just to make the rest of the partitioning look similar, we will move that pivot to the left hand side. So what we are saying is that you give me the slice L from l to r, I run median of medians on this, and I will get a pivot, and I will now take this pivot and move it to the beginning so that the rest of it can run as before.'},\n",
              " {'id': 'd428a78d-cc76-40fa-8b98-560f14813333',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So I move this pivot from wherever it is as found by the median or median thing to the left hand side and then I run everything as before. So the partitioning is as before, and the recursive calls are as before except that instead of calling quick select, of course, I am calling fast select. So the only thing I have done is I have changed the way I compute the pivot. Instead of taking the leftmost element, I am using this median of medians and then doing it. So first of all how much time does it take to run median of medians? Well that is easy. Median of medians is basically taking the entire thing, doing a linear scan. So in order n time it is reducing the problem to one fifth, and then it is again reducing the problem to one fifth. So this is one of our contracting series. And so this whole thing is going to be dominated by the time it takes at the root, as we saw. So it is going to be order n. So the median of median finding this is actually order n time. So finding the pivot is order n time, and finding the partition is also order n time because I do it in one scan. So overall as far as this fast select is concerned, this part of it remains order n. The processing time at each recursive call without the recursive call being counted is order n. It is order n to find the median of block medians and another order n to do the partitioning with respect to that median. And now what do we know? We know that like our earlier example of quick sort with 2 by 3 and 1 by 3, here we know it is 3 by 10 and 7 by 10. We know that at least that many elements have been eliminated in this, in the, or pushed into one of the two partitions. And since we have this, it is similar to our earlier thing that we will just keep contracting. The maximum of this is strictly smaller. So basically this recurrence of fast select is going to be dominated by what happens at the root, and I am going to get that fast select also runs in linear time. So this is, it should be big O of n.'},\n",
              " {'id': 'e2be7f7d-2af4-482a-b10f-72abd7455a78',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So, so t n is also big O of n. So we have moved from an n squared algorithm to a big O of n algorithm by using this very clever median of medians argument in order to find a pivot, which is in a range which is guaranteed to be between three, 3 10ths and 7 10ths of the positions in the array. So, so we can of course use the same trick in quick sort. So in quick sort also we can, instead of taking the extreme element or some fixed element as a pivot, we could call median of medians, in order n time will get a good pivot, we will reduce quick sort to taking quick sorting 3, 3 by 10 and 7 by 10, and it will become n log n. Now of course, there is a cost involved with this, so there are some constants and all that. So when I say that this is going to be order n, it is going to take some extra cost compared to just doing a partitioning and we can basically see that in quick sort on an average, it will be n log n even without doing this work. So we typically do not use this x because this overhead actually makes quick sort slower in the long run because in the cases where it would run in n log n anyway by adding this overhead of median of medians, we are actually doing extra work. But it is useful to know that quick sort can actually be made to run in n log n time in the worst case by applying this median of medians as our pivot strategy. (Refer Slide Time: 25:18) So to summarize, the median of block medians will help us to find a good pivot in linear time. And now if we use this as our strategy to choose the pivot for our partitioning, then this quick select actually becomes a linear time algorithm, and quick sort becomes an n log n algorithm in the worst case with this extra step. This also means that we can find the actual median in linear time because in particular this finds the kth largest number for any k in linear time, and if I say k is n by 2 then I can actually find the middle element in the sorted sequence in linear time. So I can actually find the real median.'},\n",
              " {'id': 'f3cb8a91-bcfb-465d-a548-d2e034aeb453',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 8},\n",
              "  'source': 'Divide and Conquer-Quick Select.pdf',\n",
              "  'content': 'So even though the median of block medians is not the real median, it is somewhere between 3 10s and 7 10s, but still I will also be able to find the exact median by using quick select with median of medians. So you can actually work it out that you can actually build in that median of medians and make it a single algorithm which will find it but it is more transparent to keep it as two separate things, as a median of block medians and then a quick select. So you apply quick select with k equal to n by 2 and call median of medians for the pivot and you will get the actual median in linear time. So historically of course, quick sort is very closely related to quick select, and in fact Tony Hoare who invented quick sort also described quick select in the same paper. So in 1962, the paper that talks about quick sort also talks about how you can use it for selection. So quick select was already known. And this clever algorithm which is therefore, median of medians was came up over 10 years later in 1973, and this is again a bunch of heavy weights in computer science. So Manuel Blum, Robert Floyd, Vaughn Pratt, Ron Rivest, and Robert Tarjan, so each one of them on their own has made seminal compute, contributions to various aspects of computer science. And together the five of them came up with this very clever idea of median of medians which makes both quick select and quick sort much faster.'},\n",
              " {'id': '8f86bbd3-5afa-4ea5-b917-71f39288f032',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor. Madhavan Mukund Common Subwords and Subsequences So, in the context of memoization and dynamic programming, we looked at this grid path problem, which is an interesting curiosity. But it is not a particularly useful problem, you might argue, because it does not have much of an application except to illustrate. So, me aspects of memorization and dynamic programming. So, now let us look at some more, quote unquote, ‘useful functions’. (Refer Slide Time: 00:33) So, these functions involve strings. So, you might want to look at two strings and find out what is the longest common segment in those two strings. So, this is called a subword. So, a subword is a segment. So, for instance, if I look at secret, and secretary, then this is a segment. And this is a segment. So, it occurs as a block. So, it has a subword of length 6, if I look at bisect, and trisect, then ‘isect’ is there in both. So, this is a common subword. So, it has length 5. If I look at bisect and secret, on the other hand, I have this ‘sec’ and I have ‘sec’, and this is the longest common segment that is there in both, so, the length is 3. And if I look at director and secretary for instance, then I find very short thing. So, I find this ‘re’ for instance. Or So, I do not know why I said it is not ‘ee’ but ‘ec’. So, I have ‘ec’ and ‘ec’. So, ‘ec’ and ‘re’ both of length 2. So, this is the question. Now the question is, if I give you two strings, can you tell me what is the longest common sub word? So, now there is obviously a bruteforce way of doing this. So, first, let us see what the problem formula says. So, I have two words. So, you is of length m. And as we normally index in Python, let us assume that the positions in the string are 0 to n minus 1, and likewise, v is of length n. So, we have a string v with position 0 to n minus 1. So, what we want to do is find common subwords.'},\n",
              " {'id': '01ffc20f-01b2-4726-958c-e5024b32a9ff',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, we want to find some sequence of length k in both of themsuch that the two sequences of letters are the same. And if we find such a length k subsequence, we say they have a common subword of length k, and we want to find the largest k for which such a thing exists. So, that is the longest common subword. So, the bruteforce technique is just to check for every starting point. So, for every starting point, So, I want to find out whether there is a long subword starting from ai and bj. Once I fix ai and bj, I can find out what is the longest word that can start from there, if ai is equal to bj, then it is 1 then I go to the next one. So, I can keep scanning, I got from ai bj, ai plus 1, bj plus 1, ai plus 2, bj plus 2, I keep going so, long as the same. And when once I find that there is a mismatch, I stop. So, from every ij, I can do a scan, and try to find out how far I can go before I hit a mismatch. And then I can record that as the longest common subword starting at those two positions. And I do it for every position, and therefore I am going to find the maximum. So, this is going to now take me m times n squared because there are m times n positions. There are of m choices for i and n choices for j So, for every i and j I am going to start this calculation and in general, I am going to be scanning. So, me thing which could be proportional to the length of the smaller word. So, if I assume that m is bigger than n, then the length could be proportional to the smaller world. So, just as a trivial example, you can see that if your sequences are like this, use this and v is this. Now, nobody has said that these longest common sub words are unique or something. So, now for every position and u and v I will find that there is a longest common sub word of length 2. So, I will be doing order n work for every position. So, I can easily achieve this m times n squared because my algorithm is not going to be able to look and say this is a special case.'},\n",
              " {'id': 'd03e0842-404a-4ff6-9406-4d6ef65ae0aa',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, I have to do it. So, this is m times n squared. The question is, can we do something better than this by looking at the inductive substructure of this problem. (Refer Slide Time: 04:19) So, this is the theme of this, these problems this week, which is problems which have an inductive structure which we can exploit and efficiently calculate. So, we want to find the largest case such that for some positions i and j, there is a string of length k starting from ai and bj which are the same. So, what we want to do is calculate for every i and j that is what we are doing by bruteforce what is the longest common word starting at i and j. So, now what is the inductive structure? Well, if the position i in uis not equal to position j in v, then there is no common word starting there because right at the beginning you are have a mismatch. So, if ai is not equal to bj, the longest common sub word starting at i comma j has to be 0. On the other hand, if ai is equal to bj, then I know that there is a longest common sub word of at least 1 what happens after that I go to the next letter. So, if I know that there is a longest common sub word starting from i plus 1, j plus 1, then this word is one more than that. So, I can just take 1 plus the longest common sub word the next position. So, this is the inductive structure, if I can include i comma j, and my longest common sub word then the rest of the sub words starts i plus 1, j plus 1. So, whatever is the longest word I can get from there, plus this letter must be the longest word that I can get from here. So, this is our inductive structure, then what about the base case? So, the base case is we are now looking at from i onwards. So, when one of the stringsget over, So, remember that the indices are 0 to n minus 1, and 0 to n minus 1. So, when I reach index m, or index n, I have gone beyond the word. So, when I the exhausted that word. So, the base case comes when I am looking at the empty string on either side.'},\n",
              " {'id': '68dec334-872f-47f0-b456-b1d333cd9e71',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, if I have the empty string on both sides, then there is no common sub words, that is 0. And in general, if I have empty string on one side, then again, there is no common sub word. So, if I look at i comma n, because n is already beyond the length of v, or if I look at m comma j, because j m is already beyond the length of u, both of these must be 0.So, these are our base cases for this problem. (Refer Slide Time: 06:32)    So, what we have seen is the sub problems are all these different combinations, i comma j. So, what we are going to use is obviously use some dynamic programming or memoization. So, let us think of it as dynamic programming. So, we have these sub problems of the form i comma j. And because this base case takes us from 0 to n minus 1 the length to the word and beyond. So, it is from 0 to m and from 0 to n. So, I have to compute this for these m times n entries, m plus 1 times n plus 1 entries.So, let me think of it as a table. So, let us look at a concrete example. One of the examples we add. So, we have secret, and bisect. So, bisect is u, and secret is v let us sayso, each of them is of length 6. So, they have legitimate position 0 to 5, and have this fictitious position 6, where I have just written a dot, which is beyond the thing. So, now I need to find out how to fill up. So, each entry here is of the form LCW, i comma j. So, this is the idea. So, I need to find out a sensible way to fill up this entry is LCW i comma j. So, what do we know? Well, we know that there is some dependency, what is the dependency it says that ij depends on i plus 1, j plus 1. So, if I want to fill this entry, I need to know this entry. If I want to fill this entry, I need to know this entry. But what about this entry? Well, this entry does not have anything beyond it. So, clearly, I can start at this entry. And again, if I look above it, this also, does not have anything.'},\n",
              " {'id': '4f185eee-eb3f-475d-bc17-67139404539f',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, the top the rightmost column has no dependencies, I could also do it in the bottom rows. So, I could do this, this way as well. But I will do it on the rightmost column. So, now, what we already said is that since we are beyond the length of secret, we are beyond n, we should just treat this as going to be 0. Now I come to this column. And here, because at this point, these two letters are the same. Since in u of 5 is equal to v of 5, I get 1 here, and everywhere else I have 0 because there is a mismatch. So, because t does not occur anywhere else in that column. Now if I go to the next column, here, I will find that. So, I am looking at this row and this column. So, this is e and therefore, I am looking at that column. So, therefore, wherever I c and e but there is only one e here, I get a 1. Now I come to the next column, I am here and I am looking at for an r on the left hand side implicitly and there is no r. So, when I got this e it is 1 plus the extension but there is nothing below it. So, I keep going. So, here for instance, I see that thanks to this, this is a 1 and now hopefully something interesting will happen. Now, when I come here I will find that this e and e generator one, but 1 plus this diagonal is going to generated 2. So, I get a 2 here, because I get a 1 from the fact that these 2 indices are both e plus 1 coming from here. So, I have this LCW y comma j is 1 plus LCW i plus 1, j plus 1 and then likewise here I find that s and s are the same. So, when I come here, I will get 3. So, I have to look at this thing and remember what is the longest common sub word it is that i comma j for which this value I have calculated is maximum. So, in this case, this happens to the maximum. So, the answer is 3. So, it says the longest common sub word between bisect and secret is length 3, which we had also, done by inspection, it is the word ‘sec’. So, now, in this, we have not really calculated the word we only calculated this length.'},\n",
              " {'id': '279fbcbf-6d8d-49c9-bafe-9d6a8c9cfeac',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, how do we get the word back? So, to read off the solution first, we find an entry which is maximum. So, we can keep track of it in as we are going along. Or we can go back and scan the table once to find the maximum, but we find the maximum here. Now, if it is the maximum, it has got to its value by adding 1 to its diagonal element. So, it added 1 to something here, which added 1 to something here. So, I should be able to walk down and find out all the entries that corresponded to that. So, I can walk down and then mark off these entries. And if I look at the corresponding entries on the two words, they will give me the words.So, these indices here, if I project it onto the column, and the top row, then I find out that it is sec which is the witness, now they could have been another word of length 3, and that would have given me possibly a different in this case, there is only one witness. So, I get ‘sec’, but just computing the length is not a restriction because from that, we can read off the answer from the final thing. (Refer Slide Time: 11:22) So, here insome simple implementation. These are I mean, most dynamic programming problems are hard to set up. But once you got it set up, then it is very easy to implement, because it is just a kind of nested loop through an array typically. So, here let us use a numpy array. So, what we will say is that we will get the length m and n of the two words, and we want one remember, which is m plus 1 times n plus 1. And because our base case is 0, it is convenient to set up this thing as a 0 length, 0 filled array. So, we use the 0 function of numpy and we get an n plus 1 by n plus 1 array of 0. Now I am going to keep track of the maximum length. So, this code does not tell you how to read off the answer that you will have to figure out for yourself. But now it is just simpleI am doing it column by column, and I am doing it from m comma n down. So, I start with n minus 1 work down to 0.'},\n",
              " {'id': '848f4e6a-670b-4306-8ca8-45822128cb06',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'And I start with m minus 1 work down to 0, because remember, the m and n column are implicitly 0 because I have already reset them to 0 here. And now I use the induction, if I have at the row and column, the values are the same, then I add 1 to the value of the next one, the next column, the diagonal limit. If they are not the same, I set it to 0. And depending on whatever I have calculated, I will update the maxlcw. So, the current lcw of this row column turns out to be more than the maximum since over updated and finally return the maximum. So, this is very straightforward. How long does this take? Well, brute force if you remember, for every starting position, m times n starting positions, we had to do a potentially order n scan, So, does m n squared, whereas here, we only have to fill up this table, which is m times n, m plus 1 times n plus 1 is an order mn table. And how much time it take us to fill up an entry? Well, which is have to look at the diagonal. So, it is a constant amount of time. So, therefore, it is mn because we have to just fill up this table of size mn. And each entry takes on a constant amount of time. So, I have saved 1 power of n from my thing. So, I brought a cubic in some sense down to a quadratic. (Refer Slide Time: 13:22) So, the longest (())(13:25) common subword is not really the more interesting problem, more interesting problem is a problem called the longest common subsequence. So, in a sub sequence, we are not restricted to taking a segment which sits there, we can drop letters. So, for instance, with secret and secretary, I already have a maximum match, and so well. So, with bisect and trisect. But if I look at bisect and secret earlier, I said that you could only match sec to sec. But now if I allow you to drop letters, you can extend this to sec t, and then drop these to and go to t. So, you can move from a longest common sub word of 3 to a longest common subsequence of 4.'},\n",
              " {'id': '7fb369c7-2ae6-4d17-8abf-ef43776a7d71',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, in a sub sequence as I am going left to right, I am allowed to delete some letters from either of the two until I find the longest match with deletions. Similarly, if I look at director and secretary earlier, we could only do two, we could do ec, or we could do re. But now I can skip along and say ectr, ectr. Or I can say retr, and r etr. So, by allowing some skipping I get a longer subsequence. (Refer Slide Time: 14:34) So, one way to think about this in terms of what we have is we are taking sections and adding them up. So, we have a longest common subword. So, let us look at bisect and secret. So, we have a longest common subword which matches these two, and then we have a separate common subword, which matches these two, and they are in sequence. So, I can take this segment, which is the sec and then take the next nonzero entry bottom and right and combine these and this will give me my longest common subsequence. But of course, this is not the useful way to calculate it. But just a way to think about the longest common subsequence consists of these blocks of things which are concatenated to each other. So, why do we, why are we interested in this because this really has many applications. For example, in genetics, it is rarely the case that the gene sequences are identical, but there are large overlaps. So, one way to check whether two species are similar is to see how much of an overlap there is in terms of this matching. So, you take and you allow yourself to drop some genes which appear in one and not the other, and see if the rest of the genome looks the same. So, genetics is one application. Another application is more down to earth. So, if you use a Unix or Linux system, there is a command line command called diff, which will take two text files and tell you how different they are. Now, this is very useful. For instance, if you are editing some code or something, it tells you which lines have changed.'},\n",
              " {'id': '1a653925-42b3-46f4-bb6e-7d1d143f4dc8',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, what it does is it tries to compute the longest common subsequence between the lines of your text file. So, it reads each line as a unit. So, if you start editing your file by breaking lines, then it will think it is a different thing. But if you are editing your file by just staying within the same lines, they will tell you the maximum match between these two lines. So, diff uses a longest common subsequence calculation to tell you what is the maximum amount to which these two text files are the same and where the differences are. So, it will say these two are different because here you have inserted this at line 5, and there, you have deleted this at line 7 or something. So, will give you some kind of minimal way of taking one file and making it (())(16:37) file. So, there are several such applications where this longest common subsequence is used. (Refer Slide Time: 16:42)   So, let us try to see how we will solve this using our inductive strategy. So, as before, we have two strings of length m and n. And now we want to find, in general, for every position i comma j, the longest common subsequence that starts at i in the first string and j in the second string. So, the first thing is that if the two positions are equal, then is it a good strategy to include this in our longest common subsequence? So, we have ai, ai plus 1, up to some am minus 1. And we have here bj, bj plus 1, up to some bn minus 1. So, now what we are saying is that these two are equal. So, should I use that in my longest common subsequence. So, I can think of a longest common subsequence of as something that pairs up letters in the first one and the second one. So, it will, so this might get paired with something else. And that guy might get paired up something else and so on. But the pairing can never cross. I cannot pair up something earlier in u with something later and v and then something later in u with something earlier and v. Because that would not be a sequence.'},\n",
              " {'id': '801c3eda-afdf-4715-bcf5-d861ab8efbf0',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'If I have, say x comma y here, and if I have y comma x here, I cannot claim that y comma x is the same sub sequences x comma y. So, this pairing must always be kind of non-crossing. So, the question is, why is it a good strategy to do this? Perhaps they would have been a better thing, if I had allowed this to go and pair up with something to the right, or this to go and pair up something to the right. So, why not that? Well, supposing this was not the right solution, supposing there was a solution which goes there. But since ai is the very first thing here.It must go before anything that ai plus 1 touches. So, I can always take this and move this back. So, this is a bit like our greedy thing where we said you can take a solution and modify so, that it looks at a solution (())(18:55). So, in this way, this is kind of a greedy strategy which says that ai is equal to bj include ai, bj in your longest common subsequence. And the reason is, ai could compete a combined the something beyond vj, it can also, combined with vj and I will get the same solution. So, the solution that you get by postponing your match for ai or postponing your match for bj cannot be any better than this because that solution can be transformed to a solution which has this structure. So, in other words if ai is equal to bj so, in the case of the longest common sub word we said if ai is equal to bj you just take 1 plus the rest here we are saying ai is equal to bj you take 1 plus the rest again. So, that part is the same. What if it is not equal? If it is not equal then we can see that both of them cannot be in the same in the final sequence because by the same logic if ai bj, if this is part of my sequence and I have not yet considered this then ai must be matching up with something later. But now that something later if bj is matching something it must be it cannot match up ai. So, it does not matching with something later. So, then I have this crisscross.'},\n",
              " {'id': '25b59e9f-7ce6-4c01-8183-43f82cf34d7f',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, if ai is in my solution bj cannot be if bj is my solution ai cannot be of course could be the neither of them is my solution that is also. So, the question is what should I do? So, I have to drop either ai or bj and proceed. But since I do not know any better, I will do both. So, I will solve it assuming that I drop bj. So, I will look at i comma j plus 1. And I will also solve it assuming a drop ai. So, I look at i plus 1 comma j, I will look at both the sub problems. In one case, I omitted ai in one case, omitted bj. And then I will decide which one is better. So, I will take the maximum of theseso, I am not ignoring anything. I am just doing all the sub problems that could contribute to this and taking the best one. So, as with the longest common sub word, the base case comes when one of the two strings becomes empty, then all the longest common subsequence is becomes 0. So, when I have either i comma m or n comma j, the longest common subsequence are going to be 0 just for the same reason as it was for the LCW case. (Refer Slide Time: 20:55)   So, once again, we have these subproblems are the form LCS, i comma j for all i comma j. So, we have these m plus 1 times n plus 1 table of values. But what is different now is the substructure. The substructure earlier, for a value here, I only needed the diagonal, but now I need the value on the right and the value below. Because either it is equal and I go to i plus 1 j plus 1, or is not equal, and I take the maximum of these two. So, I need all these three values to correctly evaluate i comma j for an arbitrary. So, everywhere I have these three contributors, which I have to keep track of. So, that means that in order to solve this, I had better have solved all of these, now solve this, I better have solved all of these. Otherwise, I am going to get into a situation where I will have to do some recursion.'},\n",
              " {'id': '6eecb374-6927-449c-b22f-2346541180ca',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, I am goto dynamic programming, I should make sure that when I reach a cell i comma j, I know what is to the right of it, what is below it and what is diagonally below it. So, as before I can start here because there is nothing to the right and below. And again for these things. I have a below but I so, I mean I need these values. But because there is nothing to the right, it effectively means I must look below. And as we said this is the base case, because I am beyond the end of the string. So, the last column is just 0. Now here is the difference. So, I get this is 1 because this t matches that t all the others are 0, but the next column I will see a difference. So, now, what I see is that this e is not equal to this t. So, LCW will save e is not equal to t then there is no common subword starting here. So, I must quit. What LCS says is I can either discard the e or discard the t if I discard the t, then I come here, then I have et with mt and that 0 but if i discard the e, then I come here I have a t matching a t. So, I take the maximum of these two and I get 1, and the same thing happens all the way up. If I look at e and c, it says well, I could either discard the e or discard the c. And if I discard the c, then I am back to the ett case. So, I get this 1, and therefore the maximum of this and this is 1. So, in all these cases here, I am taking because they are not equal because this so, here it is equal. But here I getting 1 plus 0 is 1. Here I am getting maximum of this is 1. So, in each case, it is one because either it is not equal and the maximum of the two sub problems is 1, or it is equal in the specific case of e and I get 1 plus 0 is 1. So, I keep going like this. And now here for the first time I see, when we see the c here, remember that this is where the longest common sub word initially ended. It is true because it is a match plus 1 and this 1 is coming from here.'},\n",
              " {'id': '27963e3d-c7e2-4770-b55a-e77835141bc4',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'So, there is a match further on, what it is saying is that I have a match here, and there is some match further on it issome gaps perhaps. So, therefore the total match starting at this point is 1 plus that match, which was 1. So, 1 plus 1; 2, and going backwards again, I get a 3, and then I get a 4. And unlike the longest common sub word thing, this will always be the maximum because if I start at 0 comma 0, I can always if the supposing the maximum happens that in this case, at comes at 2 comma 0. Because it is a sec. So, it is at position 2 in bisect and position 0 and secret. But I can think of it as having dropped the first two letters in basic. So, therefore even from the beginning there is a substring of length sub sequence of length two by just dropping the first two letters. So, the maximum for LCS is going to happen at 0 comma 0 always because it accumulates all the LCS is to. It is not like LCW which must start there. So, we get that now you can ask the same question. How do I recover? Well, I look at this now I know that this is my maximum. But where do I accumulate letters and my longest common substring wherever I have a match, and I go diagonally. So, I have to track, how did this entry become 1. So, this entry became 1 because of 0 and 1. But how did this entry become 2? Well, this entry became 2 because I had a 1 here. And then I added 1 to it. So, for each entry that I made my table, I know whether I got it from the diagonal entry because there was a match, or when I got to a maximum of the table entries below and to the right, because it was not a match. So, wherever there is a match, it contributes. So, I traced back this entire computation by just keeping track at every square of which of the two cases in my inductive definition I use, did I use the max, or did I use the plus 1.'},\n",
              " {'id': 'c05a596f-2b96-4862-b0f4-345d9ca2f525',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 9},\n",
              "  'source': 'Common Subwords and Subsequences.pdf',\n",
              "  'content': 'And if I use the plus 1, and this is good.So, I find out all these positions where I did a plus 1, and this gives me 4 to 3, 3 to 2, 2 to 1, and 1 to 0 either these are the four places. So, this if I now go back and map those four places there I get the corresponding segments in my word which contribute to this LCS. So, once again, by doing a little bit of extra work, after I do the length calculation, I can recover a longest common subsequence. So, finding the length is enough. And the length is the 1 that is easier to define inductively. So, you first calculate the length inductively. And then you analyze how that inductive definition unrolled in order to calculate the word. (Refer Slide Time: 26:29) So, again, the implementation is quite straightforward. It is even more straightforward than LCW because I do not have to keep this maximum. So, I just set up these this array with 0. And I just run through this and I use the two inductive cases. So, either I do 1 plus the diagonal or I do the maximum of the two things which are 1 below and 1 to the right. So, this is the easy thing and finally return the value at 0 comma 0. So, just like for LCW, LCS alsofills up this m times n table. And each entry takes constant time. So, this is also time something which takes time proportional to m times n.'},\n",
              " {'id': 'ef5edfe1-3a7a-4be5-9e17-d896b82a0487',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'Programing, Data Structures and Algorithms using Python Professor Madhavan Mukund Dynamic Programming (Refer Slide Time: 00:09) So, we will now turn our attention to a technique called dynamic programming. So, let us go back to the kind of recursive functions that we have been writing. So, typically recursive functions arise from inductive definitions. So, for instance, we can define factorial inductively. By defining the base case the factorial of 0 is 1. And for arbitrary n bigger than 1, the factorial of n can be got by multiplying the recursive call factorial n minus 1, so inductively, we know how to compute that, and multiplying it by n. We can also do this for structures, so we can do induction on the structure of a list. So, we can say that if we want to do for example, insertion sort, then for the empty list, it is already sorted. So, the insertion sort of the empty list, so this is like the 0 case is just the empty list. And if we need to sort a list from with X0 to Xn with n plus 1 elements, then what we can do is we can sort the first n elements inductively. So, we do insertion sort on X0 to Xn minus 1, and then we insert the last element into this. So, insert is a separate function which is like, so it is like multiply, so it is a separate function that we use to combine the inductive case with the element which we are currently dealing with. You can also write insert inductively, but we will not bother about that for now. So, once we have these inductive definitions, then when we write code for this, then they naturally become recursive functions. So, the factorial function will say, if n is smaller than 0 or less, return 1, otherwise return n times factorial of n minus 1. Similarly, the insertion sort function will say that if the list you are given is empty, then return that list. Otherwise, insert into the list up to but not including the last element, insert the last element.'},\n",
              " {'id': 'a8c98490-d669-4ce1-acf1-50d0d65e0e2b',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'And then, so you assume that this is sorted inductively or recursively and then you insert. So, this is how it works when we have inductive definitions, we get recursive programs. So, the reason that this works in general is that we have these sub problems. So, we have these sub problems and we are combining these sub problems to solve our original problem. So, for example, when I look at factorial of n, then I need to solve the subproblem factorial of n minus 1, but factorial of n minus 1, in turn, will ask me to solve the problem of factorial n minus 2. So, all the smaller versions of factorial n minus 1, n minus 2 up to 0, are all sub problems of the problem I am going to solve now. And once I can solve them, I can kind of bring them together and solve this problem. Similarly, if I want to sort a list, I will have to do by insertion sort, then I will have to be able to sort that prefix from 0 to n minus 1. So, if I sought from 0 to n minus 1, then I can start from 0 to n, but in general, I might end up having to sort different segments, so I should think about any sequence Xi to Xj as a sub problem of this. (Refer Slide Time: 03:07) So, now, let us look at this sub problem situation in a different context. So, remember, this interval scheduling problems. So, the interval scheduling problem is something we encountered when we looked at greedy algorithms. So, we said that we had some resource. For example, we said there is a video classroom, which is going to be used by a number of faculty members, and each faculty member has a fixed slot, so they have a time period when they would like to use the classroom. But since no two people can use the classroom at the same time, we have to choose a subset of slots to schedule during the week. And in the interval scheduling problem our criterion was to keep the maximum number of people happy. So, we wanted to satisfy as many requests as possible.'},\n",
              " {'id': '7315f6b2-e0d2-464d-b7dc-ca0a3196d905',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'So, we wanted to choose a subset of bookings, so that, the number of teachers who got their booking was maximized. We were not concerned about for example, maximizing the utilization of the room in terms of the number of hours of recording or something like that. So, here we have subproblems. So, each subset of bookings is a sub problem. And we have this greedy strategy, which says, pick based on some criterion, the first request to schedule. And once I schedule that request, there are some other requests which overlap with it, because they have the starting time or ending time is such that I cannot schedule them both together. So, I have to remove those. And once I remove that, I get what remains, and that is a subset of the original problem which I have to solve. So, that is why every subset in general is a sub problem. So, this is also there. So, this kind of inductive structure, though we did not use an inductive solution in the greedy thing there is an inductive thing going on. That is in order to solve the whole problem, I take something out and then I solve a smaller version of the same problem. (Refer Slide Time: 04:58) So, each subset is a small, is a sub problem. Now, of course, since subsets are exponential, if I have n bookings, there are 2 to the n subsets that are in principle 2 to the n different subproblems. So, ideally, I would need to look at all of these subsets. So, every subset tells me something about how it can be solved and I would have to look at the best of these. But the reason that the greedy strategy is attractive is that it does not look at all of them it only looks at a fraction of subproblems. But along with this, we have this side effect that every choice that we make rules out a large number of subproblems, so we need to make sure that the optimal solution we are looking for has not been ruled out by looking at by making a poor choice. So, whenever we make this kind of a greedy strategy, then we need to prove optimality.'},\n",
              " {'id': '0a718251-4da5-48c0-b059-a01f067a5dcc',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'Because we cannot be sure otherwise, that the choices that we have thrown away, are not going to be bad. So, we saw that there were many kinds of incorrect greedy strategy for which we could explicitly come up with counter examples. Just because you cannot come up with a counter example does not mean that the greedy strategy is correct. So, you must necessarily prove that it is correct. And we gave some arguments for it. We talked about exchange arguments or showing in this case that the interval scheduling using the greedy strategy always stays ahead of the optimal schedule, and so on. (Refer Slide Time: 06:20) So, let us look at now a variant of this problem. So, remember, we said that in this interval scheduling problem, we are only interested in maximizing the total utilization in terms of the faculty. We want to keep as many people happy as possible, but we are not looking at any resource in terms of the room utilization. But supposing we are getting something from the room. So, let us assume, for instance, that this is actually not a captive resource inside IIT Madras, but it is a commercial facility. So, in a commercial facility, so you charge rent for usage. So, each person who makes a booking is willing to pay a certain amount of money. So, there is a certain weight associated with the booking. It need not just be the length, it could also be some people for have an urgent requirement, they are willing to pay more. So, for each request, there is also a weight. You can think of it as how much money you are going to earn, if you satisfy that request. But the constraint is still the same. That is these requests are fixed in time, they must be from a starting time to an end time, they cannot shift. And you cannot have two overlapping requests at the same time. So now, our goal becomes different. We are no longer interested necessarily, in keeping the maximum number of customers happy, rather, we are interested in maximizing our revenue.'},\n",
              " {'id': '39435c73-468f-41a7-89f4-605c332a26f1',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'So, as the owners of this facility, we would like to maximize the total weight in this case, the total income that we get as rent from renting out this facility. So, if you remember the greedy strategy that we had for the earlier question, which is to maximize the number of users who use it, we said that you just take the earliest deadline. So, you look at the first job that is going to finish and schedule it first, knock out all the jobs which overlap with it. Now among those that remain, you take the first job that is going to schedule it and so on. So, we had this earliest deadline or the earliest finish time as our greedy strategy. But it is quite easy to come up with a weighted example where this does not work. So here is a trivial case. So, I have three requests, two of them have weight 1, and one of them has weight 3. So, it is very clear that the best choice for me is to use the job or to schedule the job, which is weight 3. Now, if I were just trying to maximize a number of customers, as before, I would take this one and schedule it. And then this would mean that this goes out. And then because the other one starts later I would schedule that. So, I would get two happy customers, but my revenue would be only 2. Whereas if I did not go by the earliest deadline, but somehow were able to take into account the weights that I had, then I would actually prefer to take just as one customer who is willing to pay three units, and thereby get more than I would get by servicing the other two customers. So, this earliest deadline is no longer a valid greedy strategy if I add weights, and I now want to maximize the weight of the schedule. So, what should I do? So, can I look for a different greedy strategy? So, I can start looking saying, okay, maybe I do not want to do this, maybe I want to find now.'},\n",
              " {'id': '5c4114d5-603c-4752-9b38-4aec45a2d996',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'Remember earlier also, we talked about the longest job, the shortest job, there were various other parameters that we use, so we could now also look at the most revenue earning job. Maybe we want to make that a priority or we can take revenue into time or revenue divided by time. There are many, there is no shortage of these kind of metrics that you can use in order to evaluate jobs. And then whichever one we choose, we either have to search for a counterexample or prove that it is correct. But the other way, which is more reliable, is to actually somehow exploit the fact that we have the subproblems so this is no different in a way from factorial or insertion sort or something? So, the solution to the original problem depends on subproblems. So, can we take those subproblems and build up a solution to original problem exactly as we do there. So, can we find kind of inductive structure to this weighted interval scheduling problem? (Refer Slide Time: 10:17) So, here is a way to do that systematically. So, let us just arrange the jobs in some natural order. So, we can say we can arrange them by their starting time. So, remember, every request has a starting time and an ending time. So, B1 is the first starting time B2 is a second starting time and so on. Of course, they could have multiple jobs the same starting time, in which case we will order them in some reasonable way, but let us for convenience assume that our n requests have n different starting times and B1 to Bn represents the order of the starting times in increasing order. So, let us look at B1. So, what happens with B1? Well, I do not know the solution, but I know that either B1 is there in the optimum solution or there is an optimum solution without B1, there are only two choices. Either I can find an optimum solution with B1 or if I take B1, I am not going to find an optimum solution. So, then I will explore both the possibilities.'},\n",
              " {'id': '0fd54e97-2971-40c0-8520-5d902c7d84a6',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'So, I will say that either B1 is part of an optimum solution or it is not. So, let us assume that it is part of an optimum solution, then it will rule out things, which overlap with it. And these things overlap with it will come in sequence because the starting times is in an order so maybe B2 overlaps maybe B3 overlaps maybe B4 is the first job which starts after B1 finishes, and if B4 starts after B1 finishes, so does B5 because B5 starts after B4. So, there will be a certain number of jobs B2, B3 maybe which get ruled out, so now I will have to solve the remaining problem in this particular concrete case for B4 onwards. So, if I include B1, then I must kick out B2, B3 and go to B4. On the other hand, if I do not include B1 then I just knock B1 out, and I have a resulting problem, which has B2 up to Bn. So, these are now my two subproblems. And now what do I not need to do? I do not know which one of these is the correct answer. I do not know whether I should have kept B1 or I should have dropped B1 but let us assume inductively I can solve these smaller subproblems. So, we have solved the smaller subproblems, then I can just take the maximum of the two in the sense that if I include B1 I get the revenue from B1 plus the whatever I have solved, if I exclude B1, I get the revenue for whatever else, but I do not get the revenue of B1, whichever of them gives me more revenue more weight I use that. So, this is a kind of exhaustive solution, which in some sense is guaranteed to be correct. Now, why is it guaranteed to be correct? Because it does not rule out any possibility upfront. See, the greedy solution when I make a choice, it rules out a bunch of things which I will never going to look at before look at again, this is not like that. So, if I generalize this argument for B1, if I look at any Bj any job in this sequence, either it is part of the solution or it is not part of the solution. So, in particular for B1 we have checked both cases.'},\n",
              " {'id': '3ae6e5ff-2267-4013-b57a-636a4b6c1436',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'So, B1 we have checked the case that it is there it is not there. Now, what about B2? Well, B2 if it is not in conflict with B1 will be there in this list and in this list. So, whether I include B1 or exclude B1, I will look at a next choice for B2, I will either include B2 or exclude B2. So, I will consider it in the subproblems involving B1 and the subproblems which exclude B1. On the other hand, if B2 is incompatible with B1, then it would not be here because when I choose B1 I have to throw B2 out, but it will be in the second one by guarantee because if I throw out B1, B2 still is possible. So, then I would check if B2 is in conflict to B1 I would evaluate all the answers for B2 assuming that B1 has been discarded. So, by doing it in this sequence, I know that if B2 is there B1 is not there. So, there is no problem. So, I am not ruling out the fact that B1 was there. The only reason v1 is not there is because without B1 I could not do B2. So, this now becomes a kind of recursive thing. I go to B3 and it has the same thing. So, when I get to B3, either it has been allowed by my previous choices or it has been disallowed, but every situation where it is allowed or disallowed will be considered. I am not going to rule out anything. (Refer Slide Time: 14:29)  So, what is the problem? So, here is a situation that you might well have which is that B1 and B2 are incompatible. Let us look at the picture at the bottom. So, B1 and B2 are incompatible, so B1 and B2 overlap, but they are both disjoint from what happens afterwards. So, B3, so remember these jobs are arranged in starting time order. So, B1 starts before B2, B2 starts before B3 and so on, but both B1 and B2 finish before B3 starts. So, whether I choose B1 or B2, I can freely choose between B3 to Bn, but I cannot choose both B1 and B2. So, now if I start this exhaustive strategy that I did before, what will happen is I will first look at the case where I choose B1.'},\n",
              " {'id': '42b33f13-ee9d-423b-be86-4f3e3e70688f',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'If I choose B1 I must rule out B2 and I must look at these things or I will exclude B1. And If I exclude B1 then this is gone and I am looking at these things. So, these are the two sub problems in this concrete situation, which arise by making my first choice. I keep B1, I drop B2, I keep drop B1, I keep B2 and in both cases I keep B3 to Bn. So, now, let us look at a case that arises here. So, this is it should be B2. So, let us look at the case that arises in the second case. So now in the second case I have B2 to Bn and now I must now look at the situation where I am allowed to choose B2, whether I should keep B2 or not. So, once again, just like B1 was the first choice in this choice B2 to Bn I must decide whether to keep B2 or exclude B2. Now, if I keep B2 or I exclude B2, I still have B3 to Bn, because I know that B2 is compatible with B3 to Bn, but this B3 to Bn that I am solving here, was already being solved in the case where I chose B1. So, this is the challenge. The challenge is that when I apply this inductive strategy in this way, when I break down the problem into subproblems, I cannot be guaranteed that the problem that arises from one path, the path that came from choosing B1 is different from the one that came from keeping B2 or excluding B2, so the same sub problem in this case B3 to Bn might arise from different paths. And if I use recursion, as I had done. When we said fact n is equal to n times factor n minus 1. Every time we saw an inductive dependence, we just replace it by a recursive call. So, what is going to happen in this case, is that, we are actually going to evaluate this once when I am solving for B1, and we are going to solve it again for B2 which is clearly wasteful. Because the same problem is being solved a second time, because we somehow did not recognize that this problem has been solved before. So, the question is, whether we can somehow flag this.'},\n",
              " {'id': '4f354fd1-662b-41c5-aeda-fbcb8dea9eda',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Dynamic Programming.pdf',\n",
              "  'content': 'So, we can somehow flag this problem and say that okay B3 to Bn must be considered once and not twice. So, if I have solved it once, I should somehow recognize that I solved it again. So, when I come back to this problem, later on, I can figure out that this problem is already been solved and I do not need to solve it again. So, we will basically this is what dynamic programming eventually achieves. So, we will look at dynamic programming through a number of examples and illustrate why this problem is really bad. I mean, this problem is really bad, because remember, we said that if you have n jobs, then you have 2 to the n possible subsets. So, there is really an exponential number of subproblems. And if we are going to be solving them again and again then we are going to be in real trouble. So, this could actually blow up in terms of a very simple recursive solution that we get from an inductive structure, might actually turn out to be impossible to compute simply because of this overlap. So, unless we fix this overlapping problem, what seems to be a very simple problem to compute could actually be computationally infeasible. So, we will look at these two strategies. The first one is basically, one, where we kind of remember. So, we come to a problem and we say, okay, this is a problem I have seen before, and I will just reuse the answer I have done before, and the other one is dynamic programming where we will say, okay, I will solve the problems subproblems in such a way that whenever I need one, I will have the answer. So, this will become clear, as we go along.'},\n",
              " {'id': 'e0401ebf-2cef-41b1-8157-3a9c499e537e',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Matrix Multiplication So, we have seen three examples involving strings, which were some what similar. We started with the longest common sub word, then we moved to the longest common sub sequence. And then finally we expanded that to the edit distance. And in an all cases, the recurrences looked similar. And now let us look at a problem where the recurrence is somewhat different; so this is a problem related to matrix multiplication. (Refer Slide Time: 00:32) So, how do you multiply two matrices? Well, if you have a matrix A and matrix B, and you want to multiply them and produce a new matrix; let us call it A times B. So, now if I look at the entry in the ith row and the jth column in the new matrix; I have to take the ith row in j. The jth column in B; so the ith row in A, the jth column in B; and then multiply these element by element. So, I have to take the first element here. So, I have to run k through these, k runs through these indices from 0 to n minus 1; and I take A i, 0, B 0, j. That is my first term; so that is this times this. Then I go to the next one, I get A i, 1 times B 1, j; so this is this one and this one. So, in this way I keep walking down this row and walking down this column; I pair up the elements multiply them as pairs, and then add them up. So, I do this plus this, and so that is the summation. So, is the summation of A i, k into B k, j; for k ranging from 0 to n minus 1. So, this is how you multiply two matrices. So, in order to do this, that row the ith row here and the jth column must have the same length; otherwise, you can not pair them up. So, if I look at the number of columns, so this is an m row and n column matrix; and this is an n row, this is whatever p arrow q row p column matrix. Then the size of the number of columns here must be equal to the number of rows here; so n must be equal to q.'},\n",
              " {'id': 'e7367d55-240e-45eb-93bb-5164a1e4bca2',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So, must have the same number of columns in A and in B, so that I can multiply them and how much time does it take? A times B is an m cross p matrix finally; because, I am going to get entries of the form i comma j, where i a row in A, and j is a column and B. So, computing each entry takes order n time, because we have to run through the summation. So, each entry in AB takes this much time, and how many entries are there? There are m times p entries. It has m rows and p columns. And therefore the overall cost is going to be m times p times n, which I will write as m times n times p. So, there are m times p entries and each takes order n time. So, this is the cost of doing one matrix multiplication using the standard definition of matrix multiplication. So, we are not going to try and work with; we are not trying to improve matrix multiplication itself. We are looking at a problem where we have to multiply a sequence of matrices. So, supposing we have to multiply three matrices, A times B times C. Now, we can only multiply two at a time. So, either we can first multiply A times B, and then multiply the result by C; or we can first multiply B times C, and then multiply the result by A. So, the order left to right is important, but what is not important is the sequence in which we do this pairing up. So that is what is known as associativity. So, we know from arithmetic the 6 plus 5 plus 2; it does not matter whether I do 11 plus 2, or I do 6 plus 7. So, whether I do the 5 plus 2 first, or 6 plus 5 first; it does not matter, so that is associativity. I could either put the bracket like this, or I can put the bracket like this; and both will give us the same result. Assuming I can only add two numbers at a time, I can add up anyway; and I will get the same answer, so matrix multiplications associative. So, the sequence in which we pair up and evaluate this long chain of matrix multiplication operations does not really matter in terms of the answer that we get.'},\n",
              " {'id': '2a3c23a1-c0f2-4520-9196-324a503b3106',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'But it does matter in terms of how much time it takes. Now the how much time it takes for an individual multiplication we know. If we take an m times n matrix and multiply it by an n cross p matrix; so will take m and p. But these matrices sizes should grow and shrink in the sequence as we will see. (Refer Slide Time: 04:40) So, supposing we have these 3x matrices; we have this one matrix, which is kind of just one row and it has 100 columns. And this has got 100 rows and 1 column; and this is again a single row one 100 column. So, this is what it means, 1 cross 100 means 1 row 100 columns; 100 Cross 1 means 100 rows 1 column. So, it is just a single row, a single column and a single row; and now I want to multiply A times B times C; so I can do it in these two ways as we have seen. So, let us see what happens if I do the second first; so if I do A times the product B times C. So, when I multiply B times C, what am I going to get? I am going to get this is m, this is my n and this is p. So, I am going to get an m times p matrix; I am going to get 100 by 100 matrix. So, BC the product of B and C is 100 by 100. How much time did it take? It takes m times n times p; takes 100 times 1 times 100; so this takes 10,000 steps to compute. Now, I have this 100 by 100 matrix, and here I have a 1 by 100 matrix. So, the final answer now is going to be 1 into 100 A times B times C. So, A times B times C is going to be 1 into 100; how much time is it going to take? I had a 1 by 100 here, and 100 by 100 here; so it is 1 into 100 into 100, so it is again 10,000. So, therefore computing this product by first doing B times C, and then doing a into that product takes 10,000 plus 10,000 steps. If I do it the other way, then things are very interesting. First of all, this matrix which is 1 row times 1 column, is actually just going to give me one entry. This row times is column in this product gives me one. So, I actually end up with a 1 by 1 matrix. And how much it takes?'},\n",
              " {'id': '3efb2f0f-3213-429b-a304-ebdd2089264b',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'It takes time to multiply 1 row into 1 column of length 100; so it just takes 100 steps. And now I take this 1 by 1 and I multiply it by this; I am going to get a 1 by 100. But how much time is this going to take? Well each of the strings is just one trivial, 1 by 1 is just one value. It is going to be one value one row value, times one column value. So, I am basically going to take another 100 steps, 1 into 1 into 100. So, 1 into 100 into 1 is 100; 1 into 1 into 1 is 100 and two, so I get 100 plus 100 200. So, therefore, by choosing a better order A times B first and then multiply by C; rather than the worst order A into B times C. I have made an enormous saving in terms of how much time it takes to multiply. So, the multiplication again I am trying to emphasize the multiplication of the matrices itself. I am not doing anything clever, I am just using the basic definition of matrix multiplication; I am just finding a good order in which to evaluate a long sequence of multiplications. (Refer Slide Time: 07:30) So, the general problems and this was for three A B and C; but the general problem is I will be given n such matrices. So, and we just to make everything easier for our programming, will as usual assume that the numbering is 0 to n minus 1. So, we have matrix0, matrix1, matrix2 each of them has a dimension r0 times c0, r1 times; so each has a number of rows and columns. Now, in order for this whole thing to work these dimensions must match. So, the columns of this matrix must match the rows of the next matrix; that is what this says. So, the columns of the first matrix must match the number of rows in the next matrix, as we multiply them. So, at any point if I look at rj, the jth matrix rows, it must be equal to the number of columns and j minus 1. So, rj must be equal to cj minus 1. And since the cj minus 1 must make sense, j minus 1 carry should be at least 0, so j should be at least 1.'},\n",
              " {'id': 'b5b1a0ac-d625-496e-9b08-8381732ad903',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So, J lies between 0 and it goes up to n minus 1; so j is strictly between 0 and n. And if we do that then we are guaranteed, because as we incrementally so one way to evaluate a matrix is always to go left to right. So, I can do M0 times M1; so this is the natural way if I just did not think about, I will just bracket it to the left. So, I will first compute M0 times M1, then I will take that and multiply it by M2. But at every point, the number of rows that I generate the number of columns that I generate will be compatible with the next number of rows; so this matrix product will always exist. But our goal is of course to find the best way to do this. So, what we want to do is find this kind of bracketing. So, it is amounts to basically saying how do i parenthesize the expression, how do I choose which pairs to do first, then which pair to next and so on. So that, overall I minimize the total amount of cost I spent doing this standard matrix multiplication. (Refer Slide Time: 09:17) So let us look at the inductive structure of this. So, remember we can only multiply two matrices at a time. So, our final answer is a single matrix, so that single matrix must have been got by multiplying two matrices. But because I am multiplying this whole thing left to right; the last two matrices are multiplied, so I had M0 M1 up to Mn. And the last matrix that I multiplied must have been of the form where I have basically taken some up to some k minus 1 and k. So, I have taken the product from k to n, and I have taken the product from 0 to k minus 1. In our earlier case for instance, we had A BC, and we have AB C. So, in both cases I can think of this as a trivial product, where I just have one. But if I had three for example if I had four at ABCD; then I could for example do AB times CD first, or I can do ABC times D. So, of course, ABC itself would require me to break it up; but at the last step, what I am doing is I am taking two sub orders.'},\n",
              " {'id': '3343e944-64f5-4587-b671-66ead2b13a86',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': \"So, two segments of the 0 to m n minus 1 and I'm multiplying the result. So, I have 0 to k minus 1 on one side, and I have k to n minus 1 on the other side for some k. That I am not telling you which k it is, but I know that it must be of this form. So, if I now look at the dimensions of this; then the left matrix after all this multiplication, will have the same number of rows as M0 and the same number of columns as k minus 1. So, the first one will be r0 times Ck minus 1. The second product, this one will have the same number of rows as this one that is rk, and the same number of columns as this one which is Cn minus 1. And because of this compatibility thing that we had assumed, the number of rows here and the number of columns here must be the same. So, rk must be equal to Ck minus 1, otherwise we could not have done this multiplication to begin with; so that is a constraint that is assumed. So, the cost we will now think of as the cost of doing it from the beginning to the end. So, the cost C 0 comma n minus 1 corresponds to how much time it takes, or how many steps it takes to compute the entire sequence products from 0 to n minus 1. So, what we can see from this is that this breaks up into sub cost. So first of all, there is this final step; the final step involves multiplying these two matrices. But I know the cost of that matrix, because I have not given, I have told you what the costs are. So, I have told you the rows and the columns in these two matrices. So, it is the number of rows in the first matrix times the number of columns in the first matrix, which is also the number of rows in the second matrix times the number of columns in second matrix. So, r0 times rk times C n minus 1; so this is a specific cost I get for the final multiplication. And then inductively I have incurred a cost in getting to this stage at all. So, I must have done something from 0 to k minus 1 to get this matrix.\"},\n",
              " {'id': 'b963137f-6af6-44d8-9e02-4260ac9adbdb',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'And similarly, I must have done something from k to n minus one to get this matrix. So, these were the inductive problems that solved. So, my total cost is going to be the sum of these three values, is going to be the cost of building up the first factor the left factor; the cost of building up the second factor plus the cost of combining these two factors and the final multiplication. So this is the only one that I know explicitly; the other two have to inductively compute. (Refer Slide Time: 12:34) So, of course, the choice of k is what determines how efficient it is going to be; so which k should we choose, point here is we do not know which k to choose. So this is like, even in an edit distance problem, we do not know whether we want to substitute, delete, or insert. So, similarly here we do not know whether we should choose k equal to 0 k equal to 1; we can choose any k. So, what we have to do is basically evaluate the possibilities for all sources of k, and then take the minimum; because remember again we want the minimum costs for this matrix multiplication. So, what do the subproblems look like? So if you look at this case, the subproblem consisted of starting from 0 and going to some intermediate point. And then starting from that intermediate point plus one and going to the end; but, in turn, these problems will again decompose. So, if I take for instance, this first part if I just take the first factor; then this in turn will have decomposed into something in between. So, there will be some j in between, such that is 0 to j minus 1, and j to k minus 1. So, I could have a segment, which I have to multiply in order to determine its cost, which starts from somewhere in the middle and goes to somewhere in the middle. So, in general the problem starts from some j and goes to some k. So, we will look at a generic subproblem of the form Mj Mj plus 1 up to Mk; and for that we will write the cost as C j, k. So, C j, k is the cost of multiplying Mj Mj plus with Mk.'},\n",
              " {'id': '79f78c11-c48f-4579-a0a1-945efce4c260',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'And this will now be by our calculation we said we do not know how to split it; but we have to split it at some l between j and k. So, we find some l which is between j and k, and we look at the cost of doing j to l minus 1. And the costs of doing from l to k, and plus the cost of multiplying Cj times rl times rc Ck. So, remember that rl is equal to Cl minus 1. The number of columns in the last element in the first factor is equal to the number of rows in the first element in the second factor. So, this times this times this is the final multiplication and then these two are inductive things. So, possibly I should have written the round bracket here so you can just ignore it, for now I will fix it; so this is the inductive definition of our cost. Now, what is the base case? The base case is when I actually have to just have m. So, remember that one of the cases that we had for instance was A times B times C. So, if in this case, what we had was we are multiplying a single matrix by a product of a chain of matrices. So, the base cases when the sequence consists of only one matrix; but when the sequence consists of only one matrix there is no multiplication to be done. So, if I look at j comma j as my interval, then the cost is actually 0; so I can use that as the base case for this definition. (Refer Slide Time: 15:27) So, we have these subproblems as we said, which start from i and go up to j, and i and j can be between 0 and n minus 1. So, we will naturally have a square matrix representing all the possible costs that we need to compute using our inductive definition. But there is another constraint on these costs, which is that? I know that the second index j cannot be smaller than the first index. So, i comma i is the smallest I can start with; I can not do i comma i minus 1. It does not make sense to start from a position, and multiply by a matrix before it; because I always do matrix multiplication left to right.'},\n",
              " {'id': 'e10eaa1f-8310-410c-8247-f1aca666bc13',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So, matrix multiplication is associative, but it is not commutative; commutative means that AB is equal to BA. So, for matrix multiplication, this is not the case, I cannot. First of all, it may not be possible, because if I look at AB the compatibility of the column and the row will be there; and for BA, it may not be there, but supposing they are square matrices. So, supposing this is some n times n and this is also n times n. So, I can actually multiply both AB and BA; in general the product A B is not going to be equal to the product BA. So, I am not allowed to, to flip the order. I have to go from left to right. So that means, in any sequence I must go from I and stop at a position to the right; so j is always bigger than I. So, what this means is that in this matrix, I must only look at cases where j is bigger than i. So here for instance, we look at this entry; so let us look at this entry, we mean let us assume that this is 1 and this is 1. So, if I look at this entry, then this corresponds to i equal to 0, sorry i equal to 1; so this is i, and this is j. So, row 0 and column, row 1 and column 0; and this is not allowed. So, everything in this blue zone corresponds to j which is smaller than I; so the first entry of interest is where i is actually equal to j. And after that is i is smaller than j, but below that is not a allowed. So, I only have to compute values above this so called main diagonal; so this is the diagonal of the matrix and this is the main diagonal. So, we only need to compute values on the main diagonal and above. The other thing is that our matrix product says that C i, j depends on this C i to k minus 1 and C k to j. Remember I have i up to j and I am splitting it at some k, k minus 1; so there is some value of k between i and j where it splits. And if I try to look at what that means, it means that I am looking for a value in the same row to the left, and in the same column, but below.'},\n",
              " {'id': 'c5a98c77-9fd7-4bf3-907a-e05c0b966447',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So concretely, if I take this as i and j this yellow squared is i, j; so I have i here and I have j here. If I take this as my concrete square, then the first value is to take k is equal to i itself. So, if I, sorry, if I take k is equal to i plus 1, then k minus 1 is equal to i. So, the first factor goes from i to i; so that is this one, this is the cost C i, I; and here, now this is the factor i plus 1, j. So, I have to look at this cost, the cost of this plus the cost of this; plus the product that arises by taking those as my immediate things, that is 1 factor. The other one is I move it by 1, so I now allow a chain of i, i plus 1; and here i started i plus 2 onwards, so I get this and this. And the third one is this one. So, in this particular case, if this is my position for i and j; then I have this pair, then this pair and then this pair. So, I have to look at all of these. So, what this means is that therefore I need to know the values on my left and below. And this you can see is proportional to the position where I am; so unlike our LCS or LCW matrix, where I always looked at only the three entries. So, wherever I was in the matrix in LCS or LCW or an edit distance, I only looked at my immediate neighborhood; that I only looked at the right to the bottom and diagonally to bottom at right; so it did not matter how where I was. Whereas, here depending if I am here for instance, I will need all these values; so depending on what the position is, I might need order n values to compute. So, this is the difference between this computation and the earlier computations, which is that the number of subproblems that I have to consider to evaluate a given position is not constant. It is not fixed, it depends on where I am, I mean in general it could be as bad as order n. So, what are the base cases? We saw that the base case is when i is equal to i or i is equal to j rather.'},\n",
              " {'id': '39e5d242-547a-4d7a-bda2-b27592373446',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So, we said that when I am only looking at a singleton matrix, there is no cost involved; so I can fill up this base case to begin with. Now, that I filled up the base case, then what can I fill? If I take a value here for instance, I can not fill it, because I have to look here and here, I can not even fill a value here. I can fill this value, but I can not fill this value, even if I fill this value; because if I have filled this value, and I tried to fill up this value, I still have a missing value here. So, I cannot go row by row and I cannot go column by column; I have to look at it diagonally. So, what I have to do is I have to then after filling this, I have to fill the next; because for a diagonal entries, the next diagonal above the main diagonal. For every position I do have something to his left and below; so this I have it is left and below. So, for every one of these I know both the items in the same row and the same column to the left and below; so I can fill up this, hen I can fill up the next diagonal. So, if I take a typical entry in the next diagonal, again I have all the things to its left and below. So, I can fill up this entries diagonal by diagonal. And in each case when I come to an entry on that diagonal, everything that is to the left or below has already been filled by an earlier diagonal. So, that is what I need for dynamic programming to work that when I need a value I need a subproblems answer. I am not going to reevaluate it using recursion, or the inductive definition; I am going to assume it is already been calculated. So, by doing a diagonal by diagonal I make sure that whenever I reach a problem, all the subproblems have already been calculated. So, this now becomes a little more tricky to program as we will see; so in this way you fill up this thing. And eventually, what are we going to get? We are going to say that this is the value that we want.'},\n",
              " {'id': '040bc3a3-b7bd-499f-aa25-cc9c22a35298',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'This is the cost that we incur when we break up the entire segment from 0; so this is i is equal to 0 to j is equal to n minus 1. So, if I take the entire sequence or the chain from M0 to m minus m minus, n minus 1; that is the cost 0 to n minus 1. How much cost does it take for me to do that. So, this top right corner value is going to be the answer; just like for this edit distance, the top left corner value, the 0 comma 0 value is the one I want. (Refer Slide Time: 22:13) So as I said, this is a little more tedious to program; so let us just go through the code a little bit more carefully. So, first of all, here we need some non trivial input; earlier we had two strings for example, so I could just pass it. Now, here I need this sequence of rows and columns. So let us assume that I get as input a matrix of length n, where each entries of form r, i, c, i. So, I have r0, c0, and then r1, c1; so these are the dimensions of the n matrices up to r n minus 1, c n minus 1. So, this is my input; I get a dimension matrix consisting of pairs of entries r, i, c, i. So, if I am using numpy, then I can extract the number of entries by using the shape function. So, the shape function will give me a for a one dimensional matrix, it will give me something of the form n comma 0, or n comma blank like this. So, I need to use this 0th thing to get out n. So, basically I can extract the number of entries, and then using that I can set up as usual an empty matrix to store my answer. So, C is my cost matrix, so I set up an n by n cost matrix with value 0. So, the first thing I do is I of course, is already 0; but just to emphasize I said the base case. So, for all entries from 0 to n minus 1, I said C i, i equals to 0; so strictly this is not necessary because I already have the 0s here. But this is just to emphasize that the base case is being handle. Now, I have to do this diagonal, so this diagonal is what I wanted to mention to you carefully; so let us look at a specific example.'},\n",
              " {'id': '843b67cc-b470-4724-a69f-2c60cd718c46',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': '(Refer Slide Time: 23:49) So, say that we have, say 5 by 5 matrix. So these are now 0,1,2,3,4 and 0,1,2,3,4; so I have already done this, so this is done. So, this is my main diagonal which is i is equal to j; so now I need to enumerate these pairs. So, what is the property that all these pairs share? You can look at this here it is 0 comma 1, this is the first one; the second one is 1 comma 2, the third one is 2 comma 3, the fourth one is 3 comma 4. So, the property that (wall) shared is that j minus i is equal to 1. Now, if I go to this one for instance, then it will be 0 comma 2, 1 comma 3, and 2 comma 4; so now the difference is going to be j minus i is equal to 2. So, the difference will go from 0 on the main diagonal, in the main diagonal difference means j and i is 0; because i is equal to j. Next one, it is i is equal to, j is equal to i plus 1, next one is j is equal i plus 2. And finally when we come here, when i is 0, j is n minus 1; the difference goes from 0 to n minus 1, as I go diagonal by diagonal, but the 0 case has already been handled. So, I will take my differences to run from 1 to n minus 1. Now, if I run my difference from n 1 to n minus 1, then I have to figure out what is my range for the rows. So, the rows always start at 0, but you can calculate that the last row that I get. If the difference is 0, then I go up to n minus 1, the difference is 1, then I go to n minus 2 and so on; so I go to n minus the difference. So, this is telling me how to generate the the i part of this; so I need to get generate the i part. Once I generate the i part, the j is easy, the J is just i plus difference; because j minus i is constant. So, I am generating all the i-j pairs systematically by fixing the difference. And then for each value of the difference and fixing all the valid i’s; and for each i get the j by just adding the difference; so that is what this part is doing.'},\n",
              " {'id': 'e0d7a28b-ed05-4f30-a80d-3e6853e6285b',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'So, it is saying that let the difference run from 1 to n minus 1, let i run from 0 to n minus the difference my n minus 1 minus difference actually. So, and then let j be equal to i plus; and then, then I take this minimum of those quantities. So, one way to take the minimum is to initialize the value to something large and then take the minimum. But instance, I do not want to speculate as to what the large value is; I first initialize it to a concretely the first value. I say the first value is going to be if I split i as i, i plus 1 up to j; in this smallest value is when I split it this way. So, I take C i, the cost of i to i, the cost of i plus 1 to j both of these should be known; because they are already below and to my left. And then I take the concrete cost of this multiplication as rows of i, the rows of i plus 1, and the columns of j. So, this is the concrete cost; so this is the first term in my minimum sequence. So, I use that to initialize my cost; the cost of i, j is at least this much. And then I arrange over all the possibilities and I keep taking the minimum of the current cost; and the cost I get for the new value of k. So, the rest of it is straightforward, so this part is kind of straightforward; this part is a little bit non trivial, because for minimum it is not like maximum. For maximum, you could always take a negative number or 0; because you know that you are not going to generate negative numbers in this kind of a cost calculation. But for minimum, you have to take a large number; so either you can postulate a large number which is more. So one, for example value for that you can take for the large number is the product of all the rows and the columns; you know that it is m into n into p. So, if I just take the product of all the arrays in the C i is certainly that is an upper bound on any cost. But it is simpler to actually take concretely and every time you take the minimum, take the first value getting the minimum of any least.'},\n",
              " {'id': '07b0ad98-91a6-43fe-a525-7d731058573c',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 9},\n",
              "  'source': 'Matrix Multiplication.pdf',\n",
              "  'content': 'You can take the first value as your starting value for the minimum, and then take the minimum with respect to that; so that always works. So, that is what we are doing here. But the main new thing is this part how to enumerate the diagonals. So, in this case the diagonals are the main diagonals, so the difference is constant. So, you just have to check that constant difference and then enumerate the correct values of i. And for each j, you just get it by taking I plus difference. So this works, and you will actually get that answer for example. For the example, we looked at the beginning that 1 into 100 and 100 into 1, you will get the answer 200. So, what is the cost of this whole thing? Obviously, we are filling up a table of size n squared; so that is what the amount of work we have to do. But, now remember that unlike in edit distance or in any of the earlier problems we looked, at each entry is not a constant amount of work; each entry could involve looking at a number of. So, we have this for loop inside for filling up an entry, which we did not have earlier. So, each entry in the worst case takes order n work; so this whole thing now is actually order n cube. So, that is why this is an interesting problem to look at, because unlike the earlier cases where the dependency is very local. And it only depends on your immediate neighbors compared to what value you are trying to put into the table. Here the dependency actually requires you to evaluate linear number of combinations, and then take the minimum. So, therefore, you get an extra factor in your complexity, which comes from the cost of filling the entries in your table.'},\n",
              " {'id': '61e2ac40-8800-4471-889a-eecdde3e68fc',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'Programing, Data Structures and Algorithms using Python Professor Madhavan Mukund Memoization (Refer Slide Time: 00:19) So, last time we saw that many problems have an inductive structure. But if you are not careful, then the inductive problems get repeated. So, as you approach them from different, bigger problems you might find yourself solving the same problem multiple times, and we would like to avoid this wasteful re-computation. So, we said that there were two techniques that would help us do this called memoization and dynamic programming. So, let us look at a concrete example and see how this works. (Refer Slide Time: 00:37) So, just to remind ourselves, we have these inductive definitions like factorial and insertion sort, where the value for the given input is defined in terms of these subproblems, which are smaller inputs, and you can convert these into code quite easily as recursive functions. And then when you look at the structure of the inductive definition, you can identify the subproblems. So, you can find out that for factorial of n, you need to directly compute factorial of n minus 1, which in turn will require n minus 2, and so on. So, all the problems factorial i for i smaller than n are subproblems. Similarly, for insertion sort, we said that every segment of this list to be sorted would be a potential sub problem. And the goal of inductive definitions is to use these sub problems and combine their solutions effectively to get a solution for the whole problem. (Refer Slide Time: 01:27) So, let us look at another very familiar inductive definition. So, we have seen factorial, but possibly the next most common inductive definition is the Fibonacci numbers. So, the Fibonacci numbers are defined as follows. We start with the numbers, 0 and 1.'},\n",
              " {'id': '7d7bfceb-681f-4eb0-83fa-d6b5f65834be',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, the Fibonacci number at position 0 is 0 at position 1 is 1, and after that we keep adding, so Fibonacci of 1 is 1, Fibonacci of 2 is 0 plus 1 is 1 again, Fibonacci of 3 is 1 plus 1 is 2, and so on, so we just keep adding the previous two numbers in the sequence. So, this is a very simple code for this. So, if you have something which is either 0 or 1, then remember 0 return 0, 1 returns 1. So, the value is, if the argument is less than or equal to 1 that is, if it is 0 or 1, then the answer you are going to return is the argument. So, this is just a shortcut for two cases and 1. So, 0 is 0, 1 is 1 otherwise, you are going to compute recursively, the Fibonacci for the two smaller values, and finally, we are going to return it. Now there is a reason why I have done this convoluted thing of calling this intermediate thing a value and then returning it, we could directly have written here, return n and written this expression. But there is a reason why I have specifically in both cases, stored this value to be returned in a new name called value and then returned it which will become clear as we go along. (Refer Slide Time: 02:52)  So, let us see what happens when we take this code and we actually try to evaluate a small Fibonacci number. Like say, a Fibonacci of 5. So, Fibonacci of 5, because it is not the base case, will go into this case. So, I will have to evaluate Fibonacci of 4 and Fibonacci of 3, so it will produce these two inductive subproblems, which will be called recursively by the code. So, now we will do them one by one typically left to right, so we will first to n minus 1, and then n minus 2. So, we will now look at this value, and we will learn how to evaluate it by calling this function again. So, it in turn will generate two more subproblems. For n minus 2 and n minus 1 for that, so 4 minus 1 and 4 minus 2. And again, we will, in general, go to the left of the 2 and start it again, so now we will get Fibonacci of 2 and Fibonacci of 1.'},\n",
              " {'id': 'd2cc2114-c351-44e9-bbfc-66f8319ef8af',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'Then finally, we will come to Fibonacci of 2 again, and we will get it to Fibonacci of 1 and Fibonacci of 0. Fortunately, both of these are base cases. So, for both of these, I will exit without having to call Fibonacci again. So, I will get back the values for both of these as base cases for my Fibonacci function. And then I will be here in Fibonacci of 2, so I will compute the value of Fibonacci of 2 as 1 plus 0 is 1, so I will now get back the value of Fibonacci of 2. Now, I am here and I have finished Fibonacci of 2, so I have to do Fibonacci of 1 again. So, I will again go into Fibonacci of 1. But then since Fibonacci of 1 is a base case, it will immediately return 1. And now from this 1 plus 1, I will get Fibonacci of 3 is 2. Now I am here for 4. I have done 3 and I have to do 2. So, now I will go down this Fibonacci of 2 and I will again have to start computing the whole thing again. This is what we meant by saying that the same problem comes again and again, and you have to be careful to avoid it. So, we have already computed Fibonacci of 2 but we have forgotten that we had computed it, so we will again, expand Fibonacci of 2 as 1 and 0. Again, realize that 1 and 0 are the base case and again, realize the Fibonacci of 2 is just 1. So, now with this, I have completed the evaluation of Fibonacci of 4. So, I now know that Fibonacci of 4 is 3. So, I have gone back to the original call that I made, and the first part Fibonacci of 4 has been done, but I still have to do Fibonacci of 3. Now, of course, I have done Fibonacci of 3 once before, but I have forgotten it, this is the whole problem that we are facing. So, Fibonacci 3 will again expand to 2 and 1, 2, and again expand to 1 and 0, 1 and 0 will give us back the values, so we will again get Fibonacci of 2 is 1. Once again, Fibonacci of 1 will return 1, so I will again compute Fibonacci of 3 is 2. And finally, I will get Fibonacci of 5 is 5.'},\n",
              " {'id': '96676a3c-6e71-4c69-8cd1-97e112d4e324',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, what we saw here is that we are basically recomputing the same thing many times. For example, we have recomputed, this whole thing for Fibonacci of 3, twice. So, you can look at this tree structure of the recursive calls and identify these identical computations, which we are doing more than once, because we are actually not remembering that we have done it before. So, this is true for Fibonacci of 3 but it is also true for smaller computations. For example, Fibonacci of 2, we did it 3 times. So, in general, this will be actually exponential. If you actually write a Fibonacci function like this, and you try to run it on a reasonably small input, it will be very complicated to evaluate, even for, say Fibonacci of 50 or something like that, you will find it, it takes, it will possibly not even terminate. Whereas it is quite easy for us to write off Fibonacci of 50. I can start by saying this is the zeroth one this is the first one, the next one is 1 plus 0, the next one is 2. So, I can write off clearly the Fibonacci numbers with no effort. Even though I am following the same philosophy, as this inductive definition, I am not using this kind of wasteful recursive thing. So, what am I doing? So, this is the point. (Refer Slide Time: 06:40)  So, one way to avoid this wasteful re-computation is actually to remember. So, we should remember that the value that we have seen before has been computed. So, if we come across it a second time, we will not recompute it, but rather remember what we have said before, so we will need to keep a table of values that we have computed. So, we keep a memory table, let us call it a memory table, which is basically all the values of Fibonacci, which I have already computed once in my life. So, when I have this memory table, then I can use this technique called Memoization. So, memo comes from the, it is an old kind of English term memo for something, which is a reminder.'},\n",
              " {'id': 'ea66dd27-86e4-4297-9b2d-dccfecac8685',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, it is not memorization, but memoization, you are creating this memo saying, this is a to do or this is a reminder that I have already done this before. So, this is our table, and we will this technique of looking up this table is called memoization. So, the technique consists of basically every time I compute Fibonacci of n for n for the first time, by using this recursive thing, then I will store it in this table. But if I come across it a second time, in general, when I get Fibonacci of n as a problem to solve, I will first look at the table. I will say in this table, is there already a value for Fibonacci of n, if there is already a value, pick it up, do not recompute. Otherwise, I will make the recursive call. And with this actually, it turns out that you will never recompute the same value twice, so Fibonacci in particular will come linear because we know the subproblems of say Fibonacci of n are n minus 1, n minus 2, and so on up to 0. And if we are only going to do each of them once, then obviously, the number of total calls for Fibonacci that we really use recursion on is going to be linear. So, let us see how this memoization works on the same example, with Fibonacci of 5. As before we start with Fibonacci of 5, but now we have this table. So, this table is going to be filled as and when we encounter values that we have computed. So, right now, we have not computed Fibonacci of k for any k. So, we just have an empty table, I have just put six boxes there. But in general, this table will grow as we go along. So, we know nothing about Fibonacci of 5, the table is empty, so we have to call, as usual, the recursive calls 4 and 3. Now we know nothing about 4, as usual we will do it left to right. We know nothing about 4, so again, we have to make a recursive call 3 and 2. Once again, we know nothing about 3, so we make a recursive call. So, so far, we have not done anything new.'},\n",
              " {'id': '8c8cd9e1-f45f-418a-a830-c151846db53c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'We are just following the same trajectory that we did when we did the direct recursive call. Finally, when I do 2, I get these recursive calls 1 and 0 and now I start getting answers. So, technically speaking, even the base case, I do not know until I reach the base case through one of these recursive computations. So, now I realize that if I do it left to right, that Fibonacci of 1, the function tells me is actually 1. So, I will compute it, return the answer, but I will also put it in the table. So, this is my first entry in the table. It says for k equal to 1, Fibonacci of k is 1. So, remember that this table is not necessarily getting filled in the order 0, 1, 2, 3, 4. I am filling 1 first and then because I have done it in this order, I will then find 0 and I will make an entry for 0 also. So, it says now, I know in my table that Fibonacci of 1 is 1 and Fibonacci is 0 is 0. This is different from saying it is the base case, the base case still means I have to evaluate and reach that point. And it is saying that if somebody ask me Fibonacci of 1 without executing the function, I will look at this table and tell you it is 1. So, now I come back and now I have got a value for Fibonacci of 2. So, now I put an entry here saying the Fibonacci of 2 is 1, because I have computed it and I am never going to write this tree again. So, now, I go back to this call and it ask me to go down and find out what is Fibonacci of 1. Now, again, I would like to emphasize, we are not going to call the function because we are first going to look at this table and we are going to say aha, Fibonacci of 1 is something that I have already seen before. So, I will return 1 not because it is the base case, but so when I write it in orange, now it means this function was not evaluated, it was actually looked up. So, I just, so I would first look up the table, if the table value has an entry for this argument, I will return the value from the table otherwise I will call the function.'},\n",
              " {'id': 'c5086000-7591-405c-8ef7-1855ac834425',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, this version of Fibonacci of 1 did not actually reach the code for Fibonacci of 1, it just directly returned it. So, now with this, I have also computed Fibonacci of 3. So, Fibonacci of 3 is now 1 plus 1 so that is 2, and this now gives me a new entry in my table, saying therefore k equal to 3, I know that Fibonacci of 3 is 2. So, now I come back to the call for Fibonacci of 4, and it is going to ask me to evaluate Fibonacci of 2. Now in our earlier version, we had to again expand 2, but this time I look up and I find that 2 is there. So, I will look at the table and again, it will become an orange evaluation, which is that it did not call the function, it just looked up the table and returned the value the table. So, you believe that you have computed it once, and the value does not change. So, if it was already known to be Fibonacci of 2 or already known to be 1 it is not going to change. So, it is just one, you just look it up and give it back. So, with this, now I have computed Fibonacci of 4. So, I can now make a new entry in the table saying Fibonacci of 4 is 3. So, now I finally come back to my original call Fibonacci of 5. And now I have to, again, evaluate Fibonacci of 3, but same thing. Instead of expanding this whole tree, as we had done the first time, I will realize that 3 is actually an entry in my table, so I will look it up and again, I will make an orange called Fibonacci of 3 and get back the value 2 directly by just looking up this table. So, this two came from here. And now I have 3 plus 2, so Fibonacci of 5 is a new entry in my table and I get Fibonacci of 5 is 5. So, this is how we memoize. As in when we compute a value Fibonacci of k, we make an entry in this table, you can think of it for now, it is simplest way to think of it is as a dictionary. With a key k, I put the value Fibonacci of k, so that the order in which the keys come does not matter, I just have to check if the k I am looking for is a key or not.'},\n",
              " {'id': '57297d1c-c842-4d72-ab2c-d05c6ca9bb16',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'If it is a key, then I will pick up the value if it is not a key and make a fresh entry. (Refer Slide Time: 12:51) So, this was our original Fibonacci, it was just a naive recursion. It just said that if n is less than equal to 1, then return the value as n itself, otherwise, do the recursive computation. Now, we have to embed this table in it. So, we will assume that we have this dictionary called fib table, which is defined outside, so I am not going to define it inside. So, it is a kind of global dictionary. Let us assume that it is available to us, and it is initially empty. So, what I will do is, I will check whether the argument that I have got is a key in this table. And if it is a key in this table, then I will return the value of that key. If it is not a key in this table, then as before I do this computation. And now you can see why I did not want to return directly here because I want to keep this value around, so that I can put it into it. That is the reason why we split this return n or return fib n minus 1 fib n minus 2, instead of just radically returning it, we stored it in this intermediate value, so that we can now introduce this line, which says, okay, here is my dictionary create a new entry in the dictionary for this key for which there was no entry before. Why do we know there is no entry before because if there had been an entry before, I would not have reached this point at all, I would have just returned. So, we make an entry and then we return. So, this is our first computation of fib table, fib of n, but the next time is going to be in the array in the dictionary. So, the next time I am not going to call this function again, at this point, I am going to just return without doing the recursive computation. So, this is how we would memoize our Fibonacci function. Now there is nothing very specific to the Fibonacci function, we can take any function. So, supposing we have a function of three arguments, say f of X, Y, Z.'},\n",
              " {'id': '50237928-2607-4ffc-8e90-f1530f62f821',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, now, if I see a particular combination X, Y, Z a second time, I should not compute it again. So, here now my dictionary keys will be these triples X comma Y comma Z. So, I will take the argument that I have got and I will say is this triple X comma Y comma X, is it already known to be a value in this dictionary? If it is, then I will just look up the table with that key. So, here I had fib table for fib. Here I have some arbitrary function f so I just call it f table. So, I assume that I have this empty dictionary to start with, which is defined outside somewhere, so I can access it globally from here. So, I will now take this X, Y, Z check if it is a key there, if it is not a key I will recursively compute the value of f of X, Y, Z given whatever is the inductive structure of f, it does not matter, this is a generic thing. Any f, which has an inductive structure, there will be some way to compute it from subproblems. And then at the end of this computation, before I return the value, I will make an entry. This is the crucial thing. I will update my table saying now I know the value f of X, Y, Z, so this should be, sorry, this should be f table not f. So, I will take and make an entry in f table saying f table with this key X, Y, Z should have the value, value and then I will return it. So, this is a very generic thing you can take any recursive function memoize it by just making sure you have an appropriate dictionary sitting around. (Refer Slide Time: 15:56) So, the next step from memoization is what is called dynamic programming. So, in memoization, what we did was, we recursively evaluated the function. And every time we came to a sub problem for the first time, we use the recursive structure the second time we looked up the table. So, we saw the subproblems in the order in which they arise. Now, what dynamic programming says is that, okay, you know, the sub problems are there and there must be some dependency obviously. So, the sub problems depend on.'},\n",
              " {'id': '83577069-5d92-4c84-8c4b-7c33974c9444',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'I mean, the main answer depends on the sub problem and there is rarely local. So, when I say for example, that factorial of n, I said that it depends on factorial of n minus 1 down to factorial of 0, but the dependency is not direct, I do not need to know factorial of 0 upfront to know factorial of n. I only need to know factorial of n minus 1. So, this is the immediate sub problem, which gives the answer but for that, I will need the previous sub problem and so on. So, there is a kind of dependency structure among the sub problem saying in order to evaluate this sub problem. So, Fibonacci it is n minus 1, n minus 2. If I know n minus 1 and n minus 2 I can solve n. For factorial is n minus 1, but then n minus 1 intern depends on n minus 2, n minus 2 intern depends on n minus 3, and it goes all the way down to 0. So, these must form a DAG. A DAG remember is a directed acyclic graph. It cannot have cycles because if Fibonacci of n requires n minus 1 and n minus 1 requires n, then how will like proceed. So, these have to be resolvable down to a base case otherwise your inductive definition is not well formed. So, if you have a well-formed inductive definitions, you should be able to unravel it down to the base case, and then stop at a point where you do not have any incoming dependencies. So, this is, remember, when we did this DAG we said that there will always be a node which has no incoming edges if it is a DAG. So, these form the base case, the DAG of dependencies or sub problems. So, the base case are those problems which do not have any prerequisite. So, now, we can go back to what we have seen before, which is you can evaluate DAGs, sensibly, by going from those with no prerequisites and gradually eliminating the prerequisites. This is a topological sort. So, if you evaluate the values in topological order, then you never need to make a recursive call. Because when you need that the table has already been filled.'},\n",
              " {'id': '74bc465f-b93b-4512-9002-7afddc2f34a5',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': 'So, remember that when we did memoization we did not anticipate. So, when we did fib of 5, we had not precomputed 4 and 3, even though it is sort of obvious that 4 and 3 were needed, we waited, and then we called the recursive function for 4 and then we look at 3 and then 2 and then 1, and then we work backwards. In dynamic programming, what he would say is, oh, I know that 0 and 1 are required for 2, I know that 1 and 2 are required for 3, so let me do it in that sequence. So, I do not, I start with the cases which are, the base cases do not require a recursive call, so that when it comes to the first case that requires a recursive call Fibonacci of 2, I already have the base cases. So, if you look at this DAG structure for Fibonacci, so these are the subproblems. So, Fibonacci of 5, I need 5, 4, 3, 2, 1, 0 and the DAG structure just says that 4 and 3 are both required for 5. Similarly, 2 and 3 are both required for 4. Similarly, 1 and 2 are both required for 3 and finally 0 and 1 are both required for 2 and 0 and 1 themselves are base cases and have no dependencies. So, now I can enumerate this in any order. Normally, you would enumerate it in the order 0, 1, 2, 3, 4, 5. But because these are both equal and base cases, you could also do it in the order which is what we did when we did memoization, 1, 0, 2, 3, 4, 5. Because up to this point, there are no requirements it is only here that I have the first requirement, so when I do two better I have done 0 and 1 both, but I could have done it in either order 10 or 01. So, normally when we do dynamic programming, what we will do is, we will do it in sequence. So, we will say, here are the cases that I can do right at the beginning then here is the next case now that I know this, so I can fill up that same table, that memo table that I had. I can fill it up from no prerequisites to the final answer without ever making a recursive call. So, this is dynamic program.'},\n",
              " {'id': '4a871389-ddc5-41f7-b7ca-c41c07d2d356',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 9},\n",
              "  'source': 'Memoization.pdf',\n",
              "  'content': '(Refer Slide Time: 20:10) So, to summarize, memoization says, never recompute the same problem twice. Every time you use a recursive definition to compute a value, store it in a table, so that you can look up that table before you make a wasteful recursive call the second time. On the other hand, dynamic programming says the same thing, but it says, we will avoid recursive calls altogether. We will look at the inductive structure of the problem and build in our minds or in practice, it does not really matter. Normally, it is a kind of implicit, you do not actually explicitly draw the DAG I have done before, but you try to work out this dependency. What do I need for f of k? And then you say make sure that those are evaluated first, so you work backwards and you say, okay, which is the first thing I can evaluate, start with that, and then build up this table from bottom up. So, this is another thing that you often hear about dynamic programming, it is bottom up. Whereas memorization is top down, you start with the problem you want to solve, and you keep breaking it down into subproblems, but you never do the same problem twice, whereas a dynamic programming says, anticipate all the subproblems you might need and systematically evaluate them, so that when you come to a new sub problem, the previous subproblems that it needs are already evaluated and you never make the recursive call. So, this is memorization and dynamic programming.'},\n",
              " {'id': 'c7dc11ce-dd46-4fee-888a-7dbcc6da1f71',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor. Madhavan Mukund Grid Paths (Refer Slide Time: 0:12) So, we have seen the concept of memoization and dynamic programming in the context of Fibonacci which is a kind of very artificial example. So, let us look at a more interesting example. So, this problem is called grid paths. (Refer Slide Time: 0:23) So, supposing we have these roads which are arranged in a rectangular grid, so this is often called a manhattan grid because the city of manhattan has roads like this. And let us assume that these roads are one way, so the roads are one way, so you can either go right or you can go up you cannot go left, you cannot go down. Now, we start at the beginning and we want to reach something at the top right this is our target. So, the question is if I start at 0, 0 which is the left bottom corner and I have to travel 1, 2, 3 up to m sideways and up to n upwards. So, I will reach in general a corner which is labeled m comma n. So, the question is how many different ways are there of going, I could go this way, I could go this way, so each time if I follow a different segment of road it is a different path. So, the question is how many such paths are there so these are called grid paths because this is a grid of roads and it is a very standard problem in combinatorics. (Refer Slide Time: 1:26) So, for example this blue line is one such path it only goes up or right and it takes us from 0, 0 in this concrete case where m is 5 and n is 10, it takes us from 0, 0 to 5 comma 10. Here is another path the red path it goes further to the right before it starts going up but it also never moves left or down, it always goes right and up and here is the third yellow path, so there are obviously many different paths so the question is how many such paths are there, this is what we want to count. So, our goal is to count how many such paths are there.'},\n",
              " {'id': '30c32397-ceea-4c0c-b5f5-15c227dc091d',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, there is a very standard and very elegant way of analyzing this mathematically. So, what you see is that it does not matter how I do it, I have to move 15 times to go from where I move means go from one corner to the next corner, I have to follow 15 steps to go from 0, 0 to 5, 10 because I have to go 5 times to the right and 10 times up, I have to go up this entire segment and I have to go right this entire thing, the only way I can go right is to take one segment from this road to the this corner to the next corner, so I have to do that 5 times, and I have to do 10 times going up. But notice that no matter how I do it because I never go down and I never go left, I keep making progress. So, if I go 3 times and then 4 times up those 3 times have been done so I only need to 2 more to the right I only need to do 6 more up. So, no matter which way I re or organize the path if you count the number of rights and ups in the blue path, the yellow path, the red path they are all the same. So, there are always 15 moves and in these 15 moves there are always 5 right moves and 10 up moves. So, if I tell you where all I took the right turn so if I said that I made the right turn at 1, 2, 3, 5 and 8, then it means that positions 4, 6, this is 9; 7, 8, 10, 11, 12, 13, 14, 15 at all these other numbers I must have gone up, so I have these 15 slots, in these 15 slots I can either go right or I can go up and there are only 5 where I can go right. So, I pick those 5 the remaining 10 slots must be up. So, how many ways can I pick those 5 positions where I turn right that is the question, each time I make a right turn that defines a path because the remaining ups are fixed. So, if I know where I turned right that determines the path, the number of different ways of turning right tell me the total number.'},\n",
              " {'id': 'e242b967-dbcc-4046-a3f3-08e977feb995',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': '(Refer Slide Time: 3:56) So, this is just this notation of choosing out of the 15 possible positions which I have choosing 5 of them to turn right so this is a famous factorial example. So, ncr is n factorial divided by r factorial times n minus r factorial, so this is the formula so it turns out you can explicitly evaluate it for this number 5 comma 10 and you will get 3003. Now, notice that I have said that we fixed the rights, we could also fix the ups, so there are 15 positions it is symmetric, if I tell you where all I went up then you know in the remaining 5 positions I went right. So, either I can fix the 5 rights or the 10 ups and in fact this is a general thing in this combinations and permutations you know that n choose r is the same as n choose n minus r. So, if you are choosing 3 things out of 7, it is the same as choosing 4 things out of 7 because every time I choose 4 things I have implicitly chosen 3 things, every time I choose 3 things have implicitly chosen the other 4, so the two are symmetric, so I can also think of this as 15 choose 10 by fixing the 10 up moves. So, this is a neat combinatorial solution to this problem. So, why are we interested in this? (Refer Slide Time: 5:08) Well, what if the world is not as easy as it is, so we all know in every city that we live that there is always a problem at some place or the other and the road is being dug up, it could be dug up because there is a problem with the sewerage line or somebody is laying some cables or just that the road has been damaged by rain. So, let us assume that there is this one intersection which is currently dug up we are not allowed to go through this intersection, so this intersection concretely is at 2 comma 4. So, now the question is if I am not allowed to go through 2 comma 4 now how many ways are there of going through this. So, let us see what happens.'},\n",
              " {'id': '9efbb4f1-ba4b-48b4-908b-932058946c03',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'Well, we have this blue path, this blue path unfortunately was going through 2 comma 4 so this is not a path that we could take anymore. This red path avoids 2 comma 4 so this is fine, but the yellow path also went through 2 comma 4 so the yellow path must also be skipped. So, what we have to do is we have to make sure that when we are counting we do not count paths that pass through 2 comma 4. So, we have to discard every path that passes through 2 comma 4. So, we can apply a similar combinatorial reasoning for this, we can say how do you get a path that goes through 2 comma 4. Well, it must go from here to 2 comma 4 and then must go from here to there, so I want to count the number of I know the total number of paths 3003 I want to count the number of paths which I want to throw out, saying that they pass through 2 comma 4..Well, how do I do that? Well, I count the number of paths which go from 0 to 2 comma 4 and then from 2 comma 4 to my destination because I know that every path that I am interested in will be a combination of a path of the first type and a path of the second type. So, if I go from 0 to here I am basically solving the old grid path problem for this smaller grid, so this smaller grid has got 2 rights and 4 ups. So, for this I have 2 plus 4, 6 possible moves of which I have to choose 2 and I get 15 paths so there are 15 ways of going from here to 2 comma 4 and this smaller grid has 3 rights and 6 ups because I have already finished the rest that is a smaller grid. So, here I have 9 choose 3, 3 right, 6 ups, 9 choose 3 and if you calculate it turns out to be 84. So, now any path which passes through 2 comma 4 is this one of these plus one of these, but any combination is possible so to get the total number I should multiply, I should say for every one of those 15 paths there are 84 matching paths on the other side and each one of these combinations is a different path.'},\n",
              " {'id': '76aa1db9-754c-4c6b-9f80-c51712d33c73',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': '(Refer Slide Time: 7:42) So, I have totally 15 times 84, 1260 paths which actually pass through this forbidden intersection. Now, totally I have 3003 paths, so if I subtract 1260 I get that there are 1743 paths left, so so far so good so our counting strategy using these combinations has worked for the path, the perfect situation where the grids do not have any holes and its work for this one blocked intersection case. So, of course the natural question what happens if there are 2 blocked intersections? (Refer Slide Time: 8:10) So, now I have a block at 2 comma 4 and at 4 comma 4 now what happens? So, the earlier red path which was not passing through 2 comma 4 now gets discarded because it is passing through 4 comma 4 so obviously more paths will get discarded because now I have two possible reasons why I should discard them, either they pass through 2 comma 4 or they pass through 4 comma 4. But what about that yellow path? So, that yellow path pass through both 2 comma 4 and 4 comma 4 that means if I discard it if I count the number of paths which pass through 2 comma 4 throw them out and then I separately count the number of paths which pass through 4 comma 4 and throw them out this yellow path has been counted twice, so I have thrown it out twice. So, then you have to go back and you have to add back all the paths which actually pass through both the things. Again you can do that, you can do that by again saying how many ways are there to go from 0 to 2 comma 4, 2 comma 4 to, so how many paths pass through both of these? You can count it exactly as we did before you have to multiply now 3 numbers. How many ways are going from here to there, how many ways here to there, how many ways from here to there and multiply them. And then you have to add this number back. So, this is a general counting principle called inclusion and exclusion. In some of these Venn diagram problems also you have the same thing in a different guys.'},\n",
              " {'id': '9d2bc434-042b-42fd-8860-508630c20f0e',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, you will say that you know there are so many people study physics, so many people study chemistry, so many people study biology and then so many people study two of the subjects now if you count the people who study two of the subjects implicitly it also counts people who study all three. So, if you add this up it will have an over counting so then you have to subtract the people who study all three, so you have to include, exclude at every level. So, this can be done but it becomes really tedious if I have now if I put 5 holes, then you are going to have a really tough time evaluating this using this kind of arithmetic approach. So, what can we do instead? So, can we apply some kind of inductive structure to this problem and solve it? (Refer Slide Time: 10:10) So, let us look at the problem. The problem in general is to know if I am at trying to reach so remember even for solving the whole problem I needed to know how many ways I can reach 2 by 4 or 2 comma 4. So, in general I have a target i comma j which may not be the ultimate target may not be m comma n the top right target. But I want to know how many ways can I get from 0, 0 to i comma j. So, the inductive structure comes from the fact that we have constrained our moves to be right and left so how can I reach i comma j well I could move up from the previous row so I can be in the same, I can be it in in the sorry in the previous yeah in the same row I can be in the previous column, I can go from so this should be move right I guess. And I can also move up from the previous row, so again just I will fix this but so this is the previous column and this is the previous row. But the point is there are exactly two neighboring corners from where I could have reached here in the last step. I could either come up from one row below or I would have come right from one column on the left. So, how many ways are there of reaching this?'},\n",
              " {'id': '28593900-4076-4e0d-b6c6-121bae8284d8',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'Well, I must either come here and then extend it by one path or I must come here and extend it by one path. So, if I know how many ways there are of reaching this place and I know how many ways there are reaching this place, then I know how many ways are reaching i comma j because every path which reaches my left neighbor by adding exactly that one edge there is no choice anymore I have to take that one edge to reach i comma j. Similarly, every no path that reaches the neighbor below I can add one edge. So, in this way you can say that every path to these neighbors extends to unique path and so the recurrence tells us that the number of paths p i j reaching i j is just the sum p i minus 1 j plus this p i j minus 1. The number of paths reaching my left neighbor and the below neighbor, if I just add it up I get the answer. So, we have to always when we have an inductive definition we have to realize that this keeps going down, so at some point we have to tell it to stop. So, we will tell it to stop so in particular if we are just counting the number of paths from the starting point to the starting point, there is I do not move at all but it is not saying that there is no path, not moving at all is the same as saying there is only one way to stay there so p 0, 0 the number of paths from 0, 0 to 0, 0 without doing anything is 1. So, this is important otherwise you will not get started in counting anything. (Refer Slide Time: 12:49) Now, if I look at some boundary conditions. So, if I look at this bottom row then I cannot come from below, if I look at the left column I cannot come from the left. So, if I am in the bottom row then my y coordinate is 0, so if the y coordinate is 0 then I can only come from the left so it is the same as the number of things reaching the left. And similarly if the x coordinate is 0 then I am in the left column I cannot come from the left so I can only come from below.'},\n",
              " {'id': 'ac42558d-07ef-4040-933f-d0c9139fbb4b',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, maybe this is actually yeah, so this is actually correct if I do not think about it rows and columns as x and y coordinate. So, this is saying that at the previous x coordinate and this is the previous y coordinate so that will resolve that. So, anyway so that is our recursion now. So, recursion says that p of i j is p of i minus 1 j plus p of i j minus 1 and in particular p of 0, 0 is 1 and if I am in the base case where I in the in the border case where both of these are not defined, then the one that is not defined I will just treat it as 0, there are no paths coming from that side. (Refer Slide Time: 13:55) Now, the question is why does this help? Well, the reason this helps is that what do I do at a hole? Well, for a hole I say that no matter what I can do to come so if I have a hole here, it also has 2 neighbors below and, but the problem with the hole is I cannot reach it, so regardless of the fact that this might be some K and this might be some L, there may be some K ways to reach to the left of the hole and some L ways to reach below the hole this is not K plus L but this is just the number 0. So, this is just 0 it is not K plus, normally it will be K plus L the number of ways to reach that thing will be how many ways I can come to the left, how many ways I can come below but it is not, it is just K plus L is 0. So, this is the clever thing now, so we can now wherever we see a hole in our grid we can predefine the number of paths reaching that point to be 0 and then we can apply this induction and it will work. So, let us look at this more carefully. (Refer Slide Time: 14:50) So, first let us look at this problem that we had before of memoization and dynamic programming. So, if we use this in a trivial way then of course we will be computing the same problem again and again. So, for instance look at P 5, 10.'},\n",
              " {'id': '59a9d793-cc46-4e8c-ba28-099ac9e37454',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, P 5, 10 requires us to compute the values at P 4, 10 and P 5, 9 but now both of these in turn, so this will require us to compute these two and this will require us to compute these two. So, both of them will require this value at P 4, 9. So, if I do a naive recursion P 4, 9 will be evaluated twice, so I do not want to do that so therefore I will use either memoization or I will use dynamic programming which is just saying that I will find a suitable order in which to compute p i j so that whenever I come to an intersection i comma j the values that it depends on have already been computed. (Refer Slide Time: 15:45) And to find the suitable order we need to identify this DAG structure we said what is the dependency so it is very clear, the dependency just says so these are all these arrows which are coming together. So, it is saying that each problem at a given i, j depends on the problem to its left and the problem below, so that is the DAG structure. So, now with this DAG structure we realize that this is the only node which does not have any incoming edges. So, any kind of pre computation that dynamic programming does is going to start from here. Remember dynamic programming you will take this DAG of values and it will evaluate them in topological order so that whenever you come to a node whose value needs to be computed the values that you need in that inductive definition are already computed because they are lower in the DAG. (Refer Slide Time: 16:30) So, we start at this value so we remember that P of 0, 0 is 1, so we start with 1 and now for instance we could do this row by row. So, remember that on the row we said that because there is nothing coming from below the value at any node is just the number of things coming from the left.'},\n",
              " {'id': '6486c516-796c-4755-af65-3cddce2a84d0',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, since I started with one it is very clear if I am going this way there is only one way to go that is all it saying, if I am going right there is only one way to reach that because I cannot go up and come down because that is not allowed. So, I can only go to the right. Now, I have computed the bottom row now I can look at this value. So, what is this value? This value is the value from the left which it does not exist plus the value from below so this value is going to be 1. Similarly, what is the value here? It is the value from the left the value from below, so this value is going to be 2, here it is 2 plus 1 is going to be 3. Why is this 3? Well, you can think about why it is 3, I can come like this or I can go right and turn there or I can go all the way right and come up, so that is why there are 3 ways of reaching there. So, similarly this is going to be 4, this is going to be 5, this is going to be 6. So, once I have computed the bottom row I have enough information to compute the next row from left to right. So, I can now say 1, 2, 3, 4, 5, 6. (Refer Slide Time: 17:37) Now, the same way I can say this is 1, this is 3, this is 5 and so on, so I can compute this row also as 1 3, so not yeah so, so 1, 3, 6, 10, 15 and 21. So, I keep doing this until I hit my row with the holes. So, when I hit the roll with the holes I will have 1 here, I will have 5 here but here I will not get 15 because it is a hole and if there is a hole then the answer is always 0. Similarly, here I will not get whatever I get for this value plus this I will just get a hole, so this is what I get in this row. So, I pre compute these two as 0 then inductively I get this 5 by combining 1 and 4 but this 0 is just I mean it is a fact I mean I cannot help it. But now when I come here I combine 0 plus 20 and I get 20. Again, this 0 is given to me and again when I come here I get this 56 by combining the 0 and 56.'},\n",
              " {'id': 'd407da5f-71b6-4226-ab30-71428908b814',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, the grid holes just get arbitrarily fixed to 0 because I have just declared that you cannot pass through there, so there cannot be any paths reaching there. But once you fix that 0 then everything else gets fixed along with it. So, now I can continue, so I can compute the next row. So, again now this is saying that the only way to reach here is from the left because I cannot come from below, so it is 6 plus 0. Similarly, the only way to reach here is to come from the left because I cannot come from below so it is on 26 plus 0. So, I keep doing this and eventually if I walk all the way up row by row I find out that actually with these two holes remember with the one hole we had computed by combinatorially we had computed some 1743 or something like that. Now, we have fewer obviously because two holes are there but the number of paths is exactly 1358. So, this was done row by row. Now, there is anything which respects the topological order is fine, so you can also do it differently so you can do it column by column. (Refer Slide Time: 19:28) So, you can start here and you can say ok I know this value because it comes from the left and below and I know the below value but I do not know the value on the left. So, I can fill up this entire column as 1, in some sense we are just saying that there is only one way to walk up this. And now symmetric to what we did before I can fill up the second column. And then when I come to the third column again I will put 0s and obviously since they are going to give a same answer all the other entries are going to be the same is just that I am filling up the entries now this way. But even this is not the only way. (Refer Slide Time: 20:00) So, let me look at this, now which values can I at this point which values can I solve? I can solve this value and I can solve this value also because we could do a row by row and column by column so what if I solve both these values? Now which values can I solve?'},\n",
              " {'id': '6340cfe4-38b0-47ea-8381-9c0f445f323b',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'Obviously I can solve this one and I can solve this one but I can also solve this one because it has both the incoming values. So, I can actually do it in this order diagonally, so I can fill up now 1 2 1 then I can fill up 1 3 3 1 and so on and get the same thing. So, there are many different ways in which dynamic programming can fill up this grid of these sub problems, any order which is compatible with the sub problem DAG the dependencies is fine. So, normally we choose a simple order it is kind of easier to say if you know like if you are doing row by row or column by column is just a simple nested loop for every row for every column do something. If it is this diagonal order you have to be a little more careful to enumerate all the elements in the diagonal. So, one way of thinking of the diagonal is all values where the sum is the same, so this is for example this is 0 comma 3, this is 1 comma 2, this is 2 comma 1 and this is 3 comma 0. So, basically along the diagonal all the entries are actually of the have the same sum, i comma, i plus j is the same that is how you identify things on the diagonal. So, that is how dynamic programming works on this particular problem of grid paths. Now one other question that we might ask is there a difference between dynamic programming and memoization. (Refer Slide Time: 21:41) So, let us look at this particular situation so supposing I have a scenario where my block intersections actually form this kind of a barrier, so there is a barrier right inside the boundary so the only way to go from the bottom to the top if I go inside this I get stuck because there is no way to move right or out up and get out of this, so I have to stay along the left border and go this way, so there is one path going that way and there is one path going this way.'},\n",
              " {'id': 'eb0eb104-4d2c-48fe-bcda-58d11197c62d',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, intuitively you can see that there are only two ways to reach now because I am forced to go either along the left column and then go right or I am forced to go along the bottom row and then go up. So, if I did dynamic programming what would you do you would start from here and you compute all these values and then you will hit these 0s, after a lot of calculation you will hit 0s and then you will replace the sums with the 0s. (Refer Slide Time: 22:33) But if I do memoization I will start from here and this memoization will say ok give me this value and this value and then this value will say give me that value and this is 0. So, there is no recursive call, so memoization will only compute values along the boundary, it will only explore these problems because those are the only values which will arise as recursive calls, the other values will return automatically saying it is 0. So, memoization will never explore this entire shaded region is never going to be called recursively if you start and do not call unnecessary things. (Refer Slide Time: 23:08) So, therefore in this particular case the number of entries you actually compute is going to be proportional to m plus n because I left these all these boundaries so I will have 2 m plus n may be so maybe this more accurately this should be order m plus n. Whereas, if I do dynamic programming I am going to actually fill up m times n, this is the size of the table. So, that way dynamic programming does not really care about whether a sub problem is going to be used or not it just computes it and goes ahead whereas memoization will only evaluate those problems which actually arise in the computation. Now, usually there is not much of a difference but here you can see there is a kind of quadratic difference, it is a linear thing for memoization it is a quadratic thing for the dynamic programming.'},\n",
              " {'id': 'e5017b28-ba5e-4916-bc90-3574c4ba0b4c',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 9},\n",
              "  'source': 'Grid Paths.pdf',\n",
              "  'content': 'So, but even in this such a case even though there is a potential wastefulness of doing dynamic programming; the problem with memoization is that the only way to use memoization is combining with the recursive solution. So, dynamic programming does two things, one is it exploits the inductive DAG structure to decide the order but then after that it eliminates all recursive calls. So, the entire table can be filled up just as a nested loop. So, that saving actually overrides this cost of having extra entries. So, usually you will find the dynamic programming even with some wasteful computations like in this case will still perform better than memoization because the space saving is not, is more than compensated of by the cost of recursion. So, dynamic programming does not incur that cost, so the calling and returning from a recursive function makes memoization more expensive. But there could be situations where the number of sub problems is really huge and you cannot anticipate which all you need to fill up in which case memoization is a better strategy.'},\n",
              " {'id': '9698e984-76e5-4253-bb43-36d833cbe3b1',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Edit distance So we have seen how to compute the longest common sub-word and the longest common subsequence between a pair of strings. So let us look at a very related problem that of computing what is called the Edit Distance. (Refer Slide Time: 0:22) So supposing I have 2 sentences. So in general these could be 2 larger pieces of text and entire document which read as follows. So the first one says, “The students were able to appreciate the concept optimal substructure property and its use in designing algorithms.” Second says, “The lecture taught the students to appreciate how the concept of optimal substructures can be used in designing algorithms.” Now clearly these are similar sentences and one could imagine that the second sentence was perhaps created by editing the first one. So the question is what does it mean to edit a document? So our goal is to try and find out how much editing was done or what is the minimum amount of editing needed to make the first document look like the second document. In this case, a document is just a sentence. So what do we mean by edit? Well, you can insert a character or you can delete a character or you can replace one character by another character. So every operation applies to one character at a time, one symbol at a time either you can insert a symbol, delete a symbol or modify one symbol by replacing it by another one which we call substitution. So if you have used word processing tools then sometimes they have this way in which you can track changes. So if you use track changes then it will show you what are all the insertions and deletions that you have done so that you can reconstruct what has been done to the document after editing. So here for instance we have shown in green underline what happens when we go from, so we are going from this document to this document.'},\n",
              " {'id': '51f35462-f6b9-44d8-8067-8745da57ea32',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So here it says the students and here ‘the students is’ separated by this section. So we insert the words ‘lecture taught the’. So if we insert the words ‘lecture taught the’ then we get ‘the lecture taught the’ students’. Now we come to this part, so it says ‘were able’, now here we do not have ‘were able’ we just directly go from ‘students to appreciate’. So we delete ‘were able’ here. So in this way step by step, we can take the first sentence and appropriately modify it to get the second sentence. So the three different types of modifications as we said, one is to insert a character, the other is to delete a character and the third is to substitute. So here for instance, we started with “its” and we wanted to go to “be” Now there are many ways to do this. You can delete all 3 characters and then insert b but here what we have chosen to do is to delete the i and then replace a “t\" by a “be” and “s” by an “e” So we have done 2 substitutions instead of 3 deletions and 2 insertions. So you can think of it as saying that instead of doing 5 operations, 3 deletions and 2 insertions, I have done 3 operations, 1 deletion and 2 substitutions. So we are assuming that each of these has the same cost whether you delete a character or you insert a character or you substitute a character, you are paying the same cost in terms of the editing cost. So one thing to keep in mind is that we are only looking at the non-blank letter so technically speaking you would also be inserting these spaces here and deleting spaces and so on so we have not taken that into account in this calculation. In a more careful calculation, you would also keep that in mind. So here we are only talking about the visible characters in this particular example. So we are interested in how much time it takes or what is the cost of going from one document to another document and this is what is called the edit distance.'},\n",
              " {'id': 'b0dc81fd-21fa-4685-a472-39544db4bbf3',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So the edit distance is just the number of edit operations that you need to go from here to there. And in particular since there are multiple ways as we saw when we looked at the example of transforming “its” to “be” there are many ways to do the same thing and they may all have different costs in terms of number of operations. So we are typically interested in the minimum cost. So just like the shortest path in a graph. Here we are looking for the minimum edit distance, the minimum number of changes I need to make in terms of these 3 operations to go from one document to another. So if we count what we have done above, and you look at it carefully then you will find there are 24 new characters which were inserted in going from the first sentence to the second sentence. 18 were deleted and 2 the ones in “its” the “t” in the s were substituted. So in our case this particular way of doing it we have incurred a cost of 44. So remember that the edit distance is the minimum over all possible ways of doing it. This is one particular way in which we have done. It need not be the minimum. But we certainly know that the minimum cannot be bigger than this. So we can do definitely in 44 operations. So the edit distance is utmost 44 perhaps there is a clever way of doing this. So our goal of course is try to compute this edit distance efficiently. (Refer Slide Time: 05:01) So this edit distance is often called is more properly called the Levenshtein distance because there are multiple actually ways of defining what it means to edit depending on what operations you allow. So the Levenshtein distance was discovered or invented by Vladimir Levenshtein in 1965. And why would we use edit distance? Well, imagine that you have mistyped something in a word processor or on a webpage then sometimes you get corrected, correction suggestion. So how do you suggest corrections for a misspelt word?'},\n",
              " {'id': 'f6e8c0a4-3457-4a62-9906-419db9853f42',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'Well, you look for the nearest word, so you assume that the misspelling has occurred because of some key or keys being mistyped by the person using keyboard. So if you can compute the shortest distance words in terms of edit operations to the word that has been typed which is unfamiliar then you can suggest a list of familiar words which are close enough and which are likely to be the ones that the person wanted to type. So spelling correction is one natural thing. And last time we looked at this least common subsequence, longest common subsequence as one measure of genetic similarity. Well, you can also think of the edit distance as a measure of similarity between 2 documents and therefore 2 strings of any type. So if I look at the genetic code as a string and I ask how many genes must I altered to go from one species to another species then you get some measure of how close the species are biologically. So the longest common subsequence is clearly closely related to this problem because that also measures somehow how much 2 strings overlap. So if you look at the longest common subsequence, what it says is that there are – so supposing there are m letters in u and there are n letters in v and supposing the LCS the length of the longest common subsequence turns out to be k. That means I can find k positions which are aligned in some sense if I allow - if I skip the gaps then these k positions appear in the same sequence in U and in V. So how many things are not there in this sequence? Well in the first string there are m minus k, and in the second string there are n minus k. So, the more that k is the less that m minus k is and the less it is n minus k. So the longest common subsequence also corresponds to the minimum number of edits which I can perform by just deleting. So, if my only edit allowed is to delete from one of the 2 strings then in that process I can edit the string down to the longest common subsequence with the minimum number of deletes.'},\n",
              " {'id': 'aa754775-f0f5-4203-806d-3b1034f48e10',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'Now deleting from one string can be equivalently replaced by inserting in the other string. So let us look at one of the examples that we saw last time. So if we look at the strings bisect and secret then we said that the longest common subsequence actually “sect” which is “sc” with a gap and t the second string. So how do we get from these 2 strings to this longest common subsequence? Well the natural ways to delete these letters which do not appear. So I can delete the b and the “i\" in bisect and I can delete the “r\" and the “e” in secret. But equivalently I can get down to the longest common subsequence from one string and then reconstruct the other string by insertion. So for instance I can start with bisect and delete these 2 strings and then inside this I can insert instead of deleting from the other one I can insert in this one. So there is a symmetry between inserting in one string and deleting in the other string. So it becomes a little simpler to think of it this way because now I can focus on only one string. So like what we did with the document in the previous example, we started with the first sentence and we edited down to the second sentence. Now in principle editing allows you to edit both ends, because you have 2, the source and the target and you might ask well can I change something in the source to make it closer to the target? Can I change something in the target to make it something closer to the source? Now as far as inserts and deletes are concerned, what we see is that they are symmetric. If you insert in one, it is deleting in the other; if you delete in one it is inserting in the other. So you can make all your edits in one source and you will count the same. What about substitution? Well, substitution clearly symmetric if I replace ‘a’ by ‘b’ going from here to there is the same replacing a “b” by “a” going from there to here. So therefore it is enough to focus in editing in one direction.'},\n",
              " {'id': '686844d0-76ef-4b7a-895d-d6dff9a3d451',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'And in particular the LCS is equivalent to edit distance without substitution because deleting in both strings is the same as inserting and deleting in one of the 2 strings. (Refer Slide Time: 09:34) So let us look at how we would discover some inductive structure for this problem. So as before we assume that u is of length m, so it is the sequence of letters a0 to “am” minus 1 and v is of length n. So it is b0 to bn minus 1. So remember our LCS inductive definition. We said that we look at the LCS starting from “ai” and starting from “bj”. So we call this LCS (i, j). And what we said was that if those 2 positions overlap that is a of a sub i is equal to b sub j the letter at the ith position in a and in u is the same as the letter in the bth - jth position and v. Then without loss of generality, we can include that letter in our longest common subsequence and then continue to find the longest common subsequence to its right. So we started ai plus 1 and bj plus 1. On the other hand, if they are not equal then we said that we have this situation where you have ai and bj and we do not know which one, we know that both cannot be in the sequence because if this is matched with something, and we said that this cannot be matched with anything else because it has to be to the right of ai. So, only one of them can be there. We do not know which one. So what we do is we construct this problem which has ai but does not have bj so we take i, j plus 1 and we have the other sub problem which has bj but not ai, so is the problem which is of the form i, j plus 1. And then we look at the longest common subsequence in these 2 problems and then we take the maximum so this is not like a greedy strategy we decide arbitrarily one of the two is going to be better we say we do not know which is better. So we will exhaustively try both and then take the maximum. So this is how the LCS works. So let us see how edit distance works.'},\n",
              " {'id': 'c4604c62-f199-4bb1-8df3-974f3ab3585d',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'It should be similar because we have seen the edit distance is not very different conceptually from LCS. So remember that we have said that inserts in one string are equivalent to deletes in other string. So rather than looking some like bi-direction and deletes we can assume that we are starting with u and transforming it to v. So, all changes are happening only in u. So first if ai is equal to bj then I can just proceed with the edit distance of the remaining space. There is nothing to be done because at this point the 2 strings are equal. On the other hand, if ai is not is equal to bj then instead of LCS I had 2 options. I could drop ai or drop bj but now I have I am transforming. So how do I make ai equal to bj. There are 3 ways I can do this. The first is that I can just directly replace. I can say make ai bj by just replacing it. I substitute ai by bj. But this is not the only option. So I have ai, ai plus 1, and then I have bj, bj plus 1. So the first option is just to take this letter and replace it by this letter. Another option is to delete the ai. So I have ai, ai plus 1 bj, bj plus 1. I just removed this and then I proceed because I know that if I do not substitute it, then that there is a mismatch so I must make it equal and one way to make it equal is to remove the offending letter from that string or I can insert remember I am making all my changes in u. So the letter ai does not appear at the beginning of string that I am looking at in b so I deleted or I make bj up here by pushing it up so I put it as bj ai and so on. And now in the second string is bj and so on so then I know that this bj and this bj match. So now it is enough to look at ai and bj plus 1 only. There are these three options that I can do. (Refer Slide Time: 13:22) So if I look at this and this gives me a natural recurrence it tells that if I look at the edit distance from ai and bj ed of i,j then if ai is equal to bj I have nothing to do.'},\n",
              " {'id': 'd2d3e773-3eca-4497-aeef-10719a9ab38e',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'The edit distance of i,j is the same as the edit distance from i plus 1 and j plus 1. On the other hand if it is not the same then I have to make one edit to rectify this problem. So that one edit could be a substitute. If I substitute then ai and bj are now correctly equal to each other so I go back to the earlier case and I just look at the remaining strings. So it is 1 plus the edit distance of i pus 1 and j plus 1. On the other hand if I delete ai then effectively I have started with a new string starting from ai plus 1 but the other string still starts from bj. So, then I am looking at the edit distance from i plus 1 and j. And finally if I insert bj here, as we said then we are actually deleting this thing so we have inserted bj before ai, so ai is still there but bj now has been matched up so I can look at the string from bj onwards and think of this problem as i, j plus 1. So I have now 3 options. I do not know which one is best. I am looking for the minimum edit distance rather than the maximum LCS so I take the minimum of these 3 costs and then add the unit cost for the edit operation which I was forced to do at this step. So this is now a recurrence for the edit distance. (Refer Slide Time: 14:52) Now as usual we allow the 2 strings to go off so that both of them become empty. So we go beyond the last index n minus 1 then the edit distance when we are beyond the last index in both is 0 because both have become the empty string if I look at the string starting at position m in u and n in v I get the empty string. Now what we said in the longest common subsequence is one of them is exhausted than the other one. All those positions are also 0 but here it is slightly different. So if look at the edit distance of ai to an minus 1 compared to the empty string. This edit distance is not 0. The longest common subsequence is 0 but how do I edit the first string to get there? Well, I delete all of these.'},\n",
              " {'id': '1ce5cb2a-3ed6-4564-b9ec-89b662813b76',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So I delete all of this and come here and how many things do I have to delete? Well there are i things before this a0, 2, a minus i. So I have to delete m minus i things because i things are already missing. m minus i letters are still there in my string I need to delete all those n minus i letters. So if I look at the edit distance from position i in the first string and I have gone to the empty string in the second string then the edit distance actually m minus i corresponding to deleting this entire suffix from i onwards. And symmetrically if I have the first string empty and the second string starts at bj then remember I am always operating on the first string. So instead of deleting from the second string, I will think of it as inserting into the first string. So I insert the suffix starting from bj into the first string to make them equal. Again to I have to insert j minus n minus j thing because from 0 to j minus 1, j letters which are missing out of n so n minus j letters remain and those letters must all be transported from v into u. So this now sets up our recurring fully. So we have this base case. So actually you can get these 2 cases by just expanding the base case and looking at the inductive thing but it is useful to just think of these as boundary conditions because notice that one of the 2 cases will not appear because I cannot go to i plus 1 and j plus 1 because I am already at the end of the one of two strings. So with these base cases and that inductive definition we can now set up our problem. (Refer Slide Time: 17:04) So our sub-problems are of the form of ED (i, j) for every i and j from 0 to n, n 0 to m. So, i from 0 to m, j from 0 to n and as before we will set this up in our 2 dimensional matrix of size m plus 1 times n plus 1. So same example bisect, which is of length 6 and secret which is the length 6 so we have this 7 by 7 matrix with entries all the rows and columns labelled 0 to 6.'},\n",
              " {'id': 'ec0170e9-6c74-47a5-9bfd-0d8695c0322e',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'Unlike in the edit distance, like in the LCS case, the dependence is basically looking at the 3 neighbors to the right diagonally to the bottom on the right and right below because the edit distance i,j depending on whether it is equal or not equal if it is equal then it is just the value at i plus 1 and j plus 1. If it is not equal, I have the 3 cases. I have the substitute case, which takes me to i plus 1, j plus 1 or delete and insert case, which will take me to i j plus 1 or i plus 1 j depending on which one it is. So I have the same dependence and as before this value is the one where I do not have anything on the dependence side. So for m, n I can start by initializing it to zero and this was the boundary case that we set that if I now take, if I keep n as 6 then all these values just keep increasing by 1. What it says is that to get from t to the empty string I have to delete one letter to get from ct to the empty string I have to delete 2 letters. I am going from bisect to secret. Bisect is my starting string, secret is my second string. So these correspond to the number of deletes I have to do if my target is to take the empty string which comes after secret is exhausted. So therefore if I have the entire first string bisect then I have to delete 6 characters to get from bisect to secret so that is the last column. (Refer Slide Time: 18:54)  What about the next column? Well we just follow the same thing so we see that if I have t in the second word but I have an empty string in the first word then I must insert the t to get there. So I have 1 here, so this was actually the other boundary, so this was actually 0, 1, 2, 3, 4, 5, 6 symmetric to that column. On the other hand what happens here, so here we have the t is equal to t so that means that this is a case where ai is equal to bj. So we do not have any cost at this point and then we look and we add the cost going down and right.'},\n",
              " {'id': 'e8caf2a2-2624-4e3a-896c-0de3940f34c7',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So 0 plus 0, 0 so there is no edit cost for replacing t by t or matching t to t. But if I’m at ct then I mean in the situation where I have ct on the first string and I have t on the second string. So now I have multiple things I can do. So one thing is I can go to t and t by deleting this or I can insert the t and go to tct and t. So it turns out that obviously this is the better choice so what I have to do is, I have to take the minimum of these and add one. So the minimum of these is 0. So what this is saying is really that this case, which I get by deleting the c is actually the good case because it leaves me with an edit distance of 0 and I add one and I get one. So we are just following the recurrence. We are not doing anything special. So in every case we can just do that. I mean this is just to explain what is going on but you can just do it blindly. You do not have to look at the actual operation you are doing. So for instance if I take this value it will say that this is a t and this is an s. So since this is an s and this is a t and it is not equal I must take 1 plus the minimum of my neighbors, so I look at the neighbors. So my neighbor value is 2 3 4 so the minimum of that is 2, I add 1 and I get 3. So it is a completely local blind arithmetic operation depending on the ui and the vj that correspond to this position. So you do not have to analyze why it is happening. But it will be the case in general if you pick up the value from the bottom, it is because of the delete. If you pick up the value from the right, it is because of the insert when you pick up a value from the right then i position does not change but the j position changes. And when you pick up the value from the bottom the i position changes and the j position does not change and so on. So that is how we do this and then we just keep doing this. So for every place you can just see this.'},\n",
              " {'id': '328bd69b-f510-4c68-9a81-170a3c1329c2',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So for instance here again if you look at this so this is an e and e so then if it is an e and e then the edit distance should be why is that 2 that should be 1 actually sorry because it should be the minimum it should be this plus this. So this should be a 1, so I will fix that but anyway, you can take any position and then check it. For instance if you take an i and e then it is the minimum of this plus 1. So you keep computing column by column and finally you find at this position it it says that if I take the entire string bisect and the entire string secret then I have to make 4 edits. This is compatible with what we know. You have to either delete 2 letters from both strings or equivalently delete 2 letters from bisect and insert 2 letters to make it secrete. Now how do we recover that solution? Well as before we can trace the path of how each entry in our table was made. So this 4 came as a minimum because this is an s and b. This 4 came as the minimum of 3 4 5 plus 1 so we can say that this came from here. Similarly this is an i and an s so these 3 came as a minimum of here to here. Now this is an s and s so this must have come because it is equal to the diagonal entry. So this came from here. So in this way we can trace down for each entry starting from the top left. The final edit distance is the value at 0, 0. So we can trace down all the entries as they were populated depending on which of the values I picked whenever I took the inductive definition and that is the yellow line. Now when we did LCS, the important things for us were when the longest common subsequence was extended and that happened in the diagonal. But here for us in the diagonal, the diagonal corresponds to no edits that is when ai is equal to bj and we directly skip over to ai plus 1 bj plus 1. So we are not interested in the diagonals. We are interested in the verticals and the horizontals.'},\n",
              " {'id': '977996d8-fa70-4b73-9613-3aff97a15c04',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'And the diagonal, yes, if it is a substitution but in this particular example there is no substitution so that does not matter. So here for instance we look at this first move. So this first move corresponds to a deletion. So deletion is when I remove the first letter. So, i increments to i plus 1, so i go to the next row and j remains the same. Similarly this one again is a deletion. So, I am deleting the i. Now I am doing nothing until I come here and at this point where I am at this particular thing, I move right and this corresponds to an insertion because I am keeping j fixed but I am increasing, I am keeping I fixed but I am increasing j that means I am taking the first value in the second string and inserting it and then deleting the two together. So I am actually inserting this r and then in the next step, I again insert an e. So by looking at the positions where changes happen if you see why each entry was added to this table, you can recover what is the corresponding operation that added that entry because each entry has a unique meaning horizontal line, a vertical line. The diagonal has 2 meanings but you can check that depending on whether the 2 values are equal or not. So this says you delete a b, delete an i insert an r and insert e (Refer Slide Time: 24:43) So the code for this is very straightforward as before. So you take 2 strings, you get their lengths and set up this array of zeros. Now the difference between this and the LCS and the LCW is that you cannot just be happy with the zeros for the boundary because the boundaries are not 0. So this is just setting up those 2 columns to go from 0 to n minus 1. And once you have done that then the rest is just the inductive thing. For every position in your thing, you just look at the case if you ui is equal to vj then you just copy the value from i plus 1 j plus 1 otherwise you look at the 3 neighbors take the minimum and add 1.'},\n",
              " {'id': '04a98e0b-bc3f-45cd-9fa1-0187088bd73b',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 9},\n",
              "  'source': 'Edit Distance.pdf',\n",
              "  'content': 'So the in most dynamic programming things where you are doing row by row, column by column the actual calculation is a very simple nested loop. So the main thing is to get the recurrence right, you have to get the inductive definition right and once you get the inductive definition right and the base case right, everything will follow out so the coding is very simple. (Refer Slide Time: 25:38) What about the complexity? Well the complexity is also the same as the other ones because we are filling up a table of size order m, n because it is m plus 1 into n plus 1 and using dynamic programming as we have done here but also if you use memorization you will end up adding each entry with a constant amount of work. So therefore, if I take a constant amount of time to do m, n operations so the overall thing is order m times n.'},\n",
              " {'id': 'a98d7245-8f69-4032-b39f-86af8d977574',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund String Matching-Rabin-Karp Algorithm (Refer Slide Time: 0:10) The next string matching algorithm that we will look at is the Rabin-Karp algorithm. (Refer Slide Time: 0:18) So Rabin-Karp algorithm takes a radically different approach to matching strings. So, earlier, all our string matching consisted of doing some kind of pattern match character by character between a slice of our list, this text, and the pattern itself. And what we saw with Boyer Mooore is you can optimize this a little bit by skipping forward when you find mismatches. On the other hand, what the Rabin-Karp algorithm does is it actually converts string matching to a kind of numeric equality, to arithmetic. So, it is simplest to describe when we take the alphabet to be the letter 0 to 9. So, these are the digits that we use to write numbers. And clearly, you can take any string over this sigma, this alphabet, and interpret it as a number in base 10. So, if I write a string, using the characters 0 to 9, I can think of it as a number in base 10. Of course, there will be a complication, if there are zeros at the beginning of the string, the number will be smaller than the length of the string in some sense, but we can always take care of that by making sure that we remember the zeros. So, if I now take a pattern, which has m characters in it, it is equivalent to an m digit number, an m digit number in our usual sense in our decimal notation, it is an m digit integer. So, let us call this the number corresponding to p. So, n sub p. Now, similarly, if I take every string corresponding to a slice in my pattern, which is of length m, it will again be an m digit number. So, any m digit sequence of symbols over this alphabet, 0 to 9 can be thought of as an m digit number.'},\n",
              " {'id': 'b6796539-9055-4127-82bd-c9e8922062cb',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'So that means that if I can scan that block, that slice and derive a number n b, then I can just compare it numerically with the number corresponding to the pattern. I do not have to do a one by one digit by digit. So, when I compare two numbers, numerically, I am not doing it digit by digit, so there is a kind of arithmetic comparison is going on. So that makes it much easier to do once I have converted it. So, the first job is to convert it, but if I have converted it, then checking so we earlier said that you could do a string comparison in a language like Python, which says is this slice is equal to p, but this definitely takes order m time. Because it will do it character by character. On the other hand, if I take a number and compare it to another number, then this is a numeric equality, which we can assume we have done it all the time, when we say if A is less than 7, do something, we counted those as basic operations. So, we said that these numeric equality comparisons are just single steps. So if I have converted my string to a number, then this test for each block is actually kind of unit cost. So, how do we convert a string to a number? (Refer Slide Time: 3:20) So, fortunately, this is not difficult to do, we can go left to right and we can incrementally compute it. So, for instance, if my string is 7842, then I will start with saying my number is 0, then I will take 7, 10, times 0 plus 7, so I will get the number corresponding to the first digit, then I will take my current number 7 times 0 plus 8, and I will get 78, corresponding to the first two digits, and so on. So, from 78, I will get 780 plus 4 is 784. And then I will get 7840 plus 2 is 7842. So, it is just a simple loop, which says that you walk through your string, and you accumulate this number by taking the current number multiplying by 10 and adding the next digit. And of course, in Python, we, so this should not be this should be int of t of i plus j.'},\n",
              " {'id': '320a0c66-f3e1-45a0-aa4b-4ec1e9fb6c0e',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'So, if I take the int function in Python, and if I feed it a digit, it will give me the number back. So, this is how we can do it in Python. Now, if we did this for every block from scratch, then we would spend order m time for every block. So, this work of converting our string into a sequence of m digit numbers will again cost us n times m. And n times m was anyway available to us in brute force, which is preprocessing which saves us finally by giving us a unit cost arithmetic equality check is not giving us any saving in general because the preprocessing is taking as much time as the computation step in character by character thing. But fortunately, you can slide the number from one block to the next block without having to do order m work. So, the idea is very simple. (Refer Slide Time: 5:07) So, if I want to go from the block starting at i minus 1 to the block starting at i, what do I need to do? Well, I have a t of i minus 1 up to t of i minus 1 plus m minus 1. And then I am going to look at the next one, t of i plus m minus 1, so this is my old block. And starting at t i is my new block. So, what I need to do is drop this digit and add this digit, I need to drop the leftmost digit, the highest digit in the previous number, multiply by 10, so that I can make space to add a new unit’s digit. So, that is what this thing is doing here. So, you subtract the highest digit, you subtract 10 to the power m minus 1 times, again, this should be int of this number. So, you take the digit the pattern value at position i minus 1 and multiply it by 10 to the power m minus 1 to get that number with all zeros, then you subtract it from the number you had computed at step n minus 1, and you will drop it. And now again, you will take int of the new position and add it to the number that you have after multiplying it by 10. So, this step you do once, for the new numbers, you remove the highest digit add the lowest digit. So, this takes a constant number of steps per block.'},\n",
              " {'id': '653b85bc-a443-44a2-a7e7-4dd89d3ce210',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'So, once you compute the first block in order m time after that shifting the number, as you go, one character at a time through your text takes only a constant amount of effort per shift. (Refer Slide Time: 6:40) So, this gives us this overall Python implementation of the Rabin-Karp algorithm. So, the first thing to note is that we have to do this initial setup. For the pattern, we have to convert the pattern to a number. And we have to take the first block and explicitly convert it to a number using that loop. So, you take the number for the text and the number for the pattern, initialize it to 0 and then run through the first m impositions of each and keep multiplying the current number by 10 and adding the next digit. So, this takes us order m times. Now, the first check is directly with these two values. So, remember that the first block we have to do in this loop of length m, and block, the next block onwards, we will take the existing number drop a digit add a digit. So, we have to handle the 0 case separately from t 1 onwards. So, t 0, we handle separately and if we find a match at t 0, then we append 0 to our POS list. Now, from 1 onwards, so we range through all the other positions, what we do is we first delete the highest digit. So, we delete the highest digit. So, we have n of i minus 1. So, if i is equal to 1 at the moment, we have n 0. So, for n zero, we delete the highest digit, and then we add the digit corresponding to position i. So, now we go from n i minus 1 to n i. Once again, we check whether the number we have generated is equal to the number we are searching for. Remember, the pattern is already been computed once and for all is the number here. And if it is, so then we increment the we add i to our list of positions. And if not, we go ahead. So, this, the important thing is that this shift takes only constant amount of time. So, this is an order n loop, but it takes only a constant amount of time.'},\n",
              " {'id': 'dde842da-50eb-4888-90a5-77dd8f4d1ee7',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'So, if all goes well, then this will take time order m here and order n in this loop to do the calculation, which is much better than order n times m. (Refer Slide Time: 8:41) So, the problem is that we have assumed so far that this arithmetic is easy to do. And we took as a concrete example, sigma to be 0, 1, 2, up to 9, the 10 digits and did it in base 10. Now in general, our alphabet is going to have some arbitrary number of letters. For instance, if we are doing text in a normal keyboard, there will be approximately 70 to 80 symbols that you can type, because you have the capital letters, lowercase letters, the digits, the punctuation marks, and so on. So, then, if you treat this as a number, you have to think of this as a number in base k. So, this will be a base 80 number. For example, if there are 80 symbols in your alphabet, so the units digit will hold a value between 0 and 79. The 10s digit which we call the next digit will be between 80 and 80 squared minus 1. 6500, 6399. Sorry. So, this becomes a problem now, that we have to do this arithmetic in base k. We can do the arithmetic in base k. Wherever earlier we said 10 times something plus num, 10 times num plus t of i. If we do something like this, I can say k times this plus t of i. If I want to remove the leading digit I multiply by something, k raised to the length minus 1. So, everything I can do using k instead of 10. But the problem is that this arithmetic will now be very large in terms of the numbers that I have to deal with. So, the cost that I have saved in going from strings to numbers by not having to do position by position is now transferred into the cost of doing this very large precision arithmetic. So, what is the solution for this? Well, the solution is to actually not do this arithmetic explicitly and correctly, but to reduce the numbers by going to some remainder modulo a prime. So, you pick a prime q, and you do all your calculations, modulo a prime.'},\n",
              " {'id': 'b167e35f-c67d-4f7b-92a1-9107a42cf636',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'Now the nice thing about this modulo calculation, which you can check that if I want to do a sequence of things, modulo like if I want a plus b plus c, mod q, I can do it incrementally, I can take a plus b, mod q. And then do plus c mod q, so I can keep the numbers small incrementally as I go along. It is not that I have to compute one humongous number and then take the remainder mod q, at every stage, I can reduce the number I am working with to number mod q. So, I will have one large number, one small number, one modulus, one large number, one small number, one modulus, and I keep going. So, that is how I do calculations modulo q. (Refer Slide Time: 11:18) So, if I do this, then the situation arises that we might have, of course, two different numbers, which have the same remainder modulo q. Okay. So, let us go back to our decimal example. So, let us assume that our alphabet is actually symbols from 0 to 9. But now I am not doing the calculation explicitly and correctly, but I am doing the calculations modulo some q, and the q that I have chosen is 13. So, my pattern is the sequence 31415, which, of course is the number 31,415. And my text is other long sequence of digits. And I can think of it is a humongous number. But notice that in this text, actually, my pattern occurs, I have 31415, somewhere in my think. So, what I will do is I will first compute for my pattern, what is the remainder modulo 30. So, it turns out that 31, you can divide by 13. And check for yourself that 31,415 modulo 13 is 7, or you can put into Python and use the percent operator, and you will find that it is 7. So, now I have to go through this and I have to check each other, I have to check what is the first five characters treated as a number modulo 7, I have to check what is the next five characters treated the number modulo 7 and go on. (Refer Slide Time: 12:26) So, it turns out that, for example, the first five characters 235902, modulus, 23590, modulo 7 is 8.'},\n",
              " {'id': 'bd9832ac-d4e3-4fed-b353-593d6295a78b',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'Now, if two numbers are different modulus 13, sorry, not modulo 7, modulo 13. If two numbers have different remainders, when divided by 13, they must be different. There is no doubt. So, this is not my pattern, this is not my pattern. So, as I go down, I find a number whose remainder is the same as my pattern modulo 13. In this case, it is so turns out that actually the pattern matches, but remember, I am not checking the number, I am not taking the pattern, I am only checking the remainder modulo 13. So, I am only able to check that what I am seeing in my text module 13, is what I see in my pattern, module 13. And the problem now comes out, if I go further down, and I look at this sequence 67399. And think of it as a number, this is also 7 modulo 13. So, therefore, when I find a mismatch, I am good. When I find a match, I may or may not be good, because it could be that because I am collapsing. So, remember, we discussed this when we looked at hashing very early on. So, you are taking a large space of numbers. In this case, we are taking numbers with five digits, so wea re taking numbers of the form 0 to 99999, and we are collapsing it into remainders modulo 13, which is 0 to 12. So, we only have 13 values in the range, and we have one lakh values in the domain. So, obviously, there are going to be collisions. So, when I find a collision, with the pattern I am looking for, I have to be careful. So, I get these so called false positives. So, I get something which reports the same remainder as my pattern. But then whenever I get that, I have to double check that in fact, that number is the same. So, then I have to go back. So, if I am doing arithmetic, I must calculate the actual number for that thing and check or if I am doing a string match, which might be easier in this case, I am just have to check that that string matches. So, if I find a mismatch, I am done. So, if that block does not have the same remainder as my pattern I am done.'},\n",
              " {'id': 'dbffc675-8d63-40fd-9ca0-7ef7554ec27a',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'If it does have the same remainder modulo this prime as my pattern, I cannot be sure that I have actually found the pattern till I explicitly evaluate that. So, this is the RabinKarp algorithm, the way it would work. If you extend it beyond small alphabets, you have to do some modulo arithmetic to get around the arithmetic complexity of the whole procedure. (Refer Slide Time: 14:54) So, as we saw the preprocessing time is proportional to the length of the pattern because we just have to convert this into an m digit number. And if we are doing it modulo q, then it is m digit number module. On the other hand, we could have a lot of these collisions. So, every time we have a collision, that is every time the block reports a remainder, which is the same as my number, I have to go and check it. So, I have to do a linear scan or I have to do this order m work to convert it to a number and check numerically, whatever it is I have to do order m work to validate whether this positive is a false positive or a true positive. And I could cook up the numbers in such a way that these things all are a large number happen to have the same thing. So, I could for almost all the positions end up having to do this scan, which results in a worst case of n times m. Of course, in practice, it will be very unusual that you will get such a large number of matches which have the same remainder. So, one thing we have seen is that if you have a small enough (())(15:56) like we looked at 10 then you can actually do this arithmetic explicitly, you do not have to go modulo and if you do it explicitly, then we said that it reduces to order n plus m, because you do an order m preprocessing and each shift takes me constant amount of time. So, therefore, processing, updating the number and checking equality is a constant operation.'},\n",
              " {'id': 'e1810873-72fa-4b6d-a7d9-f391c9595eb0',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 10},\n",
              "  'source': 'String Matching-Rabin-Karp Algorithm.pdf',\n",
              "  'content': 'So, in order n steps, I can process the entire string and I can think of n plus m as n in general, because I can always assume we said earlier that m is much smaller than n. The other thing is that we can be like in hashing we can see if we can choose a good q. So, by choosing the prime numbers that we are going modulo, well, we could again make sure that the number of collisions is small, but there is no fixed guarantee as such. Once you go to large alphabets that you are going to actually save anything with this Rabin-Karp method.'},\n",
              " {'id': '24913f83-09d1-432a-94ff-bf7e2938ca14',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund String Matching: Knuth-Morris-Pratt Algorithm (Refer Slide Time: 00:15) So, we will now look at the Knuth-Morris-Pratt algorithm for string match. So, what we said last time was that if we can pre-process our string into this automaton, which remembers the longest prefix match at every stage. So, if I have a prefix match up to a particular letter, and if I read a letter; what will be the new prefix set matches? So, if we have such an automaton, the state I denotes the match up to p of i. And the transition tells us how this match gets updated if I read a letter a at I; then we can do this in 1 scan of our text by starting at the initial state, following this automaton letter by letter, and seeing if we reached the final state or not. So, with a single scan of our text, we can find this pattern. (Refer Slide Time: 00:57) So, the Knuth-Morris-Pratt algorithm is essentially to compute this automaton efficiently. So, remember we said that we only need to look at the last m letters of the text we are scanning to decide, and effectively we are really looking at the last time letters of the pattern. We have to match the pattern with itself. If we are matched a certain part of the pattern and the next thing does not match; then where back in the pattern are we. So, what the Knuth-Morris-Pratt algorithm says is that you actually match p against p itself. And it computes this function match, which is defined as follows, match of a position j. So, j is going to be between 0 and n minus 1, the positions in my string. The match of 0 of j is going to be the number k, if the suffix, the longest suffix if I take my thing here; so this is my pattern, and if I take a j. So, then if I take 0 to j, so the string from 0 to j is going to be 0 to J plus 1, written as this way in a slice. The string from 0 to j has a suffix has a suffix, which corresponds to the prefix 0 to k minus 1.'},\n",
              " {'id': '28a2e340-f84c-407e-b80f-59f4095211c8',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, if I go from 0 to k minus 1 in my pattern and if I look at this word; it is the same as this word, so that is match says. So, match of j says that, if I look at the letters from 0 to j, then the last k letters of this match the first k letters of my string; the string from 0 to k minus 1, the prefix from 0 to k minus 1. So, this is what I want to try and compute in this Knuth-Morris-Pratt algorithm. So, let us assume that we have done it up to j. So, now we want to compute what happens at j plus 1. And so we have computed match of j and match of j is some k; so this is from 0 to k minus 1, and this is from 0 to j. So, match of j is k means that there is a segment of length k here, which matches a segment of length k here; now, I want to extend it. So, the good case is that this is actually equal to this. If this is equal to this, then I will just extend the match. Then, I will say that if match of j was equal to k, then match of j plus 1 is going to be equal to k plus 1, no problem. But what happens if these do not match? So, if I have some letter here which is different from some letter here, which we have indicated by saying they are of different colors. Then, our goal is to try and find some prefix here, which may not be the longest prefix. It might be a shorter suffix, such that if I take the shorter suffix. So, if I read off these letters here, then if I look I will hopefully find an extension here; I will find this orange extension, so let me draw this here. So, then I will find an orange block here; so this is what we are trying to find. So, we are trying to find a shorter suffix which matches a shorter prefix, but this orange letter happens here. So, I can extend the blue suffix by orange and I can expand the corresponding blue prefix by orange. So, this is what we are trying to do. So, although we have called it match in the literature; this is usually called the failure function.'},\n",
              " {'id': '2d94119b-db13-439f-b338-d278599aadf9',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, the failure function has a natural interpretation, which is that if I fail to match here, where should I step back and try again. So, if the match currently fails, then it means that the current prefix which I have built up does not extend anymore. So, what is the best prefix that I can fall back to after this? So that is what failure is computing. So, is exactly what we said for match; we will just use the word fail instead of match henceforth. So, we want to be able to compute this quantity. (Refer Slide Time: 05:06) So, this is how Knuth-Morris-Pratt suggests that we do this. So, we initialize this failure to be 0, which means that remember we had in that automaton that we built by hand, we said that if you see a letter C, which is not at the end of the pattern at any point; then we will go back and reset or prefix that is matched to the empty prefix. So, that is a default assumption that at any point, if I do not find a valid extension; then I will fall back to the empty prefix. So, I will just initialize this failed, if this fail is just now function, which tells me for every position from 0 to n minus 1. It tells me what will be the remember that definition. So, fail of j or match of j is the longest suffix of the first j letters, which matches the first k letters, if it is equal to k; so, the default is 0. So, now I am going to run through this, so I am going to use j to walk through this sequence; and I am going to use k to keep track of the latest prefix I have seen so far. So, k will keep track of the current match, and j will keep track of the next position which I am trying to update. So, in our previous picture, so k is this value, and j is telling me where I am trying to update in this picture. So, as we said before, if the value at position J matches the pattern at position k. So, I have this prefix up to k minus 1, because I have the value k, I have matched k letters.'},\n",
              " {'id': 'b27ab384-5a89-4a46-baea-fd20af7b7f2d',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, 0 to k minus 1 matches 0 to j or 0 to j minus 1 rather; because I am trying to update the value at j, so, I have matched up to. So, I know that in some sense, the fail of j minus 1 is equal to k, that is what I tell you. And now you want to tell me what is fail of j? So, I said that if this is equal to this, the next position is equal; then I can say fail of j is k plus 1, i increment both. On the other hand, if it is not then I have to find that longest suffix; but how can I do that? I go back here. It is effectively going back here and asking where should I go backwards. So, I take the longest suffix that matches this prefix. Notice that this prefix does not include the letter that I had before. So, I take in some sense a shorter prefix, so I take this prefix up to here. And then I look for the longest prefix suffix there, so that takes me backwards; and then I try again. If it does not match, again I go backwards. I keep going backwards, so long as I do not hit 0. Once I hit 0, then I just give up and say j equal j plus 1. If I find it before I hit 0, then at that position I will extend it; so this is how it works. So, I go backwards and I try to find a shorter prefix that matches, if I do find a shorter pre. So, I go backwards without incrementing j, so I still looking at the same j. So, I fixed a j, it had a mismatch; I try to find a shorter prefix which matches, so I go backwards. So, I find a shorter prefix, check if it matches; so if I can extend that and it is equal to j, I am looking at now; otherwise I go backwards again. So, there are two situations; one is I do find it in which case at that position this loop takes over, and I get a new value of k. So, so k is going up and down. So, k was set at something and I am now resetting it backwards. j is the one that is going forward position by position; j starts at 0 and goes to n minus 1, j never goes backwards.'},\n",
              " {'id': '524d6f91-6675-40e1-9b9d-c081f1bf6500',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, I am one by one, I am setting j values; but each time I might reduce the prefix that is matched, because I found a mismatch. And if the prefix does not match at all, then it goes back to 0 and I just leave. Notice when I just increment j on its own, I do not touch it; which means that it has this default value that I set initially remains 0. So, 0 is already set, I do not have to set it. On the other hand, if it is some non-trivial value, it will get set here. If I do find a prefix whose extension matches t of j, p of j; then I will set it here and I will proceed. So, this is how this Knuth-Morris-Pratt algorithm computes this failure function, or what we call the match function in one scan from left to right. So, how long does this take? So, the claim was that the earlier thing that we were doing when we were explicitly calculating the edges in the automaton was taking a long time. It was taking order m cubed times the size of sigma, and we wanted to claim that this thing actually takes time proportional to m. (Refer Slide Time: 09:54) So, how do we show that this takes time proportional to m? So, it is a little bit tricky. So, one thing is very clear that j steps through 0, 1, 2 and it never goes backwards; so j is going to start from 1 and go to m minus 1. So, inside this while loop, j starts at 1 and goes up to m minus 1; so j gets incremented m minus 1 time. So, there are m minus 1 iterations, where j gets incremented. But the problem is there are some iterations like this one, where the while loop iterates and decrements k without incrementing j. We have iterations with k reduces and j is unchanged; so not every iteration of the while loop makes progress with respect to j. I know that j can move forward only m minus 1 times; but what about the remaining times when it is doing nothing? How many times is k going backwards; because unless we can bound that, we cannot claim that overall this thing takes time order n.'},\n",
              " {'id': 'fc84428e-5e45-40d5-ae21-9bf02a551cdb',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'But there is a very simple observation that, you cannot reduce k more times than you increase k, so k starts at 0. There are some iterations where k goes up by 1; and there are some iterations, where k goes down, because the failure function always takes you to the left. But you cannot go left, beyond what you have gone right. So, the total number of times you decrease k cannot be more than the total number of times you increase k. If k does not increase for every time you go down, you must have a matching going up. But the matching going up happens only in one place, where I do k equal to k plus 1. So, unless I have a k equal to k plus 1, I cannot keep doing failure k; because I will be at 0 and this thing will, will not trigger the decrement. So, I have to now see whether I can control how many times k increases; and if that tells me something, then it will tell me how many times k can decrease, across all the iterations. But notice that k is incremented, only when j is incremented. So, every time k is incremented, j is also incremented; but we already saw that j increments at most m minus 1 times. So, k can also increment at most m minus 1; can be fewer, because sometimes j might go ahead without incrementing k. But certainly every time k goes up j goes up. And since j cannot go up more than m minus 1 times, k cannot go up more than 1 m minus 1 times. If it cannot go up more than m minus 1 times, it cannot go down more than m minus 1 times. So, this is a very careful and clever example of this amortized analysis. I cannot promise you that for a given j, how many times k goes down; but across all the iterations of the loop, I can tell you that j goes up at most m minus 1 times. And therefore, k goes down at most m minus 1 time; so between them they have at most two m iterations of the loop. So, the number of iterations of this loop is order n, and which is what I want.'},\n",
              " {'id': 'aa6e1f0a-279f-4d2e-b094-52f1a8fc7d35',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, this is a dramatic improvement, so when we did this naive calculation with the graph based automaton; we were effectively computing this failure function. But we were doing this complicated thing of looking at every suffix, and scanning it and comparing it to every prefix for every letter. And therefore, we are getting this order n cubed times size of sigma; whereas here we have simplified it to order m overall. So, now that we have the failure function. (Refer Slide Time: 13:21) The actual string matching is very similar to that automaton based thing; so we scan t in one scan from the beginning to the end. So, j is the position that we are looking at in our string text, and k is the indication of how many matches state in our automaton; or it is how many positions we have matched so far. So, every time I see an extension that is I am looking at position j in my text. And my next position my string, I have already matched 0 to k minus 1; so I have matched k positions. So, 0 to k minus 1 has already matched; the next 1 matches, then I proceed. And if I have matched up to position m minus 1, if k reaches m minus 1; that means I have matched from 0 to m minus 1, the first m minus 1, the first m characters. So, that means that I can return this particular value; so the particular value I have is j minus m plus 1, which is the current I, where I found a match. So, this is just exiting with the first match, so it is returning; so I am exiting at the position where I have a match. Otherwise, if I have found an equality but I have not finished matching, then I will just increment j and k exactly as we did before. So, I am just saying that my match has progressed, I have increased my prefix match by 1; and I have moved to the next position. Otherwise, like before, we will say that this was not a match; so this is not equal to. So, I will try to compute the best prefix that I have for this letter.'},\n",
              " {'id': 'fbd5d48d-7b17-4abf-b800-c390cd1cd2d1',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'Now, this as we said could take repeated decrements, but it will be the same as before. So, if it is not equal to; I have to update this prefix, exactly as we did when we computed the failure function. But here we are using the failure function which is already given to us; we do not have to re compute it. We are just computing the failure function as it is given to us. So, the failure function remember is given to us; so we have basically called that for earlier function and stored it in this array fail. And then, as before if we find that the prefix is completely vacuous, then I will just increment my position and not do anything. So, at the end if I have not exited, if I have not existed, and I reached the end of this while loop; that means that no point in my text. Did I reach the final state of my automaton, did I reach the fact that I have matched all m positions in my pattern. In this case, I must report that there is no match, I will return minus 1. So, this particular thing is very explicit like we said before, just to simplify the way it works; it only reports the first match. Now, if you want to do the next match, then you have to take this value and restart KMP at that point. So, so we will not bother about that. So, the complexity of this is again going to be order n plus m; for the same reason that this loop is going to take n steps for j. But we also said before that, we have this k going down; and so it is going to be order n actually. So, we have an order n algorithm for matching, assuming we have the failed failure function; and the failure function took order m. (Refer Slide Time: 16:36) So, so therefore, now we have this algorithm in one place; now, this Knuth-Morris-Pratt algorithm KMP as it is called. It computes the failure function first in time order m; and after pre processing, it can check the matches in order m plus n time. And overall, this KMP algorithm works in time, order m plus n.'},\n",
              " {'id': '434b2fc7-baf1-48f2-b509-ea902c0d5f5b',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 10},\n",
              "  'source': 'String Matching-Knuth-Morris-Pratt Algorithm.pdf',\n",
              "  'content': 'So, in principle, this is the best that you can do in case of worst case complexity. In the worst case complexity of string matching, we have to read every letter in our string, and we have to read every letter in our pattern. You cannot do anything better than that. So, order n plus m just says that we are taking enough time to read our entire text, and our entire pattern; and then make a decision about where the first match is. So, you cannot hope to better this in the worst case. So, asymptotically this is the best algorithm that you can come up with. But as we have seen, though Boyer-Moore has a much worse complexity in the worst case; remember it is order m times n. In practice, because you skip over, you actually do not have to see the entire text. So, when you see mismatches, you can jump ahead. And that turns out to be, in a sense more productive than keeping track of the optimum match, and doing it in this linear time. So, we said before that Boyer-Moore is often used in practice; because in real occurring texts, the benefit that you get from that heuristic skipping overcomes this m plus n thing. So, as we said the Unix utility grep for instance uses Boyer-Moore; and so do many other utilities which have a built in search function. So, string matching, so it is a little bit like if you go back to the situation of sorting. So, we said that quick sort in general has a the naive implementation of, quick sort has an order n squared behavior. But in an average case, it actually is n log n; and because of other reasons, it is very fast. So, many built in sorting functions use quick sort, even though it is not theoretically optimum; so, similarly here though this KMP algorithm is theoretically optimum. In practice, very often the sub optimal Boyer-Moore algorithm, which has a worst case (complexity) much worse than KMP, works out to be better in practice. And that is what is used.'},\n",
              " {'id': '82984d1a-4ff9-42ce-bd65-ab5d2971bac7',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': \"Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund String Matching-Tries One of the motivating examples that we gave for string matching is using something like a text editor. So, when we are editing text, then we are generating text. And as we are generating the text, we are looking for some other patterns to make changes. (Refer Slide Time: 00:25) But quite often, search happens in the context of a piece of text that is fixed. So, for instance, you might be trawling through a large body of work by a single author, like the collected works of Shakespeare. Or you might be in an organization which has a large body of reference manuals. Or in our other example that we have frequently cited, you might have genetic data for some organism or organisms that you want to search in. So, this is a more typical situation where you have a body of text, which is more or less fixed, but this text is going to be searched repeatedly. So, what would we search for, for instance, if it is Shakespeare's plays, then we might have a quotation that we want to search and find which plate came in and who spoke it. So, this is a typical example where a number of quotations are very famous from Shakespeare's plays. So, when we come across one we want to know is this a quotation from Shakespeare or not. So, we query this text, which is fixed. Similarly, if you have some question about something to do with the operation of your organization, or your plant machinery or whatever, you might want to query the reference manual for a part number, or a process. And finally, when you have a gene sequence for an entire organism, and you want to find out if a particular pattern is there, you might want to query this or you might have a database of such genetic material from a number of organisms, and you may want to look for it across this database.\"},\n",
              " {'id': '2b66929d-f577-4ff4-aad1-754ec602eb9f',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, in all these situations, the difference is that we are searching for a pattern repeatedly in a piece of text that is not changing at all. So, we could hope, since the text is not changing, we could hope to do something with the text that is to process the text and make it more efficient to search for this pattern. So, that the cost of searching for the pattern does not depend on the text at all. If you remember, when we did things like Knuth, Morris, Pratt, and so on, we said that we would do things in time order n plus m, where n was the length of the text, and m was the length of the pattern. Now, if we have a fixed pattern, we would like to make this dependence on n, or the fixed piece of text, we would like to make this dependence on n go away, we would like the search to be only proportional to the length of the pattern. So, how can we do this? (Refer Slide Time: 02:48)   So, one way of doing this is to record the words in our text in a special kind of data structure called a try. So, the word try comes from information retrieval. So, it is hidden inside the word retrieval. So, as you can imagine, in retrieval, it is pronounced tree and that is how the original inventor of the tribe intended it to be pronounced. But since we use trees, all across the place TREE all across the place in computing, it has become conventional to call this data structure at try. So, it is a special kind of tree. But to distinguish what special kind of tree it is, we call it try? What about information retrieval itself? Well, information retrieval is the traditional subject of this kind of search that we are talking about. You have a body of text and you want to search for it. So, for instance, this has been the case with libraries. For instance, if you are looking for a book in a library, or in the legal profession, you have cases and cases which have taken place in the past. And when you come across a new case, the lawyers would like to look for precedents.'},\n",
              " {'id': '238d5265-cffa-4d44-ae5a-ad97cb98061c',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, they would like to search for earlier cases which have similar features to the current case. So, information retrieval is a traditional subject, which far goes back far beyond modern computing. But in the modern context, whenever we query a search engine, what we are doing is information retrieval. We have a body of text, namely all the texts on the internet, and we are searching for a pattern against this. Now, of course this body of text on the internet is not a fixed piece of text, because it keeps changing. People keep adding and modifying documents. But what search engines do is they try to index or pre process documents once in a while. So, that effectively when you make your search, it is not looking at the current state of the documents, but at the state of the documents as they were when they were indexed. So, it is as though the internet is a fixed set of documents. And you are searching in that so all of these falls under this broad category of information retrieval, and the try was invented to deal with that. So, what does a try look like? It is a special kind of tree as we said. So, it has a root so it is like our search trees and heaps and other trees that we have seen an at every node other than the root, we have a letter from our alphabet sigma. Now, one of the properties is that if we look at any node in the tree, and we look at the labels of its children, remember that each node is labeled by a letter from sigma, these labels are all going to be different. So, you will not have two children with the same label. So, that is the property of a try. So, on the right, we see the try corresponding to a set of words. So, remember, if you have a fixed text, you can decompose if your only let us have a simple situation where we are looking for words in this text, we want to do a keyword search. So, we are only interested in recording in some sense, the dictionary or the set of words that this text contains.'},\n",
              " {'id': '3bd6c21b-27b7-47de-b972-a0db8da16a9d',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, suppose this text contains these words bear, bell, bid, bull, buy, sell, stop and stock. So, what we have is a kind of clever organization of these words in this tree form, so that if you walk down and follow any path, so for instance, if you follow this middle path, then you read off the word bid, which is one of the words in your dictionary. If you follow this path, then you read off the word buy which is also a word in your dictionary. So, every maximal path, in our try denotes a word in the dictionary. So, here for instance, we have stop, and here on the right, we have stock. Now the fact that children have distinct labels means that essentially we keep prefixes together so that we do not have one path per word. So, all the words starting with b pass through this node b, all the words starting with s pass through this node labeled list. Now buy and bull start with b and u so up to here, there is a common prefix and then they diverge. So, in this way at try compactifies the words by sharing the prefix as far as possible, and then taking us to the end. Now, there could be some ambiguity about where a word ends. So, we saw this if you remember, when we did Huffman coding, we talked about the fact that when you are decoding a code, you need to know where the symbol should be broken, in order for us to be able to recover the letters one by one. So, we said that one of the things that we like we wanted a quote to satisfy is that no encoding of a letter is a prefix of another letter. So, we need a similar property here we need to know where the words end so that we know whether a word is there or not. So, for instance in this particular thing, there is no possibly sensible word but you might ask whether the word b e a is a word in our set or not. But, the way we do it in this particular thing is to effectively add an end of letter symbol.'},\n",
              " {'id': 'b228d1d8-2bf1-4606-85ec-71c22b636c65',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, if you look notice this try here just to make things simple, we have denoted the last letter of every word that by a square rather than a circle. But in practice, what this means and we will see later is that there is one extra symbol hanging off each of these. So, each word is completed by adding a dollar to it or some symbol, which is not in my original alphabet sigma. And this signifies that the word is finished. So, that if I see a prefix of a word, then a try will report that it is not a valid word, it is just a part of the word. (Refer Slide Time: 08:18) So, we typically build a try as we said, from a set of words. So, this set of words capital S has some subset of words. So, in this particular case, if you remember, we had about eight words, and totally we have some n symbols across these words. So, if you look at this particular thing, we have 1, 2, 3, 4, 5, 6, 7, 8 words and we have 4 plus 4 plus 3, 11 15, 18, 22, 26 and 531 letters across these eight words. So, now to search for a word as we said, we follow the path. So, for example, if we look for the word bell, then we follow we just read off the path, so, the first letter is b, so we must go this way, the second letter e, so, we must go this way, the third letter is l. And then we come here and then we have implicitly hit this dollar. So, if we finish the word and we have reached a node who successes a dollar, then we have found it. (Refer Slide Time: 09:20) On the other hand, if we look for a word that for instance, is not there at all, then we would start with b u l. And now we want to look for a successor node labeled k but there is none. So, we get stuck here. So, this is one way in which we can fail. The other way in which we can fail as I said is that we will complete reading the word but we do not reach a leaf node that is we do not reach a node whose successor is dollar. So, this is how we search for a word. So, now if we look at this, what can we say about this trie?'},\n",
              " {'id': '6d7702da-a4ab-47ed-8df3-39704115f740',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, first of all, each path is a word in my set S. So, obviously the longest path is the length of the longest word. So, if I take all the words in S, and I take the length of them and I take the maximum, then the longest word will represent the longest path. So, every word is explicitly represented as a path in my try. Of course, because each node has at most, one label for each letter, as a state as a child, a note cannot have more successes, and then there are a number of letters in alphabet. So, this for instance, has three successes. But in my alphabet here, if this is my English alphabet, even in this set of words, we have used 8 or 10 words. So, we could not have more than 26 successes if we are using lowercase English letters or other anything. But not everything will have all successes. So, b has three successes here, this b i has only one successor because there is only one word with prefix b i. Similarly, s t o has two successes, because we have both stop and stock and so on. Now, each leaf in my tree corresponds to a word remember, each path is a word, and each path will terminate in a unique leaf. So, every leaf is a word. And how many words did I assume we have smallest words in my, in our set that we started with, so the number of leaves in our tree is going to be smallest. Now, in the worst case, we do not have any sharing of prefixes, supposing our set just consisted of maybe bear and say sell, if this is my set, then my try would have looked like follows, I would have had a b here, I would have had an s here, and then I would add e a r, and s e l l. So, I would have had one root node plus, I would have had one node for every letter in every word in my set. And of course, I would have had the dollar nodes which I have not counted, so I would have had two dollar nodes. So, I would have n plus 1 node at most, now in general is not going to be n plus 1.'},\n",
              " {'id': '7e714596-02a5-473d-9b7e-47e79d8137b3',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, for instance, here, remember that we said that decided, I think 31 later, so we could have had 32 nodes not counting the dollar, but we do not have we have much less than 32. If you look at this picture, we have 5 plus 6 11 plus 6 plus 5 16. So, we have about 21 instead of 31? So, because of the sharing of prefixes, the try can be smaller, but it cannot be any worse than the total number of letters in the set that you are trying to encode. (Refer Slide Time: 12:21) So, this is a vanilla try. The vanilla try just keeps track of the words that we have. Now you could use this try to keep more information. So, supposing these words were drawn from a sentence rather than a set of words. So, here is the sentence, see a bear, sell stock, see a bull, buy stock. So, in this we have positions. So, we could have 0 1 2 3, and so on. So, what we might want to do is not just record the words in my try, but also record where they occur in my try. So, if we look, for instance, for the word bid, it says it occurs at positions 47 and 58. That is because you see two occurrences a bid here, we see a bid here. And where else to see a bid so, maybe it is not in two occurrences. So, maybe it is buy, which would be so let us see another word. So, maybe that is a mistake. But let us see a word like see, see for instance. So, see occurs twice. So, the first time occurs right at the beginning. So, if you look at s e e, it occurs at position 0, and it occurs at position 24. So, in this way, what we can do is we can maintain, so when we reach the root node, instead of just saying we have found the word we can store information at that point. So, it becomes like a function, it becomes a function where the words in my text collection are the keys. So, this is a bit like a dictionary stored in a very different way. So, the keys are my paths in this try. So, each key is a word in my text, and the value that I store depends on what I want.'},\n",
              " {'id': '10d626ad-4751-488f-beb9-1b7b8fe1dc4a',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, in this particular try, I am trying to store the positions of the words as they occur in this given piece of text. So, that not only when I find a word do I know it is there, I also know where all it occurs. So, you can think of this, like, as I said, a dictionary and a dictionary indirectly is implementing if you remember a hash function. So, a trie can be seen as a clever type of hash function where the clever representation of a hash function where the key is happened to be words. So, one advantage of a try we know is that every path is unique. So, no two words will read to the same leaf. So, there will be no collision. So, in that sense, it is simpler than a hash function, where you compute some value of the key and you might end up with two different keys producing the same output. And then you have to resolve this conflict. But on the other hand, the difference that we have with a try is that we are explicitly keeping all these nodes so the penalty of not having conflicts is that we are creating as many leaf nodes as there are elements in our set of keys as many words as we have. So, therefore, a trie will be much more space expensive than a hash table. But it is another way of using a trie, which is to just use it as a way of keeping these key value stores where you store the values at the root, or the leaf nodes where the words terminate. (Refer Slide Time: 15:17) So, let us look at a very simple implementation of tries in Python. So, here is a Python class for trie. So, what we are going to do is we are going to keep this tree. So, initially, we start with a root, which is an, so we are going to maintain these children as keys in a dictionary. So, each node will be a dictionary. And it is entries will point to it is children. So, initially, I have a root node, which is the empty dictionary. And now for I can start it off with a set of words, which by default is empty, but what I will do otherwise is for every word in the set, I will add it to my trie.'},\n",
              " {'id': '98515b1c-365e-45eb-8994-dae65cff7398',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, that is this add function. So, the add function inserts a new word in the trie. So, what does it do which starts with the root, then it appends this special symbol dollar to the word, and now it walks through this word, including the dollar. So, it starts at the root, and it looks for whether the first letter in my word is a key or not, if it is a key, it goes down that way, so if there is already a node, say, say my word is able, and my tree already had a b, then what we would do is that we would walk down to a, then we will walk to the b, key of a, and so on. And then finally, when we do not have any key, then we will create a new key. So, that is the idea. So, that is how we add a word to this trie? And now, when we want to check for a word in the trie remember that we follow the path. So, there are three possibilities. The first possibility the good one is that I reached the last node and I reached a dollar after that. So, I can say yes, this word exists. Otherwise, I might reach a situation where midway I get stuck and then I can say no the word does not exist. The third possibility that I reached midway and my word ended the word I am looking for s terminated, but it was not a complete word as far as tries is concerned. So, we return three values True, False and none in this case. So, if I, start at the route and I start walking down if at any point. So, in add when we did not find something we added the key. So, here if it is not in the keys, then we return False saying that we have reached a point where we got stuck. Otherwise, we go to the next key and we keep going down. Finally, if we find that at the end, we have reached a situation where the node that we are pointing to has dollar as its next key then it means (())(17:55) the end of the word we can return True. However, if we have come out with this loop, we are finished processing the tree we have the word that we got and we did not bail out with a false but we did not reach a dollar.'},\n",
              " {'id': '7d287074-af05-4b85-967e-d0e104750ac5',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, dollar is not in the keys then I will return. So, this is an implementation a simple implementation of building a simple try and querying it. (Refer Slide Time: 18:17) So, an interesting and important extension of this notion of a trie something called a suffix trie. So, what we do in a suffix tries we take the set of words that are given to us and we expand it to include all the suffixes. So, supposing I take a word like this aba, aba, then the suffixes are the word itself. Then the word dropping the first letters of baaba, then just aaba then aba, then ba and finally a and then there is an empty suffix, there is always an empty suffix. So, this set is the set for which I want to build a trie. So, if I build a trie for this, so a suffix the suffixes of s are the set of all words such as there exists some other word v such as v followed by w is S. So, I am assuming for now, just for simplicity, that my capital S consists of a single word S. So, I am building the suffixes of a single word. So, here is a trie for instance for this set. So, here is the thing for the empty word. So, I have dollar right at the top saying that after reading no symbols are reached the end of the word, here is the suffix a, a followed by dollar. The next suffix is ba, so here is ba followed by dollar. The next suffix is aba that is aba followed by dollar. So, you can check that every suffix that we have in our word is in this suffix trie. So, as before, what we do is we take the suffixes, and we append a dollar. Or rather, we append a dollar. And then take the suffixes so because remember that if I put w and I put the dollar at the end, then every suffix will automatically have the dollar. So, I do not have to keep on adding the dollar. So, I use a dollar, to mark the end of the word, and then I build a suffix for tries for that by examining every suffix. We will see how to do that. Now, what can we do with a suffix trie?'},\n",
              " {'id': '7f8a0902-8d5f-4777-8c7e-d376b5bbfadb',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'Well, in the earlier case, remember that what we checked was that the word that we had was either of complete word if it 100 percent dollar. Or we could at best check if it is a prefix if it starts at the root and comes down and then our search terminates not because we have not got a successor, but because we did not reach a leaf node. Then we said, this is a partial match, but it is always a prefix, it is the prefix of some word. So, if I want to know whether the word occurs anywhere in a word in my set, then it need not be a prefix just a substring. So, with a suffix try, we can answer that question is w a substring? We can also answer questions like this, how many times does it occur is a substring? And which is the longest repeated substring? So, these are some of the questions that we can answer. So, with a suffix trie, we can answer more questions than we can with a vanilla trie, which just keeps track of the words themselves. (Refer Slide Time: 21:20) So, let us look at this particular suffix tries. So, we asked, for instance, for substring. So, we asked whether abaaba is there. So, as usual, what we do is we say abaaba. And since I am able to process this, it is a substring. On the other hand, if I try baabb, I will say baab, and then I get stuck. So, I cannot finish processing the word. So, when I cannot finish processing the word, then I know it is not a substring. So, why does this work? Well, because the substring is something which is sitting somewhere inside the word, but I know that I have every suffix, so there is a word corresponding to this suffix in my suffix trie. So, wherever my substring starts, it defines a suffix of my word, and that suffix is represented in my trie, so there will be a path for that suffix. And this word, if it is a part of that suffix, if it is a substring, it will process part of it and stop. So, that is why every substring will be represented, because it will be the prefix of some suffix. So, every suffix is there.'},\n",
              " {'id': 'ffc0e583-660e-4013-bde7-6655453757fa',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, if I start where the substring is supposed to start, so for instance, if I look at a valid substring of this, so like aab, then aab is part of this prefix is part of this suffix. So, I would find it by looking at aab, so there is a longer part of it, but I am not worried about that I am only looking for substring. So, this is how we would find substrings and this is why the substrings will always be found correctly. Now, if I want a more like the earlier normal try would tell us about prefixes, we can also check if it is suffix by asking whether after I have matched, I can see a dollar at the next position exactly as we did for the complete match. So, for instance, if I see baa, then I want to see a dollar after this, but there is no dollar after this. So, the word does not it cannot end in baa whereas if I do aba, then though there is an extension down there is also one way of stopping here because one of the successes of that aba node is a dollar. So, by looking for a dollar, immediately after the match among the successors of the matching node, I can check whether or not the substring is a suffix of the string. Then we also said about number of occurrences. So, supposing I want to know the number of times that aba occurs, so I come down to aba. Now every time it occurs, there will be a completion. If it occurs only once there is only one way of completing aba, but it occurs twice. So, in this case, aba occurs here. And aba also occurs here occurs in the beginning and at the end. So, if you look at the node for aba, then the completion where aba is the tail and substring is just an immediate dollar, whereas the completion whereas the initial thing is another aba followed by dollar. So, in other words, you look for the number of leaf nodes, remember a leaf node in a trie denotes a complete word. So, you look for the number of completions of this substring.'},\n",
              " {'id': '7c934d1d-ff2b-4367-b6e0-5de6687fc6ad',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'And that tells you how many times the substring occurs because every completion of the substring will represent a different suffix. So, that is one way of checking so in this case, because this aba has two leaf nodes below the aba node, I can get that this aba has two extensions in my word, so it has two occurrences. And by the same logic, you can ask, which is the longest such word. (Refer Slide Time: 25:11) So, here, if we look at this, for instance, here is a word ba, which has two below it, here is another one, a, which has two below it. And finally, we have the one that we found earlier, abs, which has two below it. Now, you can check that if you stop anywhere else in this, there are of course, technically this also has to below it because it has. So, yeah, so we will not count the empty word because empty word has every word, kind of it patches every pattern, so non empty words what we are looking at. So, you look at any other position like if you stop here, you have only one leaf below it, you stop here, you have only one leaf below it. So, the only positions are these branching positions. It is not a coincidence that the trie branches here, the trie branches here and the trie branches here. So, we look among the branching positions, which is the lowest one so we again get that not only does aba have two occurrences as we calculated it is also the longest substring which has more than one continuation. So, we are not asking for the maximum number of continuations that we could have got by counting we are asking for which is the longest one. So, we will look into the depth of the node, which has more than one leaf below it. So, these are all various interesting questions that you can answer about your fixed text using a suffix tries, which is expanding your set of words by all the suffixes. (Refer Slide Time: 26:32) So, we can do a simple extension of our earlier construction of a trie to build a suffix trie.'},\n",
              " {'id': '9aea6a5e-d725-4f14-bdb8-b0cb4cb66434',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'So, here, as we have assumed we are doing it with a single word. So, we will assume that the constructor takes as input one word. And what it does, as we said is we expand the word by adding a dollar to it. And now we have to look at all the suffixes, so we have to look at s 0, s 1. So, these are the slices starting at different starting positions up to the end. So, for every i starting from 0 and going up to n minus 1, I start at the root. And I go through the entire thing, and I do the same thing that we did earlier for adding we just add it to this trie. So, we are just processing every so this is just making sure that I get every suffix by taking every starting position in the range, 0 to length of s minus 1. Now, once we have built that, then we have this function below called follow path, which does what we did before. It takes an input and it tries to trace the path in the suffix trie and tells us what is the status of that thing? So, if I bail out in between, I return a none so what happens here is if I am not able to execute this word in my suffix trie, if I some point I get stuck, then I return a none. Otherwise, I go to the end and I return whatever I have got. Now, this whatever I have got, I can analyze. So, have I reached the end or have I not reached the end. So, for instance, if it finds a path, at all, that is the answer is not none, then I have found a substring. And this what we said before, as long as I can process the entire string and not get stuck, I have found a substring because every substring will occur as the beginning of some suffix. On the other hand, if I want to check if it is a suffix, then not only must it be a substring. But when it finishes, there must be a dollar following it. So, I will have to say not only that I have reached a node, which is not the none node I have not bailed out in the middle of follow path. But the node that I have reached has dollar as one of its keys.'},\n",
              " {'id': 'c009d6e4-acdd-4936-ace0-64d79dcbb422',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'Now remember, retrieve not dollar also as we saw those things which had b and then or a and then had dollar here, and then a b below and so on. So, we do have nodes like this, where I could stop here. And it could be that it has a valid extension as a longer suffix, but it also stops here as an terminating suffix. So, I just wanted among my successes, I have dollar. So, it just says that among the keys that I see at the node where I stopped, one of the keys is dollar. So, this is a very simple implementation of suffix tries. (Refer Slide Time: 29:15) So, we said before that one of the drawbacks of suffix tries compared to say using a hash, or a dictionary to maintain a key value store is that it is big. So, let us look in particular for suffix tries, how big can a suffix trie be? So supposing I start with one word of length n. So, can I have a suffix trie which is as long as n. And this is easy. So, if I just have a word with n copies of the same letter, then there are n suffixes from 0 to a to the power n. So, each of these represents a different node in my trie. So, I have this kind of a spine which is the length of my word. But notice that in general, if I have n letters, we have, as we said, n suffixes. Because you can chop of the first letter chop of the sec, first two letters, chop of the first three letters. So, there are order n suffixes. And we said at the beginning that the problem with the trie blowing up happens when these suffixes do not share a good prefix, so they all kind of form lots of paths. So, the question is, can I end up with n square such things that is, each of those suffixes n suffixes essentially creates a new set of nodes proportional to n. So, here is an example. So, we have na followed by nb so here for instances a cube b cube. Now what are the suffixes of this? Well, we have the empty suffix of course, but we have b, then we have bb and we have bbb, then we have abbb, aabbb and aaabbb.'},\n",
              " {'id': 'ad38efe4-10ef-4bb4-b1de-a5079e43d63a',\n",
              "  'metadata': {'chunk_idx': 15, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': \"And now you will see that there are very few common prefixes. So, here is the path for bbb and bbb. So, what we have not drawn is all those dollar nodes. So, there is a dollar node sticking of these things. And there are dollar nodes sticking of all these b’s as well. So, a followed by three b’s as a dollar, b if aa followed by three b’s as a dollar and three a followed by three b’s as a dollar. So, there are 6 suffixes. So, there are these six dollar nodes that we have not drawn. But in addition to that, what do we have? Well, we have, because of this n b's, we have one suffix here of length n, and then for each of the a's, we have that same suffix. So, we have n here, and we have n here. So, we basically end up with n squared nodes, because each of these suffixes is n times n. So, it is n by 2 times n by 2, if you think of the overall length is 2 n. So, in terms of this n, if you think of n is entire length of the word, it is n by 2, but you have n suffixes corresponding to the n a’s, and each of those n suffixes has length n. So, you end up with n squared nodes. So, you can have a kind of worst case suffix try in which you have as many nodes as there are suffixes letters. But this is something that we already knew that suffix tries are not very space efficient. (Refer Slide Time: 32:25) So, to summarize, a try is a useful way to pre process a fixed piece of text. And this is a very common occurrence. As we said in principle, an internet search engine is doing this. So, it is maintaining information about a corpus of documents at some snapshot in time. And then your queries with (())(32:45), that snapshot. Now, it might update this corpus once in a while, but it is not going to do it instantaneously. It is not so when you give a query to a search engine, it is not going out and searching in real time for documents, it is searching against this pre processed index. Now, there are many more things that go into this indexing.\"},\n",
              " {'id': '2cfc8582-9851-416c-98d1-0abc2c678a93',\n",
              "  'metadata': {'chunk_idx': 16, 'week': 10},\n",
              "  'source': 'String Matching-Tries.pdf',\n",
              "  'content': 'And this is not part of what we are looking at. But this information retrieval is a way of taking a fixed piece of text and transforming it into a representation a data structure which you can query efficiently and trie or a suffix trie is one of the starting points of doing this. So, the advantage of doing this is then once you have pre processed it, as we see, searching for a pattern, a string takes time proportional to the length of the string, you just have to run down the trie, you do not have to do anything with the rest of the trie. So, therefore searching becomes proportional to what you are looking for, rather than what you are looking at, which is what is happening in our dynamic text in our dynamic text we had to search all over the place for it, but by pre processing were able to search for it once and for all by just going through the tech the pattern once. So, suffix tries are an improvement on tries because we can make more expressive searches as we said we can look for arbitrary substrings we can look for the number of times a substring occurs and so on. And overall the main drawback that we find in using tries is the size so we there are ways of compressing tries but then if you compress it trie by the trie is going to change then you lose something so it is not always clear that compressing a try will help in a static sense, but if you have to update the try, then you might have to blow it up again and so on.'},\n",
              " {'id': '81cb99d2-ee2a-4baa-9129-fd93a372ce39',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'Programming Data Structure and Algorithms Using Python Professor Madhavan Mukund String Matching: Boyer-Moore Algorithm (Refer Slide Time: 0:14) So, let us look at the Boyer Moore algorithm for string matching. So, what we said earlier was that we can speed up the brute force algorithm which scans each slice at a time by checking whether there is letter in my text which does not appear in my pattern. So, the example we gave was this a in bananamania which does not appear in bulk. And if you are scanning from the right for instance, then we can skip forward from position 0 to position 4 in one step. Because we know that that a cannot match anything in the pattern. But this is of course a special case what do we do if we have a mismatch but that letter does appear in the pattern. Can we do anything intelligent in that case. (Refer Slide Time: 0:50) So, let us look at this example. So, we have this pattern which is called stick and we have the text which is a stack width. So, if we march along from left to right eventually we will reach say this slice that we are examining against our pattern. And Now, when we do a right to left scan, the first letter, which does not match is i in the text and k in the pattern. Now, i does not satisfy our earlier skipping criterion, because it does appear in the pattern here. But what we can Now, argue is that if I want to move this and try for a match I should try to align these i’s. There is no point in shifting this i by one because it is not going to match next time either I know, that I have seen an i. So, I need to take the pattern i and move it to that position. So, I would like to shift Now, by two positions So, that these two i’s become aligned. So, this is the second case. So, we have the first case where the pattern the letter that I have mismatching does not appear in the pattern. And I can just skip over that entire position. And the second case is when I have this match and I align. So, we are going to scan the substring.'},\n",
              " {'id': '1495c78e-a71c-499a-8adb-31d8f1ecf710',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'Next Now, how are we going to find out where this i is? Well, we can pre-compute this, we are not going to go and look at our pattern each time. What we can say is that, in the beginning, we will go through a pattern once and for every letter that appears in our pattern we can compute the last occurrence of that letter inside that pattern. So, we will use last as a dictionary which takes a character and it tells us the position of the last occurrence of that character in the pattern. And of course, if the letter does not appear in our pattern at all then it will not be a key in the dictionary. So, we will able we will be able to check whether the mismatch occurs in the pattern or not by checking the keys of the dictionary. And if it does occur in the pattern then we can find out where it occurs in the pattern by looking up this last information. So, in general, what we see is that we are at some j. So, this is our position j and then we want to shift this by that much. So, we want to shift by the difference between j and the last of c. And of course, if the special case where the pattern does not have that character, then we can shift directly to j plus 1. So, we are at position j. So, this is my last of c. So, if this does not appear occur, So, supposing this had been some other letter like z. Then I would just shift the entire thing after this to j plus 1. So, this was the earlier case that we saw. (Refer Slide Time: 3:22) So, before we formalize this as an algorithm, let us do an example a more detailed example. And this is from the original paper by Boyer and Moore where they introduced these two skipping rules. So, here the text is which finally halts full stop there are two spaces here. So, notice that the text has spaces and so, does the pattern. So, the which finally halts full stop 2 spaces at that point. And the text actually, that we are searching for the pattern is at that. So, as before we started the beginning.'},\n",
              " {'id': '4f76e1a4-7791-440f-9cd5-2b90a072df25',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'So, we are just doing this brute force scan with this optimization. So, we try to scan the string starting the beginning. So, since at space that has 7 letters we look at the first 7 letters of our text. So, we start from right to left. So, we will get a mismatch here. But what do we know, about this mismatch? Well, this mismatch involves a letter f which is not there in my pattern. So, the rule says that if it is not there in my pattern, I must shift this entire pattern to start from here. So, I must shift it by in this case to beyond the f. So, I must shift it to pattern position 7. So, I shift the pattern and now, I am scanning from the i in finally onwards. So, now again I have a mismatch at the rightmost position but this rightmost position is a space and the space occurs here. So, in such a situation I must shift by this much the difference between the two positions. So, I must shift by 4. Because the difference between this space and this space, there are 4 if I add 4 to this I come to this point. So, I must align this thing by shift by 4. So, I shifted by 4 and Now, I align l y space h a l t with at that. So, this is my next thing. So, here if I go from the right then this becomes my first mismatch. So, this mismatch again involves the letter l which is not there in my pattern at that. So, since it is not there in my pattern, I must go past it and start from here onwards. So, I shift by 6 (Refer Slide Time: 5:27)  And my next search starts at the t s of halts. So, Now, what do we see this matches this matches, but this is my first mismatch. Again, it is a space and a space occurs here and there are two apart. So, I have a space in the pattern which is 2 before the space in my text. So, I shift by 2 to align those two spaces and I come here. Now, if I go again there is a space which is my first mismatch and this time, I must shift by 3 because it is 1 2 3. So, I move this beyond that, and get the space to align.'},\n",
              " {'id': '4f241e25-6031-4012-b951-c8286d012ec0',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': \"(Refer Slide Time: 6:04)  And now, actually if you see there is a complete match between at and that So, starting at 22 I have a match. So, I would have recorded this in my list of positions where there is a match. And Now, if I have a match. Then because of the nature of the pattern supposing for instance we saw that if you had all a's in a text and you had the pattern like this. So, if I find a match here I should just move ahead and look for the next match. So, I must go just go one step forward if I have a match I cannot skip I cannot assume that the pattern does not overlap with itself. So, I will Now, move by 1. So, if I find a match I will move by 1 to the next position. And Now, again, I go and I find that the mismatch is a space and the space is the very last position and it must match with this. So, it must keep by 4. So, I move it by 4. And Now, I find at this position the letter n but this n does not appear. So, I must keep this whole thing after this. But if I move it to that position then I started a position which is less than m from the end of my text string so, I can stop. So, the thing to notice that there are 35 positions actually if you count in the string. And notice that we have only scanned about 8 of them or 9 of them 8 I think. So, in practice though the brute force thing starts for every I with this skipping heuristic you can actually skip over a large fraction of the i. In this case, we are seeing roughly about one fourth of them 35. And we are seeing only 8 starting positions. So, even if you subtract the pattern from this and 7 you can say that there should have been about 28 starting positions or and out of 28 starting positions I am seeing only 8. So, it is between three and a half to four as a factor of reduction. And of course, within each scan, because I am doing this right to left scan as soon as I find a mismatch I stop. So, it is not that every scan also takes 7 steps. So, this is the advantage of doing this heuristic.\"},\n",
              " {'id': '27f1eec9-ef86-4141-830b-db3934f6e3a2',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'So, you get this kind of skip over speed up so, you do not even see the whole text. So, that is a very important observation that you can actually go through this text without seeing all the letters. And still be sure that you have found all the occurrences of the pattern. (Refer Slide Time: 8:18) So, let us look at an implementation of this. So, first we have to set up this dictionary which tells us the last occurrence of every character in our pattern. So, we start with an empty dictionary. And then for every character, for every position in my pattern I will update the last for that character to be its position. So, we are going from left to right we want the last position. So, these are several occurrences of a letter. Then the rightmost position will be recorded in last which is what we want. Now, we can do the actual nested loop. So, because we are going to skip over positions we are not going to do a for last time for the brute force thing we had said we will take i to range over every position from 0 to the length of the text minus the length of the pattern. But now, since we are going to possibly skip over we will do this increment manually. Because sometimes we will not want to shift i by more than 1. So, we initialize i to 0 and we as before initialize as list of matching positions where we found matches to be the empty list. So, earlier as we said for i going up to length of t minus length of p. Now, we are saying while i is less than or equal to that value. So, we assume as before that for the segment that we are looking at there is a match and we started the last position in the segment because we are doing right to left. So, this is the same thing we come from right to left right and check whether there is a match or not. So, when we come out of this loop there are two possibilities we have scanned the entire segment and there is a match or we have found a mismatch. So, now we have to do appropriate things depending on what the situations.'},\n",
              " {'id': '483c26ae-e109-4118-9a82-f4e03ef5274f',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'So, if there is a match then we record it. So, we take the current starting position i and append it to our poslist. And we move to the next position in the right this is what we said before because the patterns may self overlap. So, if we find a match we just move one position to the next and restart on matching. But what if it does not? Well, if it does not match remember that we have decremented j after we found a mismatch. So, technically, the mismatch was found at j i plus j. But we have decremented j. So, I am just restoring j here back to where the mismatch was. So, I want the mismatch to be recorded at t i plus j. But since j was decremented inside the loop at the end of the loop after the mismatch was found. I am 1 position to the left of where the mismatch was in terms of what j holds. So, I restore j. Now, I have to check the two conditions. So, the two conditions are whether or not this letter occurs in p. So, if it does not occur in p let us do that easier. So, we check if it is in the keys of my dictionary last. If it is not there, then what do I do? I skip to the right of the pattern and right to that position. So, I go so, this was a mismatch at position i plus j. So, I move forward to i plus j plus 1. So, this is the easy case. What if it was in my pattern? So, if it is in my pattern I need to go by the difference between j and last. Now, the difficulty with that is that the last occurrence of this letter may actually be beyond. So, as an example, supposing it is supposing I am look at a segment which has say a x a this is my text. And my pattern was of the form b x a. So, when I come to this position here, and I find a mismatch between the a and the b a certainly in my pattern but this is the last occurrence of my pattern then it is already beyond the position where I found. So, I should not shift that a backwards and try to align it with this. Because it does not make sense to do that because I have already done that in the past.'},\n",
              " {'id': '081d7940-0478-4477-9cb7-d98be6cef651',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': \"So, if the last of j happens to be smaller than j. So, it is to the left then I will shift by the difference. If the last of j is bigger than j that is I have already seen that and have gone past it already then I will just move by 1 I will do nothing important. So, these 2 things are summarized. So, these 2 things are summarized in this rule. So, you take the maximum of these 2 quantities either j minus last j if it is positive or 1 in case j minus last j is negative than the maximum would be 1. So, this if then else is summarized by just taking the max of these 2 quantities notice that j cannot be equal to last j why can it not be equal because if it was equal that means that these 2 are actually the same letter. If the position which I am looking at is the same as the last position of that letter my pattern I am looking at that position. So, therefore, it cannot be equal to it must be either strictly greater than or strictly less than. So, this is our implementation of the Boyer Moore algorithm. (Refer Slide Time: 13:14) So, though this heuristic is attractive in practice unfortunately we cannot claim anything in the worst case. Because if I take our usual worst case pattern which is a text which has all a's and a pattern which has one b followed by m minus 1 a's then this is again going to take order n times m time even with this heuristic. But I am not going to get any non trivial shifts. So, without the dictionary there is another bottleneck which is that computing that last. So, what we said was that we will only look at the letters in our pattern and set up keys for them and not set up anything for the letters which are not in our pattern. Now, if your programming language does not support a dictionary then you have to actually set up a default value for all the letters which are not in the pattern otherwise you will end up with some kind of a mismatch when you try to search for that index.\"},\n",
              " {'id': 'd73ba10c-80ac-453f-9838-737fd66edaab',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching-Boyer-Moore Algorithm.pdf',\n",
              "  'content': 'In which case setting up that preprocessing could become proportional to the size of your alphabet, you cannot have a kind of array type of thing where you say that there is a catch all value. So, you need to do some kind of hashing in order to do with the dictionary does it for us for free. Otherwise, you have to hash it manually. So, where whatever you do you have to set up something which is order size of sigma. But fortunately in Python, we do not have to worry about it. So, the dictionary computation is actually quite easy for the preprocessing. So, if you look at Boyer Moore in practice. It is actually as we saw sub linear by which we mean that you see significantly less than the number of characters in your string. When you are processing it. We saw that long example which has this property and people have done statistical experiments. So, if you take English text and you take a 5 character pattern then typically you will see something like one fourth of a comparison per character. So, you are Getting a 75 percent saving in some sense on reading the entire text. So, this is the nature of the sub linearity and therefore, you can also imagine that because you are skipping based on the length of the pattern if you find mismatches and your pattern is longer you will make longer jumps. So, as the pattern increases in length actually Boyer Moore becomes the heuristic becomes more efficient because more characters get skipped. So, Boyer Moore though it is worst case no better than the brute force because of these very strong properties that it has with respect to realistic text. It is very often the algorithm which is implemented in utilities which requires text search. So, for instance if you use Unix or Linux there is a utility called grep which searches for patterns in a text file. So, grep actually uses Boyer Moore as its underlying algorithm.'},\n",
              " {'id': 'a5bbd49e-e5ec-4e81-ae20-44effa80acda',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms Using Python Professor Mahdavan Mukund String Machining: Regular Expressions We have seen a number of algorithms for string matching. So, we saw initially the brute force thing then we saw the Boyer Moore algorithm, then we saw the Raven Karp algorithm which transformed it into a numeric calculation. Then we saw the Knuth Morris Pratt algorithm which built an automaton of prefixes and did it in a single pass of the of the text. (Refer Slide Time: 00:30) In all of these, we were looking for a fixed query; a query was a fixed string against a fixed piece of text. So, we have a pattern which consists of 1 word or kind of set of a sequence of words, and we are looking for a match of this pattern against the text. Now, what if we want to look for a real pattern that is something which can be matched in more than 1 way. So, instead of looking for a fixed pattern, we want to look for, let me call it a flexible pattern. So, for example, supposing you are looking for a name, which is spelled differently by different people. So, let us say you are looking for the name Srivatsan and you are not sure whether the name is spelt with a t or with a th. Now, you could of course, give 2 sequential queries, you can first look for Srivatsan with a t alone and then look for Srivathsan with a t and an h. But it would be nice if you could express your query as saying look for either a copy of this. So, that is a more flexible search than looking for a fixed string. Or you might have some idea about the beginning and the end, but not something in between. So, for instance, this very often happens when we are looking for the names of files for instance, on our computer, we want a PDF file, so we know that the extension is PDF and you might remember that you started off the word by saying lecture. So, you are looking for a PDF file whose name starts with lecture, and whose name ends with PDF, but you do not remember what happens in between.'},\n",
              " {'id': 'cdb93cfb-37ee-4e6d-9279-fe9d51fbc168',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, here is an example supposing we want to look for a word that starts with the letters sub and it ends with tion. So, this should match a text which has say substitution because this starts with sub and ends with tion, but also subtraction because this also starts with sub ends with tion. So, this is a different type of pattern. So, the first type of pattern we were looking for is this or that either we want Srivatsan with an h or without an h, here, we are saying we know something happens at the beginning and later something happens and I do not want to describe what happens in between. So, what happens in between is kind of left flexible and finally, you might say that I am not very sure how many copies that are of this there is 1 or more copy of it. So, if I am looking for 1 or more copy of na, it would match it does not match pancreas because there is no na that is only an, but it does match banana because there are 2 copies. So, this is repetition. (Refer Slide Time: 03:07) So, we now have this more elaborate notion of pattern that we want to describe. So, how would you go about phrasing your question? So, you want to take a piece of text and you want to tell this algorithm that is searching for it, please find me sub followed by something followed by tion, how do you phrase that? So, we need a way of describing these patterns. So, remember that we are always working within the context of a fixed set of symbols and is usually a finite set of symbols. So, normally the symbols that we can type for instance, and we said that we will refer to this as an alphabet and we will denote it by this Greek letter sigma. So, all our patterns are with respect to specifying in advanced set of symbols from which the patterns can be built. So, we can say that the letters are a, b, c or whatever, but they are a finite set of letters. So, sigma denotes its finite set of letters, which are there in the text and which are there in the patterns we want to build.'},\n",
              " {'id': '73ab9505-5109-4701-b19d-da9cde988bb1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, when we describe a pattern, then a pattern can be thought of as something which matches a set of words. So, for instance, the word Sub dot dot dot tion matches words subtractions substitution and so on. So, I can describe in terms of this pattern, which all words it matches, so a pattern matches a set of words over this alphabet. So, now what we will do is we will describe simultaneously how to build a pattern and how to associate with that pattern, the set of words that match that pattern. So, this will give us a clear understanding of what each pattern means. A pattern the meaning of a pattern is the set of words that it is intended to match. So, the first and simplest pattern is just a single letter. So, if I take a single letter a then it matches the word consisting of that single letter. So, a matches a, b matches b, and so on. So, in general, a pattern matches a set. So, here the set is just called 1 word, which is letter a. Now, a more elaborate pattern is like Srivathsan with an h or without an h. So, if we have 1 pattern Srivatsan without an h, and 1 pattern Srivathsan with an h. So the first Srivatsan matches a certain set of things, the second Srivathsan matches a certain other set of things. So, Srivathsan plus so Srivathsan with an h plus Srivatsan without an h, will match the documents with match this or that. So, that is the union. So, if I have a pattern p which matches some set, and I have had different pattern q, it matches some set, then the way I express this union in the world of patterns is right plus. So, if I combine two patterns with a plus, it means it matches the set of either pattern. So, either it matches a pattern, which with a word which matches the pattern p or it matches a word which matches pattern q, so it matches overall, is SP union Sq. Then we had that substitution kind of example. So, we had words, which start with something and end with something.'},\n",
              " {'id': '8c0da4e8-88a4-4cb8-a0f2-f978b4531ed1',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, the way we build that is to say that we first have a pattern for the initial part, and then we have a pattern for the later part and then we combine these two, exactly as we do in strings, we can catenae it. We have this string concatenated with that string to form a longer string. So, we have this pattern concatenate with that pattern to form a longer pattern and it will match anything whose first part matches p second, match matches q. So, if I have p, which matches a set, and q with matches set, and I take a pattern of the form p followed by q, I concatenate the patterns, then it will match the set of words which decompose into the first part being from Sp and the second part be Sq. So, it will match a concatenated word where I can split the word so that the first part matches p and the second part matches q. Any word which can be split that way matches the pattern p, q. And finally, we are that banana example. So, we said we can take a pattern and we can repeat it. So, if I write p plus, it means I have 1 or more copies of the pattern. So, if I have 1 or more copies of the pattern, I know how to match 1 copy of the pattern, because p can be assumed to already know what p matches. So, p plus will match any sequence of words, where each of those words matches p. Now, p plus is 1 or more. Usually, in this theory of these patterns, you write p star, so p star matches 0 or more. So, what does it mean to match 0? It means it matches the empty string we will talk about a little later, but there is a technicality. So, p star is the usual notation. So, p star matches 0 or more occurrences of the pattern. So, there is 0 occurrence of the pattern, then it is empty. So, it is matched by the empty string, which does not have any occurrence of anything. But if it is p plus, it must have at least 1 copy of the pattern. So, with this syntax, which might be a little bit overwhelming.'},\n",
              " {'id': '16a44f3b-4c8f-4c00-84fb-6cc678414b41',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, what we had is we had a and then we had this choice, either p or q, we had this p followed by q, and we had this p plus and p star, we did not repeat repetition. (Refer Slide Time: 08:24) So, we can now look at some examples just to understand better what this does. So, if I look at the pattern, a plus b, then it must match the set consisting of the words a and b and that is because we know that each individual letter matches the set consisting of that letter alone as a single word. So, b matches the set b, a matches the set a and now I have to take the union. So, I take a union b, and I get the set consisting of 2 words a and b. So, obviously, if I do it for 2 different letters c and d, c plus d will now match the set c comma d, because c on its own matches a set c, d on its own matches set d. Now, let us look at that sequencing concatenation. So, now if I take this pattern and follow it with this pattern, so I take a plus b, followed by c plus d, then it will match anything which I can decompose so that the first part comes from the first pattern and the second comes from the second pattern. So, the first card can be a or b and the second part can be c, or the first card part can be a or b and second part can be d. So, if I look at, on this side, I have a or b, and on the side I have c or d and I look at all the words I can build by combining this so I can do a, c, I can do b, c, I can do a, d, I can do b, d. So, these are all the words I can build a c, a d, b c and b d . So, each 1 of these can be split so that the first half satisfies p, a plus b in the second half satisfies q, c plus d. So, this is how we build up patterns build up larger and larger sets of words that they can match. So, now if I take this pattern, and now I apply this plus to it, then it will match now, actually an infinite set of words, because remember that I can match any word which decomposes into parts which match this. So, remember that the words I had were a c, b c, a d and b d.'},\n",
              " {'id': 'ef4775b2-c65e-4d06-a9c0-ed913f57d47c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, if I take a c followed by a c, the first a c matches the pattern, the second a c matches the pattern. So, ac ac matches this repeated pattern. Similarly, a c, b c matches, similarly b c, a c matches, but also a c, a c, a c matches because I can have 1 or more repetition. So, I can have 3 repetition, I could have 7 repetition. So, there is no limit to this. So, once I put a plus, I blow up the set that matches typically into an infinite set, I have an infinite set of words, which matches the pattern. So, when I start with this single letters, I only have single letters, when I have plus, I only take 2 sets, which are finite, the union will be finite, if I take 2 sets, which are finite, and I concatenate them from every so I have a cross product in some sense, if you remember this cross product notation. So, I have everything from 1 set, combined with everything from the other set. So, the number of things so here, for instance, I had 2 words and 2 words. So, add 2 times 2 is equal to 4 words. So, that is why this has 4 words, because is a cross b. So, as long as I am working with plus and dot, I can only build finite sets of words, which match my pattern, but the moment I do this repetition, then I go outside this finite set, and I can now talk about an infinite set of words which matches. (Refer Slide Time: 11:32) So, this can be now extended with some shortcuts. So, for instance, supposing I want to match an a followed by b like we had earlier, the string sub followed by tion in between. So, what comes in between here is any symbol from sigma. So, if I assume that sigma consists of k letters, a1 to ak, then a1 plus a2 sells either a1 or a2 we saw that, a plus b is either a or b the word letter a. So, plus a3 will be either a, a1 or a2 over a3. So, if I write this expression, a1 plus a2 plus plus plus ak so if I take the plus and combine all the letters, it says that I can, it will match any 1 symbol from sigma.'},\n",
              " {'id': 'dcb8f8b0-512c-4694-853f-18d91baa70b5',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, the expression, if I take all the letters in sigma, and I just combine them into 1 expression with plus a1 plus a2 plus a3 plus a4 up to ak, then that expression matches 1 letter, but that 1 letter can be any letter in sigma. So, I will use the word the letters, the symbol sigma itself to symbolize that expression, because it occurs so frequently. So, it occurs so frequently that we frequently want to say any letter. So, for any letter, we will use the shortcut into writing a1 plus a2 plus a3 plus ak, we will just write the word sigma. So, now what we want is an a followed by 1 or more repetitions of any letter followed by b. So, this is the same as writing a followed by a1 plus a2 plus ak. Remember that it is finite, so a1 plus a2 plus a3 plus ak is a finite expression plus b. So, this whole thing I am just collapsing as a sigma. There is a shortcut. It is just an abbreviation. So, this is one way so this is saying an a followed by any number of letters 1 or more I do not care what they are ending with b. And of course, if I do not insist that there must be 1 or more and this is why the 0 or more becomes important, I can replace this plus by a star. So, a sigma star b says 0 or more, I have an a at the beginning of my pattern, I have a b at the end of my pattern, I have something in between there may be nothing in between also, it could just be a, b. So, if I say a sigma plus b, then I do not match the word a b, a b does not have anything between a and b. So, that will not match. So, this does not match whereas this does match here. So, sigma star is sometimes needed, if we want to express the fact that I have may have something and I may not have something also. (Refer Slide Time: 14:19) So, let us go back to that earlier example looking for Srivatsan or Srivathsan.'},\n",
              " {'id': 'eb8e03c5-4538-41fc-bfba-34262983747b',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'Now, supposing I do not care where it occurs in my text, then I will say that Srivatsan can be preceded by 0 or more letters from alphabet followed by 0 or more letters from alphabet, I just want to see in the middle the sequence s followed by r followed by remember that if I give you a word, it is like that pattern a followed by b, p dot q dot r, a dot b dot c dot d looks for a, b, c, d. So, Srivatsan these letters in that order will match exactly the sequence Srivathsan. So this is 1 possibility or I want to find any letters which surround the word Srivathsan with an h and I will not either of these so I use the plus. So, this would be the long winded way of writing this. Now, this matching anywhere in my text, that is putting a sigma star before and after, is so common that this is the default when we actually use these expressions, these patterns in searching when we use it in a library, like we do have it in Python, as I will mention shortly, then it actually matches anywhere. It is like when we do a string match. When we do a string match, we are saying look for this string anywhere in my pattern. Sometimes we say I want all the positions, sometimes I just want to know whether it is there or not. So, the default is actually anywhere. So, I we can drop the sigma stars and write that earlier pattern is just Srivatsan without an h plus Srivathsan with an h and this pattern will match these 2 words, either of these 2 words anywhere in my text. But now you obviously see that this is a redundant thing, because there is a lot of this, which is the same except for this letter h Srivatsan and Srivathsan are the same. So, it would be good to say that I have a much smaller difference between the two. It is not Srivatsan or Srivathsan, it is shrivat but with an h or without an h followed by san. So, how do you write that without an h? So, without an h means there is nothing there.'},\n",
              " {'id': '752f22ff-9ca6-46e3-8c38-773cb32eb7b9',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, we need a way of denoting nothing there and that we use this special symbol this Greek epsilon, which stands for the empty pattern. So, the empty pattern matches the empty string, as you would expect, that is no letters. So, now we have a pattern which says, start with srivat then either have an h or do not have an h do not have anything, and then finish with san. So, now this becomes a more compact pattern, which specifies the distinction between these 2 spellings. So, there are many different ways to write the same pattern and depending on how clever you are, you can compress the pattern. But remember that when you write more and more compressed patterns, you need to be able to decode also what this pattern stands for. So, writing these patterns, and then understanding whether the patterns actually express what you think they are expressing are two different things. (Refer Slide Time: 17:13) So, we have now moved to this assumption that when I write a pattern p, it matches anywhere. So, if it matches anywhere, sometimes I am really interested whether it matches the beginning or the end. For example, if I am looking for PDF files on my computer and if I am looking at a listing of all the files, I would like the PDF to be at the end. Similarly, if I am looking for all files, which are lecture something or the other, I would like the word lecture to be in the beginning. So, sometimes I need to specify this anchoring the pattern is the beginning pattern is the. So, if I want to write, so this is now an extension of this pattern notation. So this, the notation varies from, one system to the other, but these are quite conventional. So, you write this symbol, which it is on your keyboard, you will find this up arrow symbol, which is sometimes used for exponentiation, sometimes write 7 to the power 6 like this. So, it is one of the shift shifted symbols above one of the numeric keys normally 6.'},\n",
              " {'id': 'c4cc56fd-a3cd-405e-be57-487e43f33ea1',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, if I write this before my pattern, then it usually means that I am looking for this pattern anchored to the beginning of my string. So, I am only looking for p if it occurs at the beginning of the text. And symmetrically, if I want to find p at the end of my text, then I use the special symbol dollar to indicate that p must terminate. So, if we look at an example, for instance, and if I look at this pattern, it is saying b with a beginning anchor matches banana, because banana starts with ba. Similarly, na with an end anchor matches this. Whereas if I asked for instance, if I said an dollar, then do it matches a string, it does not match at the end of the string. So, an dollar would not match banana. So, if I want the entire string to match a pattern, then I could write that it must the match must begin and end. Now, this might seem strange to you, because it might look like you are actually asking for the pattern to be the string itself. So, what does it matter. But if you look at if you remember my example that I want files, who start with file names, rather we start with lecture and end with PDF. So, my pattern will be something like I start with lecture then I have my sigma star let us say and I end with PDF, and then dollar. So, this is a typical example where you know what the beginning is know what the ending is, you want the beginning to be at the beginning, you want the ending to be at the ending, you do not know what is in between. So, this is an important case where you actually want to specify both ends. So, for example, if I look at ba na dollar, this on its own does not match banana because this matches the beginning. This matches the end, but ba na on its own does not match banana. Because if I stopped with ba na, then it does not match the dollar, it matches without the dollar, you would get it. But with the dollar, you do not get it. But if I put ba and I allow multiple, 1 or more occurrences of na, then na na matches na plus.'},\n",
              " {'id': '36bab1e1-666f-490b-b271-945204b84104',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'So, na na matches na plus dollar. So, this matches there. So, this works and therefore, this complicated pattern actually matches banana from beginning to end. (Refer Slide Time: 20:43) So, what is the kind of rational behind this particular notation for patterns, where did these plus and this concatenation and the star and all come from? So, let us go back to this Knuth Morris Pratt algorithm. So, while we were explaining that Knuth Morris Pratt algorithm, we introduced this notion of a finite state automaton. Because a kind of a special graph, where each node represented a prefix of the pattern that we were trying to find, and when we read a letter, we would take an edge label by the letter which will take us from the currently matched, longest match prefix to the next longest match prefix based on the letter we have done. And of course, if I am going the right direction, each letter would progress. So, the prefix will become longer and longer and once we finish reading the entire word that is the prefix that we have read is the entire word then we will say we have found the pattern. So, we will accept this word otherwise, we will go backwards we will read the wrong letter and then the prefix we have matched may go back to a shorter prefix. So, this is how we process. So, we had this finite state automaton, but it had this kind of a structure. So, ideally, we are going forwards. So, we have the first letter like whatever, banana, but then we might come back at some point and finally, when we reach the end, we say okay, so it has a kind of a linear structure. Now, we could have these automata having branches, there would be more than one way of processing and reaching a terminating thing. So, the automaton that we had built for this Knuth, Morris Pratt explanation has a linear structure, a single path, which takes us to the finish line, and everything which deviates from that takes us back along the path to an earlier prefix.'},\n",
              " {'id': 'c6932455-f53a-4f34-bf7b-624788626191',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'On the other hand, if you take those graphs, you can actually accept on many paths, so you could have a more general graph, which starts at one place, and then it allows you depending on the word to take different paths, so very much like when we are doing things like suffix, trees or something like that, or no tries. So, that is like, I start at the top, it is like an automaton, I started the root node, and I, my letters follow a path and then if I reach the leaf, I accept it. So, we can think of that as one version of an automaton. So, an automaton could generally have multiple paths and therefore accept multiple words, not just that one prefix I am looking for. So, if I look at what kinds of so each automaton now, instead of accepting 1 word can accept a set of words. So, what are these sets of words? They can be formally described mathematically as a particular type of set called a regular set. And the reason that we are looking at these patterns is that patterns also describe sets of words, so each pattern matches a set of words. So with each pattern, I can associate a set. So, we have 2 different ways of describing sets of word, we have automata, which describe these regular sets and then we have patterns where each pattern represents the set of words which match that pattern, and it turns out that this particular syntax that we have used the particular expressions that we have used for our patterns, are precisely capturing the same sets that these automata can capture. So, these automata are well studied. So, they are a very fundamental idea and computing which go much beyond string matching, and we do not have unfortunately, this is not the course to study it, but hopefully, you will study it somewhere. So, this idea of automata is very important in order to capture properties of words and strings and various other things.'},\n",
              " {'id': 'baa803cb-5b5a-4701-aa25-da76abc5ae1c',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': 'For instance, when we use a compilers, a compiler which reads your program and decides whether there is a syntax error is implicitly using a kind of automaton to process your text of your program could decide whether it is syntactically correct or not. So, automata are used quite widely. So, we have seen 1 limited use of it, in string matching, and now we are seeing a kind of reverse use of it in generating patterns. So, the patterns that we can describe using that plus and concatenation and star and so on, are precisely those that can be described automata. As since the automata correspond to what we call regular sets, these patterns these expressions that we are writing, are called regular expressions. So, that is where terminology comes from. So these regular expressions correspond to the so called regular sets, regular sets of words accepted by finite state automaton. (Refer Slide Time: 25:07) So, formally, what that means is that if you give me an automaton any kind of a graph, which accepts a set of words, I can actually c0onstruct a pattern using the limited operators that we described for plus for choice and dot for sequencing and that iteration with a superscript star or plus, I can describe a pattern which captures exactly which matches exactly the words which the automaton describes. And conversely, if you give me a pattern, I can construct an automaton for it. So, earlier we saw that if you give me a word, I can construct that prefix automaton. But you can do it in a more general setting, if I want the all the all the words which match a pattern, I can actually build it. And then once you can build this automaton, you can actually do pattern matching for these patterns exactly as we did string matching in the Knuth Morris Pratt thing. So, what did we do in Knuth Morris Pratt we built this prefix automaton, and then we ran our word through this prefix automaton in one pass.'},\n",
              " {'id': '972d19ae-c4f7-4152-b3f0-1c75f43139a6',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 10},\n",
              "  'source': 'String Matching-Regular Expresssions.pdf',\n",
              "  'content': \"So, we got this linear pass, where we (())(26:03) at backtrack because your automaton would cleverly keep track of the longest prefix which has matched. Now, you can do the same thing in general, for this language of patterns for every pattern that we can write, using these regular expressions, you can construct a corresponding automaton, which will process your string in one pass and tell you whether the string that you have got is matches your pattern or does not match your pattern. So, we will not be able to get into the mechanics of how that works, because that requires us to understand automaton much more deeply than what we have done in this very limited introduction in this week's material. So, you can go and look it up and try to figure it out for yourself. But from a practical point of view, what you need to know is that this kind of regular expression matching is available in many programming languages, including in Python. So, look up the Python library for matching regular expressions and the syntax that we described here is a very minimal syntax. So, Python has an all programming languages to support these regular expressions have an even more expressive syntax for it. So, you can describe the syntax in more compact ways. So, basically, the patterns and more just like we wrote, for example, that sigma is a shortcut. There are other shortcuts which you can use. And this is very useful when you are trying to look for a process text and look for interesting patterns and find them. So, do get familiar with the pattern matching syntax of regular expressions in the Python library and try to make use of it whenever you are doing some non-trivial text processing, because it greatly increases the power of the kind of text processing that you can do.\"},\n",
              " {'id': '49012017-2926-4b1f-8ed0-ce864f7cf203',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms using Python Professor Madhavan Mukund String Matching Using Automata (Refer Slide Time: 0:10) So, till now we have seen two different algorithms beyond the brute force, which work on strings. So, we saw the Boyer Mooore algorithm, which uses the brute force idea with some heuristics for taking care of mismatches and skips over some letters. And then we saw the Rabin Karp algorithm, which does a completely different thing, it converts the whole string into a number and then relies on numeric comparisons rather than string comparisons. (Refer Slide Time: 0:48) So, let us go back now to string base comparison. So, let us deviate back from that numeric comparison to string base comparison. So, what we said at the beginning was that in a traditional brute force, or Boyer Mooore kind of thing, we start with our text and for every starting position, we look at the segment, or the slice which is the length of our pattern and try to match it, usually going from right to left. So, Boyer mooore tells us, that when we are doing this matching, when we find mismatches we can be clever. So, if the position, where we find a mismatch does not appear in our pattern, we can skip beyond that position. So, we can update i to i plus j plus 1. And if it does appear in my string then I can pre-compute this last of c, and shift it, so that the position where it appears in my string, aligns with the position in the text. So, this could allow again to skip over some useless matches. But though this heuristic works very well in practice as we said, it does not change the worst case from that of the Brute force algorithm, which is order n times m, because we could have this trivial pattern, where the same letter occurs repeatedly in my text. And the pattern is just a small variation with 1 b before m minus 1 is. So, now we will try to see if we can beat this worst case and get something, which is proportional to n.'},\n",
              " {'id': '5e66428f-4524-49e5-b4a3-3c23e6bdeb48',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': '(Refer Slide Time: 2:04) So, in the Boyer Moore algorithm, what we are trying to do is, learn from mismatches. So, whenever we see a mismatch, we accelerate the search by pushing our index forward. The other approach, which is what we are going to use in the new algorithm is to remember how much we have matched and see if we can update that intelligently. So, can we intelligently reuse partial matches, that we have seen so far. So, let us look at an example. So, supposing we are trying to search for this is my text, right on top and this is my pattern. So, I am searching from the left, so when I reach this a, b, a, b, and I have not yet found a mismatch in my text, I have implicitly found a prefix of my pattern in the text so far. So, that is the letters a, b, a, b, in orange in my pattern. So, we have a prefix, that we have matched so far. And this corresponds to in general, a suffix of what I have seen in the text, because the position of this text right now is at the beginning, but it could be in the middle. So, if I am in the middle of the text then the part that I have matched in my pattern will not be the entire text, it will be the last few letters in the text. So, the suffix, so suffix is when I take a word and I take something from here onward, so this is a suffix. And a prefix is from the beginning up to a point. So, there is a prefix of the pattern, which is a suffix of the text. So, the last few letters in my text, match the first few letters in my pattern, this is what we generally have as the information, that we get when we are proceeding in a block without finding a mismatch. (Refer Slide Time: 03:49) So, now the question is what happens when we extend the search. So, the good case is that the next letter that I am looking for in this block, matches the next letter in my pattern. So, earlier I had a, b, a, b, and the next letter here is a, and this matches the next letter here.'},\n",
              " {'id': '3d5d95a4-580d-453e-a7e7-12ad4d922d71',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'So, the prefix that I have matched grows from a, b, a, b, to a, b, a, b, a. So, this is good. But now what will happen next is that I will find a mismatch, because in my text I see a b, but my pattern requires me to see a c. So, if I were doing something like Boyer Moore, or brute force at this point I will now shift my entire focus and restart a scan from scratch, I will shift to a new block, I might shift to a different position in that text. But I will essentially start scanning my pattern with zero information about any match. Instead, what we can observe is that, if we are at this position, we still have a match. So, there is still some part of the text we have seen. So, we can believe that we have matched a, b, a, b, even though we failed in our last attempt to extend that original pattern to c. So, I will reset this longest match. So, now we are in the situation, where my text has come up to here, so notice now that I have a suffix of my text, which matches a prefix of my pattern. So, now if I proceed I will see that the next letter matches this, the next letter matches this, and I will report success, without having to go back. So, this is the main message here, is that if we can remember somehow, how much of the pattern match remains valid then I can believe that I have actually matched that much. So, instead of restarting from here and going forward, which is what I would do, I can already kind of you know internalize the fact that I have matched 4characters without scanning them and proceed from the fifth character onwards. So, this is what we are trying to do. So, how do we do this reset, well we are going to do it by pre computing. So, just like in the Boyer Moore algorithm, we learnt how to align letters by doing this last computation. So, here we are going to see, if I find a mismatch and if my current pattern was so much, then what is the remaining pattern, which continues to match after this. So, that is what we have to calculate now.'},\n",
              " {'id': 'd611400f-0667-4a98-87c3-6231c400a151',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': '(Refer Slide Time: 6:17) So, we will use a graph of a special kind to do this precomputation. So, let us assume that my pattern has length m as usual. So, I will have a graph in which I construct m plus 1 nodes, 0, 1, up to m. So, let us assume that m is 6, so we are doing it with our example a, b, a, b, a, c, the same example as we had before. So, this is length 6. So, I will create 7 nodes 0 to 6. Now, what I will do is, I will construct edges. So, the edge will tell me how much the match has extended. So, the interpretation of node i is that I have this sequence I have seen so far matches up to position i. So, if I have seen an a, so remember that my pattern is a, b, a, b, a, c. So, if I have seen an a, starting with the empty string, then I have matched up to position 1. So, I go from 0 to 1, in my graph, after I see an a, if I see a b, then I have matched two positions, so I go forward. So, as long as I do correct matches I keep going forward. (Refer Slide Time: 7:22) So, let us continue and finish a few more edges. So, a to b takes me from 0 to 1, b sorry from initially an a takes me from 0 to 1 after that b takes me 1 to 2, if I see an a after that then I match 3 characters and so on. Now, last time we saw in the previous example, that you could have a situation that after I see a, b, a, b, a, and then I see a b here, where I wanted to see a c. So, here what will happen is I do a, b, a, b, a. And now instead of proceeding forwards I want to be able to compute that I have now seen a prefix of length 4. So, I would like to say that if I have seen the prefix of length 5, which matches a, b, a, b, a, and the next letter is a b and not a c, then I do not give up hope, I recognize that I have still seen a, b, a, b. So, instead of seeing a length of 5, I have now seen a length of 4.'},\n",
              " {'id': '78565bd8-da88-41c3-979f-cbeee408c81d',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'So, this is what we have to do technically, so we have to go from a position i, so any position in this graph and if I read a letter a, then I must go to a new position, which tells me how much of a match survives. And this is all within the pattern. So, if the position matches then I just go forward, if the position does not match then I will look for the longest suffix, that matches. So basically, I can do it by brute force I can look at a, b, a, b, a, b, and I can look at my text a, b, a, b, a, c, and I can ask myself at this position, what is the longest prefix that matches. So, I know that the longest prefix that matches has to be less than or equal to length 6, because that is the length of my pattern. So, I can ask does a pattern of length 6 match. So, I can take 6, as my initial thing and check and then I will find that it does not match. Then I can ask whether a pattern of length 5 matches. So, if I link 5 matches I must start here, again it does not match, because this a is equal to this b, then I can ask whether a pattern of length 4 matches. So, then I will start here then I can say this b matches, this a matches, this b matches, this a same matches and I can say yes. So, I can exhaustively for every letter that I add I can take the pattern which I have right now, look at the last m characters I have seen. And go through that and look for shorter and shorter suffixes. So, I start with the longest possible suffix m and see if it matches the entire pattern, if it does not then I start with m minus 1 and start from the second last position pattern and so on. So, this is a brute force calculation, which will take me order m squared to add each edge, because I have to try m different lengths and each of these things is going to be time proportional to m, m minus 1 and so on. So, it is going to be the usual summation 1 to m. So, this is an expensive operation right now, we will see how to fix it.'},\n",
              " {'id': 'c01ea05c-c1f6-4455-8b1d-47792f00f1a3',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': \"So, I can add each of these edges explicitly, that is the but the point is I can build this graph. So, building this graph is possible, so what is graph say if I have matched up to i and then I see a letter a, what have I matched up to? So, in this graph technically, if I take my sigma to be a, b, c, then I should have letters from here for all an edge for every letter. So, we have only drawn the letters for a and b except for this one, because everywhere else if you think of a c, whenever I see a c, then the suffix ends in a c, and the only prefix that ends in a c for my pattern was a, b, a, b, c. So, there is no shorter prefix which ends in a c, because remember I am taking the beginning of the pattern. So, all c's will take me back to 0, every time I see a c. So, we have not drawn it just to avoid cluttering the graph, but every edge from here on a c will take me back. So, this graph has to be complete in that sense, it has to say that for every position and for every letter, how do I go to a new position, which marks the new prefix, if I was at this prefix before. But the point is as I said this can be done by brute force, but you cannot ignore. So, this graph has a few edges missing, simply because it clutters up, but all those edges that are missing actually take you back to 0. (Refer Slide Time: 11:45) So, this graph how do we use it? Well this graph is something that is called a finite state automaton. So, it is a special kind of graph, so we can interpret these numbers, these nodes as states. So, it is a state in the sense, it tells us something about how much the computation has progressed, it is telling us that we are in a state in which the pattern up to some position j has matched. And the edges are what are called transitions, if I see one letter I move from this state to another state. So, an example of a state example, if you take a lift for instance.\"},\n",
              " {'id': 'c74f40dc-3b96-4599-ad64-a563e71a7321',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'So, a lift has is sitting with its door open, now if the action is to press the door close button, then it goes to a state where the door is closed, now if you press a floor number, it will go to a new state, where it is on a different floor, then the door will open. So, there are many states, so we go through these states and each state goes to the next state by some action happening, the door opens, you press a button and so on. So, here the actions for us are reading a letter and the states are how much of the prefix is matched so far. So, that is the graph that we have constructed. Now, how do we use this? Well we process our word through this graph. So, we run it in some sense on this automaton. So, we start in this state, which is our initial state. So, initially we have seen nothing and therefore the prefix that we have seen is the empty prefix. So, remember that 0 corresponds to the fact that I have seen up to but not including P 0. So, this is just the empty string. So, this is how much of the pattern I have seen so far. Now, I read so remember the example was that I was reading a, b, a, b, a, b, a, c. So, this is my text and my pattern was a, b, a, b, a, c. So, when I read the first a in my text, I just follow this edge and I come here. So, it is very simple I just read the next letter of my text and follow the edge, that the automaton tells me to follow. So, I am now in state 1, I am now going to read a b, because remember it is a, b, a, b, a, b, a, c. So, we are this is the string time processing. So, a I read a b and now here, now I have finished a and b. So, I read an a and I go to 3, I read a b and I go to 4, I read an a and I go to 5. So, up to this point I have been following my pattern faithfully. (Refer Slide Time: 14:06) Now, if you remember this is where my pattern did not match, because I should have see I should have liked to have seen a c, but i see a b instead. But now my automaton from here tells me that if you see a b go to 4.'},\n",
              " {'id': '23d3ecfc-9d69-4bfd-8ada-fb29b4967113',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'So I go back to 4. Now, I have got an a and a c to do, so I see the 5, then I see a 6 and I am done. So, in this automaton, when I at any point if I reach the end of my pattern, it means I successfully saw the full prefix of my pattern. So, I actually matched it, so we have found a full match for P and notice that we were able to do this without going backwards and restarting our scan on the string. So, we just started from the beginning of the string and we just kept on going forwards and every time we saw a mismatch, the this automaton, that we had precomputed will tell us, where we are with respect to the current state of the match, how much of the match can we assume, we have done so far and just keep going. So, that going back and restarting is subsumed by this memory that I have saying that now I have seen actually a, b, a, b. So, I can go forward. So, the single scan of our text suffices. (Refer Slide Time: 15:20) So, to summarize, if we use this approach, where we are trying to remember some information about the matches, that memory we can store in this automaton, this special graph. And once we have built this graph, then we can scan our text in one pass and in order n time, we can find the match. Now, notice that what we said was that any time you reach the last state you will stop and report success. Now, of course you can then increment by one and restart this whole process by taking that suffix of your string, starting from the match plus 1. So, it is not very difficult to adapt this approach to look for all matches, but right now this approach is easiest and least messy to describe. If you are only talking about the first match. So, the first match can be found in order and time, once you have the automaton. But the problem is computing the automaton.'},\n",
              " {'id': '867128da-475b-4426-8027-9065dadebf54',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 10},\n",
              "  'source': 'String Matching Using Automata.pdf',\n",
              "  'content': 'So, what we said was that every edge with brute force, we will look at the longest prefix, longest suffix, then the second longest suffix and so on, to find out which is the largest suffix, for which I have a prefix match. So, that itself took m squared time for each letter and each i. So, I have to do it once for every letter and I have to do once for every i. So, there I have to, so every i there are how many states are there? Well, there are m, m plus 1. So, order m states and there are as many letters as there are in my alphabet. So, I have to multiply m squared by m, to count for every state and by sigma to count for every letter. So, this actual computation of this automaton, if I do it the way, we described it here, in brute force with actually going to take a lot of time, it is going to take m cube times the size of the alphabet. So, what we are going to see is that, this can actually be done in time proportional to the length of the pattern. So, in order m time and this is this clever algorithm due to Knuth Morris, and Pratt which we will see next.'},\n",
              " {'id': '97edab86-a318-4a20-aa4d-5dabc7db888d',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': 'Programming Data Structures and Algorithms Using Python Professor Madhavan Mukund String Matching So, this week, we will turn our attention to a specific class of algorithms for matching strings. (Refer Slice Time: 0:17) So, quite a lot of our computation these days deals with text. So, most of our interactions with computers is in terms of word processors or searching the internet. So, we are constantly typing text and we are manipulating text. And searching for a string in a piece of text is a fundamental operation and all these. For instance, if you have a text editor and you want to replace some erroneous text by some correct text. You need to find all occurrences of the text that you want to replace or if you make actually a query in a search engine. Then what the search engine is trying to do is to take the query that you have put in as a piece of text. And find all the documents that it has in its index database, which match that piece of text. And we have already seen applications outside this conventional text processing. For instance, in genetics, you might want to look for a particular gene sequence in some string of DNA. So, in all of these cases, we have a pattern of text which we want to compare against some given text and find out if there is a match or not. So, if we look at this formally. First let us look at it informally. So, if you look at for instance the pattern to be the word an. And we have a text which in our case here is just a small word banana. So, we want to find out where all it matches. So, we would find for instance that it matches here. So, there is the first an which appears right after the b. And then there is one more match after that. So, there are two positions where this pattern matches this piece of text. So, a general question we could ask is given a piece of text and given a pattern which is another piece of text, and in general, we will assume that this pattern is not empty.'},\n",
              " {'id': '3b4a9250-b07a-4c64-8d42-d8b235e0b86e',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': \"Because it is empty of course it makes no sense to match it. So, let us assume that we have a non-empty pattern and we have some non-empty text. So, the text will be of some length n there will be n character. The pattern will be of length m. And of course, m has to be smaller than n if you have a longer pattern than the piece of text. If I look for banana in the word an I will not find it. So, you can assume that m is less than n for sure. And usually, we will assume that it is much smaller I mean for instance, you type a search query in a search engine, it will be a few words you are searching for documents, which contain hundreds of words. So, usually the same length of the pattern we can assume is much smaller than the length of the document. So, we are really interested in doing something which is kind of optimized in both these quantities. So, these letters that we use come from some set of symbols which is usually called an alphabet. So, this Greek letter capital sigma usually denotes the set of valid characters which I will encounter in the search. Now in most of our day to day use we deal with the characters we can type on the keyboard. But you can imagine that you can do it in different languages or over different symbol sets. So, some of the algorithms they may be a dependence on what the size of this alphabet is. So, if it comes up we will refer to it explicitly using sigma. So, the search problem that we are really interested in is to find every match. So, since we assume that the pattern is of length m we are looking at the slices of length m in our text. So, start at a position i and go up to i plus m minus 1 so that is a slice i colon i plus m. We want to check which all slices of this form match p. And we would like to report every such i. So, every i so, in particular if we had a piece of text where patterns overlap. For instance, we have a a a as our text. And we have 2 a's as our pattern. So, this is my pattern and this is my text.\"},\n",
              " {'id': 'dbe9fb55-f39f-40e7-8aec-12828d770880',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': 'Then you would report position 0 because at position 0 there is a match for my pattern. But you would also report position 1 because at position 1 also if I start looking at the slice of length 2 I have a match for my pattern. So, these positions need not be disjoint. So, we want every i so, that the slice of length m starting at i matches p. (Refer Slice Time: 4:08)  So, there is a very natural brute force way to do it which is to just look at every slice starting from i and compare the letters one by one. Now, of course, in Python, you could write this as something like this. You could just compare as strings whether the slice of length m starting from ti is equal to the pattern P. But this equality here hides of course an implicit search character by character. So, to make it explicit when we use our code here we will not use string equality we will just use letter wise equality and write our explicit loops. So, what we have here now is a loop. So, we start first of all we are going to report all the positions. So, we need to keep track of all the positions in the list which we initialized to be empty, and if I am good to look for a pattern of length m in my text then it cannot start further than n minus m from the end. Because if it is less than m characters from the end then the pattern will not fit. So, I need to go only up to length of the text minus the length of the pattern. So, I start for every position from 0 to the length of the text minus the length of the pattern. And for starting from there I initially assumed that the slice matches. So, I have a Boolean matched which I assume is true which says that I have not found a mismatch in this slice yet. And I start with the first position in the slice which is i plus j is going to be I am going to go from i plus 0 i plus 1 i plus 2 up to i plus position length of p minus 1 m minus 1. So, I start with j equal to 0.'},\n",
              " {'id': '62a1efae-ac51-4b11-90f9-cd898b9ff3c3',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': \"So, long as I am within that slice and so long as I have not found a mismatch I checked the next position in the slide. So, I check whether the text that i plus j is equal to the pattern at J. If at any point I find that the text at i plus j does not match the pattern at j I immediately flag it. I say that the mismatch has occurred so, matched becomes false. And then I increment my j. So, when I come back here, I have 2 options. Either I have finished the slice so is become length of p or I flag the mismatch so, match has become false. So, when I come out to the loop if I still have match set to true it must be that I reached the end of the pattern without finding a mismatch. So, I can append the current starting position, which I started with right now, to my list of match positions. And if matched is false, then I found a mismatch. So, this i can be skipped. And I can go to the next i which is happening here in this for loop. So, this is a very natural left to right. scan position by position. And the only optimization we have done is that we have basically made sure that we come out of each slice as soon as we can. So, whenever we find a mismatch this while loop will exit. So, in particular, if I have a text, which looks like a, b, c, d, e and my pattern looks like x, y. Then the moment I look at a b I will say that a does not match x and I will quit. Then I look at b c I will look at a dozen b does not match x and I will quit. So, if there is no match at all between my pattern and my text, it will very quickly go through all the positions without going through the full slice of each position each time. Nevertheless, this is in general going to be bad because in general I would have a lot of overlap ending with a mismatch. So, I could have for instance a text which has n a's. And a pattern which has m minus 1 a's plus b.\"},\n",
              " {'id': '2617d1cd-60c6-4763-8a69-e9687667e373',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': 'So, now what I am going to do is I am going to for each slice I am going to first look at the first m minus 1 letters and believe there is a match. And then when I reach the last letter I find there is a mismatch. And only then do I get out of the loop. So, for each of these will actually go through the entire length of the pattern before it finds a mismatch. So, in general, I am going to have a complexity of n times m. So, I actually we remember that it does not actually go to n it goes to n minus m. But since m is small we can assume that the number of positions we scan starting positions we scan is order n and for each of those we are going through this slice of length n. So, this is the trivial complexity for a brute force algorithm. Now, we can do a simple modification of this which is also brute force. But the only thing is that instead of so many scan the slice i to i plus m. So, we were scanning it from i to i plus m we could also scan it in reverse. So, this is a reverse string match. So, the only difference comes here. So, we initialize j to be the last position in the slice and we go backwards. So, otherwise, there is no change. So, if you compare this code with the previous code you will find that the only difference is j equal to 0 j equal to j plus 1. And the while condition j less than length of p is matched to J equal elect p paraffin j greater than equal to 0 and j equal to j minus 1. So, all we are saying is that we are we are processing t from left to right. But each slice within t we are processing from right to left. So, there is no difference of course this is still order m n. So, this kind of a thing we will find quickly if we are doing right to left because the very first character from the right will mismatch. But if we correspondingly put a bad string which starts with the first character having a mismatch. Then we do it right to left again we spend order m time before we find the match or the mismatch. So, this will also be ordered m n.'},\n",
              " {'id': 'd6a7ce62-73f8-4b6e-bb54-7cfe97c4005c',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': 'So, why are we bothering with this? I mean, why would the reversing the scan b of any use because it just looks like a different version of the brute force. (Refer Slide Time: 9:38) So, we can it turns out speed things up. So, supposing that while we are matching we find a letter in the text which does not match any letter in the pattern. So, for instance supposing our text is this word bananamania and the pattern is bulk. So, if I scan from left to right then at this point, my first slice for instance b matches. And then the first mismatch I find is at the second position. At this position, I find the letter a in my text and the letter u in my pattern. Now, a does not appear anywhere in my pattern. So, there is no way that bulk can overlap with that position where a is because there is no letter in my pattern which can match the a. So, what we can do is in this situation? We can skip the next scan, the next scan would have been to start bulk from the position a but we already know that the position a is useless. So, we can take our pattern and go past it. So, we can directly skip one position and go and start at the third position. Suppose instead we were doing our reverse scan. So, we have the same situation. But now we start from the right, and we find a mismatch with the last letter which is again, an a against our k which has the is the last position in bulk. So, once again we know that a does not occur. So, it does not matter what is before that we know that we have to shift this entire word after the a. Because there is no point in trying any of these intermediate things because all of them would overlap this a and if I overlap an a it cannot match my pattern. Because my pattern does not have an a anywhere. So, in this case I would actually shift it to the right by 3 positions rather than by 1 position.'},\n",
              " {'id': 'ead0a90a-feb3-4fc5-9592-954409b9c4de',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 10},\n",
              "  'source': 'String Matching.pdf',\n",
              "  'content': 'So, therefore, we could have a faster skipping if we are using this kind of a heuristic to speed up our search because when we see a letter which is in our text. But not in our pattern then we can actually fast forward in some sense the scan. So, this is one special case of what is an optimization called the Boyer Moore algorithm which we shall look at next.'},\n",
              " {'id': '9e9386df-d176-4f9b-afc4-ba993b347ac9',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': 'Programming, Data Structures, and Algorithms using Python Professor Madhavan Mukund Linear Programming: Bandwidth Allocation So, let us look at another linear programming example, which will lead us to another kind of problem which we will look at as a general way of solving a large class of algorithmic problems. So, this is a problem called bandwidth allocation. (Refer Slide Time: 00:22)  So, supposing we have a network, a telecom network, basically an internet provider has a network, and there are these three users. So, we have A, B and C they are these three users, and they need to be connected. So, you can think of A, B and C as three locations of a company for instance. So, they want this company would like a certain quality of internet connection between its 3 locations, and small a small b and small c are the nearest hubs maintained by the internet service provider. So, basically each company office is linked to the nearest hub of the internet service provider and these hubs are linked to each other. And so, now the service provider has to connect these offices capital A, capital B and capital C through its network. So, these numbers which are mentioned along these links, represent bandwidth capacity, so, their capacity constraints. So, let us assume that these are in megabits per second, if you want you can think of a larger unit but it is just illustrative. So, that means that no more than 10 Mbps can flow from B to capital B to small b. So, one of the constraints that the company that is buying this internet service has asked is that it must have a minimum of 2 Mbps bandwidth between any pair of its offices. So, there must be 2 Mbps bandwidth going this way, going this way and going this way all these must have 2 Mbps bandwidth. So, the bandwidth is not affected by how many hops it takes. So, if I want to connect capital A to capital B, so, I can of course do this shortest thing, which is going from capital A to small a.'},\n",
              " {'id': '4f8db785-d1a2-4f72-88d1-aa418abbdfa3',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': 'So, that is the nearest node of my ISP, my internet service provider small a connects to small b small b is connected to capital B. So, this is the fast route, but they could also be another connection which goes this way from small a it goes to small c and then from small c goes to small b. So, this kind of indirect connection is also allowed and the sum of these two will be the capacity that I get from capital A to capital B. Now, there is another aspect to this, which is that for the company, the company that is maintaining this and providing the service, they get paid by this customer, and they get differential payment for different combinations of offices. So, some of these offices maybe for the company that is buying the service are more important. So, this company is willing to pay 300 rupees per Mbps for traffic on this route. So, you need to pay only 200 for this route. And it is willing to pay 400 for this route. So here I pay 300 here I paid 200 here i pay 400. So, now what as an ISP we need to do is allocate the bandwidth, to maximize the revenue. So, remember that this is only at least it is not that I am giving my customer exactly 2 bandwidth that I am willing to give more than 2 also, but I should not give less than 2. (Refer Slide Time: 03:30)  So, now, if I look at any 2 offices of the customer, then there are 2 ways to connect it as we saw it. So, let X denote the shorter route. So, X capital A capital B denotes how much bandwidth is being allocated to route traffic from A to B via the short route by a small a and small b and yAB will be the long route. So, this is xAB and this is yAB. So, the same thing will hold for the others. So, there will be xAC and yAC and similarly there will be an xBC and a long route which is yBC. So, there are 6 possible routes 3 short and 3 long which together give us the three desired routes which are from capital A to capital B capital A to capital C and capital B to capital C.'},\n",
              " {'id': '45e8fdc8-2691-470a-91b7-d39b2f2f1c15',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': 'Now, how do we reconcile the constraints. So, if we look at this route then it uses this edge. So, anything that flows along the short route from A to B goes along this particular connection from small b to capital B. So, does anything which flows from here to here. So, these both contribute to the capacity congestion on this edge, but so also this traffic coming from here so any traffic which comes to B follows this thing. So, this traffic and this traffic also comes. So, if we look at which are all these constraints which come here, then you have xAB and yAB that is all the traffic coming to B from a by the short route to the long route, and xBC plus yBC, which is all the traffic coming from C, and this total traffic cannot exceed the capacity of this link, which is 10. So, this link has only capacity 10. So, I have to respect that link. Similarly, of course, you have the same thing for these links. So, if I look at this one, then all the traffic that comes to A either from B or from C must add up to something less than 12. And if I look at this link, then all the traffic that comes to C cannot exceed 8. So, these are 3 constraints on these 3 links. Likewise, we will need to express the constraints on these 3 internal links between the nodes of the ISP itself. (Refer Slide Time: 06:01)  So, here we can see that for if I look at this edge, then the short edge from capital A to capital B follows this thing. The long edge from A to C follows this thing. And the long and the and the long edge from B to C follows this thing. So, each internal link is on one short edge and two long edges. So, this particular small A small B link is on xAB, the short link from A to B, I know the long links from A to C and B to C and this total cannot exceed 6. Similarly, for the other things, so this one would be on this short link and these two-long links. So, we have for each of these, we have one short link, and two long links.'},\n",
              " {'id': '185482b5-6782-4313-82a4-406d056f8303',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': \"And there is some of these two-short links, I mean, two long links and one short link cannot exceed the capacity of that edge. So, we have 6 edges, 6 constraints. And then we have this other constraint, which is the customer's service requirement, which says that the total amount of traffic from A to B, which is the sum of the traffic on the short link, and the long link must be at least 2. Similarly, for B to C and A to C. And finally, we have of course, that we cannot have negative traffic. (Refer Slide Time: 07:27) But finally, we have the revenue part. So, this is what we want to optimize as far as the ISP is concerned. We know how much we get on each of the pairwise connections between the endpoints. So, what the ISP wants to do is maximize how much revenue the customer is willing to pay. So, remember that this was 300. This was the more expensive one 400. And this was the one which was cheapest only 200. So, if I am the ISP, then I will give the minimum here on the bottom, I will make this minimum to satisfy the 2 Mbps guarantee, and no more and try to maximize in some sense how much flow between A and C because that traffic will actually earn me more revenue. (Refer Slide Time: 08:09)   So, you can feed this of course to simplex. And simplex will give you values for all these variables. So, if you look at this variable, it says that along the short route, I am getting 0, along the slow route of getting 7. Then along the short route I am getting 1.5, along the long route among again getting 1.5. But so now this is all. So, what this is really saying is that total, the total from A to B is 7, the total from B to C is 3, the total from A to C is 5. So, even though A to C gives the maximum revenue, it turns out, it is better to get because of the constraints that there in the network is better to actually give more from A to B. And as we saw them, least revenue comes from B to C.\"},\n",
              " {'id': '9ab74f58-e192-4116-8a14-a0b1667caab1',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': 'So, it is not surprising that this is the least amount of allocation that this solution gives us. Now, one thing that we observed in an earlier problem was that we had fractional solution. Now, here it is not. It is not like hiring one person or making one carpet or something which is indivisible. We can give half a bandwidth half a megabit of bandwidth, so it is not a problem. Factional solutions are okay. And if you look at this, it will turn out that everything is saturated except this edge. So, if you examine that solution, but the problem with this approach mean, setting up this problem as a linear program is that it does not scale well. And the reason it does not scale well is basically we have created one variable for every possible route between the two nodes. So, we have in this case, a kind of triangular network in which there were two possible nodes, routes, but imagine that if I had one more way of a triangle here then I would have so many more routes. I would have this route I would have this route I would have this route and so on. So, the number of routes between pairs of nodes would explode. And in general, the number of paths in a graph, paths are remembered, where you do not even repeat a vertex. But the number of paths in general is going to be exponential. So, if you are setting up a linear program, in which you construct one variable per path, then the linear program is going to be very large compared to the graph that you started with. So, this is not a good strategy actually, to model such programs, such problems and what we will do is we will actually look at a better approach. So, we will look at something which actually solves these so-called network flows more effectively. So, these are called network flows.'},\n",
              " {'id': '74f5eb19-945c-4a8d-8f45-476edb43c4b2',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Linear Programming-Bandwidth Allocation.pdf',\n",
              "  'content': 'So, we have a network and flow in this case is literal, I mean, we want to flow some internet practically it could be pipe and we will see other examples of things which do not obviously look like network flow, but which can be modelled using flows. So, what we have seen is we have seen one general technique to solve optimization problems called linear programming, and we have seen some examples. So, we saw one example involving making sweets and making an optimum combination of sweets, we saw another thing about scheduling production of carpets, and here we saw a problem where we wanted to allocate flow in a graph. So, what we will see now, after this is that we can actually solve these problems using flows in the graph more directly using the graph itself rather than converting it to linear programming. And then that in turn network flows will also turn out to be a generic problemsolving technique, which we will apply to a number of problems. So, this is a very, so these two classes of problems. So, we are not really studying in some sense, how to solve them so much as how to use them. So, that will be the focus of what we are doing, how to use these two powerful techniques to solve other algorithmic problems which can be modelled in terms of these two tools.'},\n",
              " {'id': '1b18661b-32d9-46b3-b603-b615d63d5039',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'Programming, Data Structure and Algorithm using Python Professor. Madhavan Mukund Intractability: Checking Algorithm So, till now the focus of the course has been on trying to find efficient algorithms, but sometimes efficient algorithms do not exist. So, how do you work in that situation, to show that something cannot be done efficiently? So, this is the realm of what is called intractability. So, we will look at intractability, so we will begin by looking at something called checking algorithms. (Refer Slide Time: 00:30) So, when we look at efficient problems for which we have reasonably efficient algorithms. So, we looked at shortest paths, minimum cost spanning trees, maximum flow. By efficient we typically mean that they have this polynomial time algorithm. So, polynomial time is generically taken as definition of efficient solution. Although we have seen that even in polynomial time, if you have something that runs in n cube, this is something that runs in n log n. There is a huge practical difference in what size problem we can solve with one rather than the other. But from a more abstract perspective of algorithms, anything which is of the form polynomial n to the power k for some fix k is considered to be good. So, now we have efficient algorithms, what do these efficient algorithms do? Well, they actually find a solution from a large set of possible solutions. So, if you are looking for the shortest path, there are many paths. And we said, in general, they are exponentially many paths. The same way there are exponentially many spanning trees, maximum flow also, we said, if you are not careful, the Ford Fulkerson algorithm will do something which is going to be proportional to the size of the actual flow. So, what these algorithms managed to do is somehow prune this search and find the correct solution without actually examining all these exponentially many solutions. So, there is some clever way in which you cut through this exponential.'},\n",
              " {'id': 'eaf76f32-d987-445b-93a6-3c5ccfa95d8b',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'But there is always the brute force possibility, so the brute force possibility just says enumerate every possible path, every possible spanning tree. And then check whether or not that is the best one, or find the minimum or the maximum across all of these. So, now, this is in general going to be very inefficient. It will be correct, because you are explicitly enumerating every possible solution and picking the best one. But it is not going to be computationally tractable. So, the question, as I said, is, can we always do this? Can we always somehow bypass this exponential search by finding a clever algorithm? So, do all problems admit such efficient solutions, which bypass this and unfortunately, this is not the case. So, we will see that there is a large class of problems. And these problems are very natural, they are problems which actually arise in the day to day things that we have to deal with. And therefore, it is a bit challenging that none of them is known to have a theoretically efficient algorithm of the same type that we have seen here for shortest paths, and so on. (Refer Slide Time: 02:54) So, let us look at a general question, which is the problem of solving something versus checking that a solution is correct, the problem of generating a solution versus checking a solution. So, supposing as a maths teacher, I assign you some homework. So, I give you a large number, which I know is the product of two primes, it is a product of two large primes, and I tell you find these prime numbers. So, as a student, you are given this large number N, and you have to find these primes p and q says p times q equal to n. And you can imagine that this will be complicated, because you have searched, you have to search for these large primes. Because assuming they are really large, you have to go through all the primes up to p in order to find them. So, in fact, there is no known way to do this efficiently.'},\n",
              " {'id': '0b82da6a-fa2a-4e12-8f9d-ee9f0714a3c1',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, this is the students problem, the students problem is to generate a solution. And it could take arbitrarily long depending on how much time the student is willing to spend on it, and how careful the student is about calculations. Now the student submits the homework and I have to correct it. So, what do I have to do? Well, as a teacher, the student gives me a p and a q. I am not really interested maybe in how that student arrived at p and q, I just want to check if it is correct. So, if I want to check, it is correct, I do not have to do any searching for p and q. I have given p and q and I know the target, I need that p and q is factors of n, and I know that they must multiply to n. So, I just multiply p by q, which is a relatively simple procedure, check if it is equal to n. So, as a teacher, my goal is only to check the answer. As a student, the person who is assigned this problem, the goal is to find the answer. And it seems intuitive, that generating a solution is harder than checking a solution. So, this example leads us to the notion of a checking algorithm. So, I am given a problem p, which I want to solve. And I want to talk about a checking algorithm for that problem, rather than a solution for the problem, I want to check out, talk about what would be a checking algorithm. So, checking algorithm like here, we will take us solution. So, the student has already solved the problem. So, the checking algorithm only has to validate the solution. So, it takes the input. So, it will take N in this case, it will take the input and it will take the certificate that is the solution, which is supposed to be the correct one. In this case, it will be p and q. And then you check whether the certificate, the solution that the student has given you is correct for that given input or not. So, you have to validate whether that solution is correct. So, in our situation, the input instance is the number, the large product that you need to factorize.'},\n",
              " {'id': '95952c95-ba3b-4da9-aa6d-fa352bbeb578',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'The solution that student predict presents the certificate is the set of factors. And checking consists of multiplying these two factors and validating, they actually multiply to the number you started with. So, this is a checking algorithm. (Refer Slide Time: 05:49) So, let us look at a problem now, which is very central to this whole notion of intractability. It is called Boolean satisfiability. So, as you know from programming a Boolean variable is something that can take values true and false. And we, we know that we can take Boolean values and we can operate on them. So, we can negate them for instance, true becomes false, false becomes true, we can take the disjunction x or y is true if either x is true or y is true, or both. So, at least one of them must be true, and x and y, which says that both of them are true. If either one of them is false, the answer is false. So, in in logical notation, this is conventional to write it, so you use this symbol for NOT, this V symbol for OR, and this inverted V symbol for AND. So, we will use these to write formulas. So, what we have in Boolean satisfiability is we have such a formula, but the formula consists of variables. So, we have some x1, or not x2, or x3 and so on. But this is in a particular form, the particular form is that I have only ORs here. And the NOT is attached to one of the excise. So, the excise are variables. So, each of these excise is an individual value. So, these are called literals. So, literal is either a variable, or the negation of a variable like a, not x2, or not ex four. So x 3, so these are all the literals. So, a clause consists of literals, which are connected by or, it is a disjunction of, of literals. So, each part of the clause can be a x or a not x. And they are connected by or. And now a formula will be something which is a collection of these clauses connected by and. So, it will be C1, which is a clause of this form, and C2, which is another clause of this form, and C3.'},\n",
              " {'id': 'f5aeaf14-9734-4bb1-89fb-9cc53836b38e',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, in order for this formula to be true, we need that this whole clause must evaluate to 2, this whole clause must value at 2 and so on. But within a clause because we have disjunction, it is enough for any one of those excise within the clause to be true. So, that is the challenge now, which Xi should I make true, so that all the clauses become true? (Refer Slide Time: 08:00) So, this is called the satisfiability problem. I need to assign values to each of the variables. So, every variable xi should be assigned either true or false. Once I assign the value true, or the value false, then for instance, I can evaluate the formula and check whether this particular assignment of true or false makes the formula true. If it does, then it is supposed to be satisfiable assignment. So, satisfiability says, find an assignment that makes the formula true. So, if I take this particular example here, so there are three variables, x1 x2 x3, now, I want to make this true. So, there could be many different strategies. You could say, greedily for instance, I need to make each of these clauses true. So, if I greedily start, I can say that maybe I make this one true. So, maybe I should make x1 to be true. And that will guarantee that the first clause is true. If the first clause is, if x1 is true, then this clause also has x1. So, this is also true. So, these two clauses have both become true, because I have decided to make x1 true. But I have not said anything about x2 and x3. So, I do not know anything about this one. So, let us say I want to now proceed to make that true. So, then my next step in this kind of greedy heuristic search could be, let me take the next value, which I have not yet set to true and make that true. So, now I have set x2 to be true. Now, I come to the third clause, and I find that Oh, x 1 is true. So, this is going to be false, x2 is also true. So, this is also going to be false.'},\n",
              " {'id': '69f607c2-8779-4fbd-9107-0de412f61757',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, I have because of my previous choices for x1 and x2, I have already fixed in some sense that 2 of these literals in this clause are false. So, the only way to make the whole clause true is to make this part true, which means I must set x3 to be false. So, using this kind of, this is not an algorithm is just a guessing game. I have shown you that if I make x1 true and x2 and x3 false then this formula will actually evaluate to true. So, this formula is satisfiable. Now I can add one clause to that. So, I am taking the same formula and add 1 more clause, which says x3 or not x1. Now, you can see that this particular assignment no longer satisfies this because I have thanks to this, this is false, x3 is false. And thanks to this, not x1 is also false. So, this particular form clause is not satisfied by the assignment, which I found for the original formula. And you can reason about why this is not going to work in general. Because if I, if I want to make this formula true, then you can work backwards. So, I am not going to prove it to you, but you can check for yourself that if I make x3 through here, then this is going to become false. And if this becomes false, and you work backwards, so this is true here, so this is going to become false. So, if this is false, then this must be true, I have no choice. If this is true, then this is false. And if this is false, then this must be true. So, now look at what we have, x3 was taken to be true, because I was forced to make something here. So, this became false, not x3. So, x 3 is set to true. So, not x3 becomes false, x2 became true, so not x2 becomes false, and x1 had to become true. So, not x1 becomes false. So, in the process of setting the red clause to be true, I have ended up making the last clause false. And you can check that there is no way to make this true by just exhaustively checking.'},\n",
              " {'id': '4c47a2e7-8c12-461b-8407-8127fe2557e0',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, one way to do it is there are three variables, which I call possible eight combinations, of setting x1 to be true or false, x2 to be true or false, xA to be true or false, and none of them will work. (Refer Slide Time: 11:34) So, this is a formula which has become unsatisfied. There is no satisfying assignment. So, this is the Boolean satisfiability question basically says, I give you a formula in this particular form, I give you clauses, where inside a clause I have literals with disjunctions. And then I give you these clauses connected by hands. And then your question is to answer is whether there is an assignment to these variables, which makes these clauses true or not. (Refer Slide Time: 11:56) So, now, if I have to generate a solution, as we said, we can always try every possible assignment. We can take an assignment V, which our evaluation as it is called, which assigns true or false to x1, true or false to x2, and so on. And if there are n variables, then there are two choices for each variable. So, two times two times two n times is 2 to the power n. So, I have an exponential number of different valuations which can arise. So, question is, can I do better? So, this was the whole idea of having efficient algorithms that there is an exponential search space. But there is a clever way of cutting through that exponential search space and finding the assignment or not finding the assignment quickly. Unfortunately, we do not know. So, this is literally the situation with Boolean satisfiability. That we have no idea whether there is an algorithm which is guaranteed to find this satisfying assignment without exploring all these 2 to the n possible assignments. In some cases, it may work but, whether there is something that works for all such formulas, we do not know. On the other hand, it clearly has a very efficient checking algorithm. Because a checking algorithm will take a solution. So, what is the solution?'},\n",
              " {'id': '10fa4349-6473-46b4-98b1-5b94a92441c7',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'A solution is the form what we saw before, it will say x1 is true x2 is true, x3 is false. So, this is my candidate solution. So, what you have to do is then plug in these values in the formula and then evaluate. Because you know what, or means, you know, what not means you know what and means. So, if you substitute explicit true false for each of the variables, according to the valuation, you can evaluate the formula explicitly, quickly in linear time, and tell me whether the formula is actually true under this valuation, so, whether this is a real satisfying valuation or not. So, checking is efficient, generating appears to be not efficient. Now, as I said before, it does not mean that no Boolean formula can be solved. So, we have taken this particular form where we have these clauses consisting of disjunctions, literals, and we combine these clauses using and. So, this is a particular form called conjunctive. normal form, but what if we reversed supposing we construct these clauses using AND? And we connect the clauses using OR? So, that is the opposite. So, we are now in earlier it or inside the clause and, and connecting the clauses. Now we have AND inside the clause, and we have OR outside the clause. So, we write it as C1 or C2 or Cn. Now the problem becomes very easy, because if this is a clause, I can check whether this is true or not by just walking down and saying, this says everything has to be correct. So, this forces me to set t1 x1 to be true. It forces me to set x2 to be false, it forces me to set x3 to be true and so on. So, each clause forces a valuation to make that clause true. And I have to make one of the clauses. So, each clause forces a unique valuation. So, I check whether the moment I find a clause which is satisfiable, because by that fixed valuation that that clause is enforcing. Because it is the and have some literals. So, it is saying these literals must have these values, otherwise this and will not be true.'},\n",
              " {'id': '754c40a4-3def-41f9-b877-2fbd5f01cc87',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'If that is possible, if there is no contradiction inside the clause, if I write inside a clause, if I write x1 and not x1 and something then clearly that clause is not going to be satisfiable. But otherwise, I am going to be able to find one in general. So, once I find that I am done. Because if I can satisfy C1, I do not care about the rest, because it is only an OR. The moment I can satisfy one clause, the whole disjunction is satisfiable. So, this becomes much easier. I can just try C1 that involves fixing a valuation by looking at all the literals in C1 and checking if that makes the C1 true, if true I am done. If I cannot find such evaluation, if that C1 has a contradiction, I move to C2. So, in one scan, I can fix the valuation and check whether this valuation works or not. So, therefore, it is important that the satisfiability problem is stated the way we stated it, which is that we have ORs here, and we have ANDs here. Otherwise, it becomes a trivial problem computationally. So, that is called conjunctive normal form. (Refer Slide Time: 15:53) Now, let us look at a very different problem. So, let us look at a problem which you may have heard of called a traveling salesman problem. So, this is a variation of our many shortest paths problem. So, a traveling salesman problem consists of a network of cities, and there is a distance between each pair of cities. Now you can go from anywhere to anywhere. So, you can think of this as a complete graph. So, for every pair of cities, there is a way of going directly from one city to another by paying some cost or some time or some distance. So, our goal is to find the shortest tour for the salesman that visits every city exactly once. So, tour is something which is just a cycle, which starts at some city visits every city and comes back. It should be a simple cycle, I am not allowed to visit the same city twice, it must visit all the vertices so every vertex must appear in the cycle.'},\n",
              " {'id': '4524ae1b-dad6-4639-861e-6e961fe11fb8',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'And among all cycles, it must be of minimum cost. Now, clearly, there are cycles, because it is a complete graph. So, I can find many cycles, so there is no problem of finding cycles, the problem is to find the minimum cost cycle. So, this looks clearly like a difficult problem to solve, because there are exponentially many cycles, and I have to search through all of them. But let us look at the checking part of it. Supposing somebody claims to solve have this problem? How would we validate that thing? So, how do we how can we design a checking algorithm for this problem? So, remember that a checking algorithm is take a solution and say yes or no? So, here, what would be a solution? A solution would be a cycle like this. If somebody will say, here is my cycle, this is my tour for the traveling salesman, and it is the best tour that the traveling salesman can take. So, what can we do? We can verify that it is a simple cycle, and that it visits all the vertices. So, that is easy, because that is just a graph theoretic property of that thing, we just have to check that every node is connected to the next node, every node appears there and no node, node except the starting and the ending point appear twice, everything else appears only once. So, that is easy. And of course, by looking at the edges involved in this, I look at xi xi plus 1 in this particular sequence, I can get the edge weight from the graph, and I can add it up. So, I can find the cost. But I need to find the minimum cost. So, how do I validate that this cost is the minimum among all the cycles? Because I do not have a reference for that solution, so how do I check that? It is the least cost cycle? So, there is that bottleneck. Now, for this problem to convert it into a checking problem? I do not have a way of validating the numerical answer that I am going to get. I can validate the structure of the answer that it is a cycle that visits all the cities, I can compute the numerical cost of this answer.'},\n",
              " {'id': '9242ff4a-93e7-4fd0-bd57-d2180555223d',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'But whether it is minimum or not, is something that is not obvious. (Refer Slide Time: 18:36) So, the way you get around this, is to ask a question slightly differently. So, you transform the problem and say, is there a tour for the salesman with a budget of K? So, so imagine that those costs represent some ticket costs of traveling between the two cities. So, you have a total budget of K to buy tickets, so can you achieve a traveling salesman tour with a budget of K. Now the problem is easy, because I can check it by finding out if there is a solution and verifying if the cost of the solution is within K. I am not asking whether this K is minimum or not, I am just saying is the cost within budget. Now, how do I solve the original problem? Well, I try different Ks. And how do I find different Ks? Well, I need to start with some value. So, I need to have a bound on the Ks. But there is a kind of very easy upper bound, if I add up the cost of all the edges in the graph. Now you know that the site tour traveling salesman tour is going to take some n edges. It is going to connect n vertices in a cycle. So, I am going to have n edges going from the first to the second to the third to the nth and back to the first one. So, the sum of these n edges cannot exceed the sum of the weights of all the edges in the graph. So, if I just add up the entire edge weight of the graph, I get an upper bound. So, my K that I want to achieve the minimum cost is somewhere between 0 and that maximum. So, I can start with the maximum by 2, and I can say is it achievable? If it is achievable, then I will say, can I achieve something smaller? If it is not achievable, then I need to achieve something bigger. So, I can do a binary search, I can say if my overall things adds up to 1000, then I can say, is it achievable with a cost of 500? If the answer is yes, then I will say, is it achievable 250? The answer is no, then I need to go above 500. So, I will look at 750.'},\n",
              " {'id': 'ec4318fe-68c9-465e-8ebe-2f05794b94f3',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, I just do the usual binary search so, I can find the optimum K by doing a logarithmic number of searches. So, assuming that somebody is there, who is going to produce an answer for me, I can do a logarithmic number of checks to find the solution that works. (Refer Slide Time: 20:44) So, here is another problem, this is problem is called independent set. So, we say that two edges are independent, if they are not connected by an edge. For example, two vertices, so, example 1 and 4 are independent, because there is no edge connecting one and four. So, an independent set is a set where all the vertices are independent. So, for example, supposing I take 1, 4 and 5, then there is no edge between any pair of these. 1 and 4 are not connected, 4 and five are not connected, 1 and 5 are not connected. So, this is an independent set, then nothing in there which connects each other. So, this is the basis of many interesting problems that you might solve. So, for instance, supposing these are people, and you want to form a committee, but you do not want to have a committee in which friends collaborate to achieve some outcome. So, you want to make sure that the committee consists of people who do not know each other well, so that they will come up with a neutral solution, and they will not have any bias. So, you would like to find, so these edges now represent friendship. And you would like to find a committee in which no two people are friends. So, that is an independent set. And the largest such typical thing is what we want to find. So, we want to usually find the largest independent set in a given graph. So, here, for instance, this collection 345 is an independent size 3, and it turns out, this is the largest set, we also saw another one, like 145. So, there may be more than one. But we want to find out the size of the largest independence. So, this is a standard graph theoretic problem.'},\n",
              " {'id': '0c31d6ad-d959-4403-8c21-66623cb59da6',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'Again, there is no easy way to solve this apparently, except to look at first of all, you have to identify all the independent sets, and try to find the largest one. So, now, if we want to check this, if I give you a set of vertices, you can certainly validate it is an independent set. Because you can check that no two, no two vertices in that thing form an edge. But how would you check that is minimum? Well, we have to have a budget, just like for the traveling salesman problem, I have to ask you, is this a minimum, is this an independent set whose size is at most K? So, then if you, if it is K, it is fine and independent, otherwise, you say no. (Refer Slide Time: 22:58) So, there is a dual problem called vertex cover. So, in independent set we were looking for pairs of vertices which are not connected by an edge. Here, I am looking at vertices which connect to all the edges. So, if I look at this vertex 2 for instance, then it covers all these edges, which are adjacent to 2. So, anything which is incident to 2, is covered by 2. So, node u covers every edge uv, which is incident on u. So, what we want is a vertex cover. So, a vertex cover is something which has a collection of vertices cover all edges. So, for instance, if I take this one, then I will get these edges. So, now I have a few edges which are missed out. So, I could, for instance, want to cover this edge, I could take this vertex. So, if I take this vertex, then these three edges also get covered. And now I have something at the bottom. So, I could take, for instance, this vertex and cover this. So, now if I, if I take these three vertices, then between them, they cover all the edges. Now some edges can be covered multiple times, that is not a problem, I just want to make sure that every edge is covered. So, if you imagine, for instance, that these are stretches of road and you want to put a camera at an intersection.'},\n",
              " {'id': '69c37663-ff96-4f15-81c0-2cc15081e2a4',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'And that camera can actually swivel around and scan all the roads which meet at that intersection, then what we are asking is, where should I put the camera so the minimum number of cameras are used and yet I, I am able to watch the traffic on all the roads in my network. So, this is a typical example of finding so I want in this case, I want the smallest vertex cover not the largest independent set in the earlier case, the smallest vertex cover. And again, here, for instance, one small vertex cover is 1, 2, 6 and 7. So, checking version of this problem is, is there a vertex cover of size K? Again, if I give you a set of vertices, you can check if it is a vertex cover, but is it a smallest one? Then you have to do some work. (Refer Slide Time: 24:55) So, these two problems appear to be similar, are they connected? Well, actually, yes, you can show that if U is an independent set, and it has some size K, then its complement. The set of vertices minus U is a vertex cover of size N minus K. So, why is this the case? Well, if I look at the forward direction, so supposing U is an independent set, then you know that every edge has to be at most having one endpoint in U, because it is an independent set. There are no edges between the vertices in the set. So, every edge has one endpoint outside the set. In other words, outside the set, the complement covers every edge. So, the complement is actually a vertex cover. Conversely, if I have a vertex cover, then we know that every edge has one endpoint in their vertex cover. So, if I look at any two vertices outside that vertex cover, there cannot be an edge between them. Because if there is an edge between them, one endpoint of the edge must be in the vertex cover. So, there can be no edges between them. So, the complement forms and independent set. (Refer Slide Time: 26:03) So, this basically says that independent set and vertex cover both reduce to each other.'},\n",
              " {'id': '05ce56cf-c156-4641-beab-52ea1bcc9ec4',\n",
              "  'metadata': {'chunk_idx': 14, 'week': 11},\n",
              "  'source': 'Intractability-Checking Algorithms.pdf',\n",
              "  'content': 'So, we saw last time that we can use reductions in two ways, either we can translate an efficient solution for B into an efficient solution for A or we can argue that, if there is no good solution for A, then they could not have been a good solution for B. So, here we can say that both vertex cover and independent sets seem to be difficult problems. Because we have to go through all subsets of things to find good sets on both types. And they reduce to each other. So, if I can solve one, I can solve the other because we saw that every independent set generates a vertex cover as its complement and every vertex cover generates an independent set as its complement. So, if I want to find the largest independent set, it is equivalent to finding the smallest vertex cover and vice versa. So, these reduce to each other. So, these are both as it will turn out difficult problems to solve. Just like the other problems we have been looking at like traveling salesman and Boolean satisfiability. But they are checkable. If we put this constraint of K, and they are inter reducible, if we can solve one, we can solve the other. So, this is the basis of what we are going to look at next, this inter relatability of intractable problems.'},\n",
              " {'id': 'a53a1f32-136d-4442-8aef-64a27750ce26',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'Programming, Data Structure and Algorithms Using Python Professor. Madhavan Mukund Linear Programming (Refer Slide Time: 00:23) So, this week we will look at some general powerful techniques for solving problems that can be applied to a large variety of questions that we need to answer. So, the first of these is called linear programming. So, a number of the questions that we have asked in our algorithms course have involved optimization. So, remember when we started with graphs with weighted graphs, we were looking for shortest paths, then later on we were looking for spanning trees with a minimum cost, then when we were doing dynamic programming, we looked at subsequence’s that were common to two words and we wanted the longest such common subsequence. So, in each case we are looking for some optimal value, either the shortest the smallest or the largest or the longest. And of course, these values have to be found within some constraints for instance, the shortest path between two nodes in a weighted graph must of course follow the edges that are given to us, so we are given some edges and among those edges we have to construct the shortest path. In the same way, when we want to build a minimum cost spanning tree, the spanning tree edges must come from the edges that we already have. We cannot construct new edges to make a spanning tree and of course when we find the longest common subsequence, we are looking for letters which occur in the same order in both the words that are given to us, so we cannot shuffle the order. So, linear programming is a general class of problems which follow a similar paradigm where we are trying to optimize some value subject to some constraints. The reason that this is called linear is because these constraints are given to us numerically in terms of linear functions. So, you know that a linear function is something where you have a variable multiplied by a constant but you do not have variables multiplied by each other.'},\n",
              " {'id': '960b9106-b23e-48fd-aa2f-b036581ce276',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, you cannot write x square or x times y but you can write xy and so on. So, supposing we have input variables x1 to xm, then a linear constraint would be of this form, it will say some linear combination, so we have coefficients ai and the variables xi. So, some combination ai, xi of our variables added up together is either below some value or above some value. So, the constraint could be a maximum constraint it can be that it can be no more than something or it can be no less than something. And finally subject to these constraints we have to optimize. So, the optimization is given in terms of some quantity of these variables. So, this is typically written as an objective which is another linear function. Now notice that some of these coefficients could be 0, so it could be that xj does not contribute to the to the overall optimum at all, in which case in this cost function or this objective function cj might just be 0. But this is the general form, we have these linear constraints and then we have a linear function which tells us what we are trying to optimize maximize or minimize and now we have to find the optimum values for this x1 to xm, subject to these constraints which optimizes that objective. (Refer Slide Time: 3:12) So, let us look at an example. So, grandiose sweets is a sweet shop that sells two types of sweets, cashew barfis and dry fruit halwa. So, there is a profit associated with selling each of these. For each box of barfis the shop makes rupees 100. For each box of halwa the shop earns rupees 600. Now the daily demand is known they know that the no more than 200 boxes of barfis will be sold in any given day and no more than 300 boxes of halwa will be given sold in any given day. Beyond that there is also a constraint about how many boxes they can produce? So, the staff of the shop working throughout the day can produce at most 400 boxes, these boxes can be either barfi or halwa.'},\n",
              " {'id': '1a249b6e-1b5e-42bc-b147-9681ff2c1229',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, now we are given these facts and our goal is to find out how many barfi boxes and how many halwa boxes grandiose sweet should market every day so it maximizes profit? So, to use linear programming we have to set up those linear constraints and the linear objective as we said. So, first we need to identify what the variables are? So, here it is very clear that the variables are how many barfis we produce and how many halwas we produce or how many boxes of burfi and how many boxes of halwa? So, we use two variables in this case let us call them b and h. So, b is the number of boxes of barfi we would like to produce and h is the number of boxes of halwa, this should be halwa, that we want to produce each day. So, what we know from these constraints here is that if we produce b boxes of barfi and h boxes of halwa, then the profit will be 100 for each of the b boxes and 600 for each of the halwa boxes so we will get 100 b plus 600 h. But we have some constraints, so the constraint says that there is no point in producing more than 200 barfi boxes because they will not sell. So, 200 is an upper limit on how many barfi boxes we should buy because the daily demand is at most 200. Similarly, the daily demand for halwa is at most 300. So, there is no point in making more than 300 boxes. Remember b and h are the quantities we are going to make in one day. Then we also have this production constraint. Production constraint says that, the staff as a whole cannot make more than 400 boxes so the total amount of barfi plus halwa put together cannot be more than 400. And finally, there is a kind of implicit obvious constraint which we do not need to state, which is that, we cannot make negative boxes of barfis and negative boxes of always so at least we make can make 0 boxes of one of them. So, b must be bigger than equal to 0 and h must be bigger than equal to 0. So, here we have the constraint. These are our constraints.'},\n",
              " {'id': '51c1fbbd-f964-4712-8e46-e853d4d292eb',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'We have an upper bound on each of the boxes quantities and we have this kind of cumulative bound which tells us that together we cannot make more than 400 of each. (Refer Slide Time: 6:21) So, this gives us one part of our thing, now we have to write our objective. So, our objective is clearly to maximize our profit and we saw that the profit is given because of this thing that 100 Rs per box of barfi and 600 per box of halwa, so we can get 100b plus 600h as our profit that we want to maximize subject to these constraints. So, this is now the linear program that we want to solve, we are given an objective which is a linear function of our variables. We are given constraints which are linear functions of our variables, now find the optimum values of b and h given this. (Refer Slide Time: 7:01) So, one way to think about this is to understand what combinations of b and h are actually allowed. So, we can draw in this particular case because we have only two variables, we can draw this quite easily as a two-dimensional picture, a kind of a graph. So, here on the x axis, I am looking at the number of barfis that I want to get and on the y axis I am plotting the number of halwa boxes that the shop will make. So, the first constraint, this constraint tells us that b must be less than or equal to 200. So, there is a line there indicating a boundary saying that I should not go into this region at all because this region is useless because the demand for barfi boxes is not more than 200 in a day. And similarly, there is a constraint on the halwa boxes which says that the halwas should be no more than 300 because there is no more than that demand. Then we have this demand which says that the total production capacity is limited to 400, so this becomes a line like this where b plus h is equal to 400 that gives the boundary, so everything that we produce must be below b plus h equal to 400. We cannot cross 400 because our staff cannot make that many boxes.'},\n",
              " {'id': 'cfd5ac11-ec83-41a4-9894-aa7d96aff860',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'Of course, we also have these 2 lines giving constraints saying that we must be above b equal to 0 and we must be to the right of b equal to 0 and above h equal to 0. So, with this we get what we can call a feasible region. So, any combination of b and h which lies in this yellow region is within all the constraints. It is below these constraints, it is below this constraint, it is to the left of this constraint, it is above this constraint and to the right of this constraint. So, we have to search for b and h within this feasible reason. So, if you go back to the way we have solved various problems a Brute Force Approach, would be to actually search this entire space. Now of course one complication here is that this space is continuous. So, we could look at things like we could even find things like 100, 100.01 and so on. There are arbitrarily many different things of course we cannot make fractional boxes but still there is a large space to search. (Refer Slide Time: 9:10) So, for example if we take c to be our objective function, 100 b plus ch, 600 h and then we pick a point. Say we pick this point, so at this point what happens is that we are making 0 barfi boxes and we are making a 100 halwa boxes. Since a 100 halwa boxes gives us 600 each, the total profit that I get here is 600 into 100 is 60,000. Now, I can stay within the feasible region and increase my production of halwa boxes. So, I can go up for example to 200 halwa boxes. If I go to 200 alpha boxes, then I will get 120000 or 120000 as my profit for the day. I can go still more and go 250 halwa boxes and I will get 150,000. I can start making some barfi boxes. So, for instance make 300 halwa boxes and that leaves me a capacity of 100 because my total is 400. So, I can make a 100 barfi boxes in addition to those 300 halwa boxes, remember there is no point making more than 300 halva boxes because I will go outside the feasible region.'},\n",
              " {'id': 'ff18a6f1-30af-4e6c-b831-145c433788b9',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, the maximum number of halva boxes I can make is 300 and then I use my spare capacity to make 100 barfi boxes. So, I get 180000 from my halwa and I get 10,000 from my 100 barfi boxes. So, I get 190,000 and in fact you can kind of work out even manually that this is going to be the optimum value for this particular problem. (Refer Slide Time: 10:38) So, notice that this particular value lies at a corner of this feasible region. So, it does not lie here, it does not lie inside, of course it has to lie within this yellow zone but it is lying on the boundary and more than the boundary it is lying at a corner. So, this is one interesting property about linear programs that you will find that the optimum value will always lie at a vertex. So, if I draw this particular feasible region, then what it means is the optimum value must be at one of these five points. It cannot be anywhere else. It must be at some vertex in the polygon sense not in the graph theoretic sense that we have been used to. But if you think of polygons and shapes, it is at some vertex of this feasible region that we draw by inserting all our constraints as lines. (Refer Slide Time: 11:26) So, there is a very classical algorithm which solves linear programs by exploiting this observation. So, what you do is you start at any vertex, so you first have to calculate of course that feasible region and once you calculate that feasible region you know where the vertices are which are all the corners in some sense of that feasible region so you start at any one and then you evaluate the objective there. And then once you evaluate your objective there, you look at all the next nearby vertices, you look at all the neighbouring vertices to the place where you started and if the objective improves by going to any one of them, then you move. If the objective does not move then if the objective does not improve, then you stop. So, this is what is called simplex.'},\n",
              " {'id': '5c18ead8-2cb7-4734-a29c-17ee15531db4',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, you just start at a vertex and try to find a vertex whose objective value is larger than all of its neighbours and you can actually show that this is correct. Now the problem with this algorithm is that it is not necessarily efficient. It can take an exponential amount of time but in most practical problems like the kind of question that we raised with barfi and halwa, simplex actually works well so this is quite often used in practice as a solution for these linear programs. There are theoretically clever ways to solve linear programs which are provably efficient, so you can actually do this in polynomial time but simplex is an easy thing to do and it is what most people use if they are not very worried about this extreme cases. So, we said that these solutions will exist at the feasible vertices. So, for that of course the feasible region must exist. So, the first point is the feasible region is convex, so convexity is a geometric property so intuitively something like this is not convex and something like this is convex. So, formally you can say that a shape is convex if I take any 2 points inside the shape. And I draw them, I draw a line connecting them. The entire line stays within the shape, so this is not convex whereas this is convex because I take any two points anywhere and I connect them. So, in terms of our polygons, so the usual thing holds. So, if a polygon is convex, it will look like this and if the polygon is not convex for example a typical thing will have a kind of inward facing corner. So, the feasible region for a linear program is always going to be it turns out a convex polygon. (Refer Slide Time: 14:04) And of course, the convex polygon may be empty. So, there may be no constraint with no value which satisfies the constraint. So, for instance supposing we had said that you must produce at least 250 barfi boxes and you must produce at least 250 halwa boxes but you cannot produce more than 400 together.'},\n",
              " {'id': 'cda31c14-8688-420b-b755-11c71ab86663',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'This will give me an infeasible set of constraints because the first two constraints imply that together I must make 500 but the third constraint our old production constraint, that is I cannot make more than 400. So, it is possible that the feasible region is actually empty that you do not have a polygon does not have any interior in which case you have no solution. (Refer Slide Time: 14:45) The other thing that may happen is that you have no upper bound in some direction. So, for instance if I do not have an upper bound on my halwa and I only have an upper bound on my barfi, then I say that my burfi will keep me here but I can go as high as I want, in terms of the halwa. So, then there may be no upper bound or upper limit on my objective function. (Refer Slide Time: 15:11) So, let us look at a slightly more elaborate example so supposing this sweet shop adds a third suite, so in addition to their barfis and halwas, they have now added an almond rasmalai. So, we know that for barfis and halwa we used to get a profit of 100 and 600 per box. Now rasmalai turns out to be an even more profitable item to make. The shop actually gets 1300 for each box of rasmalai. So, here the daily demand as before for barfis and halwa is known that is no more than 200 boxes of burfi and 300 boxes of halwa can be sold in a day but there is unlimited demand for rasmalai, you can make as much rasmalai as you want and people will buy it. However, we still have this capacity constraint in our shop. So, the people who are boxing these sweets cannot box more than 400 in a day. So, this was true when there were only 2 types of sweets, the same constraint holds when there are 3 types of sweets. So, of course you could now look at this and say if the rasmalai is unlimited and there is a much higher profit for rasmalai and I can make 400 boxes then the obvious solution in this case is to just make 400 boxes of rasmalai.'},\n",
              " {'id': 'fcd5e401-1924-4d82-83fe-3cb33ce605d4',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'Now it turns out that there is one more constraint which is going to come, which will prevent us from doing that and that is that, we need to use milk for both halwa and rasmalai and there is some limitation in how much milk we have. So, the milk that we have will allow us to make 600 boxes of halwa of course we will not make 600 boxes because we know that 300 is our upper bound or it will allow us to make 200 boxes of rasmalai but if I have made 200 boxes of rasmalai, I cannot make any more halwa because the halwa is already all the milk is used up so the remaining 200 should be barfi. So, maybe I should make a few less rasmalai and make a few more halwa. So, the point of this 600 and 200 says that every box of rasmalai takes uses up three times as much milk as every box of halwa. So, this is our milk constraint so if we reduce our halwa by say 100 boxes, then we can get well 100 is not good, say 90 boxes then for those 90 boxes I can make 30 boxes of rasmalai. (Refer Slide Time: 17:31) So, the question again is what is the mix that we want? So, we have a new linear program because we have one extra variable which is the number of rasmalai boxes which I will call r. So, as before we have b and h, so we want to maximize the profit that we make by selling b boxes of barfi, h boxes of halwa and r boxes of rasmalai where the profit per box is 100, 600 and 1300. So, the constraints on the production of individual boxes are only for barfi and halwa because they have the ones where there is a daily demand limit, rasmalai is unlimited. So, there is no new constraint on the individual production of rasmalai. However, together we know that we cannot make more than 400 boxes. So, that overall constraint on production has not changed even though we have added one more type of sweet to the mix and now we have this new constraint which has come from here, the milk constraints.'},\n",
              " {'id': 'bb2f0485-7c78-471d-8f7e-2e86bd1adcc6',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': \"So, what this milk constraint says is that so this is should be less than or equal to, it should say that h plus 3r must be less than 600. So, we know that if r is 0 then h can be 600, if h is 0 then r can be 200 and any combination in between is allowed but every time we remove one r we can substitute 3 h's or we need to remove 3 h's to get one r because r takes 3 times as much milk as halwa. So, if I reduce h by 3, I can add 1 to r and keep this equation because 1 to r will get multiplied by 3. So, this is our overall constraint on the halwa rasmalai mix and as before we cannot make negative amounts of anything so b, h and r must all be bigger than 0. (Refer Slide Time: 19:20) So, this is now this linear program and last time we drew a picture so let us see what the picture might look like, so this time the picture will have 3 dimensions. Because in the earlier case, I had a two-dimensional picture, 1 axis was barfi and 1 axis was halwa and now I have a 3-dimensional picture. So, I have 1 axis, so this part is my old picture and now I have added a new dimension which is our rasmalai and so if you look along this dimension, we have our old constraint. So, if I make no rasmalai, if I make no rasmalai, then I am on that plane which involves only barfi and halwa and this is exactly the picture we had before. So, this was that line which says that b plus h less than 400 and this is the line which says that b is less than 200 and this is the line which says h is less than 300. So, in that plane where r is 0, so if this r is actually all the way slide back to 0, then I get that plane there but if I start moving in this direction then something has to give. So, either the rasmalai either the halwa component comes down or the burfi component comes down. So, this is becomes a 3-dimensional picture but the old observation that our points where we are going to find the optimum are these vertices still remains.\"},\n",
              " {'id': '94d72089-3a5f-45da-a5de-485b2903c77f',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, we only have these definitive vertices on this 3-dimensional shape, it is convex, you can look at it and think it is, there are no interior facing vertices. So, in this convex thing we have to look for these vertices and we have to evaluate at each of them. (Refer Slide Time: 20:54) And in this particular case we claim that the optimum actually happens here. So, at this point we are kind of on this plane, so we are not making any barfi at all, we are making 400 boxes, so we are achieving our production capacity and in that we are using 300 boxes of halwa and 100 boxes of rasmalai. So, remember that I could make 600 boxes of halwa, so bringing it down to 300, I am releasing 300 by 100 by 3 is equal to 100 units of rasmalai worth of milk. So, that is why I am able to make 100 here. So, it is not actually a good idea to make as much rasmalai as possible it is actually better to make a little less rasmalai and make more halwa to make our production capacity and with this you can calculate if you work out the cost that you actually get 3,10,000 or 310000 rupees. So, you might ask other than you know first if you believe what I said and you evaluate it at all these corners, then you can check that this is the maximum but is there any other way to satisfy ourselves that this particular value is optimal. (Refer Slide Time: 22:05) So, is there any other validation of this fact. So, let us see if we can figure out why this particular combination of 0 barfi, 300 halwa and 100 rasmalai is optimal. So, remember the profit was given to us we got 100 profit for each box of barfi and 600 for halwa and 1300 for rasmalai. So, 100 b plus 600h and 1300 r is what we get for any combination of b, h and r.'},\n",
              " {'id': '3c54936e-ee28-41f6-88b4-36f62993710e',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': \"Now we had a number of constraints but let us focus on these three particular constraints, so we had a constraint we said number of barfi boxes is less than equal to 200, we will ignore that but we will look at the number of halwa boxes which is less than 300, then we know that the total capacity constraint is less than 400 and we know that the combination because of the milk constraint the combination of halwa plus rasmalai is bounded by h plus 3r less than 600. So, what we can do is we can, remember that when you have 2 equations you can combine them and you get a third one. So, this is how for instance you solve simultaneous equations, you create a new equation where then you can subtract one from the other and remove one variable. So, you can take 2 equations, you can take a combination of these 2 equations add them and you will get a new equation which is valid. So, in this case I am going to construct a new equation by multiplying the first one by 100, the second one by 100 and the third one by 400. So, notice what happens if I get the first one, if I multiply by 100 I get 100 h, second one gives me another 100 h, the third one gives me 400 h. So, between the 3, I get 600 h. Now if I multiply the first one, there is no other variable in the first one. So, if I multiply the second one by 100, I get 100b but there is no b in the third one, so the 400 times c does not contribute with b. So, the total b's that I get is 100 b and 400 times c, the first one does not have any r, second one has 400 r and the third one has…, second one has 100r because I am multiplying by 100. And the third one is 1200 r because 400 times 3. So, I get 1300 r and on the right-hand side, I will get 300 into 100 plus 400 into 100 plus 600 into 400 so that is…, so this will be 310000. So, if you work it out, this is what you get you will get 100 b plus 600 h plus 1300 r is less than equal to 3,10,000.\"},\n",
              " {'id': '79e436c1-3df4-448d-9ecb-ea1d502d8320',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, this is just by taking this particular combination of these 3 constraints, so if these constraints are valid then this new constraint is also a valid constraint because it is just a linear combination of the existing constraints but what is the left hand side of this? The lefthand side of this is the profit. So, we have magically got a constraint on which the left-hand side is the total profit that we can achieve for any combination of b, h and r and it says that this combination of b, h and r can be no more than this. No matter what b, h and r are. So, the left-hand side of this constraint is a profit, so if we can achieve this right-hand side at any combination then that must be an optimum. Now it is not saying this is the only optimum, there may be other optimum but certainly this value that we have found or claimed to found have found that 0, 300 and 100 is actually an optimum value. (Refer Slide Time: 25:46) So, this was very clever so how does it work? So, we derived this upper bound by taking a linear combination of constraints and how did we get this linear combination of constraints? We sort of guessed it? So, it turns out that you do not need to guess it. So, it turns out that this is always possible, it is always possible to find a linear combination of constraints which gives you an upper bound in this case because I am giving a maximization problem. It gives me an upper bound on the optimum, if it was the minimization problem it will give me a lower bound on this it cannot be lower than something. Here it is saying that the profit cannot be higher than something. So, what we did was we took 100 times 1 equation plus 100 times another equation plus 400 times another equation. So, the thing was what we had to guess was these values, so these multipliers or these coefficients. So, implicitly we also add 0 times that thing which said b is less than 200. So, this constraint had a multiplier of 0.'},\n",
              " {'id': '02667501-ed7b-47ba-8187-6ba78afee1be',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 11},\n",
              "  'source': 'Linear Programming.pdf',\n",
              "  'content': 'So, they have some constraints which we did not use where you can think the multiplier is 0 and some which we did use. Now we will always multiply by some non-negative quantity but we do not know what these quantities are. So, you can think of these as now unknown, so I want some a1 times constraint 1, plus a2 times constraint 2, plus ak times constraint k. So, if I have k constraint so this becomes my combination. So, I want to find some upper bound on this quantity and it turns out that you can solve this problem and this problem is called the dual and if you solve this problem and find those constraints, then those constraints will actually give us a solution to the other problem. So, this is a very interesting aspect about linear programming. We will not get into it but this is a very deep observation about linear programs that you can actually take the constraints of the original program and construct a new linear program which involves combining those constraints and derive from that something which gives you an upper bound on the original solution.'},\n",
              " {'id': '41c5bf2b-4f3c-4f6e-b1dc-0f44cbb13ec9',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'Programming, Data Structure and Algorithms Using Python Professor. Madhavan Mukund Intractability: P and NP So as a final lecture on difficult problems, we will look at these two classes of problems called P and NP. (Refer Slide Time: 00:20) So, last time we said that when a problem is intractable it may admit something which we call a checking algorithm. So, a checking algorithm would at least allow us to validate that the solution is correct, we may not be able to generate the solution efficiently but if we take an input x instance and a claimed solution S for it, then we can validate whether that solution S is truly a solution or not. So, if this happens, then we say that something has a checking algorithm. (Refer Slide Time: 00:48) So, the class NP is the class of problems for which this check can be done efficiently. So, I have a checking algorithm which verifies a solution in polynomial time in the size of the instance that means that the solution that I am presented must also be small because otherwise, I will not be able to validate it. So, if I give you an input i; the solution that I need to check must be polynomial i, it cannot be a gigantic thing. For instance, it cannot be that to validate that something is the shortest path you give me all the paths and say you please check that this is minimum amongst these because that would not be feasible in polynomial time because the certificate itself is too large. So, all the problems that we have been looking at which seem to be intractable factorization. Factorization remember I asked you to find two prime numbers which multiply to n; you give me p and q these are small compared to the input n because their factors of n they will be smaller than n, I have to multiply which is efficient and I can find out. Satisfiability you give me an assignment of the n variables, I just have to plug in and check.'},\n",
              " {'id': '053d46be-5436-478b-bc45-7248da13d450',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'Traveling salesman, vertex cover independent set if you give me the checking version with a budget or with a bound on the k I can check, so all of these problems are in NP. And as we said when we take these optimization problems like finding the shortest travelling salesman tour or finding the maximum independent set of the minimum vertex cover and we converted it to the budget version which is a checkable problem. We can actually think of it as doing a logarithmic number of checks, so, it is still checkable even in the optimization version, if we just add a log step. So, and that is okay for us, so why is this class called NP? Well it has a historical reason, so NP comes from non-deterministic polynomial time. So, this is a mouthful, so non-deterministic polynomial time, so we have been saying earlier that polynomial time for us means efficient. So, non-deterministic polynomial time is something which is beyond the scope of this course but essentially of this non-determinism amounts to saying that the solution that I want to check either I can assume that somebody produces it or I randomly guess it. So, non-determinism corresponds to some kind of educated guess, so you can imagine that by magic you guess a correct solution and then you can validate your correct guess in polynomial time. So, from a formal background mathematical background this comes from an area of called computability theory and these non-deterministic solutions come from a thing called a nondeterministic Turing machine. So, if you want to go back and read about this you can find it in the book but for us the class NP is going to be defined in the simple way where there is an efficient way to validate a certificate for the problem. So, it is just in terms of existence of checking algorithms.'},\n",
              " {'id': '4c4ccef5-3500-4d3c-a6ed-65610d4b06bb',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': '(Refer Slide Time: 03:38) So, P on the other hand is something that we have been dealing with, so P is the class of problems for which we have so called efficient solutions, namely those for which we have a deterministic, we have a guaranteed solution that works in polynomial time of the input in terms of worst case complexity. So, of course, if I can solve the problem from start to finish if I can generate the solid solution in polynomial time, it is as good as saying I can check it in polynomial time because if you give me a solution which you claim is correct, in order to check it I will solve the problem which I can do in polynomial time and check if my solution is equal to your solution. So, I do not even have to check your solution, I have to see whether I can generate the same solution or not. So, P is clearly included in NP, so if you want to think of it in sets the problems which can be solved in NP include the problems that can be called solved P. So, every problem in P is also an NP; the question is are there things which are not in P but which are in NP? So, this is the question which asks whether the set P is actually equal to say NP. As every algorithm which has an efficient checking solution also have an efficient generation and intuitively like we saw from the teacher example it should not be the case, it is intuitively harder to factorize a large number into two primes than it is to multiply those two numbers to validate whether it actually gives you the original number. So, we believe that checking is easier than generation and this is the question of P versus NP because NP is something for which we do not know how to generate but we can check P something for which we can both generate and check. So, the general belief is that p is not equal to NP because intuitively one reason that this is the case and is there a more formal reason.'},\n",
              " {'id': '4babd3e2-1324-4785-b564-69a88039e970',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'Well, one more formal reason is that all these problems which we have been talking about actually are in NP and they are not known to be in P, so factorization, satisfiability traveling salesman all these things. Moreover, they are all and this is very surprising, we saw that vertex cover and independent set which are very similar stated problems can be reduced to each other. So, they are inter reducible, so one is hard, the other is hard but it turns out that all of these problems are actually inter-reducible. So, let us look at some examples of this and the fact that they are all inter reducible we remember said if one is hard other is hard, alternatively; one is easy all are easy because we can transfer the solutions from one to so another. (Refer Slide Time: 06:06) So, let us look at Boolean satisfiability, right remembered Boolean satisfiability; we had x1 x2 x3 and then we said that the clause will consist of a disjunction of these literals; a literal is either an x or a not x and a formula will have a conjunction of clauses. Now if we constrain the size of these clauses, so we say that each clause can have at most three literals then we get something called three SAT, SAT for satisfiability and 3 for 3 for the number 3. So, each clause is at most three literals, so it turns out that 3-SAT is itself as hard as satisfiability, so we can reduce 3-SAT to SAT or rather SAT to 3-SAT. Remember the reduction says if I can reduce A to B; if A is hard, B is hard; if B is easy, A is easy. So, we reduce SAT to 3-SAT, how do we do this? Well, we do it by example. So, supposing we have a literal clause which has 5 literals, what I want to do is make sure it has at most 3 literals but in such a way that satisfiability is not affected. So, what I will do is I will break this up into smaller clauses and combine them so the solution does not change. So, I can say add a literal A and split it here, so remember this is an OR.'},\n",
              " {'id': '8a5e5d9a-b999-48f0-afbc-5c3d7ec0e7cc',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'So, it says one of these things must be true, so what I am saying is either the left hand side is true or the right hand side is true. So, I am adding a new literal A, so if A is true, then I can ignore these two but since not A is true one of these three must be true right. So, if A is true, then the right hand side of my original clause must produce the witness. If A is false then, if A is false, then all of these are irrelevant but because A is false one of these must be true. So therefore, my original clause said that one of those five must be true and the same is true after splitting it with A; either the left or the right must be true I mean to make both the left so I have an AND here. So, make the both the left and the right true I need only one of them to be true because by then setting A appropriately I can make both sides true. So, I can keep doing the splitting, so I can take A and split it here as we said, now we had this clause which has four things; so I can again take this and split it here with a B and now I get three 3-3-3, so by repeatedly introducing these extra variables I can split the original thing down to clauses which are of size 3. And the point is that the new formula is satisfiable exactly when the old formula is satisfiable, so it is a reduction in that sense that every solution to SAT translates to a solution for this and every solution to 3-SAT translates back to solution to SAT. So if both are either both are hard or both are easy. (Refer Slide Time: 08:41) Now I want to look at this other problem which is very different independent set and I want to claim that 3-SAT and independent SAT are in some sense related. So, 3-SAT can be reduced to independent set. If I can solve independent sat I can solve 3-SAT. So, I have let again a concrete thing so I have a formula of 3-SAT. So, I have three literals per clause at most, so some of them have less than 3, so I have 2.'},\n",
              " {'id': 'ef7c9948-71b4-420c-9ab5-44da0b2d383a',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'So, for each of these I create this triangle, I create a graph which contains one node per literal and I connect it in triangle. So, I have these four clauses, so I have these four triangles. So now, in addition to that I connect these edges between a literal and its negation if it appears anywhere. So, Y is connected to this naught Y, it is also connected to that naught Y, so there is a Y here it is connected to this naught Y, it is also connected to that not y. Similarly, this X here is connected to this not x and to that not X, so if I look at this x here it is connected here and it is connected there. So, there is an edge within each clause between the variables in the clause and there are edges across clauses connecting every literal to its negation in any other clause. (Refer Slide Time: 10:00) So now, if I have an independent set right, it can only choose one of these because if I pick the top vertex in the triangle I cannot pick either of the bottom two because there will be an edge and similarly. So, I can pick none but I can pick at most one. So, if I have, if I am able to pick one per clause, then if I have four clauses, then I will actually have four elements in my independent set. Now once I pick this then because of the edges I cannot pick this and I cannot pick this. So, it constrains across clauses how I can pick literals. So, I have these constraints which tell me that if I pick Y, so I can think of picking Y as the same as setting Y to be true. So, it says that if I set y to be true then in this clause Y is not true anymore and this clause Y is not true anymore, so I must pick something else.'},\n",
              " {'id': '333438cb-c2d3-44c0-aadb-a93b1b9e7bf4',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'So, if I pick this y here, then perhaps I must pick this X here and if I pick this X here, now I am stuck because I have a naught X there and I have a naught Y there and both of them are not pickable, so I am stuck, so actually I should probably pick this naught X in which case I cannot pick this so then I must pick this Z and then I see that I am ok because if I pick the Z I do not pick this and so on. So, I have actually got something hopefully which works, so I have picked this Y, so I am done. So, the question is whether I can find an independent set which picks one node at least in every it can cannot pick more than one, can pick exactly one node in every clause then it means that I can set every one of these to be true. So, 3-SAT reduces to independent set. (Refer Slide Time: 11:50) So we have these the various reductions right, so sat reduces to 3-SAT, 3-SAT reduces to independent set, we saw last time that independent set and vertex cover are inter reducible and notice that by combining these things I can actually do a dual reduction, I can first convert my sat problem to 3-SAT, take that 3-SAT and convert into independent set. So, if I can solve independent set I can also solve sat or if I can solve vertex cover, I can also solve sat. So ,this reduction by this combination of efficient transformations is efficient, so this reduction is transitive, so I can go from set to vertex cover, sat to vertex cover and it turns out that many of these other problems we have looked at in passing, so travelling salesman we have mentioned last time finding an short tour. When we looked at linear programming we said that if we constrain the solutions to have integer solutions it becomes intractable, so that is also something which falls into this category and all of these problems it turns out are if you take their checking versions become equally hard.'},\n",
              " {'id': '24773df0-74ea-48b7-b2e7-48663fe3a024',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': '(Refer Slide Time: 12:48) So, there is a theorem due to Cook and Levin which says that every problem in NP can be reduced to SAT, so SAT was that first problem we started with which was at Boolean satisfiability so every problem in NP you can actually transform it into SAT, so if you can solve SAT you can solve all of them, or all of them are hard because SAT is hard; that is the other way of thinking about it. So, if you can solve SAT, you can do everything. So we do not have to worry about how it is proved but it is to do with the way we characterize the original version of NP we said in terms of non-deterministic turing machines. So, this theorem translates into a definition; it says that SAT is complete for NP, so what is complete for NP mean? It means that first of all it is an NP which we know it is because it has a checking algorithm. And every problem in NP reduces to it, so it is in some sense representative all the problems in NP, if you can solve it you can solve everything and because of this inter reducibility 3-SAT is also NP complete. Because now everything reduces sat and through sat everything reduces to 3SAT because I can go from sat to 3 set. So by transitivity everything reduces to 3-SAT; 3-SAT is clearly in NP because I can check it, so 3-SAT is also NP complete. So, in general, I take if I want to show that b is NP complete, I take an A which is NP complete and I see if I can find a reduction from A to B because I know that everything reduces to A by fact that A is NP complete. So, I must show that b is in NP and everything there is some other NP complete problem it reduces to this. So, that is how for example 3-SAT became NP complete, it is NP because I have a checking algorithm for 3-SAT similar to sat I just said validate that the assignment of true-false is correct and sat is the known NP complete problem through the Cook-Levin theorem, so that is the only starting point, we need one.'},\n",
              " {'id': '65c8919d-ff3d-444f-8446-21b1ebd5831b',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'And after you do one you can use reductions to get the rest. (Refer Slide Time: 14:54) So, this is the status of what we know right, so there are these interactable problems and a large number of these intractable problems are inter reducible and have efficient checking algorithm. So, these are all these NP problems which are actually in this class called NP complete, so they are in NP and every other problem in NP is as hard as them, they are all inter reducible. So, again we go back to our question is P the same as NP or not? Well, there is a good reason to believe it is not because this class of NP complete problems contains many practical problems which you encounter in daily life. So, if you find an optimal way of scheduling given some constraints or if you want to find bin packing. So, bin packing is a problem of furniture moving, so if you are trying to rent a truck to move your sofa and this thing and you have constraints on how they can be put in; what is the optimum number of truck loads that you need to get your furniture across? So, finding that is again in general exponential and it is reduces to all these problems. We already saw this optimal tours, the traveling salesman problem. Now all of these have immense commercial importance so people have been trying to optimize these much before we had actual computers and programs, so these are problems which if one is in P all of them are and many smart people have been looking at these problems for centuries. So, these have been problems of interest to mathematicians and to businessmen and to various things for 100 of years and if you have not found an efficient algorithm yet there is reason to believe that there is not one. And if there is not one for any of these, then all of them are in the same boat, so therefore we have kind of empirical evidence that NP is different from B but we have yet not been able to prove this.'},\n",
              " {'id': '602d3020-bcb3-4304-82d8-fa7add3e941e',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 11},\n",
              "  'source': 'Intractability-P and NP.pdf',\n",
              "  'content': 'And this is actually a price it is one of the problems which is celebrated open in mathematics, the question is whether P is not equal to NP or not? And its currently carries a1 million dollar price, so this is one of the big unsolved problems, probably the big unsolved problem in computing, many people are working on it. It has been the Cook-Levin theorem is approximately 50 years old, 1971, 40 years old. So, for the last 40 years even after formulating the problem and understanding NP completeness we are no closer in some sense to understanding whether this is true or not. We strongly believe it is true; P is not equal to NP but we cannot prove it.'},\n",
              " {'id': 'd15283c6-059c-43d6-a985-29d1665b19ae',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor. Madhavan Mukund Reductions (Refer Slide Time: 00:09) So, we moved from linear programming to network flows. And we argued that network flows should also be a useful problem for modeling other things. So, let us look at an example like that. (Refer Slide Time: 00:19) So, suppose we have a situation where there are 5 teachers and 5 subjects, and some teachers are capable of teaching some of these subjects. So, our goal is to now find, of course, an allocation of teachers to subjects. So, each instructor is willing to teach a set of courses, some of them are willing to teach more than one course. So, for instance, Deb here is willing to teach both English and History, and Priya is willing to teach both Biology and Maths. On the other hand, some subjects have more than one taker. So, maths is also offered or can be offered by Kumar. And similarly, history can also be offered by Sheila. So, we now have to do this allocation, we have to allocate courses to instructor. So, what do we want to do? We want to find an allocation. So, that each course is taught by one instructor. There is only one instructor per course. But we also want to keep the instructors happy. So, every instructor should get a course, which he or she is willing to teach. So, we should not be just say, we could always do an allocation this way. Just go across and say, the first instructor gets the first course, the second instructor gets the second course, and so on. But we want to respect these edges. So, we want to make sure that the allocation respects these edges. So, this is an example of what is called bipartite matching. (Refer Slide Time: 01:31) So, in bipartite matching, we have a graph. So, we have this graph, but the graph has some extra structure. So, there are two sets of vertices. So, we have a set here, which I will call V0, and a set here, which I will call V1. And all the edges go across these two partitions.'},\n",
              " {'id': 'b7eac89b-170b-4481-9cc9-72f7f63a1e66',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, there are no edges from V0 to V0, and there are no edges from V1 to V1. So, there is no edge from a teacher to another teacher, and there is no edge from a course to another course all the edges go from the teachers to courses. And now, we want to find a subset of edges such that no two share an endpoint. So, if I choose this edge, for instance, then I am not allowed to choose this edge, because that would correspond to offering, Priya offering two courses which is not allowed. So, if I choose one edge, which Priya is incident on the other edge must go. Similarly, if I choose, for instance, this edge and allocate History to Deb, then I should not choose this edge, because if History is already part of an edge, history, as a subject has been allotted a teacher, I should not try to a lot a second teacher. So, that means formally that this node should not have two edges in the matching incident. So, we want to take a subset of edges, so that no two of them share an endpoint. (Refer Slide Time: 02:51) So, this is called a matching for obvious reasons, because you are matching up a vertex in one set with the vertex in the other set such that these matching vertices are connected by edges and the matching is guaranteed to be kind of 1 is to 1, that is no vertex is matched to more than one vertex on the other side. So, here, for instance, you could construct a matching. So, Aziz can only teach physics, Priya can only teach biology. And now, you have to make a choice. So, Sheila can only teach History. So, you should allocate history to Sheila. And now once you have allocated History to Sheila, then you should give English to Deb. And finally, once you have done this, Priya has already got Biology. So, Kumar should get maths. So, this becomes now a largest matching in which actually, every teacher has got a course and every course has got a teacher.'},\n",
              " {'id': '53c78a05-1e23-4163-8321-f26d5b360e8a',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, if you have an equal number of things on the left-hand side, if so here, the size of V0 is 5, and the size of V1 is 5, then you can hope to do this, to match everything on the left everything on the right. So, this kind of a matching is kind of, it is not only the largest, but it is one which actually matches everybody. So, this is called a perfect matching. So, one of the problems which is come arises very often is this kind of perfect matching. So, here is a question of allocating courses to teachers, you can also think of any other resource allocation. So, supposing you have a pool of vehicles and you want to allocate drivers to drive these vehicles, then the same problem, you need to allocate a driver to a vehicle that he or she is comfortable driving. And you need to make sure that all the drivers are using driving only one vehicle and each vehicle is being driven by somebody. So, in many of these resource allocation kinds of questions, you can have a match, you can also think of matching in different contexts. Like for instance, we have a bunch of students on the left-hand side and a bunch of institutions on the right-hand side. So, there it is not any more 1 is to 1 because more than one student can be matched to an institution. But you can imagine that if it is a bunch of preferences. So, supposing I have given a bunch of preferences for which courses I want to take then I can only get matched to one of them. So, matching in different forms is a very important problem. But this bipartite matching is a particularly simple and appealing version of the problem. So, we said that we are going to use flows. So, let us see how we would use flows. So, what we want to do is to convert this to a flow problem. So, remember in the flow problem, we have a source node and a sink node, so, here is a source node, and here is a sink node. So, I created a new source node and a new sink node. And now I want to think about this matching as a flow.'},\n",
              " {'id': 'bf9232d4-e6a4-4bf1-9d37-71548a11d1a8',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, I want to think about the fact that if Sheila is going to teach history, then I am going to flow something from English or History, from Sheila to history. So, now, the flow will represent what all can go from the left to the right. And now, because the flow is such that the capacity is going to be set to 1, I cannot flow 2 units of things. So, I cannot flow more than 1 unit through any of these edges. So, if I now have five edges coming out from here to the 5 instructors, the question is, can I flow 5 units if I flow 5 units, then each of them must go out. So, there must be 1 unit of flow and each of these edges, there is no storage remember so that if it goes out, it must go out of that node. So, all this flow must leave these respective nodes in some direction. And once it leaves in some direction, they will reach the other side. But now, because of the capacity constraints, supposing I allocate in this flow, supposing I allocate both Deb and Sheila to History, then I cannot push that flow out on the other side without storage, because if two units if two people are assigned History, then it is going to create a problem because only one of those flows can go to the target and then I have to store that flow. So, you can easily see now, that checking, if we have a bipartite matching corresponds to finding a maximum flow in this, if I get any maximum flow, I can translate it into a bipartite matching, and every bipartite matching will correspond to a flow. So, if I want find the maximum or the largest bipartite matching, it corresponds to finding the maximum flow. So, we want to find a maximum flow to from s to t under these constraints where I have added a source which connects to everything in V0, I have added a target which connects to everything in V1, and then I put a capacity one constraint on all the edges in the network. (Refer Slide Time: 07:07) So, this is an example, this and other things that we have seen are examples of what we call reductions.'},\n",
              " {'id': 'fd4a93e4-9e2b-4321-96dc-34d6528918d1',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, we want to solve a problem A. So, in our particular case, this could be bipartite matching. So, we want to solve A, what we know how to solve is something else. So, this is bipartite matching, and this is network flow. So, we know how to solve network flow, because we have the Ford Fulkerson algorithm, what we now do is convert our problem from bipartite matching to network flow. (Refer Slide Time: 07:41) So, this is where we add s and t and we have capacity 1 and so on. So, we convert our bipartite matching problem to a network flow problem. And once we have a network flow problem, we have a solution for that network flow problem. So, the network flow problem will tell us, there solution tell us which all edges have flow. And in this case, the edges which have flow correspond to the edges which will be allocated in the matching. So, we can now reverse this thing and reconstruct from the answer that the algorithm B gives us, we can get a reconstruct an answer for A. So, we get an answer for the original problem. So, this is what we wanted it, given A we wanted the answer that A should provide for X. So, what we do is we pass that answer, the question to B look at the answer of B, and then reconstruct. So, this is a reduction. (Refer Slide Time: 08:30) So, we say that A reduces to B. If I can solve B I can solve A. So, solving A reduces to solving B. So, if we do this, as we have seen if you could do this fast. And of course, if you can do the translation fast, that is also there. Then you can transfer an efficient solution for B to an efficient solution for A, provided it does not take you too long to do these two steps. And you know that this step is going to be quick, then combined the whole thing is going to be quick, I have after do some work, call this B, get the answer from B which will be quick and then reconstruct the answer for A. So, this is a typical scenario of using reduction. So, we have seen many such things.'},\n",
              " {'id': 'ec08e6dc-1dc0-4b53-abba-beb62473e5c1',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, we have seen specific problems which are reduced. So, when we looked at linear programming, we saw that you could take a specific problem involving sort of carpet sales or involving making boxes of sweets, and model it directly as linear programming. So, that is not a reduction in a sense because those are concrete problems, which I am modeling as reduction, whereas here we have taken one problem in one domain, I can take bipartite matching over arbitrary bipartite graphs, and any search problem. So, there is a whole class of problems called bipartite matching, I can take this entire class of problems and solve it using a class of problems called network flow. So, that is the reduction thing. I can take any bipartite matching problem and convert it to a specific network flow problem and reconstruct the answer. So, that is the reduction power. It is not saying that you model one question in terms of some formulism but you are saying that every problem of type A can be transformed into a problem of type B and the answer can be reconstructed. (Refer Slide Time: 10:07) So, when we say efficient, what we will usually mean as we will go ahead is that both these translations should be in polynomial time with respect to the size of the input of A. And the size of the output of B must also not be too large, because I have to reconstruct back in polynomial time an answer for A. (Refer Slide Time: 10:25) So, in this sense bipartite matching reduces to maximum flow. Last time, we saw that maximum flow reduces to linear programming, because we said that you can attach for each flow a variable, then we can write these constraints which express the fact that the flows respect the capacities of the edges and at each node, there is a conservation of flow, the incoming flow must add up to the outgoing flow.'},\n",
              " {'id': 'f17bdf54-ba97-4da8-9ef8-36500327390f',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, these are the constraints they said two sets of constraints one is the capacity constraints, one is this conservation constraint, and then we have this objective function which is maximize the flow through the edges coming out of the source. So, maximum flow reduces to linear programming, bipartite matching reduces to maximum flow as we have just seen and there could be other problems, which we are not going to addressed here, but we will see a few examples later on. So, the important thing is that these must be efficient. So, what we can see is that, when you do maximum flow to linear programming by adding one variable per edge, then the number of variables is now linear in the size of the graph. And the number of constraints is also linear, because we have one capacity constrained per edge, and we have one conservation constraint per node. So, the problem in this pre-processing which converts should not create to larger solution. So, that is the real goal of making a reduction effective in terms of translating efficient solutions. So, this is the positive use of reductions in some sense. So, we can take a problem that we know how to solve and use it to solve another problem. (Refer Slide Time: 11:55) Now, there is also another way of interpreting reductions. Supposing, I do not know how to solve A, supposing I do not know how to solve A, so I know that solving A is hard. Now, I want to check whether solving B is also hard. So, supposing I can reduce A to B in the same way, then I claim that B must also be hard because if I could solve B easily, then I have a simple way of solving A easily. But since I believe that A is hard to solve, it cannot be easy to solve B. So, this is a kind of an argument which would be difficult to do on your own. How do you look at a problem and say that this problem is going to all instances this problem are hard to solve.'},\n",
              " {'id': 'baeee788-c255-46ff-8a5c-bec467796a13',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, on a specific problem, it is hard, it is difficult to do this, but if I know some other problem, where somebody has already done this hard work and established that that problem is difficult to solve. If I can reduce that problem to my problem, then I can argue that my problem must be as difficult as that one because if I could solve this one easily, then through this reduction, I can transfer it back. So, this says basically that if I can solve B efficiently, I can solve A efficiently. So, conversely, if I cannot solve A efficiently, I cannot solve B efficiently. (Refer Slide Time: 13:09) So, the reason that we looked at linear programming and network flows is not to examine the algorithms for these we just gave a sketchy version of the simplex version for how simplex works for linear programming. And we kind of by illustrated how the Ford Fulkerson algorithm works for network flows without actually proving that they are correct. But the point is that these are big hammers. So, these are very powerful techniques, which are available to us to solve a lot of problems. So, many problems can be reduced to these. And these have interned because they are such useful problems. There are commercially available or even non commercially available, publicly available libraries to solve these. So, if you are using Python, or if you are using some other programming language, you will find a way to call a library which can solve a linear programming problem for you. And if you can model your problem in terms of linear programming, then all you have to do is call this library. So, therefore, it is useful to be able to do this translation. So, this is a different aspect from the rest of the course, where we will be looking at designing and analysing specific algorithms for specific problems. Here, we are saying that these are these very powerful problems which can be solved and for which solutions exist.'},\n",
              " {'id': 'e93c88bf-eeb1-4b68-a312-f42f00631613',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Reductions.pdf',\n",
              "  'content': 'So, can you take your problem and interpret it as a linear programming problem? Or can you take a problem and interpret it as a network problem and then apply the technique that we generally applies to linear programming and network flow in order to solve?'},\n",
              " {'id': 'b34f6269-91b6-4bb2-ab33-0103fc1f6cf0',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms Using Python Professor Madhavan Mukund Linear Programming: Production Planning (Refer Slide Time: 00:12) So, we saw that in linear programming, we have constraints which are given in terms of linear functions. And we also have an objective, which is another linear function in terms of the variables that we are considering. So, linear means we do not have constraints where I have x1 times x2, or xi squared or anything. So, the constraints all involve one variable multiplied by one number one coefficient. So, that is what linear means. So, I never have variable constraints in which the equation involves variables multiplied by each other. And we saw that this kind of constraint defines these lines which form a convex region. And this feasible region, we can then explore the vertices. And this is what simplex does, it starts looking at all the corners in some sense the vertices of this feasible region, and you are guaranteed that the optimum value if this feasible region is not empty, there will be some optimum value, it will always be found at one of these vertices. So, it is efficient to look at the vertices. So, simplex will start at some vertex, evaluate the objective. And if it can find a neighbouring vertex, which has got a better value of the objective, it will move to that one. So, it is like a search problem along this boundary of this feasible region. So, you start somewhere at one of the corners, look at your neighbouring corners, if you can improve you go, if you cannot improve you stops. So, in that sense, it is a greedy algorithm, you can show that this actually works. The problem is that computationally in the worst case, it is exponential, and there are better algorithms that you can use to make it actually polynomial. But it is conceptually very simple. And this is very often used in practice.'},\n",
              " {'id': '729d57e2-b318-4d4f-93b0-ac57a28e041a',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': '(Refer Slide Time: 01:49) Then we also saw that when you have an LP linear programming problem with a bunch of constraints and an objective function, you can also take the constraints and multiply it by some constants. And make that into a dual problem. And these constants are multipliers that you use the constraints on those constants are these are the new variables, the multipliers of these equations, the constraints are that these multipliers must be positive and non-negative. They can be 0, but they cannot be negative. And then when you get to solve this dual problem, then you get a solution to original problem. So, there is this thing of LP duality, which is quite powerful. So, now let us look at another aspect of linear programming. (Refer Slide Time: 02:34)  So, let us look at a problem which involves planning production, which is similar to last time we were looking at a sweet shop and we are looking at what is the production that we must achieve for a given day the sweet shop. Now, here we are looking at slightly different scenario. So, we have a company that makes hand woven carpets. And they have at the beginning of the year 30 employees and each employee during a month he makes 20 carpets and is paid 20000 rupees. So, since 20 carpets earns him a salary of 20000 rupees, the labour costs per carpet is 1000 rupees, 1000 into 20 is 20000. So, this is what it costs to make a carpet. So, now the problem is that we want to meet some demand, which varies from month to month. So, we do not always want to make so if I have 30 employees and I make 20 carpets a month, I can make 600 carpets each month. But there is a demand which sometimes drops below 600 And sometimes goes above 600. So, we must if let us assume that we know this demand, we know this demand for each month of the year. Therefore, we know the demand for January, February and so on. So, there are 12 months.'},\n",
              " {'id': '45966be2-a7c5-4ec9-94aa-a473663d8253',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, there are 12 quantities which are known in advance the demand for January the demand for February, the demand for March and so on. So, we have these 12 demand quantities. And now we want to somehow adjust our production to meet this demand. Now, there are two problems. If I if I do not have a demand, supposing I am only able to sell 500 carpets in a month, if I make 600 Then I have 100 left. On the other hand, if I am supposed if I could sell 700 And I make only 600 I am missing out on something. So, I would like to make as many carpets as the demand requires and avoid making surplus perhaps and certainly avoid not meeting the demand. So, we have to cope with this varying demand. So, as a business, what is the option that is available to us? Well, one option is to ask these employees to work more. So, then, of course for overtime, the cost goes up so they are paid 20000 rupees for making carpets and they make 20 carpets a month. So, this works out to 1000 rupees per carpet but if they work overtime you have to pay the more and how much more it is 80 percent more. So, 80 percent more translates the fact that this will become 1800 per carpet right for me new carpet they make beyond their stated target of 20 they will be paid 1800 rupees rather than 1000 rupees, but there is also a constraint saying that a single worker cannot make cannot do more than 30 percent work over time, that means that, if I have made 20 carpets in a month normally, then 30 percent, that is 6 so that you cannot make more than 26. So, one worker cannot make more than 26 carpets a month. So, if all your 30 workers were working overtime, then each of them will be making 6 extra carpets perhaps, so 6 into 30, you can get 180 carpets. So, our 600 carpets can now go to a max capacity of 780 if all our employees are paid overtime, but our demand could actually go even beyond that. So, then another option is to temporarily employ somebody to make additional carpets.'},\n",
              " {'id': '3f2a7e67-b230-490d-86d6-9f6376c1f738',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, you can expand your workforce. And of course, when the demand shrinks, you do not need this person. So, you can also reduce your workforce by firing somebody. So, there is a cost associated with this. So, there is a cost with finding a new person. So, new person finding costs you in addition to their salary, it costs you 3200 just to add somebody to your workforce, it might involve some training and searching and all that and then getting somebody to leave your company also requires some costs. So, perhaps you have to give them a kind of a bonus for hardship and so on. So, it costs 4000 to fire a person. And the third option that we said when you are going to foresee a demand next month, which you do not have the capacity to meet, but this month, you have excess capacities that you can make extra and keep it but if you keep it then you have to store it. So, there is a storage cost which is 80 rupees per carpet. So, these are all the costs involved. So, you have a standard cost for per employee of 20000 rupees. So, if you hire somebody for a month, you pay them 20000 rupees they make 20 carpets, but if you want them to make more you have to pay extra you to pay another 800 rupees per carpet, but they cannot make more than 6 extra you can hire and fire people, but that cost money or you can make extra and store it and that costs money. (Refer Slide Time: 07:19)   So, now, our target is obviously to find out what is the minimum cost that I will incur if I want to meet this monthly demand exactly for all these 12 months. So, my target is to meet the monthly demand and do it with the minimum cost by suitably hiring, firing and storing. So, for this I need to know month by month, how many people I need to employ. So, let wi be the number of workers whom I employ in a given month. So, remember that I started the year with 30 employees. So, w1, so, my i runs from 1 to 12; 1, 2, 3, 4 up to 12. 1 is January, 12 December.'},\n",
              " {'id': '24c69fd9-bc51-4504-bfb4-8f33656a82b1',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, w0 represents how many workers I had in the month before January and so that is 30. Let xi be the number of carpets that I make in month. Ideally, I would like to make di carpets in a month I have a demand of i carpets but I may make less or more depending on how many carpets I have surplus or how many carpets I want to make surplus. So, xi is not always equal to di. So, as we said before, sometimes you cannot meet the demand from your workers normal workload, so you give them overtime. So, we can count the cost of overtime in terms of number of carpets because we know that each carpet in overtime costs 1800 rupees. So, it is sufficient to count how many carpets were made in overtime. So, that is called oi. So, we also have this possibility of adding workers and removing workers. So, hi will be the number of new workers we hire at the beginning of the month i and fi will be the number of workers that we get rid of at the beginning of month i. So, for instance, we start at the beginning of January we had 30 employees, supposing we needed 27 employees in January we could have started with f1 equal to 3 that is we could have fired people at the beginning of January itself. And finally, in order to keep track of this storage cost, we need to know what surplus we are carrying I mean how much of production we have done which we have not yet sold. So, that is the surplus carpets after month i and if I have a surplus carpet from month i then in the next month is going to be costing me 80 rupees to store it. And of course, I start the year with no inventory. So, I can assume that S0 at the at the beginning of January that is the end of the previous month was 0. So, I have in some sense, one of these for every month. So, I have these 6 variables wi, xi, oi, hi, fi and si. For one i equal to 1 to 12, 6 twelves are 72. And then I need these two extra variables as we will see. For the month before we started, we need to know what the surplus was coming in.'},\n",
              " {'id': '1b6fb6a6-8314-4459-88a2-2d338364b528',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'That is a kind of boundary condition, what is the surplus coming into this year and how many workers we had coming into this year. So, we have 74 variables in our linear program. (Refer Slide Time: 10:26)  So, now we have to start expressing these constraints in terms of these variables. So, the first constraint is, of course, that everything is non-zero and non-negative, I cannot have minus five workers, nor can I hire a negative number of workers, nor can I produce a negative number of carpets, so everything here is at least 0. So, how many carpets do I make in a month? Well, every worker who is employed will make 20 carpets. So, I will get 20 times wi, if I have wi people on my staff, they are guaranteed to make 20 carpets, no more, no less. And in addition to that, some of them will make carpets because I asked them to do it in overtime. I am not particular about who does it. So, I will just count cumulatively this value that we have kept track of oi as the total overtime, contribution to my carpets made. So, notice that this wi could also change. So, it is not that I am sticking to the original 30 employees, I could now have for instance, 35 employees, as we will see because of hiring. But whoever is there with me will make 20 carpets plus some of them will make possibly overtime. So, now the number of workers I have will have to be consistent with these three numbers. How many workers I had before this month, and how many I hired and fired in this month. So, this at the end of the last month, I had wi minus one workers, this month, I hired some and I fired some. So, the hired ones add to my worker base, and the fired ones get removed from my worker base. So, my new worker set is the old workers, plus the hirings minus the firings.'},\n",
              " {'id': '2d6e91ac-97b4-40cf-8666-db88ad7d3157',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, this is a constraint which basically tells us how some of these values are connected, it says that these sorry, it says that these three values cannot be completely arbitrary, there must be some consistency between the workers in this month in the workers in next month, and how many people are hired and fired in the next month. Similarly, there is a connection between how much I have surplus and how much I have produced in the last month. So, if I look at what I had surplus before I started this month, so that is how much I had at the end of the last month. And then look at how much I make now, so xi is how much I made in this month. So, I already had si minus 1 carpets in storage, I made an additional xi this month. But I also know that I got rid of di carpets because that was a demand this month. So, I remove this from my stock. So, xi might have been smaller than di because I am using some old stock. Or it could be that xi is bigger than di because I am keeping some stock to store next time, but whatever excess happens goes into si. So, ideally si minus 1 plus xi should be equal to di have exactly as many carpets as you need to sell, but I could be selling more. But remember that everything must be bigger than 0. So, si cannot be a negative number I cannot be short selling I mean I cannot be not meeting the demand. So, I must be making at least these many carpets and possibly more with storage and what I made this month and that will translate as si. And the final thing says that this there is a constraint on this 30 percent limit in terms of production. So, though we are not really keeping track of who is producing what across all the workers, I know that each worker cannot produce more than 6 carpets. So, therefore, the total number of carpets that are produced in overtime in a given month cannot be more than 6 times the number of workers who are employed in that month. So, these are the constraints that I get in terms of these monthly variables.'},\n",
              " {'id': 'adc6a4f4-dc02-48be-b561-d121b2871358',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': \"So, these are all indexed by i. So, there are all these things happen for equal i to 1, i equal to 2 and so on. So, this is our linear program. (Refer Slide Time: 14:13) Now, we have this initial constraint which says that w0 and s0 the two which are before this year's counting starts. So, so w0 should be 30. So, not 0. So, w0 is 30, which says that before I started this year I had 30 workers and I had no surplus. Now, for i equal to 1 to 12, I know that all these 12 into 6, 72 values are not negative, then I know that the production in a month is equal to the regular production of the wi workers plus the overtime production. I know that the total number of workers respects the total number of workers I had before plus the addition due to hiring and the loss due to firing. I know that the surplus is equal to this month's production plus this month surplus from the previous month, minus this month's demand. And finally, this is our limit of 30 percent production on overtime. So, now, the objective function is to minimize the costs. So, I have these different cost rates. So, this is the salary cost. So, the salary cost for the year is 20000, times the number of workers in each month. So, 20000 times w1 is a salary costs for January and so 20000 times w12 is my salary costs for December. Then I have a hiring cost. So, for each worker that I had, remember that we had to pay 3200 rupees as a fixed cost for employing this person from scratch. So, if I just add up the number of workers I have hired across the 12 months, that times 3200 is my total hiring costs. Similarly, if I add up the number of workers who have been fired across 12 months, that times 4000 firing cost. Storage costs 80 rupees a month. So, if I add up the number of carpets stored at the end of each month, during this year, multiply by 80, I get the total storage cost incurred. And finally, remember that our overtime gives the worker an 80 percent bonus.\"},\n",
              " {'id': '17eafe17-8808-4f1f-826e-aefe48526213',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, instead of 1000 rupees per carpet, for every carpet, I make an overtime I have to pay 1800. So, if I add up the total number of carpets made an overtime across the months, and I multiply 1800 I get the extra cost I have paid for employing a worker in overtime, I will do that because sometimes it is better to get a worker in overtime to supposing I need only 6 extra carpets, it is better to make one worker work overtime rather than hire a new worker who will produce 20 carpets cost some money to hire. And I will also add up with 14 carpets surplus. So, that is why overtime sometimes balances out. (Refer Slide Time: 16:45) So, having set all this up. So, we have set up the constraints, we have set up the objective and as we said before, what we will typically do is run simplex and find a solution. So, are we done? Well, in this case, it might turn out that some of the values that we get are fractional. So, supposing it says that h3 is equal to 10.6. So, h3 is equal to 10.6 says that in March I should hire 10.6 people now obviously, I cannot hire 10.6 people because people are indivisible, I can hire 10 people, I can hire 11 people, I cannot have 0.6 people. So, then we have to deal with this problem where we are expecting integer solutions, but our linear program gives us fractional solutions. So, one way to do it is to take a fractional solution which is produced by simplex. And try to round it off. So, we can say 10.6 workers, maybe I do not want 10.6 workers, let me say if I produced if I just employed 10 workers, what will be will I meet the demand and what would be the cost if I took 11 workers will I meet the demand what will be the cost and choose the better one. So, you can round off your solutions to the nearest integer or to the integers on either side of that fraction and recompute the cost and see. So, if your values are very large.'},\n",
              " {'id': 'cc6aedac-11be-4967-821d-60947b49c88f',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 11},\n",
              "  'source': 'Linear Programming-Production Planning.pdf',\n",
              "  'content': 'So, if your solutions are things like 1800 and 1900 and so on, then you can imagine that, adding plus minus half may not change much to the overall cost. But if your values are small like 10.6, then 10 and 11 can make a huge difference, maybe 20000 rupees is what you pay for a worker. So, between 10 workers and 11 workers, you end up incurring an extra cost 20000 rupees, which might be a large fraction of your overall cost. So, the values are small, you need to be very careful when you are rounding. And in fact, there is actually no good way to do this in general. So, this if we insist that this optimization that we are doing in linear programming must have integer solutions and not arbitrary fractional solutions. Then the efficient or non-efficient even if you simplex or if you use the efficient ones that we claim exist, they will not work anymore, they will only allow you to solve the problem if you allow arbitrary solutions, including fractions. If you insist on so called integer solutions, then we get into this problem called integer linear programming. And this turns out to be computationally intractable. So, we will see in C shortly what this means. But computationally intractable really means that there is no clever way to do it. So, remember, what we said was that there is a feasible region. And in this feasible region, of course, by brute force, you can search every combination. So, in particular, if you are looking for integer points, you can search every integer combination within the feasible region. So, that will give you one way of solving it, but there could be a very large number of such integer points integer combinations to choose from. But when we say computationally intractable, it really says that some kind of exhaustive search like that is unavoidable in the worst case.'},\n",
              " {'id': '5fe1eb8b-598e-4ed5-8c47-b37f9e5ae70c',\n",
              "  'metadata': {'chunk_idx': 0, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Network Flows So, we looked at the bandwidth allocation problem, which we modeled using linear programming. And we said that it was a problem, which involved maximizing, the flow through a graph, we wanted to flow as much internet traffic as possible between multiple nodes in a graph and we said that there may be a different way of doing it. So, let us look at this generic problem called Network Flows now. (Refer Slide Time: 0:32) So, imagine we have a network of pipelines. So, let us assume that we are trying to transport oil from 1 place to another place. So, we have a network of pipelines, which is given by this kind of a directed graph. So, the edges represent the directions in which the oil will flow, and the capacities of each of the pipelines is the number indicated. So, for example we have a pipeline here of capacity 2, a pipeline here of capacity 10, and so on. So, our goal is to take as much as possible from the source to the target. So, we will have these 2 special nodes and we will think of the flow always going from the source to the target. So, 1 of the constraints, that we will impose is that the flow cannot be stored anywhere. So, all the liquid must flow, we cannot for example transport 3 units here and then keep 2 units here and move only 1 unit in that direction, so that is not allowed. So, everything must keep flowing, there can be no storage in the network. So, in this particular network, you can actually work out, that you can transport 7 units of flow. So, here we have 2 units of flow going this way and that goes there and comes down here. So, this is 2 units. And then we have here 1 unit plus 4 units, but this 1 unit goes up and then comes down, so it joins the 4 units here and then together they form 5 units, which is allowed in this pipe and another 5 units goes there.'},\n",
              " {'id': '8721683a-f262-42dd-8bf1-2b3a36c503de',\n",
              "  'metadata': {'chunk_idx': 1, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, we are actually able to achieve 7 units of flow in this particular network, even though notice that we had some capacities, which were more than 7, like we had this node, this pipe which allowed a unit flow of 10. So, the question, that we want to know is, how do we know that the 7, that we calculated by just kind of drawing some arbitrary flow is the maximum flow. (Refer Slide Time: 2:16) So, formally what we have is a graph and this is essentially, we should think about it as a directed graph, but we will see later that we will kind of add edges to it also. So, we have a special node, which is the source node and we have a special target node, which is sometime called the sync node. So, as I said before we our goal is to transport quantities from the source to the target. So, it is best to think of this quantity as some kind of liquid and these edges as pipes. So, we want to flow as much liquid as possible from the source node to the target node through these pipes. (Refer Slide Time: 2:51) So, each edge as we said has a capacity. So, we will write the capacity of an edge e as c as e. And now we want to assign a flow to each edge, we want to assign a quantity that flows through each pipe, which of course cannot exceed its capacity. So, f of e, the flow through the edge e must be less than or equal to c of e. The other constraint that we said was that, there is no storage. So, everything that comes in must go out. So, at every node except the source node and the target node, the sum of the incoming flow must equal the sum of the outgoing flows. So, for instance if I flow 3 units through this pipe here and I am only allowed to take 1 unit in that direction, then I must put 2 units in this direction, otherwise I am not going to be able to make the flow match, we will have to end up with some surplus at that node, which is not allowed. So, whenever I have some quantity flowing and it could come in through multiple sources.'},\n",
              " {'id': '9d8a9663-04e2-4acf-9311-a0458da91e09',\n",
              "  'metadata': {'chunk_idx': 2, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, now I have 1 coming in through here and maybe I have 2 coming in through there. So, now I have 3 coming into d, now 3 has to be distributed. So, maybe 2 of them can go out there and 1 can come here. So, the flow at any node must conserve the quantity. So, everything that comes in must go out, of course the only two exceptions are the source node, and the target node, because the source node is really generating flow, it does not have you can imagine that there is an unlimited supply coming this way, which is generating flow. And here we have a kind of unlimited reservoir in which to store the flow. So, at the source and the target there is no constraint, I can push out as much as I am allowed to from the source I can collect as much as I am allowed to at the target, but every intermediate node must conserve the flow. (Refer Slide Time: 4:34) So, the total volume that I can push I can measure it at either end, either I can measure it at the target, or at the source. So, let us just say that the total amount that I am actually going to flow can be computed by looking at how much flows out of the source, it might flow through different routes, but in the end it must all come from the source. So, if I just add up the flow on the 3 outgoing edges from the source. Alternatively if I add up the 2 flows and the incoming edges of the target, this must represent the flow that I have overall across all the pipes, independent of how the flow is being inside rooted and so on. (Refer Slide Time: 5:10) So, we saw last time, that we could set up a different flow problem, that bandwidth allocation problem as a linear programming problem. So, here we can try through the same thing. So, in that particular problem one of the difficulties we faced with the linear programming model was that we were using a path as a variable. And we said that there would be exponentially many paths in a given graph. So, that would be a very large encoding in terms of linear programming.'},\n",
              " {'id': 'd37ff152-cb73-4321-9c12-259f2d3b914a',\n",
              "  'metadata': {'chunk_idx': 3, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'Well you can actually do it with edges. So, let us assume that we have a variable for each flow. So, f e will represent the flow at each edge. So, we will have for every edge, so we can call the edges by their end point. So, we will have for this edge s a we will have s f s a for b d for instance we will have f b d and so on. Now, we have these constraints, which tell us an upper bound for these flows. So, f b a the flow in this edge is constrained by 10. So, we can write down for each of these variables, a constraint telling us how much capacity, that edge has as per the input graph. And now we have this conservation. So, conservation for example tells us that if I am at d, then I have these 2 edges coming in. So, I have f a b and f b d and I have 3 edges going out, I have d c d e and d t. So, the sum of the flows coming in that is f a d plus f b d must be equal to the sum of the flows going out which is f d c plus f d e plus f d t. (Refer Slide Time: 6:41) So, now that we have these constraints, this completely captures in some sense, what all is allowed in the flows and now what I want to do is maximize the flow from s to t. And as we said we can concretely capture that maximum flow, by asking how much is flowing out of the source. So, we take all the edges, which have this starting point as a source and ask what is f s a plus f s b plus f s c in this case, because there are exactly 3 edges. So, given a concrete graph you can write down this formula in terms of the edges in that graph. So, then you can solve this as we had said before using simplex for instance. And what simplex will do, is it will try to identify this feasible region and look for these corner points, these vertices and keep moving from one vertex to the other, until it finds an optimum vertex. So, what we are going to see in this lecture is a different way of interpreting that.'},\n",
              " {'id': '23585006-0c25-412d-be9e-7abd32193775',\n",
              "  'metadata': {'chunk_idx': 4, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': \"So, we will do something which will go from one solution to another solution, but we will do it kind of directly without invoking simplex. And in this process, we will get an algorithm directly for maximum flow, in this particular type of graph, where we have a source and a target. (Refer Slide Time: 7:47) So, this is called the Ford-Fulkerson algorithm. So, initially we are just given a graph with capacities and there is no flow. So, what we can do is of course add some flow, wherever there is capacity. So, for instance we choose a path from s to t, in which the capacity is not being used and we can augment the flow. So, for instance we might take some flow from here to there of unit 1, because there is capacity of 1 and at the moment I had a zero flow. So, I can now put flow s to 1, s to a and then a to t and flow 1 unit from source to target. Now, if I do that on the upper path and on the lower path, then I am able to flow a capacity of 2 and it is very easy to see in the simple graph, that you cannot flow more than 2, because there are only 2 edges going out of s and together they can only hold a capacity of 2. So, it is very clear that you cannot exceed this capacity. But what if we chose this path initially? So, right now we could have got this by choosing the upper path and the lower path cleverly, but what if we were not clever, or the network was too complex to find a clever path right in the beginning. So, supposing we take a wrong path and we actually flow 1 unit but by mistake we go down the middle. So, we come down here and go there. So, this also flows 1 unit, but now notice what' is happened, because I have taken this 1 unit if I take something down here, it cannot be flowed, because it has to pass out through b, the only way to pass out through b is to take the edge b t, but the edge b t has already been saturated by the wrong flow, that it shows initially.\"},\n",
              " {'id': 'e148b608-52c6-436d-9f03-ad36c3fa780b',\n",
              "  'metadata': {'chunk_idx': 5, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'And similarly, this edge which has capacity 1 cannot be used anymore, because I cannot reach any extra flow to a. The only way to reach flow to a is to come from s to a, but then I do not have any capacity left from s to a. So, when I have made such a bad decision, then I need a way to kind of proceed from this. So, this is imagine that this is like in simplex you have chosen 1 vertex, 1 optimum point in your feasible region and you have realized, that this is not the best point, I mean there is a different point I can go to, so how do you move to that point in this algorithm. (Refer Slide Time: 9:57) So, what you do is, you essentially allow yourself to reverse your choices. So, whenever I put something in the flow, what I say is that I am able to undo that flow. So, the capacity in the direction of the flow has now been saturated, but I have a capacity to undo the flow by reversing the direction of the flow, which is the same as stopping the flow. So, I add these reverse edges. So, I had flowed one unit through these orange edges. So, I now say that the capacity along these orange edges, which used to be 1 has become 0. But now I have added these new edges, these green edges, which are this residual capacity. It tells me that now if I really want to use s 2 a again I can push back some flow from a to s, because I have that capacity. It is basically the same as reducing the flow from s to a, but instead I think of it as pushing it back from a to b. So, I am always looking at the same type of problem, it is still the flow problem, I am not thinking of reducing a flow, but increasing the flow through new edges. So, I add this, so I reduce the capacity of every edge by the flow that I have pushed through that edge in my previous allocation. And for each such edge I introduce a reverse edge, which is equal to the capacity that I have just allocated. (Refer Slide Time: 11:19) So, let us see how it works for instance.'},\n",
              " {'id': 'b3d97079-b9b2-4dcc-8273-04edc4bae5f6',\n",
              "  'metadata': {'chunk_idx': 6, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, this is the Ford-Fulkerson algorithm, just to remind you start with the zero flow, you choose some path, now you do not have to be intelligent is the claim, you choose some path from the source to the target, or the source the sink that is not saturated. And you push as much flow as you can given the capacity of that path. Now, you build that residual graph. So, for each edge that you have used in this particular path, you replace the flow in the forward direction by the flow, the capacity minus the flow that you have put in. And you add a new edge backwards equal to the flow, that you have done. And how do you end, well you keep doing this until in this augmented graph, you are building this graph with more and more reverse edges, build in the residual way. At the end you should reach a situation, where you will not be able to push any flow even though you have added all these edges. So, this will become clear when we do this example here. So, let us start with this example. So, as you can see there are flows of 10, 20, and 30. So, I might pick initially the path, which goes from s to a, a to b and b to t. So, this allows me to flow 20 in each of these edges. So, I can the algorithm says augment the flow as much as possible. So, since I am allowed a flow of 20 on each of the edges, I can push through a flow of 20. The next step is to build the residual graph. So, I have to replace each of these edges by 0 capacity indicating that the capacity minus the flow is 0. And I have to add these reverse edges. So, I will now replace each of those edges, so sorry, so there was a mistake here. So, this should have been 30. So, there is a 30 edge there sorry. So, if I have 30 there, then even though I have 20 at the top and the bottom I can only flow 20 through these 30 edges. So, when I build the residual graph, I actually have 10 capacity left on that edge and the residual graph in the reverse direction has the flow that I actually produce.'},\n",
              " {'id': '6453296a-afd5-4c68-9df3-6c51a0734e23',\n",
              "  'metadata': {'chunk_idx': 7, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, it is 20, 20, 20, but notice that there is still a flow of 10 possible in this edge. So, it is not always the case, that the residual graph will generate a zero edge. If in some part of the flow, that you calculated you did not achieve the capacity, then in that flow you will actually keep some non-trivial capacity left for the next round. (Refer Slide Time: 13:37) So, now we work with this graph, the point is that we do not go backwards, we go forwards but with larger and larger graphs. So, we now work with this graph and decide what to do. In the limit what will happen is between every pair of edges, we will add a forward edge and a backward edge and we will keep adjusting the capacity back and forth due to residual, the residual graph criteria. But at this point now we have these extra green edges. So, I can now look at parts, which go through these green edges, for example I can look at a path which did not exist before, which goes from here to here to here, because I now have added some capacity on this new green edge from b to a, which was not there before. So, if I do that, then I can flow how much? Well I can flow 10 here, I can flow 10 out of 20 here, and I can flow another 10 there. So, I can add 10 new flow from s to b to a to t. So, I do that. So, I add a flow of 10 from s to b to a to t. And now if I compute the residual graph this will become 0, this will become 0, this will now go from 20 to 10 and the 10 will come here, I will add it to the backwards capacity. So, every time I add in the forward direction, after I construct the residual graph, I take that much quantity and transfer it from the forward direction to the backward direction. So, this becomes my new residual graph. (Refer Slide Time: 14:51) So, now I have only 10 going up here and 10 plus the old 10, 20 is now coming down here. And now this edge which had 10 has become 0 and the reverse is 10. And similarly, this edge, which had 10 has become 0 in the reverse system.'},\n",
              " {'id': 'ca8dab1e-de93-4230-b522-bab36c9a2114',\n",
              "  'metadata': {'chunk_idx': 8, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'At this point I am stuck because if I look at s and I try to push more flow, it has to go out this way, or this way and clearly, I cannot do either, because both are 0. So, I say that the graph has now reached a situation, where there is no feasible flow from s to t. So, with this I now have I just have to add up all the flows that I have put before and get the total flow. Now, some of those flows cancel out like we saw, because in this edge for instance initially we put 20 down and then we put 10 back up. So, the net flow, which is going from a to b was just 10. So, this now gives us the solution. So, this is how the Ford-Fulkerson algorithm works. So, you just start with any flow, that flow should saturate some path, then you construct this residual graph by reversing the edges with the flow, that you have created and reducing the capacities by the flow you have created in the forward direction, keep going until you get stuck. (Refer Slide Time: 16:02) Now, what about arguing that one has achieved a maximum flow. So, I have not given you a proof that, that algorithm actually terminates, or even computes the maximum flow. So, this is something which is not part of this course, the point is just to illustrate that network flows can be solved and we will see that network flows themselves are a useful tool for reducing other problems, just as we saw that we can convert problems to linear programming, we will use network flows as a kind of base problem to solve other problems. So, if I give you a solution, let us assume that somebody claims that they have a solution, how would you validate, that it is optimal. So, let us look at this graph, the first oil graph, that we constructed and let us look at these three edges. So, we have these edges from a to d, b to d and s to c. So, if you disconnect these edges, then you will see that there is no way of going from s to t, because these three edges together cut all the paths from s to t.'},\n",
              " {'id': '94116a6c-694c-4473-bd11-094c7110d3bc',\n",
              "  'metadata': {'chunk_idx': 9, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, such a set of edges, which disconnects s and t is called a s t cut, it is something which partitions your graph into something which contains the source and contains a target and the s source and the target are no longer reachable from each other. So, since this disconnects s and t, if you had any flow at all, it has to pass through this cut, because if it avoided this cut that means there is a way to go around this cut and reach the target, but we have just said that there is no way to go around this cut, everything gets disconnected with this cut. So, this cut must actually witness all the flow that passes from s to t, anything that goes from s to t has to pass through at least one of the edges in the cut. (Refer Slide Time: 17:48) So, in this case, if we look at that cut, that cut has capacity of 4 plus 1 plus 2. Now, if all the flow from s to t must go between 1 of these 3 edges, or more 1 or more of these 3 edges, then that total capacity cannot exceed 4 plus 2 plus 1. So, I cannot flow more than 7 in this particular case minus 2 t. So, this gives me an upper bound. So, clearly if I have a cut, then the capacity of the cut gives me an upper bound, which says that the flow cannot exceed that capacity. The max flow cannot exceed the capacity of min cut. But what about the reverse? Can I always guarantee that if there is a cut of this form, then I can achieve that can I always make sure that I can get 7 flow. So, for instance I can draw another cut. So, I can take a cut, which looks like this, this is also a cut. Now, this cut has capacity 3 plus 10 plus 1 plus 4, because there are these edges. So, there is this edge, there is this edge, there is this edge, and there is this edge. So, this edge has capacity 18. Now, it seems unlikely that we can put 18 flow through this thing, because there is no edge which has capacity more than 10.'},\n",
              " {'id': 'e09132fc-05b0-4ef2-8d45-b1fba5d9a1f8',\n",
              "  'metadata': {'chunk_idx': 10, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, it is not always the case that if I draw, if I find a cut and I find the capacity of the cut, that is going to guarantee the that much flow is possible, every cut will ensure that all the flow must pass through the edges in that cut, but not every cut will actually represent some flow that is realizable. So, we know that the maximum flow cannot exceed the capacity of minimum cut, but is the minimum cut itself always realizable. (Refer Slide Time: 19:24) So, it turns out this is actually true. So, this is a very powerful result, which again we will not prove, but you we will claim is called the max flow min cut theorem. So, which says, so we know that the max flow cannot exceed the min cut, but actually this theorem says that the max flow will always be equal to the min cut. Now, finding the min cut is actually difficult, because we have to look at all these subsets of edges and find out, if they are cuts and then find out which is the minimum. So, that could be a very tedious algorithm. Now, the claim we have is that the Ford-Fulkerson algorithm, that we produce just now will actually find the maximum flow. So, you can use this theorem in reverse in actual to actually compute the capacity of the minimum cut, because we know how to compute the maximum flow. So, the theorem tells us, that the minimum cut limits the maximum flow, the algorithm tells us that the maximum flow can be computed and together that means that with the maximum flow, which we can compute reasonably efficiently, we can actually get hold of the minimum cut as well. So, just to reason why this is true. So, as we said when we have a cut, it partitions our graph into these 2 parts, there is a left and right. And we know that from S you can reach everything on the left-hand side and from R you can reach everything on the right, from R you can reach the righthand side, but you cannot get from L to R.'},\n",
              " {'id': '6eb07448-be1e-413b-a8cf-3bb943d8e863',\n",
              "  'metadata': {'chunk_idx': 11, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, now if you look at any edge, which crosses this, if it is not at full capacity, that means there should be something coming back here. And if this is not at full capacity, the if this is not at zero capacity, then this is not at full capacity. So, basically you can claim in the residual graph, that if you look at edges, which cross from this side to that side, the forward edges will be saturated and the reverse edges will be 0. So, the forward edge is saturated is just saying, that we have actually reached the capacity of the minimum cut. So, this is a kind of informal hand waving argument as to why this max cut max flow min cut theorem holds. (Refer Slide Time: 21:25) So, the Ford-Fulkerson algorithm is not in itself going to be efficient unless we do things carefully. So, if we do not choose our augmenting paths, that is the paths which we get by doing this residual capacity business carefully, we might keep producing bad path one after another, eventually I claim that the Ford-Fulkerson algorithm will produce the maximum flow. But remember what we did earlier. So, we took this flow and we flowed it through this, in the very first graph, but there it was not a problem, because we just had capacity 1. So, after 1 such thing we could construct the augmenting path and rectified. Now, what is going to happen in this graph is that one such thing if I do I will flow 99 will be left as a capacity and I will get an augmenting path back of 1 here and of course I will get an augmented path back of 1 here. So, this is after 1 iteration. Now, the only path that is left through the graph is to go this way. So, I will again get a flow of 1 here. So, I will have 99 capacity here and I will have a backward flow of 1 here and I will again get a forward flow of 1 here. So, after 2 iterations, I would have flowed 1 down the middle and 1 up the middle and I would have reduced the capacity of 100, 100, 100, 100, to 99, 99, 99, 99.'},\n",
              " {'id': '24824d3b-9007-42e3-831d-2e749a3e1dd1',\n",
              "  'metadata': {'chunk_idx': 12, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, if I keep doing this, then it will take me 200 iterations, because every 2 iterations I reduce the 100 to 99, the 99 to 98 and so on. So, it will take me 200 iterations of this poorly chosen initial path to discover, that actually I should have just said that there is a flow of 100 through the top half and a flow of 100 to the bottom half. So, in that sense the Ford-Fulkerson algorithm can grow very slowly, can grow for example if your total flow is some quantity k, it could actually take k iterations to reach there, because each time your quantity will increase in this case only by 1. So, your flow is 200 and it actually takes you 200 iterations you get there. (Refer Slide Time: 23:23) Now, it turns out that there is an heuristic, which actually works for this. So, the way you do this is you take breadth first search and then you find the shortest path to the target. So, remember our goal is to find any path, which reaches from the source to the target with some capacity and flow that capacity as our next step. So, it turns out that if you choose the shortest path. So, for instance this is not the shortest path in terms of edges, this path has 3 edges, whereas the path which is in green has only 2 edges. So, b f s would choose the green path over the red path and say augment that. So, either the top or the bottom, these are both possibilities for b f s, but not the one through the middle. And then it turns out if you use this heuristic, that is you always use breadth first search to choose the next path and choose the shortest length in terms of edges, then you are going to beat this problem. And in general, now your problem will be proportional to the size of the graph, it will still be not linear, it will be the number of vertices. So, n times m, but it will not be dependent on the actual values of the flows, which is what is happening here.'},\n",
              " {'id': '02eb1108-6f1e-4a52-ba62-c4928c8a37c2',\n",
              "  'metadata': {'chunk_idx': 13, 'week': 11},\n",
              "  'source': 'Network Flows.pdf',\n",
              "  'content': 'So, here it is taking time proportional to the values, we put on the flows, whereas now we are saying it is only going to take time proportional to the size of the graph which is more reasonable.'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO_-5-2wWbY1"
      },
      "source": [
        "## [Optionally] Load/Save Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6_3U-VBWj8h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('<csv_path_here>')\n",
        "dataset = df.to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E4WrGDwTX_r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(dataset)\n",
        "df.to_csv('dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMicBHeATS3l"
      },
      "source": [
        "# Setup Qdrant Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLTtH6TKXGWn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "QDRANT_CLIENT_URL = userdata.get('QDRANT_CLIENT_URL')\n",
        "QDRANT_CLIENT_API_KEY = userdata.get('QDRANT_CLIENT_API_KEY')\n",
        "COLLECTION_NAME = \"PDSA_Transcripts_All_Google\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqYksAYCUoRV",
        "outputId": "6ce9d280-3ddb-4805-e468-65be280d0e91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dense_model_name = \"models/text-embedding-004\"\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=dense_model_name,google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "dense_embeddings = list(embedding_model.embed_documents([dataset[0][\"content\"]]))\n",
        "len(dense_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMuMZ_XYWSVJ",
        "outputId": "834c5833-0b8b-4277-d4b5-7374f7768e31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "client = QdrantClient(QDRANT_CLIENT_URL, api_key=QDRANT_CLIENT_API_KEY, timeout=600)\n",
        "client.create_collection(\n",
        "    COLLECTION_NAME,\n",
        "    vectors_config={\n",
        "        dense_model_name: models.VectorParams(\n",
        "            size=len(dense_embeddings[0]),\n",
        "            distance=models.Distance.COSINE,\n",
        "        )\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfSvPR7FX_4W"
      },
      "source": [
        "# Push To Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt36RgyrXTM5",
        "outputId": "9e460324-fa67-40ed-e393-5897e2f05555"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading points: 100%|██████████| 856/856 [06:08<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "\n",
        "# Google API limits\n",
        "RATE_LIMIT = 150  # Max requests per minute\n",
        "BATCH_SIZE = 10    # Smaller batch to avoid hitting limit\n",
        "\n",
        "# Request tracker (timestamps of last 150 requests)\n",
        "request_timestamps = deque(maxlen=RATE_LIMIT)\n",
        "\n",
        "def enforce_rate_limit():\n",
        "    \"\"\"Ensure we don't exceed 150 requests per minute.\"\"\"\n",
        "    while len(request_timestamps) >= RATE_LIMIT:\n",
        "        elapsed = time.time() - request_timestamps[0]  # Oldest request\n",
        "        if elapsed < 60:  # Wait if within a minute\n",
        "            time.sleep(60 - elapsed)\n",
        "        else:\n",
        "            break\n",
        "    request_timestamps.append(time.time())\n",
        "\n",
        "def retry(exceptions, tries=3, delay=2, backoff=2):\n",
        "    def decorator(func):\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            attempt = 0\n",
        "            wait_time = delay\n",
        "            while attempt < tries:\n",
        "                try:\n",
        "                    enforce_rate_limit()  # Apply rate limit before calling API\n",
        "                    return func(*args, **kwargs)\n",
        "                except exceptions as e:\n",
        "                    if \"RATE_LIMIT_EXCEEDED\" in str(e) or \"429\" in str(e):\n",
        "                        print(\"Rate limit exceeded. Waiting before retry...\")\n",
        "                        time.sleep(30)  # Cooldown for quota limit\n",
        "                    else:\n",
        "                        time.sleep(wait_time)  # Normal retry delay\n",
        "                    attempt += 1\n",
        "                    wait_time *= backoff\n",
        "                    if attempt == tries:\n",
        "                        raise\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "@retry((Exception,), tries=3, delay=2, backoff=2)\n",
        "def create_embedding(data):\n",
        "    return embedding_model.embed_documents([data['content']])\n",
        "\n",
        "# Prepare batches\n",
        "batches = [dataset[i:i + BATCH_SIZE] for i in range(0, len(dataset), BATCH_SIZE)]\n",
        "\n",
        "# Initialize tqdm progress bar\n",
        "with tqdm(total=len(dataset), desc=\"Uploading points\") as pbar:\n",
        "    for batch in batches:\n",
        "        points = []\n",
        "        for data in batch:\n",
        "            dense_embeddings = create_embedding(data)\n",
        "            points.append(\n",
        "                models.PointStruct(\n",
        "                    id=data[\"id\"],\n",
        "                    vector={dense_model_name: list(dense_embeddings)[0]},\n",
        "                    payload=data\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Upload batch\n",
        "        retry_count = 0\n",
        "        while retry_count < 3:\n",
        "            try:\n",
        "                enforce_rate_limit()  # Rate limit before uploading\n",
        "                client.upload_points(COLLECTION_NAME, points=points)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if \"RATE_LIMIT_EXCEEDED\" in str(e) or \"429\" in str(e):\n",
        "                    print(\"Rate limit exceeded. Cooling down...\")\n",
        "                    time.sleep(30)  # Cooldown\n",
        "                else:\n",
        "                    print(f\"Error uploading batch: {e}\")\n",
        "                    time.sleep(5)\n",
        "                retry_count += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.update(len(batch))\n",
        "\n",
        "print(\"Upload complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeKRu0QyTS3m"
      },
      "source": [
        "# Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvIYmHs8TS3m"
      },
      "outputs": [],
      "source": [
        "query = \"What is the difference between objects and classes?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtlhgcCWg4BX"
      },
      "outputs": [],
      "source": [
        "dense_query_vector = embedding_model.embed_query(query)\n",
        "prefetch = [\n",
        "    models.Prefetch(\n",
        "        query=dense_query_vector,\n",
        "        using=dense_model_name,\n",
        "        limit=20,\n",
        "    ),\n",
        "]\n",
        "results = client.query_points(\n",
        "    COLLECTION_NAME,\n",
        "    prefetch=prefetch,\n",
        "    query=models.FusionQuery(\n",
        "        fusion=models.Fusion.RRF,\n",
        "    ),\n",
        "    with_payload=True,\n",
        "    limit=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGX8wDEZho7d"
      },
      "outputs": [],
      "source": [
        "results = client.query_points(\n",
        "    COLLECTION_NAME,\n",
        "    using=dense_model_name,\n",
        "    query=dense_query_vector,\n",
        "    with_payload=True,\n",
        "    limit=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ZUHYxuxQTS3m",
        "outputId": "9f04d10b-f875-4673-8e16-46655b32c3c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Programming, Data Structures and Algorithms using Python Professor Madhavan Mukund Class and Objects So, continuing with our discussion of slightly more exotic aspects of Python, let us look at classes and objects. (Refer Slide Time: 0:15) So, most often classes and objects arise in the context of what are called abstract data types. So, we have data types as we know, in Python, we have lists, we have dictionaries. And when we have a data type, we have certain permitted operations on these. For a list, for example, you can append to it, or you can combine two lists using plus you can concatenate them, with a dictionary, you can create a new entry with the key, you can update it, and so on. You can get X, extract all the keys of a dictionary, extract all the values and so on. Now, sometimes we need to create our own data type. And this data type will typically have two parts; it will have some information that is stored in it. But there may also be some discipline or some required way of controlling access to this information. So, a typical example that most people use for this is a stack. So, what is a stack? A stack is what you think in English, it is just a pile of things come one on top of the other. Now, if I have a stack of books, for example, in a table, what can I do? I can add one more to the top of the stack. So, this is what is called in stack terminal, you ‘push’, or I can take the top most book of the stack, and this is called a ‘pop’. Now, I cannot take out this book, until I take out the box on top of it. Otherwise, things will fall down haphazardly. So, the idea of a stack is that I have a sequence of values. So, x1, x2 up to say some xn, and when I push I add a value on the end, and when I pop, I can only take out the last value. Now I may represent this information as a list. And if represented as a list, Python will allow me to take out or update, say the second element in this list. But as a stack, that is not allowed.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.points[0].payload[\"content\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03c744b180c945c5a8aa48cc67861ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0759e7860b9c4af3a57c98350d9b9f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6826a3f8f64fa6b6e808e9461467af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c8699ec3544b0aa65c07431fc78f92",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf83e387078047f9aa656f5759c84f22",
            "value": 711396
          }
        },
        "0cbc0220473e439fac2d0f4f287b8605": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1178e1a3c35b415c95d360a35e681b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9bd0ee6284043b2815c0183cfd816d8",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca357b30600f412fabb9393edde23023",
            "value": 695
          }
        },
        "12418968aafd438791dc2f7ba79db632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "179aa0c1e548431a8dadacadf6214911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e7ed181424487693b33b0ecedaddc2",
            "placeholder": "​",
            "style": "IPY_MODEL_d72584c78aeb4a4d90961c2941bbba30",
            "value": " 695/695 [00:00&lt;00:00, 19.2kB/s]"
          }
        },
        "247d059925c945fc873fc30c587c120a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24903f02cbb4dd49c875b45a2647788",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ebf646ccb494bc3a1e6eb0769d894f2",
            "value": 5
          }
        },
        "2632e543cc124ee1b30757efa7c3f600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bd058fd04f4ddbb702303d991fd366": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29289e87e09242b39ec2e7efd88e385d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e78cb824409433996e200331a11b19e",
            "placeholder": "​",
            "style": "IPY_MODEL_93043ccb1cf24581ac09f1beee57c4c5",
            "value": " 1.24k/1.24k [00:00&lt;00:00, 14.3kB/s]"
          }
        },
        "2cc176393a4d494aab423337f9a2e5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6da34b0f304a618cae119388350beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f65b50b32f34f38a676a10d9db8c3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "322cd2a1328e40e88ed33540e5b5861d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35617898bc8746c0b84ccc3dcba08ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbf5f9e784c41899eda7b082f0e9cd7",
            "placeholder": "​",
            "style": "IPY_MODEL_f58b4cf03c124b8eada8a92c78e4466d",
            "value": "config.json: 100%"
          }
        },
        "37900e749c0a4b94b5a643682cfdbd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d609f41daf4b25ad0e166ae497cfe8",
            "placeholder": "​",
            "style": "IPY_MODEL_4631cb25b4e744c998863c56694d81be",
            "value": " 712k/712k [00:00&lt;00:00, 8.94MB/s]"
          }
        },
        "3817b88203be480b9bf1120f545f099f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba942cc87c2e451198d6e04844d4c37e",
            "max": 742,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b94edf6ded420f8133bf5366d80ca6",
            "value": 742
          }
        },
        "384f428f60624a88bba12daa4dfd314c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "386f3596d82a48ff93883a08ec624a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9157582164a6408ab0a00a0316775176",
            "placeholder": "​",
            "style": "IPY_MODEL_8d3f0eb9c4f548f8a9e4130974a2f965",
            "value": " 711k/711k [00:00&lt;00:00, 3.77MB/s]"
          }
        },
        "389df603f6374836bcd80a2cc092836d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a61cf2ddba348928c9ebdbdde5e973e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a266567e594987a70815ca19f8526d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb17703507cc4cb5b9ab2b9cbb7ca280",
              "IPY_MODEL_8f0ca15f0a7044e58c0b1341fcbf227a",
              "IPY_MODEL_dbced11902f743bcb2550aa23417bf05"
            ],
            "layout": "IPY_MODEL_aa4cc8ef3e6a4ac78f224fc36f1d11db"
          }
        },
        "43aa3fce7957432eab003335be571012": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4422b8b44dc6474d9fcce0b1867b871e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c8699ec3544b0aa65c07431fc78f92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4631cb25b4e744c998863c56694d81be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47aedfcbc2884d07a271c293d3f552fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b572bbacae743f89eeb4a485e859897": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebf646ccb494bc3a1e6eb0769d894f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518aa0c39298497f86f78a4fdf14e76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_665689518bc94a898f14b8aa0852c76b",
              "IPY_MODEL_f2adcf5691af4500acee1f43321821a4",
              "IPY_MODEL_93c363d2fa934e0f9162a0dab39ef962"
            ],
            "layout": "IPY_MODEL_ea505c293b224465b4c1f80253ad9bad"
          }
        },
        "53ea61f462e943a6baccb07de031a10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e94bab72d374f9bbaef6e9d6f306bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_d4ddd20c798649cb92b20594282faeb7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "541bce507e564522b885cbbcd76fb76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5528380c295b4c36bede3a9614e8670c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5648a30db2d648b592f7a5c2b2519165": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5879710aa4dc40efbf1ab451bc2b7417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c996de02e594113805e28a6dc2e2153": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d76f9ccaf444fff8bfbf1c51c8a6b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96da0900296142f99cea126ff1d20970",
            "max": 1381,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd42a2085b904b75bb7ecad890776ad6",
            "value": 1381
          }
        },
        "5e78cb824409433996e200331a11b19e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "626822c8cad6450fa56e3dcd05a174f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_798ec3e3b0ff4bde859a9a6b16570d63",
            "placeholder": "​",
            "style": "IPY_MODEL_d69bb433408b406b974bfa5097b5f349",
            "value": " 532M/532M [00:12&lt;00:00, 41.7MB/s]"
          }
        },
        "665689518bc94a898f14b8aa0852c76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5648a30db2d648b592f7a5c2b2519165",
            "placeholder": "​",
            "style": "IPY_MODEL_d98a5c7d2c2741cab27ed29bfe24d6f2",
            "value": "Fetching 5 files: 100%"
          }
        },
        "6689f7363c6a41f4832b889167778e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68464db71815482fababc1d7beadbb38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d531d0d58fe4f95b6a8efef706e3bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e94bab72d374f9bbaef6e9d6f306bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6faabd1034e24fa0adc4681749d21610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d777105c7bbb480a88ea78ce271650ba",
              "IPY_MODEL_247d059925c945fc873fc30c587c120a",
              "IPY_MODEL_fe441943811f4cab9653d00eb7beb0c0"
            ],
            "layout": "IPY_MODEL_4b572bbacae743f89eeb4a485e859897"
          }
        },
        "730efe0367f94a41aab3276b4dddb8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "735bb3b36f7c444392465049a19e3c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35617898bc8746c0b84ccc3dcba08ceb",
              "IPY_MODEL_3817b88203be480b9bf1120f545f099f",
              "IPY_MODEL_9bc93cf3874e4d17a3d0197609dcb7c2"
            ],
            "layout": "IPY_MODEL_bcfb5202065449c4b7d816770cb4b4b6"
          }
        },
        "73d609f41daf4b25ad0e166ae497cfe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7565061c24a74fe4bdb5b5c1a5d0d588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ae9c16ba8304a82a0d8a96d063cf605",
            "placeholder": "​",
            "style": "IPY_MODEL_e2caa672fe8042b0b6a7d33b353c26ed",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "76e7ed181424487693b33b0ecedaddc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7943bb934de74a38a83a11c5610d10eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "798ec3e3b0ff4bde859a9a6b16570d63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d02fcc535f34403a4f3849256dd9e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba9f2dea5d97481aa64d7884c14e745f",
              "IPY_MODEL_e571f4e49caa4c0f9bd652a59803fd34",
              "IPY_MODEL_a86c4f7af04f4e4f9e2c41b49d791842"
            ],
            "layout": "IPY_MODEL_2632e543cc124ee1b30757efa7c3f600"
          }
        },
        "80690fea5efe45da850e0fb97a01dbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53ea61f462e943a6baccb07de031a10b",
              "IPY_MODEL_1178e1a3c35b415c95d360a35e681b47",
              "IPY_MODEL_e73b63df43b94ccbb6f3a4dbfa0dcf9a"
            ],
            "layout": "IPY_MODEL_322cd2a1328e40e88ed33540e5b5861d"
          }
        },
        "80c46799b1fb4ce7acab05ef0452ff6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81539e6f059c4fefa5309b7afbca9cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b40718499ab24fb6bc29c29b0c0c705d",
              "IPY_MODEL_8236184193e24f20a8d8cf0ce3d34ecd",
              "IPY_MODEL_37900e749c0a4b94b5a643682cfdbd62"
            ],
            "layout": "IPY_MODEL_4422b8b44dc6474d9fcce0b1867b871e"
          }
        },
        "8236184193e24f20a8d8cf0ce3d34ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c996de02e594113805e28a6dc2e2153",
            "max": 711649,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c727a9d4ce4d41a2adb631d7e128c752",
            "value": 711649
          }
        },
        "84c149740a904599aeb947e0e8ff7ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a43479a3d044202842aa0cc25842b70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c7db7cc08f043d5a76ac6b140dae7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d3f0eb9c4f548f8a9e4130974a2f965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df701b1219642fcbd9a8cbdb15c73d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f0ca15f0a7044e58c0b1341fcbf227a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47aedfcbc2884d07a271c293d3f552fd",
            "max": 1336854282,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c909bb8d15254fab9871aa5afad04f0c",
            "value": 1336854282
          }
        },
        "8f8b4c3af4774768a11f5a6b09879392": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9157582164a6408ab0a00a0316775176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93043ccb1cf24581ac09f1beee57c4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93c363d2fa934e0f9162a0dab39ef962": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0d6eac752e74b8687cb0e002d064d07",
            "placeholder": "​",
            "style": "IPY_MODEL_0759e7860b9c4af3a57c98350d9b9f31",
            "value": " 5/5 [00:33&lt;00:00, 19.52s/it]"
          }
        },
        "96da0900296142f99cea126ff1d20970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae9c16ba8304a82a0d8a96d063cf605": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc93cf3874e4d17a3d0197609dcb7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c744b180c945c5a8aa48cc67861ce6",
            "placeholder": "​",
            "style": "IPY_MODEL_b7310ce91c4249d09a87a568bafd2585",
            "value": " 742/742 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "9c29d802eccc4d36bec62b503dc69600": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a049384504c240168ef666652b3be148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1e49bfd09d5469d95609c51a3e9a801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a39ec2676f744cc38521923f068b61a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9173c7d6de04e7c8b0de68c600bd3ec",
            "placeholder": "​",
            "style": "IPY_MODEL_a049384504c240168ef666652b3be148",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a53d97a1e27f49e3b792a1bc56691a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a43479a3d044202842aa0cc25842b70",
            "placeholder": "​",
            "style": "IPY_MODEL_2f65b50b32f34f38a676a10d9db8c3f1",
            "value": " 1.38k/1.38k [00:00&lt;00:00, 26.1kB/s]"
          }
        },
        "a86c4f7af04f4e4f9e2c41b49d791842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b465f880d0b2447cba67e17dee4df38c",
            "placeholder": "​",
            "style": "IPY_MODEL_2d6da34b0f304a618cae119388350beb",
            "value": " 755/755 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "a9bbe440540c4ca099ebe7fc45463abe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa4cc8ef3e6a4ac78f224fc36f1d11db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d6eac752e74b8687cb0e002d064d07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b40718499ab24fb6bc29c29b0c0c705d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389df603f6374836bcd80a2cc092836d",
            "placeholder": "​",
            "style": "IPY_MODEL_d144fffa76a74b40a1e5e3e5cf68a441",
            "value": "tokenizer.json: 100%"
          }
        },
        "b465f880d0b2447cba67e17dee4df38c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7310ce91c4249d09a87a568bafd2585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba942cc87c2e451198d6e04844d4c37e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9f2dea5d97481aa64d7884c14e745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c452ef2ed9f4406d8231a6120f30a97a",
            "placeholder": "​",
            "style": "IPY_MODEL_730efe0367f94a41aab3276b4dddb8e1",
            "value": "config.json: 100%"
          }
        },
        "bcfb5202065449c4b7d816770cb4b4b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf83e387078047f9aa656f5759c84f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c452ef2ed9f4406d8231a6120f30a97a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c67da774287f4b208f835b11829f07bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6d62f46ac634c6f81fc0c2eb443ddcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e923e4fa15e5484daaafe3a89b6d029a",
              "IPY_MODEL_f7d4040ef87c48e98d63a7fab9322f77",
              "IPY_MODEL_626822c8cad6450fa56e3dcd05a174f5"
            ],
            "layout": "IPY_MODEL_6689f7363c6a41f4832b889167778e7e"
          }
        },
        "c727a9d4ce4d41a2adb631d7e128c752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7fad31fa0e7497cbf2454151f432991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c149740a904599aeb947e0e8ff7ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_f087e85c47184c16ae1edc7700e4ea6a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c909bb8d15254fab9871aa5afad04f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca357b30600f412fabb9393edde23023": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca55f3b769fb46a9af5a74ec8316117b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb17703507cc4cb5b9ab2b9cbb7ca280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384f428f60624a88bba12daa4dfd314c",
            "placeholder": "​",
            "style": "IPY_MODEL_e23b4fe2a7dc412b8f4d246e301e3893",
            "value": "model.onnx: 100%"
          }
        },
        "cd42a2085b904b75bb7ecad890776ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa0ccc5852a421c85e7f1bd9db96aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbeed427137549239c4ee7bcbc7ac161",
              "IPY_MODEL_0b6826a3f8f64fa6b6e808e9461467af",
              "IPY_MODEL_386f3596d82a48ff93883a08ec624a28"
            ],
            "layout": "IPY_MODEL_2cc176393a4d494aab423337f9a2e5f3"
          }
        },
        "d144fffa76a74b40a1e5e3e5cf68a441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d24903f02cbb4dd49c875b45a2647788": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b94edf6ded420f8133bf5366d80ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4ddd20c798649cb92b20594282faeb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d69bb433408b406b974bfa5097b5f349": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72584c78aeb4a4d90961c2941bbba30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d777105c7bbb480a88ea78ce271650ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df701b1219642fcbd9a8cbdb15c73d1",
            "placeholder": "​",
            "style": "IPY_MODEL_7943bb934de74a38a83a11c5610d10eb",
            "value": "Fetching 5 files: 100%"
          }
        },
        "d98a5c7d2c2741cab27ed29bfe24d6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9bd0ee6284043b2815c0183cfd816d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbced11902f743bcb2550aa23417bf05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba46ade662145d68e5d768ee9c49800",
            "placeholder": "​",
            "style": "IPY_MODEL_9c29d802eccc4d36bec62b503dc69600",
            "value": " 1.34G/1.34G [00:32&lt;00:00, 42.8MB/s]"
          }
        },
        "dbeed427137549239c4ee7bcbc7ac161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e31416f58938447caebe41eddc6164f9",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a37a10cd1e4957829bf4b16baa7acf",
            "value": "tokenizer.json: 100%"
          }
        },
        "e23b4fe2a7dc412b8f4d246e301e3893": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2caa672fe8042b0b6a7d33b353c26ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31416f58938447caebe41eddc6164f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e571f4e49caa4c0f9bd652a59803fd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5528380c295b4c36bede3a9614e8670c",
            "max": 755,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1e49bfd09d5469d95609c51a3e9a801",
            "value": 755
          }
        },
        "e5a37a10cd1e4957829bf4b16baa7acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e73b63df43b94ccbb6f3a4dbfa0dcf9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a61cf2ddba348928c9ebdbdde5e973e",
            "placeholder": "​",
            "style": "IPY_MODEL_ca55f3b769fb46a9af5a74ec8316117b",
            "value": " 695/695 [00:00&lt;00:00, 8.54kB/s]"
          }
        },
        "e82109b40a79451ea92eb82f8c7a4492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8aadeb61e804bf5a61cb7b6a5652f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a39ec2676f744cc38521923f068b61a7",
              "IPY_MODEL_5d76f9ccaf444fff8bfbf1c51c8a6b3e",
              "IPY_MODEL_a53d97a1e27f49e3b792a1bc56691a57"
            ],
            "layout": "IPY_MODEL_6d531d0d58fe4f95b6a8efef706e3bb5"
          }
        },
        "e923e4fa15e5484daaafe3a89b6d029a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43aa3fce7957432eab003335be571012",
            "placeholder": "​",
            "style": "IPY_MODEL_5879710aa4dc40efbf1ab451bc2b7417",
            "value": "model.onnx: 100%"
          }
        },
        "ea505c293b224465b4c1f80253ad9bad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba46ade662145d68e5d768ee9c49800": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f087e85c47184c16ae1edc7700e4ea6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f254b9e36a4f42979f31b431573c33ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27bd058fd04f4ddbb702303d991fd366",
            "max": 695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c7db7cc08f043d5a76ac6b140dae7e3",
            "value": 695
          }
        },
        "f2adcf5691af4500acee1f43321821a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68464db71815482fababc1d7beadbb38",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e82109b40a79451ea92eb82f8c7a4492",
            "value": 5
          }
        },
        "f2c0492d43df443d86b6746b25f23a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7fad31fa0e7497cbf2454151f432991",
              "IPY_MODEL_f3a08d680e094a548c34a33ab1e485cb",
              "IPY_MODEL_29289e87e09242b39ec2e7efd88e385d"
            ],
            "layout": "IPY_MODEL_0cbc0220473e439fac2d0f4f287b8605"
          }
        },
        "f2f6e6e5857a4810972b7a1c5f085c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3a08d680e094a548c34a33ab1e485cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12418968aafd438791dc2f7ba79db632",
            "max": 1242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2f6e6e5857a4810972b7a1c5f085c64",
            "value": 1242
          }
        },
        "f4e10a9c48f545cd93dcb8a6b6866ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7565061c24a74fe4bdb5b5c1a5d0d588",
              "IPY_MODEL_f254b9e36a4f42979f31b431573c33ba",
              "IPY_MODEL_179aa0c1e548431a8dadacadf6214911"
            ],
            "layout": "IPY_MODEL_a9bbe440540c4ca099ebe7fc45463abe"
          }
        },
        "f58b4cf03c124b8eada8a92c78e4466d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7d4040ef87c48e98d63a7fab9322f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8b4c3af4774768a11f5a6b09879392",
            "max": 532091260,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c67da774287f4b208f835b11829f07bb",
            "value": 532091260
          }
        },
        "f9173c7d6de04e7c8b0de68c600bd3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbbf5f9e784c41899eda7b082f0e9cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe441943811f4cab9653d00eb7beb0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80c46799b1fb4ce7acab05ef0452ff6d",
            "placeholder": "​",
            "style": "IPY_MODEL_541bce507e564522b885cbbcd76fb76c",
            "value": " 5/5 [00:13&lt;00:00,  7.65s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
